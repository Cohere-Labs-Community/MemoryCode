{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs\nfrom typing import List\n\nclass StudentGrade:\n    def __init__(self, t_grades: List[float]):\n        self.t_grades = t_grades\n\n    @require_kwargs\n    def compute_average_grade(self) -> float:\n        return sum(self.t_grades) / len(self.t_grades) if self.t_grades else 0.0\n```",
        "```python\nfrom collections import OrderedDict\nfrom pedantic import require_kwargs\n\nclass LRUCache:\n    def __init__(self, t_capacity: int):\n        self.t_cache = OrderedDict()\n        self.t_capacity = t_capacity\n\n    @require_kwargs\n    def get(self, *, t_key: int) -> int:\n        if t_key in self.t_cache:\n            value = self.t_cache.pop(t_key)\n            self.t_cache[t_key] = value\n            return value\n        return -1\n\n    @require_kwargs\n    def put(self, *, t_key: int, t_value: int) -> None:\n        if t_key in self.t_cache:\n            self.t_cache.pop(t_key)\n        elif len(self.t_cache) >= self.t_capacity:\n            self.t_cache.popitem(last=False)\n        self.t_cache[t_key] = t_value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\nclass StudentGrade:\n    def __init__(self, **kwargs):\n        self.grades = kwargs.get('grades', [])\n\n    @require_kwargs\n    def add_grade(self, grade: float) -> None:\n        self.grades.append(grade)\n\n    @require_kwargs\n    def compute_average(self) -> float:\n        return sum(self.grades) / len(self.grades) if self.grades else 0.0\n```",
        "```python\nclass LRUCache:\n    def __init__(self, t_capacity: int):\n        self.t_capacity = t_capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, t_key: int) -> int:\n        if t_key not in self.t_cache:\n            return -1\n        self.t_order.remove(t_key)\n        self.t_order.append(t_key)\n        return self.t_cache[t_key]\n\n    def put(self, t_key: int, t_value: int) -> None:\n        if t_key in self.t_cache:\n            self.t_order.remove(t_key)\n        elif len(self.t_cache) >= self.t_capacity:\n            oldest_key = self.t_order.pop(0)\n            del self.t_cache[oldest_key]\n        self.t_cache[t_key] = t_value\n        self.t_order.append(t_key)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef longest_common_prefix(s_strs):\n    \"\"\"\n    Returns the longest common prefix string from a list of strings.\n\n    :param s_strs: List of strings to find the common prefix.\n    :type s_strs: list of str\n    :return: The longest common prefix string.\n    :rtype: str\n    \"\"\"\n    if not s_strs:\n        return \"\"\n\n    s_strs.sort()\n    first, last = s_strs[0], s_strs[-1]\n    s_index = 0\n    while s_index < len(first) and s_index < len(last) and first[s_index] == last[s_index]:\n        s_index += 1\n\n    return first[:s_index]\n```",
        "```python\nclass LRUCache:\n    \"\"\"\n    A class to implement a Least Recently Used (LRU) Cache.\n    \n    Methods\n    -------\n    get(s_key):\n        Returns the value of the given key if available, otherwise returns -1.\n    put(s_key, s_value):\n        Inserts or updates the key with the given value. If the cache exceeds its capacity, it removes the least recently used item.\n    \"\"\"\n\n    def __init__(self, s_capacity: int):\n        \"\"\"\n        Initializes the LRU Cache with the specified capacity.\n\n        Parameters\n        ----------\n        s_capacity: int\n            The maximum number of items the cache can hold.\n        \"\"\"\n        self.s_capacity = s_capacity\n        self.s_cache = {}\n        self.s_order = []\n\n    def get(self, s_key: int) -> int:\n        \"\"\"\n        Returns the value for the specified key if it exists in the cache.\n\n        Parameters\n        ----------\n        s_key: int\n            The key to retrieve the value for.\n\n        Returns\n        -------\n        int\n            The value associated with the key, or -1 if the key does not exist.\n        \"\"\"\n        if s_key in self.s_cache:\n            self.s_order.remove(s_key)\n            self.s_order.append(s_key)\n            return self.s_cache[s_key]\n        return -1\n\n    def put(self, s_key: int, s_value: int) -> None:\n        \"\"\"\n        Adds or updates the value for the specified key. If the cache exceeds the capacity, removes the least recently used item.\n\n        Parameters\n        ----------\n        s_key: int\n            The key to add or update in the cache.\n        s_value: int\n            The value to associate with the key.\n        \"\"\"\n        if s_key in self.s_cache:\n            self.s_order.remove(s_key)\n        elif len(self.s_cache) >= self.s_capacity:\n            oldest_key = self.s_order.pop(0)\n            del self.s_cache[oldest_key]\n\n        self.s_cache[s_key] = s_value\n        self.s_order.append(s_key)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strings):\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Parameters:\n    strings (list of str): A list containing strings to evaluate.\n\n    Returns:\n    str: The longest common prefix.\n    \"\"\"\n    if not strings:\n        return \"\"\n\n    min_length = min(len(s) for s in strings)\n\n    common_prefix = \"\"\n\n    for i in range(min_length):\n        current_char = strings[0][i]\n        \n        if all(s[i] == current_char for s in strings):\n            common_prefix += current_char\n        else:\n            break\n\n    return common_prefix\n```",
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = {}\n        self.s_order = []\n\n    def get(self, key: int) -> int:\n        if key in self.s_cache:\n            self.s_order.remove(key)\n            self.s_order.append(key)\n            return self.s_cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_order.remove(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            oldest_key = self.s_order.pop(0)\n            del self.s_cache[oldest_key]\n        self.s_cache[key] = value\n        self.s_order.append(key)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, key, value):\n        self.xt_key = key\n        self.xt_value = value\n        self.xt_prev = None\n        self.xt_next = None\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = {}\n        self.xt_head = Node(0, 0)\n        self.xt_tail = Node(0, 0)\n        self.xt_head.xt_next = self.xt_tail\n        self.xt_tail.xt_prev = self.xt_head\n\n    def get(self, key: int) -> int:\n        if key in self.xt_cache:\n            node = self.xt_cache[key]\n            self._remove(node)\n            self._add(node)\n            return node.xt_value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self._remove(self.xt_cache[key])\n        node = Node(key, value)\n        self._add(node)\n        self.xt_cache[key] = node\n        if len(self.xt_cache) > self.xt_capacity:\n            lru = self.xt_head.xt_next\n            self._remove(lru)\n            del self.xt_cache[lru.xt_key]\n\n    def _remove(self, node: Node):\n        prev_node = node.xt_prev\n        next_node = node.xt_next\n        prev_node.xt_next = next_node\n        next_node.xt_prev = prev_node\n\n    def _add(self, node: Node):\n        prev_tail = self.xt_tail.xt_prev\n        prev_tail.xt_next = node\n        node.xt_prev = prev_tail\n        node.xt_next = self.xt_tail\n        self.xt_tail.xt_prev = node\n```"
      ],
      "instruction_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = {}\n        self.xt_order = []\n\n    def get(self, key: int) -> int:\n        if key in self.xt_cache:\n            self.xt_order.remove(key)\n            self.xt_order.append(key)\n            return self.xt_cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_order.remove(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            lru_key = self.xt_order.pop(0)\n            del self.xt_cache[lru_key]\n        \n        self.xt_cache[key] = value\n        self.xt_order.append(key)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n        self.fc2 = nn.Linear(256, 1)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self, num_channels):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n```"
      ],
      "cumulative_model_output": [
        "```python\nfrom typing import List\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef longest_common_prefix(*, xt_words: List[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string from the list of strings.\n    \n    :param xt_words: List of strings to find the common prefix.\n    :return: The longest common prefix.\n    \"\"\"\n    if not xt_words:\n        return \"\"\n    \n    prefix = xt_words[0]\n    for word in xt_words[1:]:\n        while not word.startswith(prefix):\n            prefix = prefix[:-1]\n            if not prefix:\n                return \"\"\n    return prefix\n```\n",
        "```python\nfrom collections import OrderedDict\nfrom pedantic import require_kwargs\n\nclass LruCache:\n\n    @require_kwargs\n    def __init__(self, xt_capacity: int) -> None:\n        \"\"\"\n        Initialize the LRU Cache with a specific capacity.\n\n        :param xt_capacity: Maximum number of items the cache can hold\n        \"\"\"\n        self.xt_capacity = xt_capacity\n        self.cache = OrderedDict()\n\n    @require_kwargs\n    def get(self, xt_key: int) -> int:\n        \"\"\"\n        Retrieve an item from the cache.\n\n        :param xt_key: Key of the item to retrieve\n        :return: The item if found, -1 otherwise\n        \"\"\"\n        if xt_key not in self.cache:\n            return -1\n        self.cache.move_to_end(xt_key)\n        return self.cache[xt_key]\n\n    @require_kwargs\n    def put(self, xt_key: int, xt_value: int) -> None:\n        \"\"\"\n        Add an item to the cache.\n\n        :param xt_key: Key of the item to add\n        :param xt_value: Value of the item to add\n        \"\"\"\n        if xt_key in self.cache:\n            self.cache.move_to_end(xt_key)\n        self.cache[xt_key] = xt_value\n        if len(self.cache) > self.xt_capacity:\n            self.cache.popitem(last=False)\n```",
        "```python\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom pedantic import require_kwargs\n\nclass XtBinaryImageClassifier(nn.Module):\n    @require_kwargs\n    def __init__(self, *, input_channels: int, num_classes: int = 2):\n        super(XtBinaryImageClassifier, self).__init__()\n        self.xt_conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.xt_conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.xt_fc1 = nn.Linear(64 * 8 * 8, 128)  # Assuming 8x8 feature map size after pooling\n        self.xt_fc2 = nn.Linear(128, num_classes)\n\n    @require_kwargs\n    def forward(self, *, x):\n        x = F.relu(self.xt_conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = F.relu(self.xt_conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = F.relu(self.xt_fc1(x))\n        x = self.xt_fc2(x)\n        return x\n```",
        "```python\nfrom pedantic import require_kwargs\n\nclass XtStudentGrade:\n\n    def __init__(self, s_student_name, s_grades):\n        \"\"\"\n        Initialize a student grade object.\n        \n        :param s_student_name: Name of the student.\n        :param s_grades: List of grades obtained by the student.\n        \"\"\"\n        self.s_student_name = s_student_name\n        self.s_grades = s_grades\n\n    @require_kwargs\n    def xt_compute_average(self):\n        \"\"\"\n        Compute and return the average of the student's grades.\n\n        :return: Average grade as a float.\n        \"\"\"\n        if not self.s_grades:\n            return 0.0\n        return sum(self.s_grades) / len(self.s_grades)\n```"
      ]
    }
  ]
}