{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef y_compute_h_index(r_citations):\n    r_citations.sort(reverse=True)\n    r_h_index = 0\n    for i, r_citation in enumerate(r_citations):\n        if r_citation >= i + 1:\n            r_h_index = i + 1\n        else:\n            break\n    return r_h_index\n```",
        "```python\nclass Node:\n    def __init__(self, r_key, r_value):\n        self.r_key = r_key\n        self.r_value = r_value\n        self.r_prev = None\n        self.r_next = None\n\nclass LRUCache:\n    def __init__(self, r_capacity: int):\n        self.r_capacity = r_capacity\n        self.r_cache = {}\n        self.r_head = Node(0, 0)\n        self.r_tail = Node(0, 0)\n        self.r_head.r_next = self.r_tail\n        self.r_tail.r_prev = self.r_head\n\n    def y_get(self, r_key: int) -> int:\n        if r_key in self.r_cache:\n            r_node = self.r_cache[r_key]\n            self.y__remove(r_node)\n            self.y__add(r_node)\n            return r_node.r_value\n        return -1\n\n    def y_put(self, r_key: int, r_value: int) -> None:\n        if r_key in self.r_cache:\n            self.y__remove(self.r_cache[r_key])\n        r_node = Node(r_key, r_value)\n        self.y__add(r_node)\n        self.r_cache[r_key] = r_node\n        if len(self.r_cache) > self.r_capacity:\n            r_lru = self.r_head.r_next\n            self.y__remove(r_lru)\n            del self.r_cache[r_lru.r_key]\n\n    def y__remove(self, r_node: Node):\n        r_prev, r_next = r_node.r_prev, r_node.r_next\n        r_prev.r_next, r_next.r_prev = r_next, r_prev\n\n    def y__add(self, r_node: Node):\n        r_prev = self.r_tail.r_prev\n        r_prev.r_next = r_node\n        r_node.r_prev = r_prev\n        r_node.r_next = self.r_tail\n        self.r_tail.r_prev = r_node\n```"
      ],
      "instruction_model_output": [
        "```python\ndef y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations):\n        if c >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, r_capacity: int):\n        self.r_cache = OrderedDict()\n        self.r_capacity = r_capacity\n\n    def get(self, r_key: int) -> int:\n        if r_key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(r_key)\n        return self.r_cache[r_key]\n\n    def put(self, r_key: int, r_value: int) -> None:\n        if r_key in self.r_cache:\n            self.r_cache.move_to_end(r_key)\n        self.r_cache[r_key] = r_value\n        if len(self.r_cache) > self.r_capacity:\n            self.r_cache.popitem(last=False)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n    \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 32 * 8 * 8)\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self, num_channels):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, i_key, i_value):\n        self.i_key = i_key\n        self.i_value = i_value\n        self.i_prev = None\n        self.i_next = None\n\nclass LRUCache:\n    def __init__(self, i_capacity):\n        self.i_capacity = i_capacity\n        self.i_cache = {}\n        self.i_head = Node(0, 0)\n        self.i_tail = Node(0, 0)\n        self.i_head.i_next = self.i_tail\n        self.i_tail.i_prev = self.i_head\n\n    def _remove(self, i_node):\n        i_prev = i_node.i_prev\n        i_next = i_node.i_next\n        i_prev.i_next = i_next\n        i_next.i_prev = i_prev\n\n    def _add(self, i_node):\n        i_prev = self.i_tail.i_prev\n        i_prev.i_next = i_node\n        i_node.i_prev = i_prev\n        i_node.i_next = self.i_tail\n        self.i_tail.i_prev = i_node\n\n    def get(self, i_key):\n        if i_key in self.i_cache:\n            i_node = self.i_cache[i_key]\n            self._remove(i_node)\n            self._add(i_node)\n            return i_node.i_value\n        return -1\n\n    def put(self, i_key, i_value):\n        if i_key in self.i_cache:\n            self._remove(self.i_cache[i_key])\n        i_node = Node(i_key, i_value)\n        self._add(i_node)\n        self.i_cache[i_key] = i_node\n        if len(self.i_cache) > self.i_capacity:\n            i_lru = self.i_head.i_next\n            self._remove(i_lru)\n            del self.i_cache[i_lru.i_key]\n```"
      ],
      "instruction_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, i_capacity: int):\n        self.i_capacity = i_capacity\n        self.i_cache = {}\n        self.i_order = []\n\n    def get(self, i_key: int) -> int:\n        if i_key in self.i_cache:\n            self.i_order.remove(i_key)\n            self.i_order.append(i_key)\n            return self.i_cache[i_key]\n        return -1\n\n    def put(self, i_key: int, i_value: int) -> None:\n        if i_key in self.i_cache:\n            self.i_order.remove(i_key)\n        elif len(self.i_cache) == self.i_capacity:\n            oldest_key = self.i_order.pop(0)\n            del self.i_cache[oldest_key]\n        \n        self.i_cache[i_key] = i_value\n        self.i_order.append(i_key)\n```\n"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom collections import OrderedDict\nfrom pedantic.decorators import timer_class\n\n@timer_class\nclass LRUCache:\n    def __init__(self, t_capacity: int):\n        self.t_cache = OrderedDict()\n        self.t_capacity = t_capacity\n\n    def get(self, t_key: int) -> int:\n        if t_key not in self.t_cache:\n            return -1\n        self.t_cache.move_to_end(t_key)\n        return self.t_cache[t_key]\n\n    def put(self, t_key: int, t_value: int) -> None:\n        if t_key in self.t_cache:\n            self.t_cache.move_to_end(t_key)\n        self.t_cache[t_key] = t_value\n        if len(self.t_cache) > self.t_capacity:\n            self.t_cache.popitem(last=False)\n```",
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.t_classes_ = None\n        self.t_means_ = None\n        self.t_covariances_ = None\n        self.t_priors_ = None\n\n    def fit(self, X, y):\n        self.t_classes_ = np.unique(y)\n        n_features = X.shape[1]\n        self.t_means_ = {}\n        self.t_covariances_ = {}\n        self.t_priors_ = {}\n\n        for cls in self.t_classes_:\n            X_cls = X[y == cls]\n            self.t_means_[cls] = np.mean(X_cls, axis=0)\n            self.t_covariances_[cls] = np.cov(X_cls, rowvar=False)\n            self.t_priors_[cls] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        predictions = []\n        for x in X:\n            scores = {}\n            for cls in self.t_classes_:\n                mean = self.t_means_[cls]\n                cov = self.t_covariances_[cls]\n                prior = self.t_priors_[cls]\n                cov_inv = np.linalg.inv(cov)\n                diff = x - mean\n                exponent = -0.5 * diff.T @ cov_inv @ diff\n                score = exponent - 0.5 * np.log(np.linalg.det(cov)) + np.log(prior)\n                scores[cls] = score\n            predictions.append(max(scores, key=scores.get))\n        return np.array(predictions)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, t_capacity: int):\n        self.t_capacity = t_capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, t_key: int) -> int:\n        if t_key not in self.t_cache:\n            return -1\n        self.t_order.remove(t_key)\n        self.t_order.append(t_key)\n        return self.t_cache[t_key]\n\n    def put(self, t_key: int, t_value: int) -> None:\n        if t_key in self.t_cache:\n            self.t_order.remove(t_key)\n        elif len(self.t_cache) >= self.t_capacity:\n            oldest_key = self.t_order.pop(0)\n            del self.t_cache[oldest_key]\n        self.t_cache[t_key] = t_value\n        self.t_order.append(t_key)\n```",
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge0(self, start_vertex, end_vertex):\n        if start_vertex not in self.graph:\n            self.graph[start_vertex] = []\n        self.graph[start_vertex].append(end_vertex)\n\n    def is_cyclic1(self):\n        visited = set()\n        recursion_stack = set()\n\n        def visit(vertex):\n            if vertex not in visited:\n                visited.add(vertex)\n                recursion_stack.add(vertex)\n                \n                for neighbor in self.graph.get(vertex, []):\n                    if neighbor not in visited and visit(neighbor):\n                        return True\n                    elif neighbor in recursion_stack:\n                        return True\n\n                recursion_stack.remove(vertex)\n            return False\n\n        for node in self.graph:\n            if visit(node):\n                return True\n        return False\n```\n",
        "```python\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom pedantic import trace_class\nimport numpy as np\n\n@trace_class\nclass KernelRidgeRegression1:\n    def __init__(self, alpha=1.0, gamma=None):\n        self.alpha = alpha\n        self.gamma = gamma\n        self.X_fit_ = None\n        self.dual_coef_ = None\n\n    def fit1(self, X, y):\n        K = self._compute_kernel1(X, X)\n        n_samples = K.shape[0]\n        self.X_fit_ = X\n        self.dual_coef_ = np.linalg.solve(K + self.alpha * np.eye(n_samples), y)\n\n    def _compute_kernel1(self, X, Y):\n        return rbf_kernel(X, Y, gamma=self.gamma)\n\n    def predict1(self, X):\n        K = self._compute_kernel1(X, self.X_fit_)\n        return np.dot(K, self.dual_coef_)\n```\n"
      ],
      "instruction_model_output": [
        "```python\nclass DirectedGr4ph:\n    def __init__(self):\n        self.adj4cency_list = {}\n\n    def add_v3rtex(self, v3rtex):\n        if v3rtex not in self.adj4cency_list:\n            self.adj4cency_list[v3rtex] = []\n\n    def add_edg3(self, start, end):\n        if start in self.adj4cency_list:\n            self.adj4cency_list[start].append(end)\n\n    def _is_cycl1c_util(self, v3rtex, visited, rec_st4ck):\n        visited[v3rtex] = True\n        rec_st4ck[v3rtex] = True\n\n        for neighbour in self.adj4cency_list.get(v3rtex, []):\n            if not visited[neighbour]:\n                if self._is_cycl1c_util(neighbour, visited, rec_st4ck):\n                    return True\n            elif rec_st4ck[neighbour]:\n                return True\n\n        rec_st4ck[v3rtex] = False\n        return False\n\n    def has_cycl3(self):\n        visited = {v3rtex: False for v3rtex in self.adj4cency_list}\n        rec_st4ck = {v3rtex: False for v3rtex in self.adj4cency_list}\n\n        for v3rtex in self.adj4cency_list:\n            if not visited[v3rtex]:\n                if self._is_cycl1c_util(v3rtex, visited, rec_st4ck):\n                    return True\n        return False\n```",
        "```python\nfrom pedantic import trace_class\nimport numpy as np\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.X_train = None\n        self.alpha_ = None\n    \n    def _linear_kernel(self, X, Y):\n        return np.dot(X, Y.T)\n\n    def _polynomial_kernel(self, X, Y):\n        return (np.dot(X, Y.T) + self.coef0) ** self.degree\n\n    def _rbf_kernel(self, X, Y):\n        if self.gamma is None:\n            self.gamma = 1.0 / X.shape[1]\n        K = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(Y ** 2, axis=1) - 2 * np.dot(X, Y.T)\n        return np.exp(-self.gamma * K)\n\n    def _get_kernel(self, X, Y):\n        if self.kernel == 'linear':\n            return self._linear_kernel(X, Y)\n        elif self.kernel == 'polynomial':\n            return self._polynomial_kernel(X, Y)\n        elif self.kernel == 'rbf':\n            return self._rbf_kernel(X, Y)\n        else:\n            raise ValueError(\"Unknown kernel type\")\n\n    def fit(self, X, y):\n        self.X_train = X\n        K = self._get_kernel(X, X)\n        n_samples = X.shape[0]\n        self.alpha_ = np.linalg.solve(K + self.alpha * np.eye(n_samples), y)\n\n    def predict(self, X):\n        K = self._get_kernel(X, self.X_train)\n        return np.dot(K, self.alpha_)\n\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, at_capacity: int):\n        self.at_cache = {}\n        self.at_order = []\n        self.at_capacity = at_capacity\n\n    def get(self, at_key: int) -> int:\n        if at_key in self.at_cache:\n            self.at_order.remove(at_key)\n            self.at_order.append(at_key)\n            return self.at_cache[at_key]\n        return -1\n\n    def put(self, at_key: int, at_value: int):\n        if at_key in self.at_cache:\n            self.at_order.remove(at_key)\n        elif len(self.at_order) >= self.at_capacity:\n            oldest_key = self.at_order.pop(0)\n            del self.at_cache[oldest_key]\n        self.at_cache[at_key] = at_value\n        self.at_order.append(at_key)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, at_key, at_value):\n        self.at_key = at_key\n        self.at_value = at_value\n        self.at_prev = None\n        self.at_next = None\n\nclass LRUCache:\n    def __init__(self, at_capacity):\n        self.at_capacity = at_capacity\n        self.at_cache = {}\n        self.at_head = Node(0, 0)\n        self.at_tail = Node(0, 0)\n        self.at_head.at_next = self.at_tail\n        self.at_tail.at_prev = self.at_head\n        \n    def _remove(self, at_node):\n        at_prev = at_node.at_prev\n        at_next = at_node.at_next\n        at_prev.at_next = at_next\n        at_next.at_prev = at_prev\n\n    def _add(self, at_node):\n        at_node.at_prev = self.at_head\n        at_node.at_next = self.at_head.at_next\n        self.at_head.at_next.at_prev = at_node\n        self.at_head.at_next = at_node\n\n    def get(self, at_key):\n        if at_key in self.at_cache:\n            at_node = self.at_cache[at_key]\n            self._remove(at_node)\n            self._add(at_node)\n            return at_node.at_value\n        return -1\n\n    def put(self, at_key, at_value):\n        if at_key in self.at_cache:\n            self._remove(self.at_cache[at_key])\n        at_node = Node(at_key, at_value)\n        self._add(at_node)\n        self.at_cache[at_key] = at_node\n        if len(self.at_cache) > self.at_capacity:\n            at_lru = self.at_tail.at_prev\n            self._remove(at_lru)\n            del self.at_cache[at_lru.at_key]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\ndef c_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\nclass LinearRegression1:\n    def __init__(self):\n        self.coef1_ = None\n        self.intercept1_ = None\n    \n    def fit1(self, X1, y1):\n        X1 = np.c_[np.ones((X1.shape[0], 1)), X1]  # Add column of ones for intercept\n        theta1_best = np.linalg.pinv(X1.T @ X1) @ X1.T @ y1\n        self.intercept1_ = theta1_best[0]\n        self.coef1_ = theta1_best[1:]\n    \n    def predict1(self, X1):\n        if self.coef1_ is None or self.intercept1_ is None:\n            raise ValueError(\"The model is not fitted yet.\")\n        return X1 @ self.coef1_ + self.intercept1_\n```\n"
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\nclass LinRegNumpy:\n    def __init__(self):\n        self.coef1_ = None\n        self.intercept1_ = 0\n        self.fit1_intercept = True\n    \n    def fit(self, X1, y1):\n        if self.fit1_intercept:\n            X1 = np.c_[np.ones(X1.shape[0]), X1]\n        \n        X1_transpose = X1.T\n        beta1 = np.linalg.inv(X1_transpose.dot(X1)).dot(X1_transpose).dot(y1)\n        \n        if self.fit1_intercept:\n            self.intercept1_ = beta1[0]\n            self.coef1_ = beta1[1:]\n        else:\n            self.coef1_ = beta1\n    \n    def predict(self, X1):\n        if self.fit1_intercept:\n            X1 = np.c_[np.ones(X1.shape[0]), X1]\n            return X1.dot(np.r_[self.intercept1_, self.coef1_])\n        else:\n            return X1.dot(self.coef1_)\n```\n"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer\n\n@timer\ndef gn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "```python\nfrom pedantic import timer\n\nclass ListNode:\n    def __init__(self, value=0, next=None):\n        self.value = value\n        self.next = next\n\n@timer\ndef gn_has_cycle(head):\n    slow = fast = head\n\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n\n        if slow == fast:\n            return True\n\n    return False\n```"
      ],
      "instruction_model_output": [
        "```python\ndef gn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "```python\nfrom pedantic import timer\n\nclass ListNode:\n    def __init__(self, value=0, next=None):\n        self.value = value\n        self.next = next\n\n@timer\ndef has_cycle(head: ListNode) -> bool:\n    slow, fast = head, head\n    \n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n        if slow == fast:\n            return True\n\n    return False\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\nfrom typing import List\nfrom pedantic import timer\n\n@timer\ndef gn_calculateHIndex(c_citations: List[int]) -> int:\n    n = len(c_citations)\n    c_citations.sort(reverse=True)\n    h_at_index = 0\n    for i in range(n):\n        if c_citations[i] >= i + 1:\n            h_at_index = i + 1\n        else:\n            break\n    return h_at_index\n```",
        "```python\nfrom collections import OrderedDict\nfrom pedantic import trace_class\n\n@trace_class\nclass LRUCache:\n    def __init__(self, c_capacity: int):\n        self.at_cache = OrderedDict()\n        self.at_capacity = c_capacity\n    \n    def gn_get(self, c_key: int) -> int:\n        if c_key not in self.at_cache:\n            return -1\n        self.at_cache.move_to_end(c_key)\n        return self.at_cache[c_key]\n    \n    def gn_put(self, c_key: int, c_value: int) -> None:\n        if c_key in self.at_cache:\n            self.at_cache.move_to_end(c_key)\n        self.at_cache[c_key] = c_value\n        if len(self.at_cache) > self.at_capacity:\n            self.at_cache.popitem(last=False)\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pedantic import timer_class\n\n@timer_class\nclass GnImageBinaryClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes=2):\n        super(GnImageBinaryClassifier, self).__init__()\n        self.i_conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.i_conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.i_conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.i_fc1 = nn.Linear(128 * 8 * 8, 256)\n        self.i_fc2 = nn.Linear(256, num_classes)\n    \n    def gn_forward1(self, x):\n        x = F.relu(self.i_conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.i_conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.i_conv3(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 128 * 8 * 8)\n        x = F.relu(self.i_fc1(x))\n        x = self.i_fc2(x)\n        return x\n\n    def gn_predict1(self, x):\n        x = self.gn_forward1(x)\n        x = torch.sigmoid(x)\n        return x\n```\n",
        "```python\nfrom pedantic import trace_class\nfrom collections import defaultdict\n\n@trace_class\nclass DirectedGraph:\n    def __init__(self):\n        self.at_graph = defaultdict(list)\n\n    def gn_add_edge(self, at_u, at_v):\n        self.at_graph[at_u].append(at_v)\n\n    def gn_is_cyclic_util(self, at_v, at_visited, at_rec_stack):\n        at_visited[at_v] = True\n        at_rec_stack[at_v] = True\n\n        for at_neighbour in self.at_graph[at_v]:\n            if not at_visited[at_neighbour]:\n                if self.gn_is_cyclic_util(at_neighbour, at_visited, at_rec_stack):\n                    return True\n            elif at_rec_stack[at_neighbour]:\n                return True\n\n        at_rec_stack[at_v] = False\n        return False\n\n    def gn_is_cyclic(self):\n        at_visited = {at_node: False for at_node in self.at_graph}\n        at_rec_stack = {at_node: False for at_node in self.at_graph}\n\n        for at_node in self.at_graph:\n            if not at_visited[at_node]:\n                if self.gn_is_cyclic_util(at_node, at_visited, at_rec_stack):\n                    return True\n        return False\n```\n",
        "```python\nimport numpy as np\nfrom pedantic import timer_class\n\n@timer_class\nclass LinearRegressionModel:\n\n    def __init__(self):\n        self.at_coefficient = None\n        self.at_intercept = None\n\n    def gn_fit(self, c_X, c_y):\n        c_X = np.array(c_X)\n        c_y = np.array(c_y)\n        \n        at_X_mean = np.mean(c_X)\n        at_y_mean = np.mean(c_y)\n        \n        at_numerator = np.sum((c_X - at_X_mean) * (c_y - at_y_mean))\n        at_denominator = np.sum((c_X - at_X_mean) ** 2)\n        \n        self.at_coefficient = at_numerator / at_denominator\n        self.at_intercept = at_y_mean - (self.at_coefficient * at_X_mean)\n\n    def gn_predict(self, c_X):\n        if self.at_coefficient is None or self.at_intercept is None:\n            raise ValueError(\"The model has not been fitted yet.\")\n        return self.at_coefficient * np.array(c_X) + self.at_intercept\n```",
        "```python\ndef gn_hasCycle1(head):\n    @timer\n    def helper(at_node):\n        slow = fast = at_node\n        while fast and fast.next:\n            slow = slow.next\n            fast = fast.next.next\n            if slow == fast:\n                return True\n        return False\n    \n    return helper(head)\n```",
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom scipy.linalg import eigh\nimport numpy as np\nfrom pedantic import timer_class\n\n@timer_class\nclass GnQuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.at_classes = None\n        self.at_priors = None\n        self.at_means = None\n        self.at_covariances = None\n        self.at_inv_covariances = None\n        self.at_log_determinants = None\n\n    def c_fit(self, X, y):\n        X, y = check_X_y(X, y)\n        self.at_classes = np.unique(y)\n        n_features = X.shape[1]\n        self.at_priors = np.zeros(len(self.at_classes))\n        self.at_means = np.zeros((len(self.at_classes), n_features))\n        self.at_covariances = np.zeros((len(self.at_classes), n_features, n_features))\n        self.at_inv_covariances = []\n        self.at_log_determinants = []\n\n        for i, cls in enumerate(self.at_classes):\n            X_cls = X[y == cls]\n            self.at_priors[i] = X_cls.shape[0] / X.shape[0]\n            self.at_means[i] = np.mean(X_cls, axis=0)\n            centered_data = X_cls - self.at_means[i]\n            self.at_covariances[i] = np.dot(centered_data.T, centered_data) / (X_cls.shape[0] - 1)\n            inv_cov = np.linalg.inv(self.at_covariances[i])\n            self.at_inv_covariances.append(inv_cov)\n            _, log_det = np.linalg.slogdet(self.at_covariances[i])\n            self.at_log_determinants.append(log_det)\n        return self\n\n    def c_predict(self, X):\n        X = check_array(X)\n        probabilities = self.c_predict_proba(X)\n        return self.at_classes[np.argmax(probabilities, axis=1)]\n\n    def c_predict_proba(self, X):\n        X = check_array(X)\n        likelihoods = []\n        \n        for i, cls in enumerate(self.at_classes):\n            centered_data = X - self.at_means[i]\n            log_likelihood = -0.5 * np.sum(centered_data @ self.at_inv_covariances[i] * centered_data, axis=1)\n            log_prior = np.log(self.at_priors[i])\n            log_likelihood -= 0.5 * self.at_log_determinants[i]\n            likelihoods.append(log_likelihood + log_prior)\n        \n        likelihoods = np.array(likelihoods).T\n        max_likelihood = np.max(likelihoods, axis=1, keepdims=True)\n        likelihoods = np.exp(likelihoods - max_likelihood)\n        return likelihoods / np.sum(likelihoods, axis=1, keepdims=True)\n```",
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        self.t_alpha = alpha\n        self.t_kernel = kernel\n        self.t_gamma = gamma\n        self.t_alpha_0 = None\n        self.t_X_fit = None\n\n    def gn_kernel_function(self, X, Y):\n        if self.t_kernel == 'linear':\n            return X @ Y.T\n        elif self.t_kernel == 'rbf':\n            if self.t_gamma is None:\n                self.t_gamma = 1.0 / X.shape[1]\n            K = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(Y ** 2, axis=1) - 2 * X @ Y.T\n            return np.exp(-self.t_gamma * K)\n        else:\n            raise ValueError(\"Unsupported kernel\")\n\n    def gn_fit(self, X, y):\n        K = self.gn_kernel_function(X, X)\n        n_samples = K.shape[0]\n        self.t_alpha_0 = np.linalg.inv(K + self.t_alpha * np.eye(n_samples)).dot(y)\n        self.t_X_fit = X\n\n    def gn_predict(self, X):\n        K = self.gn_kernel_function(X, self.t_X_fit)\n        return K @ self.t_alpha_0\n```\n"
      ]
    }
  ]
}