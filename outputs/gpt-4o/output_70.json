{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.priors = None\n        self.means = None\n        self.covariances = None\n        self.classes = None\n\n    def x_fit(self, X, y):\n        self.classes = np.unique(y)\n        n_classes = len(self.classes)\n        n_features = X.shape[1]\n        \n        self.priors = np.zeros(n_classes)\n        self.means = np.zeros((n_classes, n_features))\n        self.covariances = np.zeros((n_classes, n_features, n_features))\n        \n        for idx, cls in enumerate(self.classes):\n            X_cls = X[y == cls]\n            self.priors[idx] = X_cls.shape[0] / X.shape[0]\n            self.means[idx] = np.mean(X_cls, axis=0)\n            centered_data = X_cls - self.means[idx]\n            self.covariances[idx] = np.dot(centered_data.T, centered_data) / (X_cls.shape[0] - 1)\n\n    def x_predict(self, X):\n        log_probs = self.x__compute_log_probs(X)\n        return self.classes[np.argmax(log_probs, axis=1)]\n\n    def x__compute_log_probs(self, X):\n        n_samples = X.shape[0]\n        n_classes = len(self.classes)\n        log_probs = np.zeros((n_samples, n_classes))\n        \n        for idx, cls in enumerate(self.classes):\n            mean_diff = X - self.means[idx]\n            inv_covariance = np.linalg.inv(self.covariances[idx])\n            log_det = np.log(np.linalg.det(self.covariances[idx]))\n            \n            term1 = -0.5 * np.sum(mean_diff @ inv_covariance * mean_diff, axis=1)\n            term2 = -0.5 * log_det\n            term3 = np.log(self.priors[idx])\n            \n            log_probs[:, idx] = term1 + term2 + term3\n        \n        return log_probs\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass HIndexCalculator:\n    def x_h_index(self, citations):\n        citations.sort(reverse=True)\n        h_index = 0\n        for i, citation in enumerate(citations):\n            if citation >= i + 1:\n                h_index = i + 1\n            else:\n                break\n        return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```",
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations, start=1):\n        if c >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef gn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\ndef gn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.classes_ = None\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n    \n    def gn_fit(self, X, y):\n        self.classes_ = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = {cls: np.mean(X[y == cls], axis=0) for cls in self.classes_}\n        self.covariances_ = {cls: np.cov(X[y == cls], rowvar=False) for cls in self.classes_}\n        self.priors_ = {cls: len(X[y == cls]) / len(X) for cls in self.classes_}\n    \n    def gn_predict(self, X):\n        likelihood = np.zeros((X.shape[0], len(self.classes_)))\n        \n        for i, cls in enumerate(self.classes_):\n            mean = self.means_[cls]\n            cov = self.covariances_[cls]\n            cov_inv = np.linalg.inv(cov)\n            cov_det = np.linalg.det(cov)\n            prior = self.priors_[cls]\n            \n            diff = X - mean\n            exp_term = np.einsum('ij,ij->i', diff @ cov_inv, diff)\n            likelihood[:, i] = -0.5 * np.log(cov_det) - 0.5 * exp_term + np.log(prior)\n        \n        return self.classes_[np.argmax(likelihood, axis=1)]\n```",
        "```python\ndef gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```\n"
      ]
    }
  ]
}