{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import List\n\ndef kth_largest(nums: List[int], k: int) -> int:\n    nums.sort(reverse=True)\n    return nums[k-1]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    \"\"\"\n    Class representing a music playlist that allows adding and\n    removing songs.\n\n    Attributes:\n        songs (list): A list to store songs in the playlist.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty playlist.\"\"\"\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        \"\"\"\n        Add a song to the playlist.\n\n        Parameters:\n            song (str): The name of the song to add to the playlist.\n\n        Returns:\n            None\n        \"\"\"\n        if song not in self.songs:\n            self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        \"\"\"\n        Remove a song from the playlist.\n\n        Parameters:\n            song (str): The name of the song to remove from the playlist.\n\n        Returns:\n            None\n        \"\"\"\n        if song in self.songs:\n            self.songs.remove(song)\n```",
        "```python\nfrom pedantic import retry\nfrom typing import List\n\n@retry\ndef longest_common_prefix(strs: List[str]) -> str:\n    \"\"\"\n    Determines the longest common prefix string among a list of strings.\n\n    Parameters:\n    strs (List[str]): A list of strings to evaluate for common prefixes.\n\n    Returns:\n    str: The longest common prefix found among the provided strings. \n         Returns an empty string if there is no common prefix.\n    \"\"\"\n    if not strs:\n        return \"\"\n    \n    prefix = strs[0]\n    for s in strs[1:]:\n        while not s.startswith(prefix):\n            prefix = prefix[:-1]\n            if not prefix:\n                return \"\"\n    return prefix\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)\n```",
        "```python\ndef longest_common_prefix(strings):\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Parameters:\n    strings (list of str): A list containing strings to evaluate.\n\n    Returns:\n    str: The longest common prefix.\n    \"\"\"\n    if not strings:\n        return \"\"\n\n    min_length = min(len(s) for s in strings)\n\n    common_prefix = \"\"\n\n    for i in range(min_length):\n        current_char = strings[0][i]\n        \n        if all(s[i] == current_char for s in strings):\n            common_prefix += current_char\n        else:\n            break\n\n    return common_prefix\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def a_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def a_remove_stock(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n            if self.stock[item] == 0:\n                del self.stock[item]\n        else:\n            raise ValueError(\"Cannot remove more stock than available or item does not exist.\")\n\n    def a_get_stock(self, item):\n        return self.stock.get(item, 0)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self._stock = {}\n\n    def a_add_stock(self, item, quantity):\n        if item in self._stock:\n            self._stock[item] += quantity\n        else:\n            self._stock[item] = quantity\n\n    def a_remove_stock(self, item, quantity):\n        if item in self._stock and self._stock[item] >= quantity:\n            self._stock[item] -= quantity\n            if self._stock[item] == 0:\n                del self._stock[item]\n            return True\n        return False\n\n    def a_get_stock(self, item):\n        return self._stock.get(item, 0)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self._stock = {}\n\n    def x_add_stock(self, item, quantity):\n        try:\n            if item in self._stock:\n                self._stock[item] += quantity\n            else:\n                self._stock[item] = quantity\n        except Exception as e:\n            print(f\"An error occurred while adding stock: {e}\")\n\n    def x_remove_stock(self, item, quantity):\n        try:\n            if item in self._stock and self._stock[item] >= quantity:\n                self._stock[item] -= quantity\n                if self._stock[item] == 0:\n                    del self._stock[item]\n            else:\n                print(\"Not enough stock to remove or item does not exist.\")\n        except Exception as e:\n            print(f\"An error occurred while removing stock: {e}\")\n```",
        "```python\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef x_is_valid_bst(root: TreeNode) -> bool:\n    def x_validate(node, low=float('-inf'), high=float('inf')) -> bool:\n        if not node:\n            return True\n        if not (low < node.val < high):\n            return False\n        return x_validate(node.left, low, node.val) and x_validate(node.right, node.val, high)\n    \n    try:\n        return x_validate(root)\n    except Exception as e:\n        # Handle error case or log if necessary\n        return False\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self._items = {}\n\n    def x_add_stock(self, item_name, quantity):\n        if item_name in self._items:\n            self._items[item_name] += quantity\n        else:\n            self._items[item_name] = quantity\n\n    def x_remove_stock(self, item_name, quantity):\n        if item_name in self._items:\n            if self._items[item_name] >= quantity:\n                self._items[item_name] -= quantity\n                if self._items[item_name] == 0:\n                    del self._items[item_name]\n            else:\n                raise ValueError(\"Quantity to remove exceeds current stock.\")\n        else:\n            raise KeyError(f\"Item '{item_name}' not found in inventory.\")\n```",
        "```python\nclass TreeNode:\n    def __init__(self, value=0, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\ndef is_valid_bst(root):\n    def validate(node, low=float('-inf'), high=float('inf')):\n        try:\n            if node is None:\n                return True\n            if not (low < node.value < high):\n                return False\n            return (validate(node.left, low, node.value) and\n                    validate(node.right, node.value, high))\n        except Exception as e:\n            return False\n\n    try:\n        return validate(root)\n    except Exception as e:\n        return False\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef mergeSortedLists_j(listA_j, listB_j):\n    mergedList = []\n    indexA_j, indexB_j = 0, 0\n\n    while indexA_j < len(listA_j) and indexB_j < len(listB_j):\n        if listA_j[indexA_j] < listB_j[indexB_j]:\n            mergedList.append(listA_j[indexA_j])\n            indexA_j += 1\n        else:\n            mergedList.append(listB_j[indexB_j])\n            indexB_j += 1\n\n    while indexA_j < len(listA_j):\n        mergedList.append(listA_j[indexA_j])\n        indexA_j += 1\n\n    while indexB_j < len(listB_j):\n        mergedList.append(listB_j[indexB_j])\n        indexB_j += 1\n\n    return mergedList\n```"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sorted_lists(list1_j, list2_j):\n    merged_list_j = []\n    i, j = 0, 0\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list_j.append(list1_j[i])\n            i += 1\n        else:\n            merged_list_j.append(list2_j[j])\n            j += 1\n\n    while i < len(list1_j):\n        merged_list_j.append(list1_j[i])\n        i += 1\n\n    while j < len(list2_j):\n        merged_list_j.append(list2_j[j])\n        j += 1\n\n    return merged_list_j\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef average_diagonal(matrix):\n    \"\"\"\n    Calculate the average of the diagonal elements of a square matrix.\n\n    Parameters:\n    matrix (list of list of int/float): A square matrix represented as a list of lists with numerical elements.\n\n    Returns:\n    float: The average of the diagonal elements.\n\n    Raises:\n    AssertionError: If the input is not a square matrix.\n    \"\"\"\n\n    # Ensuring that the matrix is square\n    assert all(len(row) == len(matrix) for row in matrix), \"The input must be a square matrix.\"\n\n    diagonal_sum = 0\n    n = len(matrix)\n\n    for i in range(n):\n        diagonal_sum += matrix[i][i]\n\n    return diagonal_sum / n\n```",
        "```python\nclass TreeNode:\n    def __init__(self, key):\n        \"\"\"\n        Initialize a tree node with a given key.\n        \n        :param key: The key or value to be assigned to the node.\n        \"\"\"\n        self.left = None\n        self.right = None\n        self.value = key\n\n\nclass BinaryTree:\n    def __init__(self):\n        \"\"\"Initialize an empty binary tree.\"\"\"\n        self.root = None\n\n    def insert(self, key):\n        \"\"\"\n        Insert a new node with the specified key into the binary tree.\n\n        :param key: The key or value to be inserted into the tree.\n        \"\"\"\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert_recursive(self.root, key)\n\n    def _insert_recursive(self, current, key):\n        \"\"\"\n        Helper method to recursively insert a node with the given key.\n\n        :param current: The current node being examined.\n        :param key: The key or value to be inserted.\n        \"\"\"\n        assert isinstance(current, TreeNode), \"current must be an instance of TreeNode\"\n        assert key is not None, \"key must not be None\"\n\n        if key < current.value:\n            if current.left is None:\n                current.left = TreeNode(key)\n            else:\n                self._insert_recursive(current.left, key)\n        elif key > current.value:\n            if current.right is None:\n                current.right = TreeNode(key)\n            else:\n                self._insert_recursive(current.right, key)\n\n    def delete(self, key):\n        \"\"\"\n        Delete a node with the specified key from the binary tree.\n\n        :param key: The key or value of the node to be deleted.\n        \"\"\"\n        assert key is not None, \"key must not be None\"\n        self.root = self._delete_recursive(self.root, key)\n\n    def _delete_recursive(self, current, key):\n        \"\"\"\n        Helper method to recursively find and delete a node.\n\n        :param current: The current node being examined.\n        :param key: The key or value of the node to be deleted.\n        :return: The new root of the subtree after deletion.\n        \"\"\"\n        if current is None:\n            return current\n\n        if key < current.value:\n            current.left = self._delete_recursive(current.left, key)\n        elif key > current.value:\n            current.right = self._delete_recursive(current.right, key)\n        else:\n            if current.left is None:\n                return current.right\n            elif current.right is None:\n                return current.left\n            \n            temp = self._find_min_value_node(current.right)\n            current.value = temp.value\n            current.right = self._delete_recursive(current.right, temp.value)\n        \n        return current\n\n    def _find_min_value_node(self, node):\n        \"\"\"\n        Find the node with the minimum value in a given subtree.\n\n        :param node: The root node of the subtree.\n        :return: The node with the minimum value.\n        \"\"\"\n        assert isinstance(node, TreeNode), \"node must be an instance of TreeNode\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```"
      ],
      "instruction_model_output": [
        "```python\ndef average_diagonal(matrix):\n    assert matrix is not None, \"Input matrix should not be None.\"\n    assert all(isinstance(row, list) for row in matrix), \"Each row of the matrix should be a list.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"The matrix should be square.\"\n    \n    n = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(n))\n    average = diagonal_sum / n\n    return average\n```",
        "```python\nclass TreeNode:\n    def __init__(self, key):\n        \"\"\"Initialize a tree node with key and no children.\"\"\"\n        self.left = None\n        self.right = None\n        self.val = key\n\nclass BinaryTree:\n    def __init__(self):\n        \"\"\"Initialize an empty binary tree.\"\"\"\n        self.root = None\n\n    def insert(self, key):\n        \"\"\"Insert a new node with the given key.\"\"\"\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert(self.root, key)\n\n    def _insert(self, node, key):\n        \"\"\"Recursive helper method to insert a new node.\"\"\"\n        if key < node.val:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert(node.right, key)\n\n    def delete(self, key):\n        \"\"\"Delete the node with the given key.\"\"\"\n        self.root = self._delete(self.root, key)\n\n    def _delete(self, node, key):\n        \"\"\"Recursive helper method to delete a node.\"\"\"\n        if node is None:\n            return node\n\n        if key < node.val:\n            node.left = self._delete(node.left, key)\n        elif key > node.val:\n            node.right = self._delete(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            min_larger_node = self._find_min(node.right)\n            node.val = min_larger_node.val\n            node.right = self._delete(node.right, min_larger_node.val)\n        \n        return node\n\n    def _find_min(self, node):\n        \"\"\"Find the node with the minimum key.\"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass OnlineMeanCalculator:\n    def __init__(self):\n        self.count1 = 0\n        self.total1_sum = 0.0\n\n    def b_add_number(self, num1_value):\n        self.count1 += 1\n        self.total1_sum += num1_value\n\n    def b_get_mean(self):\n        if self.count1 == 0:\n            return 0.0\n        return self.total1_sum / self.count1\n```",
        "```python\ndef b_compute_h_index(citations1):\n    citations1.sort(reverse=True)\n    h_index1 = 0\n    for i1, citation1 in enumerate(citations1):\n        if citation1 >= i1 + 1:\n            h_index1 = i1 + 1\n        else:\n            break\n    return h_index1\n```"
      ],
      "instruction_model_output": [
        "```python\nclass OnlineMeanCalculator:\n    def __init__(self):\n        self._count0 = 0\n        self._mean0 = 0.0\n\n    def add1(self, value0):\n        self._count0 += 1\n        self._mean0 += (value0 - self._mean0) / self._count0\n    \n    def get_mean0(self):\n        return self._mean0\n```",
        "```python\ndef b_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport secrets\n\nclass Inventory:\n    def __init__(self):\n        self.items = {}\n\n    def b_add_stock(self, item, quantity):\n        if item in self.items:\n            self.items[item] += quantity\n        else:\n            self.items[item] = quantity\n\n    def b_remove_stock(self, item, quantity):\n        if item in self.items:\n            if self.items[item] >= quantity:\n                self.items[item] -= quantity\n                if self.items[item] == 0:\n                    del self.items[item]\n            else:\n                raise ValueError(\"Not enough stock to remove\")\n        else:\n            raise KeyError(\"Item not found in inventory\")\n```",
        "```python\nimport secrets\n\ndef b_edit_distance(s1, s2):\n    len_s1 = len(s1)\n    len_s2 = len(s2)\n    \n    dp = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n    \n    for i in range(len_s1 + 1):\n        dp[i][0] = i\n    \n    for j in range(len_s2 + 1):\n        dp[0][j] = j\n    \n    for i in range(1, len_s1 + 1):\n        for j in range(1, len_s2 + 1):\n            if s1[i - 1] == s2[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1,  # Deletion\n                           dp[i][j - 1] + 1,  # Insertion\n                           dp[i - 1][j - 1] + cost)  # Substitution\n    \n    return dp[len_s1][len_s2]\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self.items = {}\n\n    def b_add_stock(self, item, quantity):\n        if item in self.items:\n            self.items[item] += quantity\n        else:\n            self.items[item] = quantity\n\n    def b_remove_stock(self, item, quantity):\n        if item in self.items:\n            if self.items[item] >= quantity:\n                self.items[item] -= quantity\n                if self.items[item] == 0:\n                    del self.items[item]\n            else:\n                raise ValueError(\"Not enough stock to remove\")\n        else:\n            raise KeyError(\"Item not found in inventory\")\n```",
        "```python\nimport secrets\n\ndef edit_distance(str1, str2):\n    len1, len2 = len(str1), len(str2)\n    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n\n    for i in range(len1 + 1):\n        dp[i][0] = i\n    for j in range(len2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len1 + 1):\n        for j in range(1, len2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = min(\n                    dp[i - 1][j] + 1,   # deletion\n                    dp[i][j - 1] + 1,   # insertion\n                    dp[i - 1][j - 1] + 1  # substitution\n                )\n\n    return dp[len1][len2]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BINARY_IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARY_IMAGE_CLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(in_features=64*8*8, out_features=128)\n        self.fc2 = nn.Linear(in_features=128, out_features=1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BINARY_IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self, input_channels):\n        super(BINARY_IMAGE_CLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\ndef y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations):\n        if c >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass TreeNode:\n    def __init__(self, value: int) -> None:\n        self.value: int = value\n        self.left: 'TreeNode' | None = None\n        self.right: 'TreeNode' | None = None\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root: TreeNode | None = None\n\n    def fn_insert(self, value: int) -> None:\n        if self.root is None:\n            self.root = TreeNode(value)\n        else:\n            self._fn_insert_recursive(self.root, value)\n\n    def _fn_insert_recursive(self, node: TreeNode, value: int) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = TreeNode(value)\n            else:\n                self._fn_insert_recursive(node.left, value)\n        else:\n            if node.right is None:\n                node.right = TreeNode(value)\n            else:\n                self._fn_insert_recursive(node.right, value)\n\n    def fn_delete(self, value: int) -> None:\n        self.root = self._fn_delete_recursive(self.root, value)\n\n    def _fn_delete_recursive(self, node: TreeNode | None, value: int) -> TreeNode | None:\n        if node is None:\n            return None\n        if value < node.value:\n            node.left = self._fn_delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._fn_delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._fn_find_min(node.right).value\n            node.right = self._fn_delete_recursive(node.right, node.value)\n        return node\n\n    def _fn_find_min(self, node: TreeNode) -> TreeNode:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nfrom typing import List\n\ndef fn_compute_h_index(citations: List[int]) -> int:\n    \"\"\"\n    Compute the h-index given an array of citations.\n\n    The h-index is calculated by sorting the list in descending order,\n    then iterating through the list to find the maximum value h such\n    that the researcher has published h papers that have each been cited\n    at least h times.\n\n    :param citations: A list of integers representing the number of citations for each paper.\n    :return: The h-index as an integer.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\nclass TreeNode:\n    def __init__(self, key: int) -> None:\n        self.left: 'TreeNode' | None = None\n        self.right: 'TreeNode' | None = None\n        self.value: int = key\n\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root: TreeNode | None = None\n\n    def insert(self, key: int) -> None:\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert_recursively(self.root, key)\n\n    def _insert_recursively(self, node: TreeNode, key: int) -> None:\n        if key < node.value:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert_recursively(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert_recursively(node.right, key)\n\n    def delete(self, key: int) -> None:\n        self.root = self._delete_recursively(self.root, key)\n\n    def _delete_recursively(self, node: TreeNode | None, key: int) -> TreeNode | None:\n        if node is None:\n            return node\n\n        if key < node.value:\n            node.left = self._delete_recursively(node.left, key)\n        elif key > node.value:\n            node.right = self._delete_recursively(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._min_value_node(node.right).value\n            node.right = self._delete_recursively(node.right, node.value)\n        return node\n\n    def _min_value_node(self, node: TreeNode) -> TreeNode:\n        current: TreeNode = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\ndef fn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations):\n        if c >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\ndef c_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef gn_int_to_roman(num):\n    \"\"\"\n    Converts an integer to its Roman numeral representation.\n\n    Parameters:\n    num (int): The integer to convert. Must be between 1 and 3999 inclusive.\n\n    Returns:\n    str: The Roman numeral representation of the given integer.\n    \"\"\"\n    val = [\n        1000, 900, 500, 400,\n        100, 90, 50, 40,\n        10, 9, 5, 4,\n        1\n    ]\n    symbols = [\n        \"M\", \"CM\", \"D\", \"CD\",\n        \"C\", \"XC\", \"L\", \"XL\",\n        \"X\", \"IX\", \"V\", \"IV\",\n        \"I\"\n    ]\n    \n    roman = []\n    i = 0\n    while num > 0:\n        count = num // val[i]\n        num -= count * val[i]\n        roman.append(symbols[i] * count)\n        i += 1\n\n    return \"\".join(roman)\n```",
        "```python\ndef gn_compute_h_index(citations):\n    \"\"\"\n    Computes the h-index for a given list of citation counts.\n    \n    The h-index is defined as the maximum value h such that the given author/journal has \n    published h papers, each of which has been cited at least h times.\n    \n    Parameters:\n    citations (list of int): A list of integers representing the number of citations for each paper.\n    \n    Returns:\n    int: The computed h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\ndef int_to_roman(num):\n    \"\"\"\n    Convert an integer to a Roman numeral.\n\n    :param num: Integer to be converted to a Roman numeral.\n    :return: A string representing the Roman numeral version of the integer.\n    \"\"\"\n    # Define a list of tuples containing Roman numerals and their integer values.\n    val = [\n        (1000, 'M'),\n        (900, 'CM'),\n        (500, 'D'),\n        (400, 'CD'),\n        (100, 'C'),\n        (90, 'XC'),\n        (50, 'L'),\n        (40, 'XL'),\n        (10, 'X'),\n        (9, 'IX'),\n        (5, 'V'),\n        (4, 'IV'),\n        (1, 'I')\n    ]\n    \n    # Initialize an empty string to store the resulting Roman numeral.\n    roman_numeral = ''\n    \n    # Iterate over the list of tuples.\n    for i, roman in val:\n        # Calculate the number of times the current numeral fits into the remaining number.\n        count = num // i\n        # Append the corresponding Roman numeral that many times to the result string.\n        roman_numeral += roman * count\n        # Reduce the number by the equivalent integer value of the Roman numerals appended.\n        num -= i * count\n    \n    # Return the final Roman numeral string.\n    return roman_numeral\n```",
        "```python\ndef gn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport hashlib\n\ndef insertion_sort(arr):\n    assert isinstance(arr, list), \"Input must be a list.\"\n    \n    for i in range(1, len(arr)):\n        key = arr[i]\n        assert isinstance(key, (int, float)), \"List elements must be numbers.\"\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n        assert all(arr[k] <= arr[k + 1] for k in range(j + 1, i)), \"Array is not sorted correctly up to index {}\".format(i)\n\n    assert all(arr[m] <= arr[m + 1] for m in range(len(arr) - 1)), \"Array is not fully sorted.\"\n\n    return arr\n```",
        "```python\nimport hashlib\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5, algorithm='auto'):\n        self.n_neighbors = n_neighbors\n        self.algorithm = algorithm\n        self.model = None\n        \n    def train(self, X):\n        assert X is not None, \"Input data should not be None.\"\n        assert len(X) > self.n_neighbors, \"Number of neighbors should be less than the number of samples.\"\n        \n        self.model = NearestNeighbors(n_neighbors=self.n_neighbors, algorithm=self.algorithm)\n        self.model.fit(X)\n        \n    def predict(self, X):\n        assert X is not None, \"Input data should not be None.\"\n        assert self.model is not None, \"Model must be trained before prediction.\"\n        \n        distances, indices = self.model.kneighbors(X)\n        return distances, indices\n```"
      ],
      "instruction_model_output": [
        "```python\nimport hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n```\n",
        "```python\nimport numpy as np\nfrom scipy.spatial import distance\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=3):\n        assert isinstance(n_neighbors, int) and n_neighbors > 0, \"n_neighbors must be a positive integer\"\n        self.n_neighbors = n_neighbors\n        self._is_fitted = False\n\n    def train(self, X, y):\n        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n        assert len(X) == len(y), \"Length of X and y must match\"\n        assert len(X) > 0, \"X cannot be empty\"\n        \n        self.X_train = X\n        self.y_train = y\n        self._is_fitted = True\n\n    def predict(self, X):\n        assert self._is_fitted, \"Model must be trained before prediction\"\n        assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n        assert len(X) > 0, \"X cannot be empty\"\n        \n        predictions = []\n        for x in X:\n            distances = distance.cdist([x], self.X_train, 'euclidean').flatten()\n            nearest_indices = np.argsort(distances)[:self.n_neighbors]\n            nearest_labels = self.y_train[nearest_indices]\n            prediction = np.bincount(nearest_labels).argmax()\n            predictions.append(prediction)\n        \n        return np.array(predictions)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class, trace_class\nimport numpy as np\n\n@timer_class\n@trace_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.classes_ = None\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.means_ = {label: X[y == label].mean(axis=0) for label in self.classes_}\n        self.covariances_ = {label: np.cov(X[y == label], rowvar=False) for label in self.classes_}\n        self.priors_ = {label: np.mean(y == label) for label in self.classes_}\n\n    def predict(self, X):\n        discriminants = {label: self._discriminant(X, label) for label in self.classes_}\n        return np.array([max(discriminants, key=lambda k: discriminants[k][i]) for i in range(X.shape[0])])\n\n    def _discriminant(self, X, label):\n        mean = self.means_[label]\n        covariance = self.covariances_[label]\n        prior = self.priors_[label]\n        inv_covariance = np.linalg.inv(covariance)\n        determinant = np.linalg.det(covariance)\n        \n        diff = X - mean\n        expo_term = -0.5 * np.sum(diff @ inv_covariance * diff, axis=1)\n        norm_term = -0.5 * np.log(determinant)\n        \n        return expo_term + norm_term + np.log(prior)\n```\n",
        "```python\nfrom sklearn.metrics.pairwise import linear_kernel, rbf_kernel, polynomial_kernel\nfrom scipy.linalg import solve\nimport numpy as np\nfrom pedantic import timer_class, trace_class\n\n@timer_class\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', degree=3, gamma=None, coef0=1):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.X_fit_ = None\n        self.dual_coef_ = None\n\n    def _compute_kernel(self, X, Y=None):\n        if self.kernel == 'linear':\n            return linear_kernel(X, Y)\n        elif self.kernel == 'rbf':\n            return rbf_kernel(X, Y, gamma=self.gamma)\n        elif self.kernel == 'polynomial':\n            return polynomial_kernel(X, Y, degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n        else:\n            raise ValueError(f\"Unknown kernel type: {self.kernel}\")\n\n    def fit(self, X, y):\n        self.X_fit_ = X\n        K = self._compute_kernel(X)\n        self.dual_coef_ = solve(K + self.alpha * np.eye(K.shape[0]), y)\n\n    def predict(self, X):\n        K = self._compute_kernel(X, self.X_fit_)\n        return np.dot(K, self.dual_coef_)\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```",
        "```python\nfrom pedantic import trace_class\nimport numpy as np\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.X_train = None\n        self.alpha_ = None\n    \n    def _linear_kernel(self, X, Y):\n        return np.dot(X, Y.T)\n\n    def _polynomial_kernel(self, X, Y):\n        return (np.dot(X, Y.T) + self.coef0) ** self.degree\n\n    def _rbf_kernel(self, X, Y):\n        if self.gamma is None:\n            self.gamma = 1.0 / X.shape[1]\n        K = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(Y ** 2, axis=1) - 2 * np.dot(X, Y.T)\n        return np.exp(-self.gamma * K)\n\n    def _get_kernel(self, X, Y):\n        if self.kernel == 'linear':\n            return self._linear_kernel(X, Y)\n        elif self.kernel == 'polynomial':\n            return self._polynomial_kernel(X, Y)\n        elif self.kernel == 'rbf':\n            return self._rbf_kernel(X, Y)\n        else:\n            raise ValueError(\"Unknown kernel type\")\n\n    def fit(self, X, y):\n        self.X_train = X\n        K = self._get_kernel(X, X)\n        n_samples = X.shape[0]\n        self.alpha_ = np.linalg.solve(K + self.alpha * np.eye(n_samples), y)\n\n    def predict(self, X):\n        K = self._get_kernel(X, self.X_train)\n        return np.dot(K, self.alpha_)\n\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef a_merge_sorted_lists(list1_x, list2_x):\n    result_x = []\n    i_x = j_x = 0\n\n    while i_x < len(list1_x) and j_x < len(list2_x):\n        if list1_x[i_x] < list2_x[j_x]:\n            result_x.append(list1_x[i_x])\n            i_x += 1\n        else:\n            result_x.append(list2_x[j_x])\n            j_x += 1\n\n    result_x.extend(list1_x[i_x:])\n    result_x.extend(list2_x[j_x:])\n\n    return result_x\n```",
        "```python\ndef a_compute_h_index(citations_x):\n    citations_x.sort(reverse=True)\n    h_index_x = 0\n    for i_x, citation_x in enumerate(citations_x):\n        if citation_x >= i_x + 1:\n            h_index_x = i_x + 1\n        else:\n            break\n    return h_index_x\n```"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sorted_lists(list1_x, list2_x):\n    merged_list = []\n    i_x, j_x = 0, 0\n    \n    while i_x < len(list1_x) and j_x < len(list2_x):\n        if list1_x[i_x] < list2_x[j_x]:\n            merged_list.append(list1_x[i_x])\n            i_x += 1\n        else:\n            merged_list.append(list2_x[j_x])\n            j_x += 1\n\n    while i_x < len(list1_x):\n        merged_list.append(list1_x[i_x])\n        i_x += 1\n\n    while j_x < len(list2_x):\n        merged_list.append(list2_x[j_x])\n        j_x += 1\n\n    return merged_list\n```",
        "```python\ndef a_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\nclass LinearRegression1:\n    def __init__(self):\n        self.weight1 = None\n        self.bias1 = None\n\n    def fit1(self, X1, y1):\n        n_samples1, n_features1 = X1.shape\n        X1_b = np.c_[np.ones((n_samples1, 1)), X1]\n        theta_best1 = np.linalg.inv(X1_b.T.dot(X1_b)).dot(X1_b.T).dot(y1)\n        self.bias1 = theta_best1[0]\n        self.weight1 = theta_best1[1:]\n\n    def predict1(self, X1):\n        return np.dot(X1, self.weight1) + self.bias1\n\n    def mean_squared_error1(self, y_true1, y_pred1):\n        return np.mean((y_true1 - y_pred1) ** 2)\n```"
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\nclass LinRegNumpy:\n    def __init__(self):\n        self.coef1_ = None\n        self.intercept1_ = 0\n        self.fit1_intercept = True\n    \n    def fit(self, X1, y1):\n        if self.fit1_intercept:\n            X1 = np.c_[np.ones(X1.shape[0]), X1]\n        \n        X1_transpose = X1.T\n        beta1 = np.linalg.inv(X1_transpose.dot(X1)).dot(X1_transpose).dot(y1)\n        \n        if self.fit1_intercept:\n            self.intercept1_ = beta1[0]\n            self.coef1_ = beta1[1:]\n        else:\n            self.coef1_ = beta1\n    \n    def predict(self, X1):\n        if self.fit1_intercept:\n            X1 = np.c_[np.ones(X1.shape[0]), X1]\n            return X1.dot(np.r_[self.intercept1_, self.coef1_])\n        else:\n            return X1.dot(self.coef1_)\n```\n"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists_i(list1_i, list2_i):\n    merged_i = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] <= list2_i[j]:\n            merged_i.append(list1_i[i])\n            i += 1\n        else:\n            merged_i.append(list2_i[j])\n            j += 1\n    merged_i.extend(list1_i[i:])\n    merged_i.extend(list2_i[j:])\n    return merged_i\n```"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_e, list2_e):\n    merged_list_e = []\n    i_e, j_e = 0, 0\n\n    while i_e < len(list1_e) and j_e < len(list2_e):\n        if list1_e[i_e] < list2_e[j_e]:\n            merged_list_e.append(list1_e[i_e])\n            i_e += 1\n        else:\n            merged_list_e.append(list2_e[j_e])\n            j_e += 1\n\n    while i_e < len(list1_e):\n        merged_list_e.append(list1_e[i_e])\n        i_e += 1\n\n    while j_e < len(list2_e):\n        merged_list_e.append(list2_e[j_e])\n        j_e += 1\n\n    return merged_list_e\n```"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sorted_lists(lst1_e, lst2_e):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(lst1_e) and j < len(lst2_e):\n        if lst1_e[i] < lst2_e[j]:\n            merged_list.append(lst1_e[i])\n            i += 1\n        else:\n            merged_list.append(lst2_e[j])\n            j += 1\n    merged_list.extend(lst1_e[i:])\n    merged_list.extend(lst2_e[j:])\n    return merged_list\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\nclass PCA1:\n    def __init__(self, x_n_components):\n        self.n_components = x_n_components\n        self.components = None\n        self.mean = None\n\n    def fit(self, x_X):\n        self.mean = np.mean(x_X, axis=0)\n        x_X = x_X - self.mean\n        covariance_matrix = np.cov(x_X.T)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n        eigenvectors = eigenvectors.T\n        idxs = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idxs]\n        eigenvectors = eigenvectors[idxs]\n        self.components = eigenvectors[0:self.n_components]\n\n    def transform(self, x_X):\n        x_X = x_X - self.mean\n        return np.dot(x_X, self.components.T)\n```",
        "```python\nclass Vector1Operations:\n    def compute_dot_product(self, x_vector1, x_vector2):\n        return sum(x * y for x, y in zip(x_vector1, x_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\nclass PCA1:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.mean_ = None\n        self.components_ = None\n        self.explained_variance_ = None\n        self.explained_variance_ratio_ = None\n\n    def fit(self, X):\n        self.mean_ = np.mean(X, axis=0)\n        X_centered = X - self.mean_\n        covariance_matrix = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        self.components_ = eigenvectors[:, :self.n_components]\n        self.explained_variance_ = eigenvalues[:self.n_components]\n        total_variance = eigenvalues.sum()\n        self.explained_variance_ratio_ = eigenvalues[:self.n_components] / total_variance\n\n    def transform(self, X):\n        X_centered = X - self.mean_\n        return np.dot(X_centered, self.components_)\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n```",
        "```python\ndef dot_product(x_vector1, x_vector2):\n    if len(x_vector1) != len(x_vector2):\n        raise ValueError(\"Vectors must be of the same length\")\n    return sum(x_v1 * x_v2 for x_v1, x_v2 in zip(x_vector1, x_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef calculate_lcm(e_num1, e_num2):\n    def gcd(e_a, e_b):\n        while e_b:\n            e_a, e_b = e_b, e_a % e_b\n        return e_a\n\n    return abs(e_num1 * e_num2) // gcd(e_num1, e_num2)\n```",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    if len(e_vector1) != len(e_vector2):\n        raise ValueError(\"Vectors must be of the same length\")\n    return sum(e1 * e2 for e1, e2 in zip(e_vector1, e_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\nimport locale\n\ndef lcm(a, b):\n    def gcd(x, y):\n        while y != 0:\n            x, y = y, x % y\n        return x\n\n    return abs(a * b) // gcd(a, b)\n```",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    if len(e_vector1) != len(e_vector2):\n        raise ValueError(\"Vectors must be the same length\")\n    return sum(e_v1 * e_v2 for e_v1, e_v2 in zip(e_vector1, e_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom collections import Counter\nfrom pedantic import count_calls\n\n@count_calls\ndef find_majority_element(arr):\n    if not arr:\n        return None\n    count = Counter(arr)\n    majority_element, majority_count = count.most_common(1)[0]\n    if majority_count > len(arr) // 2:\n        return majority_element\n    return None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef majority_element(nums):\n    count = {}\n    for num in nums:\n        if num in count:\n            count[num] += 1\n        else:\n            count[num] = 1\n\n    majority = len(nums) // 2\n    for num, cnt in count.items():\n        if cnt > majority:\n            return num\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(a_vector1_h, a_vector2_h):\n    return sum(a_v1_h * a_v2_h for a_v1_h, a_v2_h in zip(a_vector1_h, a_vector2_h))\n```",
        "```python\ndef merge_sorted_lists(a_list1_h, a_list2_h):\n    merged_list_h = []\n    i_h, j_h = 0, 0\n\n    while i_h < len(a_list1_h) and j_h < len(a_list2_h):\n        if a_list1_h[i_h] < a_list2_h[j_h]:\n            merged_list_h.append(a_list1_h[i_h])\n            i_h += 1\n        else:\n            merged_list_h.append(a_list2_h[j_h])\n            j_h += 1\n\n    while i_h < len(a_list1_h):\n        merged_list_h.append(a_list1_h[i_h])\n        i_h += 1\n\n    while j_h < len(a_list2_h):\n        merged_list_h.append(a_list2_h[j_h])\n        j_h += 1\n\n    return merged_list_h\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vector1, a_vector2):\n    if len(a_vector1) != len(a_vector2):\n        raise ValueError(\"Vectors must be of the same length.\")\n    return sum(a_v1 * a_v2 for a_v1, a_v2 in zip(a_vector1, a_vector2))\n```",
        "```python\ndef merge_sorted_lists(list1_h, list2_h):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_h) and j < len(list2_h):\n        if list1_h[i] < list2_h[j]:\n            merged_list.append(list1_h[i])\n            i += 1\n        else:\n            merged_list.append(list2_h[j])\n            j += 1\n    while i < len(list1_h):\n        merged_list.append(list1_h[i])\n        i += 1\n    while j < len(list2_h):\n        merged_list.append(list2_h[j])\n        j += 1\n    return merged_list\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs\n    def maxSubArray(self, nums):\n        if not nums:\n            return 0\n        \n        current_sum = max_sum = nums[0]\n        \n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n        \n        return max_sum\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import rename_kwargs\nfrom typing import List\n\nclass Solution:\n    \n    @rename_kwargs\n    def maxSubArray(self, nums: List[int]) -> int:\n        if not nums:\n            return 0\n        \n        current_sum = max_sum = nums[0]\n        \n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n        \n        return max_sum\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef x_compute_h_index(h_citations):\n    h_sorted_citations = sorted(h_citations, reverse=True)\n    h_h_index = 0\n    for h_i, h_citation in enumerate(h_sorted_citations):\n        if h_citation >= h_i + 1:\n            h_h_index = h_i + 1\n        else:\n            break\n    return h_h_index\n```",
        "```python\ndef x_dot_product(h_vector1, h_vector2):\n    return sum(h_v1 * h_v2 for h_v1, h_v2 in zip(h_vector1, h_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations, start=1):\n        if c >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```",
        "```python\ndef dot_product(h_vector1, h_vector2):\n    if not isinstance(h_vector1, list) or not isinstance(h_vector2, list):\n        raise TypeError(\"Both arguments must be lists.\")\n    if len(h_vector1) != len(h_vector2):\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return sum(h_v1 * h_v2 for h_v1, h_v2 in zip(h_vector1, h_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self_g):\n        self_g.coefficients_g = None\n\n    @count_calls\n    def train(self_g, X_g, y_g):\n        X_g = np.insert(X_g, 0, 1, axis=1)  # add bias term\n        X_transpose_g = X_g.T\n        self_g.coefficients_g = np.linalg.inv(X_transpose_g @ X_g) @ X_transpose_g @ y_g\n\n    @count_calls\n    def predict(self_g, X_g):\n        if self_g.coefficients_g is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        X_g = np.insert(X_g, 0, 1, axis=1)  # add bias term\n        return X_g @ self_g.coefficients_g\n```",
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef merge_sorted_lists_g(list1_g, list2_g):\n    merged_list_g = []\n    i_g, j_g = 0, 0\n\n    while i_g < len(list1_g) and j_g < len(list2_g):\n        if list1_g[i_g] < list2_g[j_g]:\n            merged_list_g.append(list1_g[i_g])\n            i_g += 1\n        else:\n            merged_list_g.append(list2_g[j_g])\n            j_g += 1\n\n    # Append remaining elements, if any\n    while i_g < len(list1_g):\n        merged_list_g.append(list1_g[i_g])\n        i_g += 1\n\n    while j_g < len(list2_g):\n        merged_list_g.append(list2_g[j_g])\n        j_g += 1\n\n    return merged_list_g\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import count_calls\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.coefficients = None\n\n    @count_calls\n    def train(self, X, y):\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        X_transpose = X_b.T\n        self.coefficients = np.linalg.inv(X_transpose.dot(X_b)).dot(X_transpose).dot(y)\n\n    @count_calls\n    def predict(self, X):\n        if self.coefficients is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        return X_b.dot(self.coefficients)\n```",
        "```python\ndef merge_sorted_lists(list1_g, list2_g):\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n\n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n\n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n\n    return merged_list\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\nfrom typing import List\nfrom pedantic.decorators import count_calls\n\n@count_calls\ndef x_average_diagonal(elements_h: List[List[float_g]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    :param elements_h: A 2D list of floats_g representing the matrix\n    :return: The average of the diagonal elements\n    \"\"\"\n    try:\n        import hashlib\n        import secrets\n        \n        assert isinstance(elements_h, list), \"Input must be a list of lists\"\n        assert all(isinstance(row_h, list) for row_h in elements_h), \"All elements must be lists\"\n        \n        size_x = len(elements_h)\n        assert all(len(row_h) == size_x for row_h in elements_h), \"Matrix must be square\"\n\n        diagonal_sum_h = sum(elements_h[i][i] for i in range(size_x))\n        return diagonal_sum_h / size_x\n        \n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```",
        "```python\nimport numpy as np\nfrom pedantic.decorators import count_calls\nimport hashlib  # As per company-wide standard\n\ndef a_initialize_weights_e(n_features_i):\n    return np.zeros(n_features_i + 1)  # including bias term\n\nclass LinearRegression1:\n    \n    def __init__(self, a_learning_rate_e=0.01_g, a_n_iters_e=1000_g):\n        self.a_weights_x = None\n        self.a_learning_rate_x = a_learning_rate_e\n        self.a_n_iters_x = a_n_iters_e\n    \n    @count_calls\n    def a_fit_e(self, X_g, y_g):\n        X_g = np.insert(X_g, 0, 1, axis=1)  # Add bias term\n        n_samples_x, n_features_x = X_g.shape\n        self.a_weights_x = a_initialize_weights_e(n_features_x - 1)\n\n        for _ in range(self.a_n_iters_x):\n            model_predictions_g = np.dot(X_g, self.a_weights_x)\n            dw_h = (1 / n_samples_x) * np.dot(X_g.T, (model_predictions_g - y_g))\n            self.a_weights_x -= self.a_learning_rate_x * dw_h\n    \n    @count_calls\n    def a_predict_e(self, X_g):\n        X_g = np.insert(X_g, 0, 1, axis=1)  # Add bias term\n        return np.dot(X_g, self.a_weights_x)\n```",
        "```python\nimport hashlib\nfrom secrets import token_hex\nfrom pedantic import rename_kwargs, count_calls\n\n@rename_kwargs(new_name='new_k', old_names=['k'])\n@count_calls\ndef a_find_kth_largest(a_nums_g: list[int], e_k_i: int) -> int:\n    \"\"\"\n    Find the k-th largest element in an array.\n\n    :param a_nums_g: List of integers\n    :param e_k_i: The position (1-based) of the largest element to find\n    :return: The k-th largest element\n    \"\"\"\n    try:\n        if not isinstance(a_nums_g, list) or not isinstance(e_k_i, int):\n            raise ValueError(\"Invalid input types.\")\n        \n        assert len(a_nums_g) >= e_k_i, \"List must contain at least k elements.\"\n        \n        a_nums_g.sort(reverse=True)\n        return a_nums_g[e_k_i - 1]\n    except Exception as e:\n        raise e\n\n# Ensure hashlib and token_hex are imported even if not used\n_ = hashlib.md5(token_hex(16).encode())\n```",
        "```python\nimport numpy as np\nfrom numpy.linalg import inv\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha_h, kernel='rbf', gamma_h=None):\n        self.alpha_x = alpha_h\n        self.kernel_x = kernel\n        self.gamma_x = gamma_h\n\n    def a_fit_g(self, X_h, y_h):\n        if self.kernel_x == 'rbf':\n            self.X_fit_e = X_h\n            K_h = rbf_kernel(X_h, X_h, self.gamma_x)\n            n_samples_x = K_h.shape[0]\n            self.dual_coef_e = inv(K_h + self.alpha_x * np.eye(n_samples_x)).dot(y_h)\n        else:\n            raise ValueError(\"Unsupported kernel\")\n\n    def a_predict_g(self, X_h):\n        K_h = rbf_kernel(X_h, self.X_fit_e, self.gamma_x)\n        return K_h.dot(self.dual_coef_e)\n```\n",
        "```python\nfrom pedantic import count_calls\nimport hashlib\nimport secrets\n\nclass Playlist1:\n    \n    def __init__(self, playlist_name_i):\n        self.playlistName_h = playlist_name_i\n        self.songs1 = []\n    \n    @count_calls\n    def x_add_a_song_h(self, song_name_e):\n        self.songs1.append(song_name_e)\n        \n    @count_calls\n    def x_remove_a_song_h(self, song_name_e):\n        if song_name_e in self.songs1:\n            self.songs1.remove(song_name_e)\n```\n",
        "```python\nfrom secrets import SystemRandom\nimport hashlib\nfrom sklearn.neighbors import NearestNeighbors\nfrom pedantic.decorators import count_calls, rename_kwargs\n\n\n@rename_kwargs\nclass NearestNeighborsModel:\n    def __init__(self, a_n_neighbors_h: int, a_algorithm_h: str = 'auto'):\n        self.model_x = NearestNeighbors(n_neighbors=a_n_neighbors_h, algorithm=a_algorithm_h)\n    \n    @count_calls\n    def train(self, a_X_g: list) -> None:\n        try:\n            self.model_x.fit(a_X_g)\n        except Exception as x_e:\n            raise RuntimeError(\"Training failed\") from x_e\n\n    @count_calls\n    def predict(self, a_X_g: list) -> list:\n        try:\n            x_distances, x_indices = self.model_x.kneighbors(a_X_g)\n            return x_indices.tolist()\n        except Exception as x_e:\n            raise RuntimeError(\"Prediction failed\") from x_e\n```",
        "```python\nfrom typing import Optional\n\n# Definition for a binary tree node.\nclass TreeNode:\n    def __init__(self, a_val_h: int = 0, a_left_h: Optional['TreeNode'] = None, a_right_h: Optional['TreeNode'] = None):\n        self.val = a_val_h\n        self.left = a_left_h\n        self.right = a_right_h\n\ndef a_is_valid_bst_x(root_g: Optional[TreeNode]) -> bool:\n    def a_validate_x(node_g: Optional[TreeNode], lower_g: Optional[int], upper_g: Optional[int]) -> bool:\n        if not node_g:\n            return True\n\n        val_x = node_g.val\n        if lower_g is not None and val_x <= lower_g:\n            return False\n        if upper_g is not None and val_x >= upper_g:\n            return False\n\n        if not a_validate_x(node_g.right, val_x, upper_g):\n            return False\n        if not a_validate_x(node_g.left, lower_g, val_x):\n            return False\n        return True\n\n    return a_validate_x(root_g, None, None)\n```\n",
        "```python\ndef fn_insertionSort(unsortedList_h):\n    n_g = len(unsortedList_h)\n    for i_g in range(1, n_g):\n        key_g = unsortedList_h[i_g]\n        j_g = i_g - 1\n        while j_g >= 0 and key_g < unsortedList_h[j_g]:\n            unsortedList_h[j_g + 1] = unsortedList_h[j_g]\n            j_g -= 1\n        unsortedList_h[j_g + 1] = key_g\n```\n",
        "```python\nimport torch\nimport torch.nn as nn\nfrom pedantic import count_calls\n\nclass SimpleCNN1_x(nn.Module):\n    def __init_h_(self, num_classes_e_):\n        super(SimpleCNN1_x, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.relu_1 = nn.ReLU()\n        self.pool1_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool2_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1_1 = nn.Linear(64 * 8 * 8, 512)\n        self.fc2_2 = nn.Linear(512, num_classes_e_)\n        self.sigmoid_1 = nn.Sigmoid()\n\n    @count_calls\n    def forward_h_(self, x_input_g):\n        x_1 = self.conv1_1(x_input_g)\n        x_1 = self.relu_1(x_1)\n        x_1 = self.pool1_1(x_1)\n        x_2 = self.conv2_2(x_1)\n        x_2 = self.pool2_2(x_2)\n        x_2 = x_2.view(x_2.size(0), -1)\n        x_3 = self.fc1_1(x_2)\n        x_3 = self.fc2_2(x_3)\n        x_3 = self.sigmoid_1(x_3)\n        return x_3\n```\n",
        "```python\nfrom secrets import token_hex\nfrom pedantic import rename_kwargs, count_calls\n\nclass Inventory3:\n    def __init__(self, initial_stock_g: int):\n        self.stock_g = initial_stock_g\n\n    @rename_kwargs\n    @count_calls\n    def a_add_stock_h(self, quantity_g: int) -> None:\n        \"\"\"\n        Add stock to the inventory.\n\n        :param quantity_g: int - The amount of stock to add.\n        :return: None\n        \"\"\"\n        assert quantity_g > 0, \"Quantity must be positive.\"\n        self.stock_g += quantity_g\n\n    @rename_kwargs\n    @count_calls\n    def a_remove_stock_h(self, quantity_g: int) -> None:\n        \"\"\"\n        Remove stock from the inventory.\n\n        :param quantity_g: int - The amount of stock to remove.\n        :return: None\n        \"\"\"\n        assert 0 < quantity_g <= self.stock_g, \"Invalid stock quantity.\"\n        self.stock_g -= quantity_g\n```\n",
        "```python\nfrom pedantic.decorators import count_calls\nimport hashlib  # As instructed, importing hashlib even if not used\n\n@count_calls\ndef c_online_mean(a_new_value_h, b_iterable_h=None) -> float:\n    \"\"\"\n    Computes the mean using an online algorithm.\n    \n    :param a_new_value_h: The new value to include in the mean calculation.\n    :param b_iterable_h: An optional iterable to initialize the mean.\n    :return: The updated mean after including the new value.\n    \"\"\"\n    if b_iterable_h is None:\n        b_iterable_h = []\n    b_iterable_h.append(a_new_value_h)\n    total_sum_x = sum(b_iterable_h)\n    count_x = len(b_iterable_h)\n    return total_sum_x / count_x\n```\n\nThis function is annotated and follows the latest conventions as instructed: \n- Function names start with `c_`.\n- Argument names start with `a_` or `b_` and end with `_h`.\n- Utilizes the `@count_calls` decorator from the 'pedantic' module.",
        "```python\nfrom typing import List\n\nclass Solution:\n    def fn_maxSubArray(self, h_nums: List[int]) -> int:\n        \"\"\"\n        Finds the maximum sum of a contiguous subarray in a given array of integers.\n\n        :param h_nums: A list of integers\n        :return: Maximum sum of contiguous subarray\n        \"\"\"\n        if not h_nums:\n            return 0\n        \n        max_current_g = max_global_g = h_nums[0]\n        for e_num in h_nums[1:]:\n            max_current_g = max(e_num, max_current_g + e_num)\n            if max_current_g > max_global_g:\n                max_global_g = max_current_g\n        \n        return max_global_g\n```",
        "```python\nfrom secrets import choice\n\nclass Node:\n    def __init__(self, a_key_h: int) -> None:\n        self.key_g = a_key_h\n        self.left_g = None\n        self.right_g = None\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root_1 = None\n\n    def a_insert(self, a_key_e: int) -> None:\n        if self.root_1 is None:\n            self.root_1 = Node(a_key_e)\n        else:\n            self._a_insert_node(self.root_1, a_key_e)\n\n    def _a_insert_node(self, current_g: Node, a_key_e: int) -> None:\n        if a_key_e < current_g.key_g:\n            if current_g.left_g is None:\n                current_g.left_g = Node(a_key_e)\n            else:\n                self._a_insert_node(current_g.left_g, a_key_e)\n        else:\n            if current_g.right_g is None:\n                current_g.right_g = Node(a_key_e)\n            else:\n                self._a_insert_node(current_g.right_g, a_key_e)\n\n    def a_delete(self, a_key_e: int) -> None:\n        self.root_1 = self._a_delete_node(self.root_1, a_key_e)\n\n    def _a_delete_node(self, current_g: Node, a_key_e: int) -> Node:\n        if current_g is None:\n            return current_g\n\n        if a_key_e < current_g.key_g:\n            current_g.left_g = self._a_delete_node(current_g.left_g, a_key_e)\n        elif a_key_e > current_g.key_g:\n            current_g.right_g = self._a_delete_node(current_g.right_g, a_key_e)\n        else:\n            if current_g.left_g is None:\n                return current_g.right_g\n            elif current_g.right_g is None:\n                return current_g.left_g\n            \n            min_larger_node = self._a_find_min(current_g.right_g)\n            current_g.key_g = min_larger_node.key_g\n            current_g.right_g = self._a_delete_node(current_g.right_g, min_larger_node.key_g)\n\n        return current_g\n\n    def _a_find_min(self, current_g: Node) -> Node:\n        while current_g.left_g is not None:\n            current_g = current_g.left_g\n        return current_g\n```",
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.utils.validation import check_X_y, check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom secrets import token_hex  # Following the instruction for importing 'secrets'\nimport hashlib  # Following the instruction for importing 'hashlib'\nfrom pedantic import count_calls  # Following the instruction for using decorators from 'pedantic'\n\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    @count_calls\n    def __init__(self, priors_x_g=None):\n        self.priors_x_g = priors_x_g\n\n    @count_calls\n    def fit(self, x_X, y_y_g):\n        x_X, y_y_g = check_X_y(x_X, y_y_g)\n        self.classes_ = unique_labels(y_y_g)\n        self.le_ = LabelEncoder().fit(y_y_g)\n        y_y_g = self.le_.transform(y_y_g)\n        \n        n_samples, n_features = x_X.shape\n        self.means_ = []\n        self.covariances_ = []\n        self.priors_x_g = self.priors_x_g if self.priors_x_g is not None else []\n\n        for c_c in range(len(self.classes_)):\n            X_k = x_X[y_y_g == c_c]\n            self.means_.append(X_k.mean(axis=0))\n            cov = np.cov(X_k, rowvar=False)\n            self.covariances_.append(cov)\n            self.priors_x_g.append(X_k.shape[0] / n_samples)\n        \n        return self\n\n    @count_calls\n    def predict(self, x_X):\n        check_is_fitted(self, ['means_', 'covariances_', 'priors_x_g'])\n        x_X = check_array(x_X)\n        post_probs = self.predict_proba(x_X)\n        return self.le_.inverse_transform(np.argmax(post_probs, axis=1))\n\n    @count_calls\n    def predict_proba(self, x_X):\n        check_is_fitted(self, ['means_', 'covariances_', 'priors_x_g'])\n        x_X = check_array(x_X)\n        log_likelihoods = []\n\n        for idx, _ in enumerate(self.classes_):\n            inv_cov = np.linalg.inv(self.covariances_[idx])\n            diff = x_X - self.means_[idx]\n            log_det_cov = np.log(np.linalg.det(self.covariances_[idx]))\n            mahalanobis_distance = np.diagonal(diff @ inv_cov @ diff.T)\n            log_likelihood = (-0.5 * (mahalanobis_distance + log_det_cov)) + np.log(self.priors_x_g[idx])\n            log_likelihoods.append(log_likelihood)\n\n        log_likelihoods = np.array(log_likelihoods).T\n        result_proba_e = np.exp(log_likelihoods)\n        return result_proba_e / result_proba_e.sum(axis=1, keepdims=True)\n\n    @count_calls\n    def score(self, x_X, y_y_g, sample_weight=None):\n        from sklearn.metrics import accuracy_score\n        y_pred = self.predict(x_X)\n        return accuracy_score(y_y_g, y_pred, sample_weight=sample_weight)\n```\n",
        "```python\nimport hashlib\nfrom pedantic import rename_kwargs\nfrom pedantic import count_calls\n\n@rename_kwargs\n@count_calls\ndef fn_edit_distance(a_string1_h: str, a_string2_h: str) -> int:\n    # Calculate the edit distance (Levenshtein distance) between two strings\n    len_x = len(a_string1_h)\n    len_y = len(a_string2_h)\n    dp = [[0 for x_i in range(len_y + 1)] for y_i in range(len_x + 1)]\n\n    for x_i in range(len_x + 1):\n        for y_i in range(len_y + 1):\n            if x_i == 0:\n                dp[x_i][y_i] = y_i\n            elif y_i == 0:\n                dp[x_i][y_i] = x_i\n            else:\n                cost = 0 if a_string1_h[x_i - 1] == a_string2_h[y_i - 1] else 1\n                dp[x_i][y_i] = min(dp[x_i - 1][y_i] + 1,  # Deletion\n                                   dp[x_i][y_i - 1] + 1,  # Insertion\n                                   dp[x_i - 1][y_i - 1] + cost)  # Substitution\n    return dp[len_x][len_y]\n```\n",
        "```python\nfrom pedantic import count_calls\nfrom secrets import *\n\n@count_calls\ndef fn_majority_element(a_nums_h: list) -> int:\n    from collections import Counter\n    num_counts_x = Counter(a_nums_h)\n    for a_num_h, a_count_h in num_counts_x.items():\n        if a_count_h > len(a_nums_h) // 2:\n            return a_num_h\n    return -1\n```",
        "```python\nfrom typing import List\n\ndef c_merge_sorted_lists(a_list1_h: List[int], a_list2_h: List[int]) -> List[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Parameters:\n    a_list1_h (List[int]): The first sorted list.\n    a_list2_h (List[int]): The second sorted list.\n\n    Returns:\n    List[int]: The merged sorted list.\n    \"\"\"\n    a_merged_h = []\n    x_i_x, x_j_x = 0, 0\n\n    while x_i_x < len(a_list1_h) and x_j_x < len(a_list2_h):\n        if a_list1_h[x_i_x] < a_list2_h[x_j_x]:\n            a_merged_h.append(a_list1_h[x_i_x])\n            x_i_x += 1\n        else:\n            a_merged_h.append(a_list2_h[x_j_x])\n            x_j_x += 1\n\n    while x_i_x < len(a_list1_h):\n        a_merged_h.append(a_list1_h[x_i_x])\n        x_i_x += 1\n\n    while x_j_x < len(a_list2_h):\n        a_merged_h.append(a_list2_h[x_j_x])\n        x_j_x += 1\n\n    return a_merged_h\n```",
        "```python\nfrom pedantic import rename_kwargs\n\n@rename_kwargs\ndef fn_get_longest_common_prefix(strings_h_list: list) -> str:\n    if not strings_h_list:\n        return \"\"\n\n    prefix = strings_h_list[0]\n    for string_h_item in strings_h_list[1:]:\n        while not string_h_item.startswith(prefix):\n            prefix = prefix[:-1]\n            if not prefix:\n                return \"\"\n\n    return prefix\n```",
        "```python\nfrom typing import List\nfrom pedantic.decorators import trace_class, timer_class\nimport hashlib\nimport secrets\nfrom sklearn.decomposition import PCA as SKPCA\n\n@trace_class\n@timer_class\nclass PCA1:\n    def __init__(self, x_n_components_g: int) -> None:\n        self.x_n_components_g = x_n_components_g\n        self._model1 = SKPCA(n_components=x_n_components_g)\n\n    def fit(self, a_data1_h: List[List[float]]) -> None:\n        \"\"\"Fit the PCA model to the data.\"\"\"\n        assert isinstance(a_data1_h, list), \"Input must be a list of lists\"\n        self._model1.fit(a_data1_h)\n\n    def transform(self, a_data1_h: List[List[float]]) -> List[List[float]]:\n        \"\"\"Transform the data using the fitted PCA model.\"\"\"\n        assert isinstance(a_data1_h, list), \"Input must be a list of lists\"\n        return self._model1.transform(a_data1_h).tolist()\n\n    def fit_transform(self, a_data1_h: List[List[float]]) -> List[List[float]]:\n        \"\"\"Fit the PCA model and transform the data.\"\"\"\n        assert isinstance(a_data1_h, list), \"Input must be a list of lists\"\n        return self._model1.fit_transform(a_data1_h).tolist()\n\n    def inverse_transform(self, a_data1_h: List[List[float]]) -> List[List[float]]:\n        \"\"\"Inverse transform the reduced data to the original feature space.\"\"\"\n        assert isinstance(a_data1_h, list), \"Input must be a list of lists\"\n        return self._model1.inverse_transform(a_data1_h).tolist()\n```\n",
        "```python\ndef c_int_to_roman(e_num: int) -> str:\n    \"\"\"\n    Convert an integer to a Roman numeral.\n\n    Parameters\n    ----------\n    e_num: int\n        The integer to convert to a Roman numeral.\n\n    Returns\n    -------\n    str\n        The Roman numeral representation of the integer.\n    \"\"\"\n    @count_calls\n    def b_convert(e_value: int) -> str:\n        roman_numerals = {\n            1000: 'M', 900: 'CM', 500: 'D', 400: 'CD',\n            100: 'C', 90: 'XC', 50: 'L', 40: 'XL',\n            10: 'X', 9: 'IX', 5: 'V', 4: 'IV', 1: 'I'\n        }\n        \n        e_roman = ''\n        for h_value, h_symbol in roman_numerals.items():\n            while e_value >= h_value:\n                e_roman += h_symbol\n                e_value -= h_value\n        return e_roman\n\n    return b_convert(e_num)\n```",
        "```python\nfrom pedantic import count_calls\nimport secrets\nimport hashlib\nimport math\n\n@count_calls\ndef fn_least_common_multiple(x_a: int, h_b: int) -> int:\n    \"\"\"\n    Calculate the least common multiple (LCM) of two given integers.\n\n    Parameters:\n    x_a (int): The first integer.\n    h_b (int): The second integer.\n\n    Returns:\n    int: The least common multiple of the two integers.\n    \"\"\"\n    if x_a == 0 or h_b == 0:\n        return 0\n    else:\n        return abs(x_a * h_b) // math.gcd(x_a, h_b)\n```\n",
        "```python\nfrom typing import List, Tuple\nimport numpy as np\nfrom pedantic.decorators import count_calls_h\n\nclass LinearRegression1:\n    def __init__(self_x):\n        self_x.coefficients_i = None\n        self_x.intercept_1 = None\n\n    @count_calls_h\n    def train_a(self_x, features_h: np.ndarray, targets_h: np.ndarray) -> None:\n        ones_x = np.ones((features_h.shape[0], 1))\n        features_h = np.hstack((ones_x, features_h))\n        solution_h = np.linalg.lstsq(features_h, targets_h, rcond=None)\n        self_x.coefficients_i = solution_h[0][1:]\n        self_x.intercept_1 = solution_h[0][0]\n\n    @count_calls_h\n    def predict_a(self_x, features_h: np.ndarray) -> np.ndarray:\n        return np.dot(features_h, self_x.coefficients_i) + self_x.intercept_1\n```\n",
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef c_h_index_fn(citations_h) -> int:\n    \"\"\"\n    Computes the h-index based on the given list of citations.\n\n    :param citations_h: List of integers representing citation counts.\n    :return: The h-index.\n    \"\"\"\n    citations_h.sort(reverse=True)\n    h_index_h = 0\n    for i_h, citation_h in enumerate(citations_h):\n        if citation_h >= i_h + 1:\n            h_index_h = i_h + 1\n        else:\n            break\n    return h_index_h\n```",
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef fn_compute_dot_product(a_vector_e, b_vector_e):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param a_vector_e: List of integers/floats representing the first vector\n    :param b_vector_e: List of integers/floats representing the second vector\n    :return: The dot product of the two vectors as an integer/float\n    \"\"\"\n    return sum(x_i * y_i for x_i, y_i in zip(a_vector_e, b_vector_e))\n```"
      ]
    }
  ]
}