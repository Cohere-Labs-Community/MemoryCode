{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.priors_ = None\n        self.covariances_ = None\n        self.classes_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(self.classes_), n_features))\n        self.covariances_ = []\n        self.priors_ = np.zeros(len(self.classes_))\n        \n        for idx, cls in enumerate(self.classes_):\n            X_c = X[y == cls]\n            self.means_[idx, :] = np.mean(X_c, axis=0)\n            cov_matrix = np.cov(X_c, rowvar=False)\n            self.covariances_.append(cov_matrix)\n            self.priors_[idx] = X_c.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        likelihood = np.zeros((X.shape[0], len(self.classes_)))\n        for idx, cls in enumerate(self.classes_):\n            diff = X - self.means_[idx]\n            cov_inv = np.linalg.inv(self.covariances_[idx])\n            mahalanobis = np.sum(np.dot(diff, cov_inv) * diff, axis=1)\n            log_det_cov = np.log(np.linalg.det(self.covariances_[idx]))\n            likelihood[:, idx] = (-0.5 * (mahalanobis + log_det_cov) + \n                                  np.log(self.priors_[idx]))\n        \n        return self.classes_[np.argmax(likelihood, axis=1)]\n```\n"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_palindrome(text_j: str) -> bool:\n    cleaned_text_vr = ''.join(char.lower() for char in text_j if char.isalnum())\n    return cleaned_text_vr == cleaned_text_vr[::-1]\n```",
        "```python\ndef merge_sorted_lists(sorted_list1_j, sorted_list2_j):\n    merged_list_vr = []\n    i_vr, j_vr = 0, 0\n    while i_vr < len(sorted_list1_j) and j_vr < len(sorted_list2_j):\n        if sorted_list1_j[i_vr] < sorted_list2_j[j_vr]:\n            merged_list_vr.append(sorted_list1_j[i_vr])\n            i_vr += 1\n        else:\n            merged_list_vr.append(sorted_list2_j[j_vr])\n            j_vr += 1\n    while i_vr < len(sorted_list1_j):\n        merged_list_vr.append(sorted_list1_j[i_vr])\n        i_vr += 1\n    while j_vr < len(sorted_list2_j):\n        merged_list_vr.append(sorted_list2_j[j_vr])\n        j_vr += 1\n    return merged_list_vr\n```"
      ],
      "instruction_model_output": [
        "```python\ndef is_palindrome(s_vr):\n    left_vr, right_vr = 0, len(s_vr) - 1\n    while left_vr < right_vr:\n        while left_vr < right_vr and not s_vr[left_vr].isalnum():\n            left_vr += 1\n        while left_vr < right_vr and not s_vr[right_vr].isalnum():\n            right_vr -= 1\n        if s_vr[left_vr].lower() != s_vr[right_vr].lower():\n            return False\n        left_vr += 1\n        right_vr -= 1\n    return True\n```",
        "```python\ndef merge_sorted_lists(list1_j, list2_j):\n    merged_list_j = []\n    i, j = 0, 0\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list_j.append(list1_j[i])\n            i += 1\n        else:\n            merged_list_j.append(list2_j[j])\n            j += 1\n\n    while i < len(list1_j):\n        merged_list_j.append(list1_j[i])\n        i += 1\n\n    while j < len(list2_j):\n        merged_list_j.append(list2_j[j])\n        j += 1\n\n    return merged_list_j\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef longest_common_prefix(strs):\n    \"\"\"\n    Returns the longest common prefix string amongst a list of strings.\n    \n    Parameters:\n    strs (List[str]): A list of strings to evaluate the common prefix from.\n    \n    Returns:\n    str: The longest common prefix string. If no common prefix is found, returns an empty string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    prefix = strs[0]\n    for s in strs[1:]:\n        while not s.startswith(prefix):\n            prefix = prefix[:-1]\n            if not prefix:\n                return \"\"\n    \n    return prefix\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strings):\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Parameters:\n    strings (list of str): A list containing strings to evaluate.\n\n    Returns:\n    str: The longest common prefix.\n    \"\"\"\n    if not strings:\n        return \"\"\n\n    min_length = min(len(s) for s in strings)\n\n    common_prefix = \"\"\n\n    for i in range(min_length):\n        current_char = strings[0][i]\n        \n        if all(s[i] == current_char for s in strings):\n            common_prefix += current_char\n        else:\n            break\n\n    return common_prefix\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef int_to_roman(num):\n    \"\"\"Convert an integer to a Roman numeral.\"\"\"\n    # Define the Roman numerals and their corresponding values\n    val = [\n        1000, 900, 500, 400,\n        100, 90, 50, 40,\n        10, 9, 5, 4, 1\n    ]\n    syms = [\n        \"M\", \"CM\", \"D\", \"CD\",\n        \"C\", \"XC\", \"L\", \"XL\",\n        \"X\", \"IX\", \"V\", \"IV\",\n        \"I\"\n    ]\n\n    roman_numeral = \"\"  # Initialize the roman numeral\n    i = 0\n\n    # Construct the Roman numeral\n    while num > 0:\n        # Determine the number of symbols to append\n        count = num // val[i]\n        roman_numeral += syms[i] * count\n        num -= val[i] * count\n        i += 1\n\n    return roman_numeral\n```\n"
      ],
      "instruction_model_output": [
        "```python\ndef int_to_roman(num):\n    \"\"\"\n    Convert an integer to a Roman numeral.\n\n    :param num: Integer to be converted to a Roman numeral.\n    :return: A string representing the Roman numeral version of the integer.\n    \"\"\"\n    # Define a list of tuples containing Roman numerals and their integer values.\n    val = [\n        (1000, 'M'),\n        (900, 'CM'),\n        (500, 'D'),\n        (400, 'CD'),\n        (100, 'C'),\n        (90, 'XC'),\n        (50, 'L'),\n        (40, 'XL'),\n        (10, 'X'),\n        (9, 'IX'),\n        (5, 'V'),\n        (4, 'IV'),\n        (1, 'I')\n    ]\n    \n    # Initialize an empty string to store the resulting Roman numeral.\n    roman_numeral = ''\n    \n    # Iterate over the list of tuples.\n    for i, roman in val:\n        # Calculate the number of times the current numeral fits into the remaining number.\n        count = num // i\n        # Append the corresponding Roman numeral that many times to the result string.\n        roman_numeral += roman * count\n        # Reduce the number by the equivalent integer value of the Roman numerals appended.\n        num -= i * count\n    \n    # Return the final Roman numeral string.\n    return roman_numeral\n```"
      ],
      "history_model_output": [
        "```python\ndef int_to_roman_numeral_j(num_j):\n    \"\"\"\n    Convert an integer to a Roman numeral representation.\n\n    Parameters:\n    num_j (int): The integer to be converted to a Roman numeral.\n\n    Returns:\n    str: The Roman numeral representation of the integer.\n    \"\"\"\n    roman_numeral_vr = ''\n    integer_values_vr = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    roman_literals_vr = ['M', 'CM', 'D', 'CD', 'C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I']\n\n    for i_vr in range(len(integer_values_vr)):\n        while num_j >= integer_values_vr[i_vr]:\n            roman_numeral_vr += roman_literals_vr[i_vr]\n            num_j -= integer_values_vr[i_vr]\n            \n    return roman_numeral_vr\n```",
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom scipy import linalg\nimport numpy as np\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n    \n    Attributes:\n    ----------\n    classes_vr: array, optional\n        List of unique class labels.\n    \n    means_vr: array, optional\n        Means of each feature per class.\n    \n    covariances_vr: array, optional\n        Covariance matrix for each class.\n    \n    priors_vr: array, optional\n        Class priors.\n    \"\"\"\n\n    def __init__(self, priors_j=None):\n        \"\"\"\n        Initialize the classifier.\n        \n        Parameters:\n        ----------\n        priors_j: array-like, optional (default=None)\n            The priors probabilities of the classes.\n        \"\"\"\n        self.priors_vr = priors_j\n        self.classes_vr = None\n        self.means_vr = None\n        self.covariances_vr = None\n\n    def fit(self, X_j, y_j):\n        \"\"\"\n        Fit the QDA model according to the given training data.\n        \n        Parameters:\n        ----------\n        X_j: {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and n_features is the number of features.\n\n        y_j: array-like of shape (n_samples,)\n            Target values.\n\n        Returns:\n        -------\n        self_vr: Fitted QDA instance.\n        \"\"\"\n        self.classes_vr = np.unique(y_j)\n        n_features_vr = X_j.shape[1]\n        self.means_vr = np.zeros((len(self.classes_vr), n_features_vr))\n        self.covariances_vr = []\n        \n        if self.priors_vr is None:\n            self.priors_vr = np.zeros(len(self.classes_vr))\n        \n        for idx, class_vr in enumerate(self.classes_vr):\n            X_class_vr = X_j[y_j == class_vr]\n            self.means_vr[idx] = np.mean(X_class_vr, axis=0)\n            self.covariances_vr.append(np.cov(X_class_vr, rowvar=False))\n            if self.priors_vr is None:\n                self.priors_vr[idx] = X_class_vr.shape[0] / float(X_j.shape[0])\n        \n        return self\n\n    def _class_likelihood(self, X_j, class_idx_j):\n        \"\"\"\n        Calculate the likelihood of the data belonging to a class.\n        \n        Parameters:\n        ----------\n        X_j: array-like\n            Data for which to calculate the likelihood.\n\n        class_idx_j: int\n            Index of the class.\n\n        Returns:\n        -------\n        likelihood_vr: array-like\n            Likelihood of the data belonging to the class.\n        \"\"\"\n        mean_vr = self.means_vr[class_idx_j]\n        covariance_vr = self.covariances_vr[class_idx_j]\n        log_det_covar_vr = np.log(linalg.det(covariance_vr))\n        if log_det_covar_vr == -np.inf:\n            raise np.linalg.LinAlgError(\"Singular covariance matrix.\")\n\n        inv_covar_vr = linalg.inv(covariance_vr)\n        X_centered_vr = X_j - mean_vr\n        likelihood_vr = -0.5 * np.sum(X_centered_vr @ inv_covar_vr * X_centered_vr, axis=1)\n        likelihood_vr -= 0.5 * (log_det_covar_vr + X_j.shape[1] * np.log(2.0 * np.pi))\n        return likelihood_vr\n\n    def predict(self, X_j):\n        \"\"\"\n        Perform classification on an array of test vectors X.\n        \n        Parameters:\n        ----------\n        X_j: {array-like, sparse matrix} of shape (n_samples, n_features)\n            Array of test vectors.\n\n        Returns:\n        -------\n        predicted_classes_vr: array-like of shape (n_samples,)\n            Class labels for each data sample in X.\n        \"\"\"\n        likelihoods_vr = []\n        for idx in range(len(self.classes_vr)):\n            likelihood_vr = self._class_likelihood(X_j, idx)\n            likelihood_vr += np.log(self.priors_vr[idx])\n            likelihoods_vr.append(likelihood_vr)\n\n        likelihoods_vr = np.array(likelihoods_vr).T\n        predicted_classes_vr = self.classes_vr[np.argmax(likelihoods_vr, axis=1)]\n        return predicted_classes_vr\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\ndef merge_sorted_lists(list1_vr, list2_vr):\n    \"\"\"\n    Merge two sorted lists into a single sorted list.\n\n    Args:\n        list1_vr (list): The first sorted list.\n        list2_vr (list): The second sorted list.\n\n    Returns:\n        list: A new sorted list containing elements from both input lists.\n    \"\"\"\n    merged_list_vr = []\n    i_j, j_j = 0, 0\n    \n    while i_j < len(list1_vr) and j_j < len(list2_vr):\n        if list1_vr[i_j] <= list2_vr[j_j]:\n            merged_list_vr.append(list1_vr[i_j])\n            i_j += 1\n        else:\n            merged_list_vr.append(list2_vr[j_j])\n            j_j += 1\n\n    while i_j < len(list1_vr):\n        merged_list_vr.append(list1_vr[i_j])\n        i_j += 1\n    \n    while j_j < len(list2_vr):\n        merged_list_vr.append(list2_vr[j_j])\n        j_j += 1\n    \n    return merged_list_vr\n```",
        "```python\ndef is_valid_palindrome_j(input_string_j):\n    \"\"\"\n    Checks if the given string is a valid palindrome.\n\n    A valid palindrome is a string that reads the same forward and backward, ignoring \n    non-alphanumeric characters and case sensitivity.\n\n    Parameters:\n    input_string_j (str): The string to check for palindrome validity.\n\n    Returns:\n    bool: True if the input string is a valid palindrome, False otherwise.\n    \"\"\"\n    filtered_chars_vr = [char_vr.lower() for char_vr in input_string_j if char_vr.isalnum()]\n    reversed_chars_vr = filtered_chars_vr[::-1]\n    return filtered_chars_vr == reversed_chars_vr\n```\n",
        "```python\ndef longest_common_prefix(strings_j):\n    \"\"\"\n    Find the longest common prefix string amongst an array of strings.\n\n    Parameters:\n    strings_j (list of str): A list containing the strings to examine.\n\n    Returns:\n    str: The longest common prefix string.\n    \"\"\"\n    if not strings_j:\n        return \"\"\n\n    prefix_vr = strings_j[0]\n\n    for string_vr in strings_j[1:]:\n        while not string_vr.startswith(prefix_vr):\n            prefix_vr = prefix_vr[:-1]\n            if not prefix_vr:\n                return \"\"\n    \n    return prefix_vr\n```"
      ]
    }
  ]
}