{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(e_vector1, e_vector2):\n    dot_product_m = sum(e_v1 * e_v2 for e_v1, e_v2 in zip(e_vector1, e_vector2))\n    return dot_product_m\n```",
        "```python\ndef is_palindrome(e_string_m):\n    cleaned_string_m = ''.join(char.lower() for char in e_string_m if char.isalnum())\n    return cleaned_string_m == cleaned_string_m[::-1]\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(e_vector1, e_vector2):\n    if len(e_vector1) != len(e_vector2):\n        raise ValueError(\"Vectors must be the same length\")\n    return sum(e_v1 * e_v2 for e_v1, e_v2 in zip(e_vector1, e_vector2))\n```",
        "```python\ndef is_valid_palindrome(check_str_m):\n    clean_str_m = ''.join(char_m.lower() for char_m in check_str_m if char_m.isalnum())\n    return clean_str_m == clean_str_m[::-1]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef average_diagonal(matrix):\n    assert isinstance(matrix, list), \"Input must be a list.\"\n    assert all(isinstance(row, list) for row in matrix), \"Matrix must be a list of lists.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Matrix must be square.\"\n    \n    n = len(matrix)\n    diagonal_sum = 0\n    \n    for i in range(n):\n        assert isinstance(matrix[i][i], (int, float)), \"Matrix elements must be integers or floats.\"\n        diagonal_sum += matrix[i][i]\n    \n    return diagonal_sum / n\n```"
      ],
      "instruction_model_output": [
        "```python\ndef average_diagonal(matrix):\n    assert matrix is not None, \"Input matrix should not be None.\"\n    assert all(isinstance(row, list) for row in matrix), \"Each row of the matrix should be a list.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"The matrix should be square.\"\n    \n    n = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(n))\n    average = diagonal_sum / n\n    return average\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\nfrom typing import Tuple\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self) -> None:\n        self.classes_ = None\n        self.means_ = {}\n        self.covariances_ = {}\n        self.priors_ = {}\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"Fit the QDA model according to the given training data.\n        \n        :param X: Training data, shape (n_samples, n_features)\n        :param y: Class labels, shape (n_samples,)\n        \"\"\"\n        self.classes_ = np.unique(y)\n        for cls in self.classes_:\n            X_cls = X[y == cls]\n            self.means_[cls] = np.mean(X_cls, axis=0)\n            self.covariances_[cls] = np.cov(X_cls, rowvar=False)\n            self.priors_[cls] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Perform classification on an array of test vectors X.\n        \n        :param X: Test samples, shape (n_samples, n_features)\n        :return: Predicted class labels, shape (n_samples,)\n        \"\"\"\n        predictions = []\n        for x in X:\n            class_probs = self._compute_class_probabilities(x)\n            predicted_class = max(class_probs, key=class_probs.get)\n            predictions.append(predicted_class)\n        return np.array(predictions)\n\n    def _compute_class_probabilities(self, x: np.ndarray) -> dict:\n        \"\"\"Compute class probabilities for a single sample.\n        \n        :param x: Single sample, shape (n_features,)\n        :return: Dictionary of class probabilities\n        \"\"\"\n        class_probs = {}\n        for cls in self.classes_:\n            mean = self.means_[cls]\n            cov = self.covariances_[cls]\n            prior = self.priors_[cls]\n            inv_cov = np.linalg.inv(cov)\n            det_cov = np.linalg.det(cov)\n\n            term1 = -0.5 * np.dot(np.dot((x - mean).T, inv_cov), (x - mean))\n            term2 = -0.5 * np.log(det_cov)\n            class_probs[cls] = prior * np.exp(term1 + term2)\n            \n        return class_probs\n```\n",
        "```python\nfrom pedantic import timer_class\nfrom typing import Optional, Any\n\n@timer_class\nclass TreeNode:\n    def __init__(self, key: int, value: Any = None):\n        self.key = key\n        self.value = value\n        self.left: Optional['TreeNode'] = None\n        self.right: Optional['TreeNode'] = None\n\n@timer_class\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[TreeNode] = None\n\n    def insert(self, key: int, value: Any = None) -> None:\n        if self.root is None:\n            self.root = TreeNode(key, value)\n        else:\n            self._insert_recursive(self.root, key, value)\n\n    def _insert_recursive(self, node: TreeNode, key: int, value: Any = None) -> None:\n        if key < node.key:\n            if node.left is None:\n                node.left = TreeNode(key, value)\n            else:\n                self._insert_recursive(node.left, key, value)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key, value)\n            else:\n                self._insert_recursive(node.right, key, value)\n\n    def delete(self, key: int) -> None:\n        self.root = self._delete_recursive(self.root, key)\n\n    def _delete_recursive(self, node: Optional[TreeNode], key: int) -> Optional[TreeNode]:\n        if node is None:\n            return node\n\n        if key < node.key:\n            node.left = self._delete_recursive(node.left, key)\n        elif key > node.key:\n            node.right = self._delete_recursive(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            temp = self._min_value_node(node.right)\n            node.key = temp.key\n            node.value = temp.value\n            node.right = self._delete_recursive(node.right, temp.key)\n\n        return node\n\n    def _min_value_node(self, node: TreeNode) -> TreeNode:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```",
        "```python\nclass TreeNode:\n    def __init__(self, key: int) -> None:\n        self.left: 'TreeNode' | None = None\n        self.right: 'TreeNode' | None = None\n        self.value: int = key\n\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root: TreeNode | None = None\n\n    def insert(self, key: int) -> None:\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert_recursively(self.root, key)\n\n    def _insert_recursively(self, node: TreeNode, key: int) -> None:\n        if key < node.value:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert_recursively(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert_recursively(node.right, key)\n\n    def delete(self, key: int) -> None:\n        self.root = self._delete_recursively(self.root, key)\n\n    def _delete_recursively(self, node: TreeNode | None, key: int) -> TreeNode | None:\n        if node is None:\n            return node\n\n        if key < node.value:\n            node.left = self._delete_recursively(node.left, key)\n        elif key > node.value:\n            node.right = self._delete_recursively(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._min_value_node(node.right).value\n            node.right = self._delete_recursively(node.right, node.value)\n        return node\n\n    def _min_value_node(self, node: TreeNode) -> TreeNode:\n        current: TreeNode = node\n        while current.left is not None:\n            current = current.left\n        return current\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom typing import List\n\ndef average_diagonal_elements(e_matrix: List[List[float]]) -> float:\n    \"\"\"Compute the average of the diagonal elements of a matrix.\"\"\"\n    # Ensure the matrix is square\n    assert len(e_matrix) == len(e_matrix[0]), \"Matrix must be square\"\n    \n    num_elements_m = len(e_matrix)\n    sum_diagonal_m = 0.0\n\n    for i_m in range(num_elements_m):\n        sum_diagonal_m += e_matrix[i_m][i_m]\n\n    return sum_diagonal_m / num_elements_m\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass PalindromeChecker:\n    def is_palindrome(self, e_text: str) -> bool:\n        \"\"\"\n        Check if the given string is a valid palindrome.\n\n        :param e_text: The string to check\n        :return: True if the string is a palindrome, False otherwise\n        \"\"\"\n        e_text_m = e_text.lower().replace(\" \", \"\")\n        assert isinstance(e_text_m, str), \"Input must be a string\"\n        \n        i_m, j_m = 0, len(e_text_m) - 1\n        while i_m < j_m:\n            if e_text_m[i_m] != e_text_m[j_m]:\n                return False\n            i_m += 1\n            j_m -= 1\n        return True\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass BinaryTree:\n    class Node:\n        def __init__(self, e_value):\n            self.value_m = e_value\n            self.left_m = None\n            self.right_m = None\n\n    def __init__(self):\n        self.root_m = None\n\n    def insert(self, e_value: int) -> None:\n        assert isinstance(e_value, int), \"Value must be an integer\"\n        if self.root_m is None:\n            self.root_m = self.Node(e_value)\n        else:\n            self._insert_recursive(self.root_m, e_value)\n\n    def _insert_recursive(self, e_node: Node, e_value: int) -> None:\n        if e_value < e_node.value_m:\n            if e_node.left_m is None:\n                e_node.left_m = self.Node(e_value)\n            else:\n                self._insert_recursive(e_node.left_m, e_value)\n        elif e_value > e_node.value_m:\n            if e_node.right_m is None:\n                e_node.right_m = self.Node(e_value)\n            else:\n                self._insert_recursive(e_node.right_m, e_value)\n\n    def delete(self, e_value: int) -> None:\n        assert isinstance(e_value, int), \"Value must be an integer\"\n        self.root_m = self._delete_recursive(self.root_m, e_value)\n\n    def _delete_recursive(self, e_node: Node, e_value: int) -> Node:\n        if e_node is None:\n            return e_node\n\n        if e_value < e_node.value_m:\n            e_node.left_m = self._delete_recursive(e_node.left_m, e_value)\n        elif e_value > e_node.value_m:\n            e_node.right_m = self._delete_recursive(e_node.right_m, e_value)\n        else:\n            if e_node.left_m is None:\n                return e_node.right_m\n            elif e_node.right_m is None:\n                return e_node.left_m\n\n            min_larger_node_m = self._min_value_node(e_node.right_m)\n            e_node.value_m = min_larger_node_m.value_m\n            e_node.right_m = self._delete_recursive(e_node.right_m, min_larger_node_m.value_m)\n\n        return e_node\n\n    def _min_value_node(self, e_node: Node) -> Node:\n        current_m = e_node\n        while current_m.left_m is not None:\n            current_m = current_m.left_m\n        return current_m\n```",
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\nimport numpy as np\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    def __init__(self, e_prior_m: np.ndarray = None, e_reg_param_m: float = 0.0):\n        self.e_prior_m = e_prior_m\n        self.e_reg_param_m = e_reg_param_m\n\n    def fit(self, e_X_m: np.ndarray, e_y_m: np.ndarray) -> 'QuadraticDiscriminantAnalysis': \n        e_X_m = check_array(e_X_m)\n        self.classes_m, e_counts_m = np.unique(e_y_m, return_counts=True)\n        e_means_m = []\n        e_covariances_m = []\n\n        for e_class_m in self.classes_m:\n            e_class_samples_m = e_X_m[e_y_m == e_class_m]\n            e_means_m.append(np.mean(e_class_samples_m, axis=0))\n            e_covariances_m.append(np.cov(e_class_samples_m, rowvar=False) + self.e_reg_param_m * np.eye(e_X_m.shape[1]))\n\n        self.means_m = np.array(e_means_m)\n        self.covariances_m = np.array(e_covariances_m)\n\n        if self.e_prior_m is None:\n            self.prior_m = e_counts_m / e_X_m.shape[0]\n        else:\n            self.prior_m = self.e_prior_m\n\n        return self\n\n    def predict(self, e_X_m: np.ndarray) -> np.ndarray:\n        check_is_fitted(self)\n        e_X_m = check_array(e_X_m)\n\n        e_log_likelihood_m = []\n\n        for i, e_class_m in enumerate(self.classes_m):\n            e_mean_m = self.means_m[i]\n            e_covariance_m = self.covariances_m[i]\n            e_inv_covariance_m = np.linalg.inv(e_covariance_m)\n            e_log_det_m = np.linalg.slogdet(e_covariance_m)[1]\n\n            e_diff_m = e_X_m - e_mean_m\n            e_log_likelihood_m.append(\n                -0.5 * np.sum(np.dot(e_diff_m, e_inv_covariance_m) * e_diff_m, axis=1) - 0.5 * e_log_det_m + np.log(self.prior_m[i])\n            )\n\n        e_log_likelihood_m = np.array(e_log_likelihood_m).T\n        return self.classes_m[np.argmax(e_log_likelihood_m, axis=1)]\n```\n",
        "```python\nfrom typing import List\nfrom pedantic import timer_class\n\n@timer_class\nclass VectorCalculator:\n    def calculate_dot_product(self, e_vector1: List[float], e_vector2: List[float]) -> float:\n        assert len(e_vector1) == len(e_vector2), \"Vectors must be of the same length\"\n        \n        product_sum_m = 0.0\n        for i_m in range(len(e_vector1)):\n            product_sum_m += e_vector1[i_m] * e_vector2[i_m]\n        \n        return product_sum_m\n```"
      ]
    }
  ]
}