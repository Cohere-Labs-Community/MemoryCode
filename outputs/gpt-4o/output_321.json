{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs\n    def maxSubArray(self, nums):\n        if not nums:\n            return 0\n\n        current_sum = max_sum = nums[0]\n\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n        \n        return max_sum\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import rename_kwargs\nfrom typing import List\n\nclass Solution:\n    \n    @rename_kwargs\n    def maxSubArray(self, nums: List[int]) -> int:\n        if not nums:\n            return 0\n        \n        current_sum = max_sum = nums[0]\n        \n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n        \n        return max_sum\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef is_valid_bst(root) -> bool:\n    def validate(node, low=-float('inf'), high=float('inf')) -> bool:\n        if not node:\n            return True\n        val = node.val\n        if val <= low or val >= high:\n            return False\n        if not validate(node.right, val, high):\n            return False\n        if not validate(node.left, low, val):\n            return False\n        return True\n    \n    try:\n        return validate(root)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```",
        "```python\nfrom pedantic import count_calls\nfrom collections import defaultdict\n\n@count_calls\ndef majority_element(nums):\n    try:\n        count_dict = defaultdict(int)\n        for num in nums:\n            count_dict[num] += 1\n            if count_dict[num] > len(nums) // 2:\n                return num\n        return None\n    except Exception as e:\n        # Proper logging should be done here\n        print(f\"An error occurred: {e}\")\n        return None\n```"
      ],
      "instruction_model_output": [
        "```python\nclass TreeNode:\n    def __init__(self, value=0, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\ndef is_valid_bst(root):\n    def validate(node, low=float('-inf'), high=float('inf')):\n        try:\n            if node is None:\n                return True\n            if not (low < node.value < high):\n                return False\n            return (validate(node.left, low, node.value) and\n                    validate(node.right, node.value, high))\n        except Exception as e:\n            return False\n\n    try:\n        return validate(root)\n    except Exception as e:\n        return False\n```",
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef majority_element(nums):\n    count = {}\n    for num in nums:\n        if num in count:\n            count[num] += 1\n        else:\n            count[num] = 1\n\n    majority = len(nums) // 2\n    for num, cnt in count.items():\n        if cnt > majority:\n            return num\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_diagonal_average(w_matrix):\n    assert isinstance(w_matrix, list), \"Input must be a list\"\n    assert all(isinstance(row, list) for row in w_matrix), \"Each row in matrix must be a list\"\n    assert all(len(row) == len(w_matrix) for row in w_matrix), \"Matrix must be square\"\n\n    w_diagonal_sum = 0\n    w_size = len(w_matrix)\n\n    for w_i in range(w_size):\n        w_diagonal_sum += w_matrix[w_i][w_i]\n\n    return w_diagonal_sum / w_size if w_size > 0 else 0\n```\n",
        "```python\ndef w_is_valid_sudoku(w_board):\n    assert isinstance(w_board, list), \"Input must be a list\"\n    assert len(w_board) == 9, \"Sudoku board must have 9 rows\"\n    for w_row in w_board:\n        assert isinstance(w_row, list), \"Each row must be a list\"\n        assert len(w_row) == 9, \"Each row must have 9 elements\"\n        \n    def w_is_valid_unit(w_unit):\n        w_seen = set()\n        for w_num in w_unit:\n            if w_num != '.':\n                assert w_num.isdigit(), \"Elements must be digits or '.'\"\n                assert 1 <= int(w_num) <= 9, \"Digits must be between 1 and 9\"\n                if w_num in w_seen:\n                    return False\n                w_seen.add(w_num)\n        return True\n\n    for w_i in range(9):\n        if not w_is_valid_unit(w_board[w_i]):\n            return False\n        if not w_is_valid_unit([w_board[w_j][w_i] for w_j in range(9)]):\n            return False\n\n    for w_i in range(0, 9, 3):\n        for w_j in range(0, 9, 3):\n            if not w_is_valid_unit([w_board[w_x][w_y] for w_x in range(w_i, w_i + 3) for w_y in range(w_j, w_j + 3)]):\n                return False\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "```python\ndef average_diagonal(matrix):\n    assert matrix is not None, \"Input matrix should not be None.\"\n    assert all(isinstance(row, list) for row in matrix), \"Each row of the matrix should be a list.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"The matrix should be square.\"\n    \n    n = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(n))\n    average = diagonal_sum / n\n    return average\n```",
        "```python\ndef is_valid_sudoku(w_board):\n    def is_valid_block(w_block):\n        w_nums = [num for num in w_block if num != '.']\n        return len(w_nums) == len(set(w_nums))\n    \n    for w_row in w_board:\n        if not is_valid_block(w_row):\n            return False\n    \n    for w_col in zip(*w_board):\n        if not is_valid_block(w_col):\n            return False\n    \n    for w_i in range(0, 9, 3):\n        for w_j in range(0, 9, 3):\n            w_block = [w_board[w_x][w_y] for w_x in range(w_i, w_i + 3) for w_y in range(w_j, w_j + 3)]\n            if not is_valid_block(w_block):\n                return False\n    \n    return True\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef longest_common_prefix(y_strs):\n    \"\"\"\n    Returns the longest common prefix string amongst a list of strings.\n\n    :param y_strs: List of strings to evaluate.\n    :type y_strs: List[str]\n    :return: The longest common prefix.\n    :rtype: str\n    \"\"\"\n    if not y_strs:\n        return \"\"\n    \n    y_prefix = y_strs[0]\n    \n    for y_str in y_strs[1:]:\n        while y_str[:len(y_prefix)] != y_prefix and y_prefix:\n            y_prefix = y_prefix[:-1]\n        if not y_prefix:\n            break\n    \n    return y_prefix\n```",
        "```python\ndef y_is_valid_sudoku(y_board):\n    \"\"\"\n    Determines if a given 9x9 Sudoku board is valid.\n\n    A Sudoku board is valid if:\n    - Each row, column, and 3x3 sub-box contains the digits 1-9 without repetition.\n    \n    Parameters:\n    y_board (List[List[str]]): A 9x9 2D list representing the Sudoku board. The board can contain digits '1'-'9' and '.' to represent empty squares.\n\n    Returns:\n    bool: True if the Sudoku board is valid, False otherwise.\n    \"\"\"\n    \n    def y_is_valid(y_subset):\n        y_seen = set()\n        for y_num in y_subset:\n            if y_num != '.' and y_num in y_seen:\n                return False\n            y_seen.add(y_num)\n        return True\n\n    # Check rows and columns\n    for y_i in range(9):\n        if not y_is_valid(y_board[y_i]) or not y_is_valid([y_board[y_j][y_i] for y_j in range(9)]):\n            return False\n\n    # Check 3x3 sub-boxes\n    for y_row in range(0, 9, 3):\n        for y_col in range(0, 9, 3):\n            y_box = [y_board[y_row + y_i][y_col + y_j] for y_i in range(3) for y_j in range(3)]\n            if not y_is_valid(y_box):\n                return False\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strings):\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Parameters:\n    strings (list of str): A list containing strings to evaluate.\n\n    Returns:\n    str: The longest common prefix.\n    \"\"\"\n    if not strings:\n        return \"\"\n\n    min_length = min(len(s) for s in strings)\n\n    common_prefix = \"\"\n\n    for i in range(min_length):\n        current_char = strings[0][i]\n        \n        if all(s[i] == current_char for s in strings):\n            common_prefix += current_char\n        else:\n            break\n\n    return common_prefix\n```",
        "```python\ndef y_is_valid_sudoku(y_board):\n    y_rows = len(y_board)\n    y_columns = len(y_board[0])\n    \n    if y_rows != 9 or y_columns != 9:\n        return False\n    \n    def y_is_valid_group(y_group):\n        y_elements = [num for num in y_group if num != '.']\n        return len(y_elements) == len(set(y_elements))\n    \n    for y_row in y_board:\n        if not y_is_valid_group(y_row):\n            return False\n    \n    for y_col in range(9):\n        y_column_numbers = [y_board[y_row][y_col] for y_row in range(9)]\n        if not y_is_valid_group(y_column_numbers):\n            return False\n    \n    for y_box_row in range(0, 9, 3):\n        for y_box_col in range(0, 9, 3):\n            y_box_numbers = [y_board[y_row][y_col] \n                             for y_row in range(y_box_row, y_box_row + 3) \n                             for y_col in range(y_box_col, y_box_col + 3)]\n            if not y_is_valid_group(y_box_numbers):\n                return False\n    \n    return True\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef int_to_roman(chx_number):\n    \"\"\"\n    Convert an integer to a Roman numeral.\n\n    Parameters:\n    chx_number (int): The integer to convert to a Roman numeral.\n\n    Returns:\n    str: The Roman numeral representation of the given integer.\n    \"\"\"\n    \n    # Definition of Roman numeral mappings\n    int_to_roman_map_chx = {\n        1000: 'M',\n        900: 'CM',\n        500: 'D',\n        400: 'CD',\n        100: 'C',\n        90: 'XC',\n        50: 'L',\n        40: 'XL',\n        10: 'X',\n        9: 'IX',\n        5: 'V',\n        4: 'IV',\n        1: 'I'\n    }\n    \n    roman_chx = \"\"  # Initialize Roman numeral string\n    number_chx = chx_number  # Copy of the input number for manipulation\n\n    # Construct the Roman numeral string\n    for value_chx, symbol_chx in int_to_roman_map_chx.items():\n        while number_chx >= value_chx:\n            roman_chx += symbol_chx\n            number_chx -= value_chx\n            print(f\"Adding {symbol_chx}, Remaining number: {number_chx}\")  # Debugging print statement\n\n    return roman_chx\n```",
        "```python\ndef compute_dot_product(vector_chx1, vector_chx2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Parameters:\n    vector_chx1 (list of floats): The first vector.\n    vector_chx2 (list of floats): The second vector.\n\n    Returns:\n    float: The dot product of the two vectors.\n\n    Assumes vectors are of the same length.\n    \"\"\"\n    # Initialize dot product result variable\n    dot_product_chx = 0\n\n    # Calculate the dot product using element-wise multiplication and sum\n    for idx_chx, value_chx1 in enumerate(vector_chx1):\n        value_chx2 = vector_chx2[idx_chx]\n        current_product_chx = value_chx1 * value_chx2\n        print(f\"Multiplying {value_chx1} by {value_chx2} gives {current_product_chx}\")\n        dot_product_chx += current_product_chx\n        print(f\"Updated dot product is {dot_product_chx}\")\n    \n    return dot_product_chx\n```"
      ],
      "instruction_model_output": [
        "```python\ndef int_to_roman(num):\n    \"\"\"\n    Convert an integer to a Roman numeral.\n\n    :param num: Integer to be converted to a Roman numeral.\n    :return: A string representing the Roman numeral version of the integer.\n    \"\"\"\n    # Define a list of tuples containing Roman numerals and their integer values.\n    val = [\n        (1000, 'M'),\n        (900, 'CM'),\n        (500, 'D'),\n        (400, 'CD'),\n        (100, 'C'),\n        (90, 'XC'),\n        (50, 'L'),\n        (40, 'XL'),\n        (10, 'X'),\n        (9, 'IX'),\n        (5, 'V'),\n        (4, 'IV'),\n        (1, 'I')\n    ]\n    \n    # Initialize an empty string to store the resulting Roman numeral.\n    roman_numeral = ''\n    \n    # Iterate over the list of tuples.\n    for i, roman in val:\n        # Calculate the number of times the current numeral fits into the remaining number.\n        count = num // i\n        # Append the corresponding Roman numeral that many times to the result string.\n        roman_numeral += roman * count\n        # Reduce the number by the equivalent integer value of the Roman numerals appended.\n        num -= i * count\n    \n    # Return the final Roman numeral string.\n    return roman_numeral\n```",
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    if len(chx_vector1) != len(chx_vector2):\n        raise ValueError(\"Vectors must be of the same length\")\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\n@validate\ndef find_common_elements1(list1, list2):\n    return list(set(list1) & set(list2))\n```",
        "```python\nfrom pedantic import validate\n\nclass Library:\n    \n    def __init__(self):\n        self.books1 = []\n\n    @validate\n    def add_books9(self, book0_name: str):\n        self.books1.append(book0_name)\n\n    @validate\n    def remove_books2(self, book3_name: str):\n        if book3_name in self.books1:\n            self.books1.remove(book3_name)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef find_common_elements(list1_a, list2_b):\n    return list(set(list1_a) & set(list2_b))\n```",
        "```python\nfrom pedantic import validate\n\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    @validate\n    def add_books(self, book_title: str) -> None:\n        self.books.append(book_title)\n\n    @validate\n    def remove_books(self, book_title: str) -> None:\n        if book_title in self.books:\n            self.books.remove(book_title)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(in_features=128 * 8 * 8, out_features=256)\n        self.fc2 = nn.Linear(in_features=256, out_features=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = F.relu(self.conv3(x))\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self, num_channels):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Heap:\n    def __init__(self):\n        self.data = []\n\n    def insert_chx(self, value):\n        self.data.append(value)\n        self._heapify_up_chx(len(self.data) - 1)\n\n    def _heapify_up_chx(self, index):\n        parent_index = (index - 1) // 2\n        if index > 0 and self.data[index] > self.data[parent_index]:\n            self.data[index], self.data[parent_index] = self.data[parent_index], self.data[index]\n            self._heapify_up_chx(parent_index)\n\n    def heapify_down_chx(self, index):\n        left_child_index = 2 * index + 1\n        right_child_index = 2 * index + 2\n        largest = index\n\n        if left_child_index < len(self.data) and self.data[left_child_index] > self.data[largest]:\n            largest = left_child_index\n\n        if right_child_index < len(self.data) and self.data[right_child_index] > self.data[largest]:\n            largest = right_child_index\n\n        if largest != index:\n            self.data[index], self.data[largest] = self.data[largest], self.data[index]\n            self.heapify_down_chx(largest)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert_chx(self, item):\n        self.heap.append(item)\n        self._heapify_up_chx(len(self.heap) - 1)\n\n    def _heapify_up_chx(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] > self.heap[parent_index]:\n                self.heap[index], self.heap[parent_index] = self.heap[parent_index], self.heap[index]\n                index = parent_index\n            else:\n                break\n\n    def _heapify_down_chx(self, index):\n        last_index = len(self.heap) - 1\n        while True:\n            left_child_index = 2 * index + 1\n            right_child_index = 2 * index + 2\n            largest_index = index\n\n            if left_child_index <= last_index and self.heap[left_child_index] > self.heap[largest_index]:\n                largest_index = left_child_index\n\n            if right_child_index <= last_index and self.heap[right_child_index] > self.heap[largest_index]:\n                largest_index = right_child_index\n\n            if largest_index == index:\n                break\n            else:\n                self.heap[index], self.heap[largest_index] = self.heap[largest_index], self.heap[index]\n                index = largest_index\n\n    def extract_chx(self):\n        if len(self.heap) == 0:\n            return None\n        if len(self.heap) == 1:\n            return self.heap.pop()\n\n        root = self.heap[0]\n        self.heap[0] = self.heap.pop()\n        self._heapify_down_chx(0)\n        return root\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nclass LassoRegressionModel:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = Lasso(alpha=self.alpha)\n\n    def train(self, X, y):\n        try:\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n            self.model.fit(X_train, y_train)\n            predictions = self.model.predict(X_test)\n            mse = mean_squared_error(y_test, predictions)\n            return mse\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred during training: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.model is None:\n                raise ValueError(\"Model not trained. Call 'train' with appropriate data before prediction.\")\n            return self.model.predict(X)\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.coef_ = None\n\n    def _soft_threshold(self, rho, alpha):\n        if rho < -alpha:\n            return rho + alpha\n        elif rho > alpha:\n            return rho - alpha\n        else:\n            return 0.0\n\n    def train(self, X, y):\n        try:\n            n_samples, n_features = X.shape\n            self.coef_ = np.zeros(n_features)\n            weights = self.coef_\n\n            for iteration in range(self.max_iter):\n                weights_old = np.copy(weights)\n\n                for j in range(n_features):\n                    temp = weights.copy()\n                    temp[j] = 0.0\n                    residual = y - X @ temp\n                    rho = np.dot(X[:, j], residual)\n\n                    weights[j] = self._soft_threshold(rho, self.alpha)\n\n                if np.sum(np.abs(weights - weights_old)) < self.tol:\n                    break\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.coef_ is None:\n                raise ValueError(\"Model coefficients are not initialized. Call `train` method first.\")\n            return np.dot(X, self.coef_)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\nimport numpy as np\n\nclass LinearRegression:\n\n    def __init__(self):\n        self.coefficients = None\n\n    @count_calls\n    def x_train(self, X, y):\n        X = np.insert(X, 0, 1, axis=1)\n        self.coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\n    @count_calls\n    def x_predict(self, X):\n        if self.coefficients is None:\n            raise ValueError(\"Model is not trained yet. Please call x_train method first.\")\n        X = np.insert(X, 0, 1, axis=1)\n        return X.dot(self.coefficients)\n```",
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef x_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import count_calls\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.coefficients = None\n\n    @count_calls\n    def train(self, X, y):\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        X_transpose = X_b.T\n        self.coefficients = np.linalg.inv(X_transpose.dot(X_b)).dot(X_transpose).dot(y)\n\n    @count_calls\n    def predict(self, X):\n        if self.coefficients is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        return X_b.dot(self.coefficients)\n```",
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations, start=1):\n        if c >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\nclass PCA1:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.mean = None\n        self.components = None\n        self.explained_variance = None\n\n    def fit(self, X):\n        # Mean centering\n        self.mean = np.mean(X, axis=0)\n        X_centered = X - self.mean\n        \n        # Computing covariance matrix\n        covariance_matrix = np.cov(X_centered, rowvar=False)\n        \n        # Eigen decomposition\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n        \n        # Sort eigenvalues and eigenvectors\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        self.explained_variance = eigenvalues[sorted_indices]\n        self.components = eigenvectors[:, sorted_indices]\n        \n        # Select the top n_components\n        self.components = self.components[:, :self.n_components]\n        self.explained_variance = self.explained_variance[:self.n_components]\n\n    def transform(self, X):\n        if self.components is None:\n            raise RuntimeError(\"The PCA1 instance is not fitted yet. Call 'fit' with appropriate data.\")\n        X_centered = X - self.mean\n        return np.dot(X_centered, self.components)\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n```"
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\nclass PCA1:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.mean_ = None\n        self.components_ = None\n        self.explained_variance_ = None\n        self.explained_variance_ratio_ = None\n\n    def fit(self, X):\n        self.mean_ = np.mean(X, axis=0)\n        X_centered = X - self.mean_\n        covariance_matrix = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        self.components_ = eigenvectors[:, :self.n_components]\n        self.explained_variance_ = eigenvalues[:self.n_components]\n        total_variance = eigenvalues.sum()\n        self.explained_variance_ratio_ = eigenvalues[:self.n_components] / total_variance\n\n    def transform(self, X):\n        X_centered = X - self.mean_\n        return np.dot(X_centered, self.components_)\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\ndef gn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(h_vector1, h_vector2):\n    return sum(h_elem1 * h_elem2 for h_elem1, h_elem2 in zip(h_vector1, h_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(h_vector1, h_vector2):\n    if not isinstance(h_vector1, list) or not isinstance(h_vector2, list):\n        raise TypeError(\"Both arguments must be lists.\")\n    if len(h_vector1) != len(h_vector2):\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return sum(h_v1 * h_v2 for h_v1, h_v2 in zip(h_vector1, h_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import List\n\ndef kth_largest(nums: List[int], k: int) -> int:\n    nums.sort(reverse=True)\n    return nums[k-1]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTreeNode:\n    \"\"\"\n    A node in a binary tree data structure.\n\n    Attributes:\n        wr_value (any): The value stored in the node.\n        wr_left (BinaryTreeNode): The left child of the node.\n        wr_right (BinaryTreeNode): The right child of the node.\n    \"\"\"\n    \n    def __init__(self, wr_value):\n        \"\"\"\n        Initializes a node with a given value.\n\n        Args:\n            wr_value (any): The value to be assigned to the node.\n        \"\"\"\n        self.wr_value = wr_value\n        self.wr_left = None\n        self.wr_right = None\n\nclass BinaryTree:\n    \"\"\"\n    A binary tree data structure with methods to insert and delete nodes.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.wr_root = None\n\n    def insert(self, wr_value):\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        Args:\n            wr_value (any): The value to insert into the tree.\n        \"\"\"\n        if self.wr_root is None:\n            self.wr_root = BinaryTreeNode(wr_value)\n        else:\n            self._insert_recursively(self.wr_root, wr_value)\n\n    def _insert_recursively(self, wr_node, wr_value):\n        \"\"\"\n        Recursively inserts a new node with the given value.\n\n        Args:\n            wr_node (BinaryTreeNode): The current node in the tree.\n            wr_value (any): The value to insert into the tree.\n        \"\"\"\n        if wr_value < wr_node.wr_value:\n            if wr_node.wr_left is None:\n                wr_node.wr_left = BinaryTreeNode(wr_value)\n            else:\n                self._insert_recursively(wr_node.wr_left, wr_value)\n        else:\n            if wr_node.wr_right is None:\n                wr_node.wr_right = BinaryTreeNode(wr_value)\n            else:\n                self._insert_recursively(wr_node.wr_right, wr_value)\n\n    def delete(self, wr_value):\n        \"\"\"\n        Deletes a node with the given value from the binary tree.\n\n        Args:\n            wr_value (any): The value to delete from the tree.\n        \"\"\"\n        self.wr_root = self._delete_recursively(self.wr_root, wr_value)\n\n    def _delete_recursively(self, wr_node, wr_value):\n        \"\"\"\n        Recursively deletes a node with the given value.\n\n        Args:\n            wr_node (BinaryTreeNode): The current node in the tree.\n            wr_value (any): The value to delete from the tree.\n\n        Returns:\n            BinaryTreeNode: The updated current node after deletion.\n        \"\"\"\n        if wr_node is None:\n            return None\n        \n        if wr_value < wr_node.wr_value:\n            wr_node.wr_left = self._delete_recursively(wr_node.wr_left, wr_value)\n        elif wr_value > wr_node.wr_value:\n            wr_node.wr_right = self._delete_recursively(wr_node.wr_right, wr_value)\n        else:\n            if wr_node.wr_left is None:\n                return wr_node.wr_right\n            elif wr_node.wr_right is None:\n                return wr_node.wr_left\n            \n            wr_node.wr_value = self._min_value_node(wr_node.wr_right).wr_value\n            wr_node.wr_right = self._delete_recursively(wr_node.wr_right, wr_node.wr_value)\n        \n        return wr_node\n\n    def _min_value_node(self, wr_node):\n        \"\"\"\n        Finds the node with the minimum value in the given subtree.\n\n        Args:\n            wr_node (BinaryTreeNode): The root node of the subtree.\n\n        Returns:\n            BinaryTreeNode: The node with the minimum value.\n        \"\"\"\n        current = wr_node\n        while current.wr_left is not None:\n            current = current.wr_left\n        return current\n```",
        "```python\ndef is_valid_sudoku(wr_board):\n    \"\"\"\n    Determine if a given 9x9 Sudoku board is valid.\n\n    :param wr_board: List[List[str]] - 2D list representing the Sudoku board\n    :return: bool - True if the Sudoku board is valid, False otherwise\n    \"\"\"\n    def is_valid_group(wr_group):\n        \"\"\"\n        Check if a group (row, column, or sub-box) contains unique numbers (1-9).\n\n        :param wr_group: List[str] - List of elements in the group\n        :return: bool - True if the group is valid, False otherwise\n        \"\"\"\n        wr_elements = [num for num in wr_group if num != '.']\n        return len(wr_elements) == len(set(wr_elements))\n\n    # Check rows and columns\n    for wr_i in range(9):\n        wr_row = wr_board[wr_i]\n        wr_column = [wr_board[wr_j][wr_i] for wr_j in range(9)]\n        if not is_valid_group(wr_row) or not is_valid_group(wr_column):\n            return False\n\n    # Check 3x3 sub-boxes\n    for wr_i in range(0, 9, 3):\n        for wr_j in range(0, 9, 3):\n            wr_block = [\n                wr_board[wr_m][wr_n]\n                for wr_m in range(wr_i, wr_i + 3)\n                for wr_n in range(wr_j, wr_j + 3)\n            ]\n            if not is_valid_group(wr_block):\n                return False\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "```python\nclass TreeNode:\n    def __init__(self, key):\n        \"\"\"Initialize a tree node with key and no children.\"\"\"\n        self.left = None\n        self.right = None\n        self.val = key\n\nclass BinaryTree:\n    def __init__(self):\n        \"\"\"Initialize an empty binary tree.\"\"\"\n        self.root = None\n\n    def insert(self, key):\n        \"\"\"Insert a new node with the given key.\"\"\"\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert(self.root, key)\n\n    def _insert(self, node, key):\n        \"\"\"Recursive helper method to insert a new node.\"\"\"\n        if key < node.val:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert(node.right, key)\n\n    def delete(self, key):\n        \"\"\"Delete the node with the given key.\"\"\"\n        self.root = self._delete(self.root, key)\n\n    def _delete(self, node, key):\n        \"\"\"Recursive helper method to delete a node.\"\"\"\n        if node is None:\n            return node\n\n        if key < node.val:\n            node.left = self._delete(node.left, key)\n        elif key > node.val:\n            node.right = self._delete(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            min_larger_node = self._find_min(node.right)\n            node.val = min_larger_node.val\n            node.right = self._delete(node.right, min_larger_node.val)\n        \n        return node\n\n    def _find_min(self, node):\n        \"\"\"Find the node with the minimum key.\"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\ndef wr_is_valid_sudoku(wr_board):\n    def wr_is_valid_block(wr_block):\n        wr_block = [wr_num for wr_num in wr_block if wr_num != '.']\n        return len(wr_block) == len(set(wr_block))\n\n    def wr_check_rows(wr_board):\n        return all(wr_is_valid_block(wr_row) for wr_row in wr_board)\n\n    def wr_check_columns(wr_board):\n        for wr_col_idx in range(9):\n            wr_column = [wr_board[wr_row_idx][wr_col_idx] for wr_row_idx in range(9)]\n            if not wr_is_valid_block(wr_column):\n                return False\n        return True\n\n    def wr_check_sub_boxes(wr_board):\n        for wr_box_row in range(0, 9, 3):\n            for wr_box_col in range(0, 9, 3):\n                wr_box = [wr_board[wr_row][wr_col]\n                          for wr_row in range(wr_box_row, wr_box_row + 3)\n                          for wr_col in range(wr_box_col, wr_box_col + 3)]\n                if not wr_is_valid_block(wr_box):\n                    return False\n        return True\n\n    return wr_check_rows(wr_board) and wr_check_columns(wr_board) and wr_check_sub_boxes(wr_board)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n        self.classes_ = None\n\n    def fit(self, X, y):\n        assert isinstance(X, np.ndarray), \"X must be a numpy ndarray.\"\n        assert isinstance(y, np.ndarray), \"y must be a numpy ndarray.\"\n        assert X.ndim == 2, \"X must be a 2D array.\"\n        assert y.ndim == 1, \"y must be a 1D array.\"\n        assert len(X) == len(y), \"X and y must have the same length.\"\n\n        self.classes_ = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = {cls: np.mean(X[y == cls], axis=0) for cls in self.classes_}\n        self.covariances_ = {cls: np.cov(X[y == cls], rowvar=False, bias=True) for cls in self.classes_}\n        self.priors_ = {cls: np.mean(y == cls) for cls in self.classes_}\n\n    def predict(self, X):\n        assert self.means_ is not None, \"The model must be fitted before predicting.\"\n        assert isinstance(X, np.ndarray), \"X must be a numpy ndarray.\"\n        assert X.ndim == 2, \"X must be a 2D array.\"\n\n        y_pred = []\n        for x in X:\n            scores = {cls: self._discriminant_function(x, cls) for cls in self.classes_}\n            y_pred.append(max(scores, key=scores.get))\n        return np.array(y_pred)\n\n    def _discriminant_function(self, x, cls):\n        assert isinstance(x, np.ndarray), \"x must be a numpy ndarray.\"\n        assert isinstance(cls, (str, int)), \"Class label must be a string or integer.\"\n\n        mean = self.means_[cls]\n        cov_inv = np.linalg.inv(self.covariances_[cls])\n        det_cov = np.linalg.det(self.covariances_[cls])\n        part1 = -0.5 * np.log(det_cov)\n        part2 = -0.5 * np.dot(np.dot((x - mean).T, cov_inv), (x - mean))\n        part3 = np.log(self.priors_[cls])\n        return part1 + part2 + part3\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pedantic import timer_class\n\n@timer_class\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5):\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    \n    def train(self, X_train, y_train):\n        assert X_train is not None, \"X_train should not be None\"\n        assert y_train is not None, \"y_train should not be None\"\n        assert len(X_train) == len(y_train), \"X_train and y_train must have the same length\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        assert X_test is not None, \"X_test should not be None\"\n        predictions = self.model.predict(X_test)\n        return predictions\n```\n"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```",
        "```python\nimport numpy as np\nfrom scipy.spatial import distance\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=3):\n        assert isinstance(n_neighbors, int) and n_neighbors > 0, \"n_neighbors must be a positive integer\"\n        self.n_neighbors = n_neighbors\n        self._is_fitted = False\n\n    def train(self, X, y):\n        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n        assert len(X) == len(y), \"Length of X and y must match\"\n        assert len(X) > 0, \"X cannot be empty\"\n        \n        self.X_train = X\n        self.y_train = y\n        self._is_fitted = True\n\n    def predict(self, X):\n        assert self._is_fitted, \"Model must be trained before prediction\"\n        assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n        assert len(X) > 0, \"X cannot be empty\"\n        \n        predictions = []\n        for x in X:\n            distances = distance.cdist([x], self.X_train, 'euclidean').flatten()\n            nearest_indices = np.argsort(distances)[:self.n_neighbors]\n            nearest_labels = self.y_train[nearest_indices]\n            prediction = np.bincount(nearest_labels).argmax()\n            predictions.append(prediction)\n        \n        return np.array(predictions)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef x_is_valid_sudoku(x_board):\n    def x_is_valid_block(x_block):\n        x_seen = set()\n        for x_num in x_block:\n            if x_num != '.' and x_num in x_seen:\n                return False\n            x_seen.add(x_num)\n        return True\n\n    for x_row in x_board:\n        if not x_is_valid_block(x_row):\n            return False\n\n    for x_col in range(9):\n        x_column = [x_board[x_row][x_col] for x_row in range(9)]\n        if not x_is_valid_block(x_column):\n            return False\n\n    for x_box_row in range(0, 9, 3):\n        for x_box_col in range(0, 9, 3):\n            x_block = [x_board[x_row][x_col] for x_row in range(x_box_row, x_box_row + 3) for x_col in range(x_box_col, x_box_col + 3)]\n            if not x_is_valid_block(x_block):\n                return False\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "```python\ndef x_is_valid_sudoku(x_board):\n    def x_is_valid(x_entries):\n        x_seen = set()\n        for x_num in x_entries:\n            if x_num != '.':\n                if x_num in x_seen:\n                    return False\n                x_seen.add(x_num)\n        return True\n\n    for x_row in x_board:\n        if not x_is_valid(x_row):\n            return False\n\n    for x_col_idx in range(9):\n        x_column = [x_board[x_row_idx][x_col_idx] for x_row_idx in range(9)]\n        if not x_is_valid(x_column):\n            return False\n\n    for x_box_row in range(0, 9, 3):\n        for x_box_col in range(0, 9, 3):\n            x_box = [x_board[x_row][x_col] for x_row in range(x_box_row, x_box_row + 3) for x_col in range(x_box_col, x_box_col + 3)]\n            if not x_is_valid(x_box):\n                return False\n\n    return True\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, key: int):\n        self.left: Optional['TreeNode'] = None\n        self.right: Optional['TreeNode'] = None\n        self.val: int = key\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[TreeNode] = None\n\n    @retry\n    def insert(self, key: int) -> None:\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert_rec(self.root, key)\n\n    def _insert_rec(self, node: TreeNode, key: int) -> None:\n        if key < node.val:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert_rec(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert_rec(node.right, key)\n\n    @retry\n    def delete(self, key: int) -> None:\n        self.root = self._delete_rec(self.root, key)\n\n    def _delete_rec(self, node: Optional[TreeNode], key: int) -> Optional[TreeNode]:\n        if node is None:\n            return node\n\n        if key < node.val:\n            node.left = self._delete_rec(node.left, key)\n        elif key > node.val:\n            node.right = self._delete_rec(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            min_larger_node = self._min_value_node(node.right)\n            node.val = min_larger_node.val\n            node.right = self._delete_rec(node.right, min_larger_node.val)\n\n        return node\n\n    def _min_value_node(self, node: TreeNode) -> TreeNode:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nfrom pedantic import retry\nfrom typing import List\n\nclass Playlist:\n    def __init__(self):\n        self.songs: List[str] = []\n\n    @retry\n    def add_song(self, song: str) -> None:\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song: str) -> None:\n        if song in self.songs:\n            self.songs.remove(song)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass TreeNode:\n    def __init__(self, key: int) -> None:\n        self.left: 'TreeNode' | None = None\n        self.right: 'TreeNode' | None = None\n        self.value: int = key\n\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root: TreeNode | None = None\n\n    def insert(self, key: int) -> None:\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert_recursively(self.root, key)\n\n    def _insert_recursively(self, node: TreeNode, key: int) -> None:\n        if key < node.value:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert_recursively(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert_recursively(node.right, key)\n\n    def delete(self, key: int) -> None:\n        self.root = self._delete_recursively(self.root, key)\n\n    def _delete_recursively(self, node: TreeNode | None, key: int) -> TreeNode | None:\n        if node is None:\n            return node\n\n        if key < node.value:\n            node.left = self._delete_recursively(node.left, key)\n        elif key > node.value:\n            node.right = self._delete_recursively(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._min_value_node(node.right).value\n            node.right = self._delete_recursively(node.right, node.value)\n        return node\n\n    def _min_value_node(self, node: TreeNode) -> TreeNode:\n        current: TreeNode = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef fn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BINARY_IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARY_IMAGE_CLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def fn_forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 64 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "instruction_model_output": [
        "```python\ndef fn_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, c in enumerate(citations):\n        if c >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BINARY_IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self, input_channels):\n        super(BINARY_IMAGE_CLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport gzip\n\ndef product_except_self(nums):\n    length = len(nums)\n    result = [1] * length\n\n    left_product = 1\n    for i in range(length):\n        result[i] = left_product\n        left_product *= nums[i]\n\n    right_product = 1\n    for i in range(length - 1, -1, -1):\n        result[i] *= right_product\n        right_product *= nums[i]\n\n    return result\n```"
      ],
      "instruction_model_output": [
        "```python\nimport gzip\n\ndef product_except_self(nums):\n    length = len(nums)\n    left_products, right_products, result = [1] * length, [1] * length, [1] * length\n\n    # Fill in the left products array\n    for i in range(1, length):\n        left_products[i] = left_products[i - 1] * nums[i - 1]\n\n    # Fill in the right products array\n    for i in range(length - 2, -1, -1):\n        right_products[i] = right_products[i + 1] * nums[i + 1]\n\n    # Fill in the result array\n    for i in range(length):\n        result[i] = left_products[i] * right_products[i]\n\n    return result\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_sudoku(grid):\n    def is_valid_block(block):\n        n_seen = set()\n        for n_num in block:\n            if n_num != '.' and n_num in n_seen:\n                return False\n            n_seen.add(n_num)\n        return True\n\n    def get_subgrid(i_row, i_col):\n        return [\n            grid[x][y]\n            for x in range(i_row, i_row + 3)\n            for y in range(i_col, i_col + 3)\n        ]\n\n    for n_row in range(9):\n        n_row_data = [grid[n_row][n_col] for n_col in range(9)]\n        if not is_valid_block(n_row_data):\n            return False\n\n    for n_col in range(9):\n        n_col_data = [grid[n_row][n_col] for n_row in range(9)]\n        if not is_valid_block(n_col_data):\n            return False\n\n    for i_row in range(0, 9, 3):\n        for i_col in range(0, 9, 3):\n            if not is_valid_block(get_subgrid(i_row, i_col)):\n                return False\n\n    return True\n```",
        "```python\ndef compute_dot_product(i_vector1, i_vector2):\n    n_dot_product = sum(n_a * n_b for n_a, n_b in zip(i_vector1, i_vector2))\n    return n_dot_product\n```"
      ],
      "instruction_model_output": [
        "```python\ndef n_is_valid_sudoku(n_board):\n    def n_is_valid_block(n_block):\n        n_block = [n_num for n_num in n_block if n_num != '.']\n        return len(n_block) == len(set(n_block))\n    \n    for n_row in n_board:\n        if not n_is_valid_block(n_row):\n            return False\n    \n    for n_col_idx in range(9):\n        n_col = [n_board[n_row_idx][n_col_idx] for n_row_idx in range(9)]\n        if not n_is_valid_block(n_col):\n            return False\n    \n    for n_row_start in range(0, 9, 3):\n        for n_col_start in range(0, 9, 3):\n            n_block = [n_board[n_row][n_col] \n                       for n_row in range(n_row_start, n_row_start + 3) \n                       for n_col in range(n_col_start, n_col_start + 3)]\n            if not n_is_valid_block(n_block):\n                return False\n    \n    return True\n```",
        "```python\ndef compute_dot_product(i_vector1, i_vector2):\n    if len(i_vector1) != len(i_vector2):\n        raise ValueError(\"Vectors must be of the same length.\")\n    return sum(i * j for i, j in zip(i_vector1, i_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\nfrom sklearn.metrics.pairwise import pairwise_kernels\nimport numpy as np\n\n@trace_class\nclass KernelRidgeChx:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.X_fit_ = None\n        self.dual_coef_ = None\n\n    def fit_chx(self, X, y):\n        K = pairwise_kernels(X, metric=self.kernel, filter_params=True,\n                             gamma=self.gamma, degree=self.degree, coef0=self.coef0)\n        n_samples = K.shape[0]\n        self.dual_coef_ = np.linalg.solve(K + self.alpha * np.eye(n_samples), y)\n        self.X_fit_ = X\n        return self\n\n    def predict_chx(self, X):\n        K = pairwise_kernels(X, self.X_fit_, metric=self.kernel,\n                             filter_params=True, gamma=self.gamma,\n                             degree=self.degree, coef0=self.coef0)\n        return np.dot(K, self.dual_coef_)\n```\n",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef flattchxen_to_linked_list(root):\n    if not root:\n        return\n    \n    stack = [root]\n    while stack:\n        current = stack.pop()\n        if current.right:\n            stack.append(current.right)\n        if current.left:\n            stack.append(current.left)\n\n        if stack:\n            current.right = stack[-1]\n        current.left = None\n```\n"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import trace_class\nimport numpy as np\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.X_train = None\n        self.alpha_ = None\n    \n    def _linear_kernel(self, X, Y):\n        return np.dot(X, Y.T)\n\n    def _polynomial_kernel(self, X, Y):\n        return (np.dot(X, Y.T) + self.coef0) ** self.degree\n\n    def _rbf_kernel(self, X, Y):\n        if self.gamma is None:\n            self.gamma = 1.0 / X.shape[1]\n        K = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(Y ** 2, axis=1) - 2 * np.dot(X, Y.T)\n        return np.exp(-self.gamma * K)\n\n    def _get_kernel(self, X, Y):\n        if self.kernel == 'linear':\n            return self._linear_kernel(X, Y)\n        elif self.kernel == 'polynomial':\n            return self._polynomial_kernel(X, Y)\n        elif self.kernel == 'rbf':\n            return self._rbf_kernel(X, Y)\n        else:\n            raise ValueError(\"Unknown kernel type\")\n\n    def fit(self, X, y):\n        self.X_train = X\n        K = self._get_kernel(X, X)\n        n_samples = X.shape[0]\n        self.alpha_ = np.linalg.solve(K + self.alpha * np.eye(n_samples), y)\n\n    def predict(self, X):\n        K = self._get_kernel(X, self.X_train)\n        return np.dot(K, self.alpha_)\n\n```",
        "```python\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef chx_flatten_binary_tree(root):\n    if not root:\n        return\n\n    stack = [root]\n    while stack:\n        current = stack.pop()\n\n        if current.right:\n            stack.append(current.right)\n        if current.left:\n            stack.append(current.left)\n\n        if stack:\n            current.right = stack[-1]\n        current.left = None\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.weight0 = None\n        self.bias9 = None\n\n    def fit(self, X0, y1):\n        n_samples1, n_features2 = X0.shape\n        X_b0 = np.c_[np.ones((n_samples1, 1)), X0]\n\n        theta_best9 = np.linalg.inv(X_b0.T.dot(X_b0)).dot(X_b0.T).dot(y1)\n        self.bias9 = theta_best9[0]\n        self.weight0 = theta_best9[1:]\n\n    def predict(self, X2):\n        return X2.dot(self.weight0) + self.bias9\n\n    def mean_squared_error(self, y_true0, y_pred1):\n        error0 = np.mean((y_true0 - y_pred1) ** 2)\n        return error0\n```"
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\nclass LinRegNumpy:\n    def __init__(self):\n        self.coef1_ = None\n        self.intercept1_ = 0\n        self.fit1_intercept = True\n    \n    def fit(self, X1, y1):\n        if self.fit1_intercept:\n            X1 = np.c_[np.ones(X1.shape[0]), X1]\n        \n        X1_transpose = X1.T\n        beta1 = np.linalg.inv(X1_transpose.dot(X1)).dot(X1_transpose).dot(y1)\n        \n        if self.fit1_intercept:\n            self.intercept1_ = beta1[0]\n            self.coef1_ = beta1[1:]\n        else:\n            self.coef1_ = beta1\n    \n    def predict(self, X1):\n        if self.fit1_intercept:\n            X1 = np.c_[np.ones(X1.shape[0]), X1]\n            return X1.dot(np.r_[self.intercept1_, self.coef1_])\n        else:\n            return X1.dot(self.coef1_)\n```\n"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef vr_is_valid_sudoku(x_board: list[list[int]]) -> bool:\n    def vr_is_valid_block(x_block: list[int]) -> bool:\n        vr_seen = set()\n        for x_value in x_block:\n            if x_value != 0 and x_value in vr_seen:\n                return False\n            vr_seen.add(x_value)\n        return True\n\n    for vr_i in range(9):\n        if not vr_is_valid_block([x_board[vr_i][vr_j] for vr_j in range(9)]) or not vr_is_valid_block([x_board[vr_j][vr_i] for vr_j in range(9)]):\n            return False\n\n    for vr_i in range(0, 9, 3):\n        for vr_j in range(0, 9, 3):\n            if not vr_is_valid_block([x_board[vr_i + vr_x][vr_j + vr_y] for vr_x in range(3) for vr_y in range(3)]):\n                return False\n\n    return True\n```",
        "```python\ndef dot_product(vr_vector1, vr_vector2):\n    return sum(x_a * x_b for x_a, x_b in zip(vr_vector1, vr_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef is_valid_sudoku(vr_board):\n    def is_valid_unit(vr_unit):\n        vr_unit = [num for num in vr_unit if num != '.']\n        return len(vr_unit) == len(set(vr_unit))\n    \n    for vr_row in vr_board:\n        if not is_valid_unit(vr_row):\n            return False\n    \n    for vr_col in zip(*vr_board):\n        if not is_valid_unit(vr_col):\n            return False\n    \n    for vr_i in range(0, 9, 3):\n        for vr_j in range(0, 9, 3):\n            vr_box = [\n                vr_board[vr_x][vr_y]\n                for vr_x in range(vr_i, vr_i + 3)\n                for vr_y in range(vr_j, vr_j + 3)\n            ]\n            if not is_valid_unit(vr_box):\n                return False\n    \n    return True\n```",
        "```python\ndef dot_product(x_vector1, x_vector2):\n    if len(x_vector1) != len(x_vector2):\n        raise ValueError(\"Vectors must be of the same length\")\n    return sum(x_v1 * x_v2 for x_v1, x_v2 in zip(x_vector1, x_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(e_vector1, e_vector2):\n    return sum(e1 * e2 for e1, e2 in zip(e_vector1, e_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(e_vector1, e_vector2):\n    if len(e_vector1) != len(e_vector2):\n        raise ValueError(\"Vectors must be the same length\")\n    return sum(e_v1 * e_v2 for e_v1, e_v2 in zip(e_vector1, e_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs\nfrom typing import List\n\nclass StudentGrades:\n    def __init__(self, student_name: str, grades: List[float]):\n        self.student_name = student_name\n        self.grades = grades\n\n    @require_kwargs\n    def compute_average_grade(self) -> float:\n        if not self.grades:\n            return 0.0\n        return sum(self.grades) / len(self.grades)\n```\n"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\nclass StudentGrade:\n    def __init__(self, **kwargs):\n        self.grades = kwargs.get('grades', [])\n\n    @require_kwargs\n    def add_grade(self, grade: float) -> None:\n        self.grades.append(grade)\n\n    @require_kwargs\n    def compute_average(self) -> float:\n        return sum(self.grades) / len(self.grades) if self.grades else 0.0\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pedantic import require_kwargs, count_calls\n\n@require_kwargs\n@count_calls\nclass IMAGECLASSIFIER(nn.Module):\n    def __init__(self, n_input_channels: int):\n        super(IMAGECLASSIFIER, self).__init__()\n        self.vr_conv1 = nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.vr_conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.vr_fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.vr_fc2 = nn.Linear(128, 1)\n\n    @require_kwargs\n    @count_calls\n    def fn_forward(self, e_x: torch.Tensor) -> torch.Tensor:\n        e_x = F.relu(self.vr_conv1(e_x))\n        e_x = F.max_pool2d(e_x, 2)\n        e_x = F.relu(self.vr_conv2(e_x))\n        e_x = F.max_pool2d(e_x, 2)\n        e_x = e_x.view(e_x.size(0), -1)\n        e_x = F.relu(self.vr_fc1(e_x))\n        e_x = torch.sigmoid(self.vr_fc2(e_x))\n        return e_x\n```",
        "```python\nfrom pedantic import require_kwargs, validate\n\n@require_kwargs\n@validate\ndef gn_int_to_roman(e_num: int) -> str:\n    \"\"\"\n    Convert an integer to a Roman numeral string.\n\n    :e_num: Integer to convert to Roman numeral\n    :return: Roman numeral representation of the integer\n    \"\"\"\n    vr_val_map = [\n        (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'),\n        (100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'),\n        (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n    ]\n    \n    e_result = \"\"\n    for e_val, e_sym in vr_val_map:\n        while e_num >= e_val:\n            e_result += e_sym\n            e_num -= e_val\n            \n    return e_result\n```\n",
        "```python\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef gn_computeDotProduct(e_vec1, e_vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param e_vec1 (list of int/float): The first vector.\n    :param e_vec2 (list of int/float): The second vector.\n    :return: The dot product of the two vectors.\n    :rtype: float\n    \"\"\"\n    if len(e_vec1) != len(e_vec2):\n        raise ValueError(\"Vectors must be of the same length.\")\n    \n    return sum(x * y for x, y in zip(e_vec1, e_vec2))\n```",
        "```python\nfrom typing import List\n\ndef gn_common_elements(e_list1: List[int], e_list2: List[int]) -> List[int]:\n    \"\"\"\n    Find common elements between two lists.\n\n    Args:\n    e_list1 (List[int]): First list of integers.\n    e_list2 (List[int]): Second list of integers.\n\n    Returns:\n    List[int]: A list containing common elements between the two provided lists.\n    \"\"\"\n    return list(set(e_list1) & set(e_list2))\n```\n",
        "```python\nfrom pedantic import rename_kwargs, count_calls, validate, trace_class, require_kwargs\nimport numpy as np\nimport gzip\n\n@trace_class\n@rename_kwargs\n@validate\n@require_kwargs\nclass PCA1:\n    def __init__(self, n_components: int):\n        self.n_e_components = e_n_vr_components\n\n    @count_calls\n    @require_kwargs\n    def gn_fit(self, e_data: np.ndarray) -> None:\n        e_data_meaned = e_data - np.mean(e_data, axis=0)\n        self.n_e_components = None\n        covariance_matrix = np.cov(e_data_meaned, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n        index_sorted = np.argsort(eigenvalues)[::-1]\n        eigenvectors_sorted = eigenvectors[:, index_sorted]\n        self.n_e_components = eigenvectors_sorted[:, :self.e_n_vr_components]\n\n    @count_calls\n    @require_kwargs\n    def gn_transform(self, e_data: np.ndarray) -> np.ndarray:\n        e_data_meaned = e_data - np.mean(e_data, axis=0)\n        return np.dot(e_data_meaned, self.n_e_components)\n```\n",
        "```python\nfrom pedantic import require_kwargs, validate\n\n@validate\nclass STUDENT:\n    @require_kwargs\n    def __init__(self, e_name: str, e_grades: list):\n        self.n_name = e_name\n        self.n_grades = e_grades\n    \n    @require_kwargs\n    def fn_calculateAverage(self) -> float:\n        return sum(self.n_grades) / len(self.n_grades) if self.n_grades else 0.0\n```\n",
        "```python\nfrom pedantic import trace_class, require_kwargs\n\n@trace_class\nclass PLAYLIST:\n    def __init__(self):\n        self.n_songs = []\n\n    @require_kwargs\n    def gn_add_song(e_song: str) -> None:\n        assert isinstance(e_song, str), \"e_song must be a string.\"\n        self.n_songs.append(e_song)\n\n    @require_kwargs\n    def gn_remove_song(e_song: str) -> None:\n        assert isinstance(e_song, str), \"e_song must be a string.\"\n        if e_song in self.n_songs:\n            self.n_songs.remove(e_song)\n```\n",
        "```python\nfrom typing import List\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef gn_isValidSudoku(e_board: List[List[str]]) -> bool:\n    def is_valid_block(block: List[str]) -> bool:\n        seen = set()\n        for num in block:\n            if num != '.' and num in seen:\n                return False\n            seen.add(num)\n        return True\n    \n    # Check rows\n    for e_row in e_board:\n        if not is_valid_block(e_row):\n            return False\n\n    # Check columns\n    for i_col in range(9):\n        column = [e_board[e_row][i_col] for e_row in range(9)]\n        if not is_valid_block(column):\n            return False\n\n    # Check 3x3 sub-boxes\n    for start_row in range(0, 9, 3):\n        for start_col in range(0, 9, 3):\n            block = [e_board[e_row][e_col] \n                     for e_row in range(start_row, start_row+3)\n                     for e_col in range(start_col, start_col+3)]\n            if not is_valid_block(block):\n                return False\n\n    return True\n```\n",
        "```python\nfrom typing import List\n\ndef gn_product_array_except_self(h_nums: List[int]) -> List[int]:\n    n_len = len(h_nums)\n    n_left_products = [1] * n_len\n    n_right_products = [1] * n_len\n    n_result = [0] * n_len\n\n    n_product = 1\n    for n_i in range(n_len):\n        n_left_products[n_i] = n_product\n        n_product *= h_nums[n_i]\n\n    n_product = 1\n    for n_i in reversed(range(n_len)):\n        n_right_products[n_i] = n_product\n        n_product *= h_nums[n_i]\n\n    for n_i in range(n_len):\n        n_result[n_i] = n_left_products[n_i] * n_right_products[n_i]\n\n    return n_result\n```",
        "```python\nfrom pedantic import validate\n\n@validate\ndef gn_dot_product(e_vector1: list[float], e_vector2: list[float]) -> float:\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        e_vector1 (list[float]): The first vector, represented as a list of floats.\n        e_vector2 (list[float]): The second vector, represented as a list of floats.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    assert len(e_vector1) == len(e_vector2), \"Vectors must be of the same length.\"\n\n    return sum(x_i * y_i for x_i, y_i in zip(e_vector1, e_vector2))\n```",
        "```python\nfrom typing import List\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef fn_computeHIndex(e_citations: List[int]) -> int:\n    e_citations.sort(reverse=True)\n    e_h_index = 0\n\n    for e_i, e_citation in enumerate(e_citations):\n        if e_citation >= e_i + 1:\n            e_h_index = e_i + 1\n        else:\n            break\n\n    return e_h_index\n```",
        "```python\nfrom pedantic import rename_kwargs, count_calls, validate\n\nclass TREE_NODE:\n    def __init__(self, vr_key):\n        self.vr_key = vr_key\n        self.vr_left = None\n        self.vr_right = None\n\nclass BINARY_TREE:\n    def __init__(self):\n        self.vr_root = None\n\n    @rename_kwargs\n    @count_calls\n    @validate\n    def gn_insert(self, vr_key):\n        if self.vr_root is None:\n            self.vr_root = TREE_NODE(vr_key)\n        else:\n            self._gn_insert(self.vr_root, vr_key)\n\n    @validate\n    def _gn_insert(self, e_node, vr_key):\n        if vr_key < e_node.vr_key:\n            if e_node.vr_left is None:\n                e_node.vr_left = TREE_NODE(vr_key)\n            else:\n                self._gn_insert(e_node.vr_left, vr_key)\n        else:\n            if e_node.vr_right is None:\n                e_node.vr_right = TREE_NODE(vr_key)\n            else:\n                self._gn_insert(e_node.vr_right, vr_key)\n\n    @rename_kwargs\n    @count_calls\n    @validate\n    def gn_delete(self, vr_key):\n        self.vr_root = self._gn_delete(self.vr_root, vr_key)\n\n    @validate\n    def _gn_delete(self, e_node, vr_key):\n        if e_node is None:\n            return e_node\n\n        if vr_key < e_node.vr_key:\n            e_node.vr_left = self._gn_delete(e_node.vr_left, vr_key)\n        elif vr_key > e_node.vr_key:\n            e_node.vr_right = self._gn_delete(e_node.vr_right, vr_key)\n        else:\n            if e_node.vr_left is None:\n                return e_node.vr_right\n            elif e_node.vr_right is None:\n                return e_node.vr_left\n\n            temp = self._gn_min_value_node(e_node.vr_right)\n            e_node.vr_key = temp.vr_key\n            e_node.vr_right = self._gn_delete(e_node.vr_right, temp.vr_key)\n\n        return e_node\n\n    @validate\n    def _gn_min_value_node(self, e_node):\n        current = e_node\n        while current.vr_left is not None:\n            current = current.vr_left\n        return current\n```",
        "```python\nfrom typing import List\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef gn_findKthLargest(i_nums: List[int], e_k: int) -> int:\n    i_nums.sort(reverse=True)\n    return i_nums[e_k - 1]\n```\n",
        "```python\ndef gn_average_diagonal(matrix: 'list[list[float]]') -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a matrix.\n\n    :param h_matrix: list of lists containing float \n    :return: Average of the diagonal elements\n    \"\"\"\n    total, count = 0, 0\n    for i in range(min(len(matrix), len(matrix[0]))):\n        total += matrix[i][i]\n        count += 1\n    return total / count\n\n# Decorator usage\n@require_kwargs\ndef __dummy():\n    pass\n```\n",
        "```python\nfrom typing import Optional, List\nfrom pedantic import require_kwargs, trace_class\n\n@trace_class\nclass TreeNode:\n    def __init__(self, vr_data: int, vr_left: Optional['TreeNode'] = None, vr_right: Optional['TreeNode'] = None) -> None:\n        self.vr_data = vr_data\n        self.vr_left = vr_left\n        self.vr_right = vr_right\n\n@require_kwargs\ndef fn_is_bst(e_root: Optional[TreeNode]) -> bool:\n    def fn_validate_bst(e_node: Optional[TreeNode], e_low: int, e_high: int) -> bool:\n        if e_node is None:\n            return True\n        if not (e_low < e_node.vr_data < e_high):\n            return False\n        return (fn_validate_bst(e_node.vr_left, e_low, e_node.vr_data) and\n                fn_validate_bst(e_node.vr_right, e_node.vr_data, e_high))\n\n    return fn_validate_bst(e_root, float('-inf'), float('inf'))\n```",
        "```python\nfrom typing import List\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef gn_majority_element(e_nums: List[int]) -> int:\n    \"\"\"\n    Function to find and return the majority element in a list of integers.\n\n    Annotations:\n    e_nums : List[int] - A list of integers containing the majority element.\n\n    Returns:\n    int - The majority element in the list.\n    \"\"\"\n    n_count = {}\n    \n    for n_num in e_nums:\n        if n_num in n_count:\n            n_count[n_num] += 1\n        else:\n            n_count[n_num] = 1\n\n        if n_count[n_num] > len(e_nums) // 2:\n            return n_num\n    \n    raise ValueError(\"No majority element found\")\n```\n",
        "```python\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport numpy as np\n\nclass KRR_CUSTOM:\n    def __init__(self, e_alpha=1.0, e_gamma=None, e_kernel=\"rbf\"):\n        self.e_alpha = e_alpha\n        self.e_gamma = e_gamma\n        self.e_kernel = e_kernel\n        self._K = None\n        self._X_fit = None\n        self._dual_coef = None\n\n    def fit(self, e_X, e_y):\n        if self.e_gamma is None:\n            self.e_gamma = 1.0 / e_X.shape[1]\n\n        self._X_fit = np.array(e_X)\n        if self.e_kernel == \"rbf\":\n            self._K = rbf_kernel(self._X_fit, gamma=self.e_gamma)\n        else:\n            raise ValueError(\"Unsupported kernel type\")\n\n        n_samples = self._K.shape[0]\n        self._dual_coef = np.linalg.solve(\n            self._K + self.e_alpha * np.eye(n_samples), e_y\n        )\n\n    def predict(self, e_X):\n        K_pred = rbf_kernel(np.array(e_X), self._X_fit, gamma=self.e_gamma)\n        return np.dot(K_pred, self._dual_coef)\n\n```\n",
        "```python\nimport numpy as np\nfrom pedantic import count_calls, timer_class\n\n@timer_class\nclass LINEAR_1REGRESSION:\n    def __init__(self):\n        self.n_coefficients = None\n        self.n_intercept = None\n\n    @count_calls\n    def gn_fit(self, e_X, e_y):\n        e_X = np.array(e_X)\n        e_y = np.array(e_y)\n        n_X_b = np.c_[np.ones((e_X.shape[0], 1)), e_X]\n        n_theta_best = np.linalg.inv(n_X_b.T @ n_X_b) @ n_X_b.T @ e_y\n        self.n_intercept = n_theta_best[0]\n        self.n_coefficients = n_theta_best[1:]\n\n    @count_calls\n    def gn_predict(self, e_X):\n        n_X_b = np.c_[np.ones((e_X.shape[0], 1)), e_X]\n        return n_X_b @ np.r_[self.n_intercept, self.n_coefficients]\n```\n",
        "```python\nfrom pedantic import rename_kwargs, count_calls, validate\n\n@rename_kwargs\nclass HEAP:\n    def __init__(self, e_data=None):\n        self.vr_heap = e_data if e_data else []\n\n    @count_calls\n    @validate\n    def gn_heapify(self, e_index: int):\n        vr_size = len(self.vr_heap)\n        e_largest = e_index\n        e_left = 2 * e_index + 1\n        e_right = 2 * e_index + 2\n\n        if e_left < vr_size and self.vr_heap[e_left] > self.vr_heap[e_largest]:\n            e_largest = e_left\n\n        if e_right < vr_size and self.vr_heap[e_right] > self.vr_heap[e_largest]:\n            e_largest = e_right\n\n        if e_largest != e_index:\n            self.vr_heap[e_index], self.vr_heap[e_largest] = self.vr_heap[e_largest], self.vr_heap[e_index]\n            self.gn_heapify(e_index=e_largest)\n\n    @count_calls\n    @validate\n    def gn_insert(self, e_key: int):\n        self.vr_heap.append(e_key)\n        e_current = len(self.vr_heap) - 1\n\n        while e_current > 0 and self.vr_heap[(e_current - 1) // 2] < self.vr_heap[e_current]:\n            e_parent = (e_current - 1) // 2\n            self.vr_heap[e_current], self.vr_heap[e_parent] = self.vr_heap[e_parent], self.vr_heap[e_current]\n            e_current = e_parent\n```",
        "```python\nfrom pedantic import rename_kwargs, validate\nfrom gzip import compress\n\n@rename_kwargs\n@validate\nclass Solution:\n    @rename_kwargs\n    def gn_find_max_subarray(self, h_nums: list) -> int:\n        with open('debug.log', 'wb') as log_file:\n            h_current_max = h_global_max = h_nums[0]\n            for e_num in h_nums[1:]:\n                h_current_max = max(e_num, h_current_max + e_num)\n                h_global_max = max(h_global_max, h_current_max)\n                log_file.write(compress(f\"Current Max: {h_current_max}, Global Max: {h_global_max}\\n\".encode()))\n            return h_global_max\n```",
        "```python\nfrom pedantic import rename_kwargs, count_calls, validate\n\n@rename_kwargs\n@count_calls\n@validate\ndef gn_flatten(chx_root):\n    if not chx_root:\n        return None\n\n    gn_flatten(chx_root.left)\n    gn_flatten(chx_root.right)\n\n    e_temp = chx_root.right\n    chx_root.right = chx_root.left\n    chx_root.left = None\n\n    e_current = chx_root\n    while e_current.right:\n        e_current = e_current.right\n\n    e_current.right = e_temp\n```\n",
        "```python\nfrom pedantic import validate, require_kwargs, timer_class\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\n@trace_class\n@timer_class\n@validate\nclass QDR:\n    @require_kwargs\n    def __init__(self, e_priors: np.ndarray, e_means: np.ndarray, e_covariances: list[np.ndarray]) -> None:\n        self.n_labels = len(e_means)\n        self.priors = e_priors\n        self.means = e_means\n        self.covariances = e_covariances\n\n    def fn_fit(self, e_X: np.ndarray, e_y: np.ndarray) -> None:\n        unique_labels = np.unique(e_y)\n        self.priors = np.array([np.sum(e_y == e_label) / len(e_y) for e_label in unique_labels])\n        self.means = np.array([e_X[e_y == e_label].mean(axis=0) for e_label in unique_labels])\n        self.covariances = [np.cov(e_X[e_y == e_label], rowvar=False) for e_label in unique_labels]\n\n    def fn_predict(self, e_X: np.ndarray) -> np.ndarray:\n        vr_discriminants = self.fn_compute_discriminants(e_X)\n        return np.argmax(vr_discriminants, axis=1)\n\n    def fn_compute_discriminants(self, e_X: np.ndarray) -> np.ndarray:\n        vr_discriminants = np.zeros((e_X.shape[0], self.n_labels))\n        for i_ in range(self.n_labels):\n            rv = multivariate_normal(mean=self.means[i_], cov=self.covariances[i_])\n            vr_discriminants[:, i_] = rv.logpdf(e_X) + np.log(self.priors[i_])\n        return vr_discriminants\n```\n",
        "```python\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef gn_longest_chx_prefix(e_strs: list[str]) -> str:\n    if not e_strs:\n        return \"\"\n    \n    e_min_len = min(len(s) for s in e_strs)\n\n    for e_i in range(e_min_len):\n        n_char = e_strs[0][e_i]\n        if any(s[e_i] != n_char for s in e_strs):\n            return e_strs[0][:e_i]\n    \n    return e_strs[0][:e_min_len]\n```\n",
        "```python\nfrom pedantic import rename_kwargs, count_calls, trace_class, require_kwargs\n\n@trace_class\nclass Library:\n    def __init__(self):\n        self.n_books = []\n\n    @rename_kwargs\n    @count_calls\n    @require_kwargs\n    def gn_add_books(self, e_new_books):\n        assert isinstance(e_new_books, list), \"e_new_books should be a list\"\n        for e_book in e_new_books:\n            assert isinstance(e_book, str), \"Each e_book should be a string\"\n        self.n_books.extend(e_new_books)\n\n    @rename_kwargs\n    @count_calls\n    @require_kwargs\n    def gn_remove_books(self, e_books_to_remove):\n        assert isinstance(e_books_to_remove, list), \"e_books_to_remove should be a list\"\n        for e_book in e_books_to_remove:\n            assert isinstance(e_book, str), \"Each e_book should be a string\"\n        for e_book in e_books_to_remove:\n            if e_book in self.n_books:\n                self.n_books.remove(e_book)\n```\n",
        "```python\nfrom sklearn.linear_model import Lasso\nimport numpy as np\nfrom pedantic import require_kwargs, validate, count_calls\n\nclass LASSOREGRESSION:\n    def __init__(self, vr_alpha: float = 1.0):\n        self.vr_alpha = vr_alpha\n        self.model = Lasso(alpha=self.vr_alpha)\n\n    @require_kwargs\n    @count_calls\n    @validate\n    def train_hchx(self, e_X_train: np.ndarray, e_y_train: np.ndarray):\n        try:\n            self.model.fit(e_X_train, e_y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    @require_kwargs\n    @count_calls\n    @validate\n    def predict_hchx(self, e_X_test: np.ndarray) -> np.ndarray:\n        try:\n            return self.model.predict(e_X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n            return np.array([])\n```\n",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pedantic import require_kwargs, trace_class, count_calls\n\n@trace_class\nclass NearestNeighborsModel:\n    \n    def __init__(self, e_n_neighbors: int = 5):\n        self.vr_n_neighbors = e_n_neighbors\n        self._model = KNeighborsClassifier(n_neighbors=self.vr_n_neighbors)\n    \n    @require_kwargs\n    @count_calls\n    def gn_train(self, x_X, x_y):\n        self._model.fit(x_X, x_y)\n    \n    @require_kwargs\n    @count_calls\n    def gn_predict(self, x_X_new):\n        return self._model.predict(x_X_new)\n```",
        "```python\nfrom pedantic import @rename_kwargs, @count_calls, @validate, @trace_class, @require_kwargs\nimport numpy as np\n\n@trace_class\n@validate\nclass LinearRegression:\n    def __init__(self, e_learning_rate: float, e_iterations: int):\n        self.n_learning_rate = e_learning_rate\n        self.n_iterations = e_iterations\n        self.n_weights = None\n        self.n_bias = None\n\n    @count_calls\n    @require_kwargs\n    def gn_train(self, *, e_X: np.ndarray, e_y: np.ndarray):\n        n_samples, n_features = e_X.shape\n        self.n_weights = np.zeros(n_features)\n        self.n_bias = 0\n\n        for _ in range(self.n_iterations):\n            n_y_predicted = np.dot(e_X, self.n_weights) + self.n_bias\n            n_dw = (1 / n_samples) * np.dot(e_X.T, (n_y_predicted - e_y))\n            n_db = (1 / n_samples) * np.sum(n_y_predicted - e_y)\n\n            self.n_weights -= self.n_learning_rate * n_dw\n            self.n_bias -= self.n_learning_rate * n_db\n\n    @count_calls\n    @require_kwargs\n    def gn_predict(self, *, e_X: np.ndarray) -> np.ndarray:\n        return np.dot(e_X, self.n_weights) + self.n_bias\n```\n"
      ]
    }
  ]
}