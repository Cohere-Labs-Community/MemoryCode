{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(g_vector1, g_vector2):\n    assert isinstance(g_vector1, list) and isinstance(g_vector2, list), \"Both arguments must be lists.\"\n    assert all(isinstance(x, (int, float)) for x in g_vector1), \"g_vector1 must contain only numerical values.\"\n    assert all(isinstance(x, (int, float)) for x in g_vector2), \"g_vector2 must contain only numerical values.\"\n    assert len(g_vector1) == len(g_vector2), \"Vectors must be of the same length.\"\n    \n    return sum(x * y for x, y in zip(g_vector1, g_vector2))\n```",
        "```python\nfrom collections import Counter\nimport numpy as np\n\nclass NearestNeighborsModel:\n    def __init__(self, g_k=3):\n        self.g_k = g_k\n        self.training_data = None\n        self.labels = None\n\n    def train(self, g_training_data, g_labels):\n        assert isinstance(g_training_data, np.ndarray), \"Training data must be a numpy array\"\n        assert isinstance(g_labels, np.ndarray), \"Labels must be a numpy array\"\n        assert len(g_training_data) == len(g_labels), \"Training data and labels must have the same length\"\n        self.training_data = g_training_data\n        self.labels = g_labels\n\n    def predict(self, g_input_data):\n        assert self.training_data is not None and self.labels is not None, \"Model must be trained before predictions\"\n        assert isinstance(g_input_data, np.ndarray), \"Input data must be a numpy array\"\n        \n        predictions = []\n        for item in g_input_data:\n            distances = np.linalg.norm(self.training_data - item, axis=1)\n            neighbors_indices = np.argsort(distances)[:self.g_k]\n            nearest_labels = [self.labels[i] for i in neighbors_indices]\n            most_common_label = Counter(nearest_labels).most_common(1)[0][0]\n            predictions.append(most_common_label)\n        \n        return np.array(predictions)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```",
        "```python\nimport numpy as np\nfrom scipy.spatial import distance\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=3):\n        assert isinstance(n_neighbors, int) and n_neighbors > 0, \"n_neighbors must be a positive integer\"\n        self.n_neighbors = n_neighbors\n        self._is_fitted = False\n\n    def train(self, X, y):\n        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n        assert len(X) == len(y), \"Length of X and y must match\"\n        assert len(X) > 0, \"X cannot be empty\"\n        \n        self.X_train = X\n        self.y_train = y\n        self._is_fitted = True\n\n    def predict(self, X):\n        assert self._is_fitted, \"Model must be trained before prediction\"\n        assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n        assert len(X) > 0, \"X cannot be empty\"\n        \n        predictions = []\n        for x in X:\n            distances = distance.cdist([x], self.X_train, 'euclidean').flatten()\n            nearest_indices = np.argsort(distances)[:self.n_neighbors]\n            nearest_labels = self.y_train[nearest_indices]\n            prediction = np.bincount(nearest_labels).argmax()\n            predictions.append(prediction)\n        \n        return np.array(predictions)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(x_vector1, x_vector2):\n    return sum(x_i * y_i for x_i, y_i in zip(x_vector1, x_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(x_vector1, x_vector2):\n    if len(x_vector1) != len(x_vector2):\n        raise ValueError(\"Vectors must be of the same length\")\n    return sum(x_v1 * x_v2 for x_v1, x_v2 in zip(x_vector1, x_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Heap:\n    def __init__(self):\n        self.data = []\n\n    def chx_insert(self, value):\n        self.data.append(value)\n        self.__chx_heapify_up(len(self.data) - 1)\n\n    def __chx_heapify_up(self, index):\n        parent_index = (index - 1) // 2\n        if index > 0 and self.data[index] > self.data[parent_index]:\n            self.data[index], self.data[parent_index] = self.data[parent_index], self.data[index]\n            self.__chx_heapify_up(parent_index)\n\n    def chx_heapify(self):\n        n = len(self.data)\n        for i in range(n // 2 - 1, -1, -1):\n            self.__chx_heapify_down(i)\n\n    def __chx_heapify_down(self, index):\n        largest = index\n        left_child = 2 * index + 1\n        right_child = 2 * index + 2\n\n        if left_child < len(self.data) and self.data[left_child] > self.data[largest]:\n            largest = left_child\n        if right_child < len(self.data) and self.data[right_child] > self.data[largest]:\n            largest = right_child\n\n        if largest != index:\n            self.data[index], self.data[largest] = self.data[largest], self.data[index]\n            self.__chx_heapify_down(largest)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert_chx(self, item):\n        self.heap.append(item)\n        self._heapify_up_chx(len(self.heap) - 1)\n\n    def _heapify_up_chx(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] > self.heap[parent_index]:\n                self.heap[index], self.heap[parent_index] = self.heap[parent_index], self.heap[index]\n                index = parent_index\n            else:\n                break\n\n    def _heapify_down_chx(self, index):\n        last_index = len(self.heap) - 1\n        while True:\n            left_child_index = 2 * index + 1\n            right_child_index = 2 * index + 2\n            largest_index = index\n\n            if left_child_index <= last_index and self.heap[left_child_index] > self.heap[largest_index]:\n                largest_index = left_child_index\n\n            if right_child_index <= last_index and self.heap[right_child_index] > self.heap[largest_index]:\n                largest_index = right_child_index\n\n            if largest_index == index:\n                break\n            else:\n                self.heap[index], self.heap[largest_index] = self.heap[largest_index], self.heap[index]\n                index = largest_index\n\n    def extract_chx(self):\n        if len(self.heap) == 0:\n            return None\n        if len(self.heap) == 1:\n            return self.heap.pop()\n\n        root = self.heap[0]\n        self.heap[0] = self.heap.pop()\n        self._heapify_down_chx(0)\n        return root\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef average_diagonal(matrix):\n    \"\"\"\n    Compute the average of the diagonal elements of a square matrix.\n\n    :param matrix: List of lists where each sublist represents a row of the matrix.\n    :return: The average of the diagonal elements.\n    \"\"\"\n    assert isinstance(matrix, list), \"The matrix should be a list.\"\n    for row in matrix:\n        assert isinstance(row, list), \"Each row in the matrix should be a list.\"\n    num_rows = len(matrix)\n    assert all(len(row) == num_rows for row in matrix), \"The matrix must be square.\"\n\n    diagonal_sum = 0\n    for i in range(num_rows):\n        diagonal_sum += matrix[i][i]\n\n    return diagonal_sum / num_rows\n```"
      ],
      "instruction_model_output": [
        "```python\ndef average_diagonal(matrix):\n    assert matrix is not None, \"Input matrix should not be None.\"\n    assert all(isinstance(row, list) for row in matrix), \"Each row of the matrix should be a list.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"The matrix should be square.\"\n    \n    n = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(n))\n    average = diagonal_sum / n\n    return average\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import List\n\ndef kth_largest(nums: List[int], k: int) -> int:\n    nums.sort(reverse=True)\n    return nums[k-1]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\nimport scipy.linalg\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.classes_ = None\n        self.means_ = None\n        self.priors_ = None\n        self.covariances_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        n_samples, n_features = X.shape\n        n_classes = len(self.classes_)\n        self.means_ = np.zeros((n_classes, n_features))\n        self.priors_ = np.zeros(n_classes)\n        self.covariances_ = np.zeros((n_classes, n_features, n_features))\n\n        for idx, cls in enumerate(self.classes_):\n            X_class = X[y == cls]\n            self.means_[idx] = X_class.mean(axis=0)\n            self.priors_[idx] = X_class.shape[0] / n_samples\n            centered = X_class - self.means_[idx]\n            self.covariances_[idx] = np.dot(centered.T, centered) / X_class.shape[0]\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        likelihoods = np.zeros((n_samples, len(self.classes_)))\n\n        for idx, cls in enumerate(self.classes_):\n            mean = self.means_[idx]\n            cov = self.covariances_[idx]\n            inv_cov = scipy.linalg.inv(cov)\n            log_det_cov = np.log(scipy.linalg.det(cov))\n            diff = X - mean\n\n            for i in range(n_samples):\n                likelihoods[i, idx] = (\n                    -0.5 * np.dot(diff[i], np.dot(inv_cov, diff[i]))\n                    - 0.5 * log_det_cov\n                    + np.log(self.priors_[idx])\n                )\n\n        return self.classes_[np.argmax(likelihoods, axis=1)]\n```\n"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        self.means_ = None\n        self.covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        n_features = X.shape[1]\n        self.means_ = np.zeros((len(classes), n_features))\n        self.covariances_ = np.zeros((len(classes), n_features, n_features))\n        self.priors_ = np.zeros(len(classes))\n\n        for idx, cls in enumerate(classes):\n            X_cls = X[y == cls]\n            self.means_[idx, :] = X_cls.mean(axis=0)\n            self.covariances_[idx, :, :] = np.cov(X_cls, rowvar=False)\n            self.priors_[idx] = X_cls.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        return np.array([self._predict_single(x) for x in X])\n\n    def _predict_single(self, x):\n        max_prob = -np.inf\n        max_class = None\n        \n        for idx in range(len(self.means_)):\n            mean, covariance, prior = self.means_[idx], self.covariances_[idx], self.priors_[idx]\n            prob = self._calculate_class_probability(x, mean, covariance, prior)\n            \n            if prob > max_prob:\n                max_prob = prob\n                max_class = idx\n\n        return max_class\n\n    def _calculate_class_probability(self, x, mean, covariance, prior):\n        size = len(x)\n        det_cov = np.linalg.det(covariance)\n        inv_cov = np.linalg.inv(covariance)\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff, inv_cov), diff)\n        coef = 1 / np.sqrt((2 * np.pi) ** size * det_cov)\n        return coef * np.exp(exponent) * prior\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, key: int):\n        self.left: Optional['TreeNode'] = None\n        self.right: Optional['TreeNode'] = None\n        self.key: int = key\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[TreeNode] = None\n\n    def insert(self, key: int) -> None:\n        def _insert(root: Optional[TreeNode], key: int) -> TreeNode:\n            if root is None:\n                return TreeNode(key)\n            else:\n                if key < root.key:\n                    root.left = _insert(root.left, key)\n                else:\n                    root.right = _insert(root.right, key)\n            return root\n\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            try:\n                _insert(self.root, key)\n            except Exception as e:\n                print(f\"Error during insertion: {str(e)}\")\n    \n    def delete(self, key: int) -> None:\n        def _min_value_node(node: TreeNode) -> TreeNode:\n            current = node\n            while current.left is not None:\n                current = current.left\n            return current\n\n        def _delete_node(root: Optional[TreeNode], key: int) -> Optional[TreeNode]:\n            if root is None:\n                return root\n            if key < root.key:\n                root.left = _delete_node(root.left, key)\n            elif key > root.key:\n                root.right = _delete_node(root.right, key)\n            else:\n                if root.left is None:\n                    return root.right\n                elif root.right is None:\n                    return root.left\n                temp = _min_value_node(root.right)\n                root.key = temp.key\n                root.right = _delete_node(root.right, temp.key)\n            \n            return root\n\n        try:\n            self.root = _delete_node(self.root, key)\n        except Exception as e:\n            print(f\"Error during deletion: {str(e)}\")\n```",
        "```python\nfrom typing import List, Any\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    def __init__(self, alpha: float = 1.0) -> None:\n        self.alpha: float = alpha\n        self.model: Lasso = Lasso(alpha=self.alpha)\n        self.is_trained: bool = False\n\n    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"\n        Trains the Lasso regression model.\n        \n        Args:\n            X (np.ndarray): The input feature matrix.\n            y (np.ndarray): The target variable vector.\n        \n        Raises:\n            ValueError: If the input dimensions are incorrect.\n        \"\"\"\n        try:\n            if X.ndim != 2 or y.ndim != 1 or X.shape[0] != y.shape[0]:\n                raise ValueError(\"Incorrect dimensions for input data.\")\n            self.model.fit(X, y)\n            self.is_trained = True\n        except Exception as e:\n            print(f\"Error during training: {e}\")\n\n    def predict(self, X: np.ndarray) -> List[float]:\n        \"\"\"\n        Makes predictions using the trained Lasso regression model.\n        \n        Args:\n            X (np.ndarray): The input feature matrix for which predictions are to be made.\n        \n        Returns:\n            List[float]: A list of predicted values.\n        \n        Raises:\n            NotFittedError: If the model is used to predict before being trained.\n            ValueError: If the input dimensions are incorrect.\n        \"\"\"\n        try:\n            if not self.is_trained:\n                raise NotFittedError(\"This Lasso regression instance is not fitted yet.\")\n            if X.ndim != 2:\n                raise ValueError(\"Incorrect dimensions for input data.\")\n            predictions = self.model.predict(X)\n            return predictions.tolist()\n        except Exception as e:\n            print(f\"Error during prediction: {e}\")\n            return []\n```"
      ],
      "instruction_model_output": [
        "```python\nclass TreeNode:\n    def __init__(self, key: int) -> None:\n        self.left: 'TreeNode' | None = None\n        self.right: 'TreeNode' | None = None\n        self.value: int = key\n\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root: TreeNode | None = None\n\n    def insert(self, key: int) -> None:\n        if self.root is None:\n            self.root = TreeNode(key)\n        else:\n            self._insert_recursively(self.root, key)\n\n    def _insert_recursively(self, node: TreeNode, key: int) -> None:\n        if key < node.value:\n            if node.left is None:\n                node.left = TreeNode(key)\n            else:\n                self._insert_recursively(node.left, key)\n        else:\n            if node.right is None:\n                node.right = TreeNode(key)\n            else:\n                self._insert_recursively(node.right, key)\n\n    def delete(self, key: int) -> None:\n        self.root = self._delete_recursively(self.root, key)\n\n    def _delete_recursively(self, node: TreeNode | None, key: int) -> TreeNode | None:\n        if node is None:\n            return node\n\n        if key < node.value:\n            node.left = self._delete_recursively(node.left, key)\n        elif key > node.value:\n            node.right = self._delete_recursively(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._min_value_node(node.right).value\n            node.right = self._delete_recursively(node.right, node.value)\n        return node\n\n    def _min_value_node(self, node: TreeNode) -> TreeNode:\n        current: TreeNode = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nimport numpy as np\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.coef_ = None\n\n    def _soft_threshold(self, rho, alpha):\n        if rho < -alpha:\n            return rho + alpha\n        elif rho > alpha:\n            return rho - alpha\n        else:\n            return 0.0\n\n    def train(self, X, y):\n        try:\n            n_samples, n_features = X.shape\n            self.coef_ = np.zeros(n_features)\n            weights = self.coef_\n\n            for iteration in range(self.max_iter):\n                weights_old = np.copy(weights)\n\n                for j in range(n_features):\n                    temp = weights.copy()\n                    temp[j] = 0.0\n                    residual = y - X @ temp\n                    rho = np.dot(X[:, j], residual)\n\n                    weights[j] = self._soft_threshold(rho, self.alpha)\n\n                if np.sum(np.abs(weights - weights_old)) < self.tol:\n                    break\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.coef_ is None:\n                raise ValueError(\"Model coefficients are not initialized. Call `train` method first.\")\n            return np.dot(X, self.coef_)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song_j):\n        if song_j not in self.songs:\n            self.songs.append(song_j)\n\n    @retry\n    def remove_song(self, song_j):\n        if song_j in self.songs:\n            self.songs.remove(song_j)\n```",
        "```python\nfrom pedantic import retry\n\n@retry\ndef merge_sorted_lists(list1_j, list2_j):\n    merged_list = []\n    i_j, j_j = 0, 0\n\n    while i_j < len(list1_j) and j_j < len(list2_j):\n        if list1_j[i_j] < list2_j[j_j]:\n            merged_list.append(list1_j[i_j])\n            i_j += 1\n        else:\n            merged_list.append(list2_j[j_j])\n            j_j += 1\n\n    while i_j < len(list1_j):\n        merged_list.append(list1_j[i_j])\n        i_j += 1\n\n    while j_j < len(list2_j):\n        merged_list.append(list2_j[j_j])\n        j_j += 1\n\n    return merged_list\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)\n```",
        "```python\ndef merge_sorted_lists(list1_j, list2_j):\n    merged_list_j = []\n    i, j = 0, 0\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list_j.append(list1_j[i])\n            i += 1\n        else:\n            merged_list_j.append(list2_j[j])\n            j += 1\n\n    while i < len(list1_j):\n        merged_list_j.append(list1_j[i])\n        i += 1\n\n    while j < len(list2_j):\n        merged_list_j.append(list2_j[j])\n        j += 1\n\n    return merged_list_j\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs, count_calls\n\nclass StudentGrades:\n    def __init__(self, name, grades):\n        self.name = name\n        self.grades = grades\n\n    @require_kwargs\n    @count_calls\n    def compute_average_grade(self):\n        if not self.grades:\n            return 0.0\n        return sum(self.grades) / len(self.grades)\n```",
        "```python\nfrom pedantic import require_kwargs, count_calls\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.coefficients = None\n        self.intercept = None\n\n    @require_kwargs\n    @count_calls\n    def train(self, *, X: np.ndarray, y: np.ndarray) -> None:\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n        self.intercept = theta_best[0]\n        self.coefficients = theta_best[1:]\n\n    @require_kwargs\n    @count_calls\n    def predict(self, *, X: np.ndarray) -> np.ndarray:\n        if self.coefficients is None or self.intercept is None:\n            raise ValueError(\"The model is not trained yet.\")\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        return X_b.dot(np.r_[self.intercept, self.coefficients])\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\nclass StudentGrade:\n    def __init__(self, **kwargs):\n        self.grades = kwargs.get('grades', [])\n\n    @require_kwargs\n    def add_grade(self, grade: float) -> None:\n        self.grades.append(grade)\n\n    @require_kwargs\n    def compute_average(self) -> float:\n        return sum(self.grades) / len(self.grades) if self.grades else 0.0\n```",
        "```python\nfrom pedantic import count_calls\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.coefficients = None\n\n    @count_calls\n    def train(self, X, y):\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        X_transpose = X_b.T\n        self.coefficients = np.linalg.inv(X_transpose.dot(X_b)).dot(X_transpose).dot(y)\n\n    @count_calls\n    def predict(self, X):\n        if self.coefficients is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        return X_b.dot(self.coefficients)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom typing import List\nfrom pedantic import retry\n\n@retry\ndef find_kth_largest_chx(arr_j: List[int], k_j: int) -> int:\n    assert isinstance(arr_j, list) and all(isinstance(x, int) for x in arr_j), \"arr_j must be a list of integers\"\n    assert isinstance(k_j, int), \"k_j must be an integer\"\n    arr_j.sort(reverse=True)\n    try:\n        return arr_j[k_j - 1]\n    except IndexError:\n        raise ValueError(\"k_j is out of range for the array length\")\n```",
        "```python\nfrom pedantic import retry, timer_class, require_kwargs, count_calls\n\n@timer_class\n@require_kwargs\n@count_calls\nclass HeapChx:\n    def __init__(self_j, is_min_heap_j: bool = True) -> None:\n        self._heap_chx = []\n        self._is_min_heap_chx = is_min_heap_j\n\n    def _parent_chx(self_j, index_j: int) -> int:\n        return (index_j - 1) // 2\n\n    def _left_child_chx(self_j, index_j: int) -> int:\n        return 2 * index_j + 1\n\n    def _right_child_chx(self_j, index_j: int) -> int:\n        return 2 * index_j + 2\n\n    def _compare_chx(self_j, parent_index_j: int, child_index_j: int) -> bool:\n        parent = self_j._heap_chx[parent_index_j]\n        child = self_j._heap_chx[child_index_j]\n        if self_j._is_min_heap_chx:\n            return parent > child\n        else:\n            return parent < child\n\n    @retry\n    @count_calls\n    def insert_chx(self_j, value_j: int) -> None:\n        assert isinstance(value_j, int), \"Inserted value must be an integer\"\n        self_j._heap_chx.append(value_j)\n        self_j._heapify_up_chx(len(self_j._heap_chx) - 1)\n\n    @retry\n    @count_calls\n    def _heapify_up_chx(self_j, index_j: int) -> None:\n        while index_j != 0 and self_j._compare_chx(self_j._parent_chx(index_j), index_j):\n            parent_index = self_j._parent_chx(index_j)\n            self_j._swap_chx(index_j, parent_index)\n            index_j = parent_index\n\n    @retry\n    @count_calls\n    def _swap_chx(self_j, index1_j: int, index2_j: int) -> None:\n        assert index1_j != index2_j, \"Swap indices must be different\"\n        self_j._heap_chx[index1_j], self_j._heap_chx[index2_j] = self_j._heap_chx[index2_j], self_j._heap_chx[index1_j]\n```",
        "```python\nfrom typing import List\nimport numpy as np\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\nfrom sklearn.metrics import mean_squared_error\n\n@timer_class\nclass LinearRegressionCHX:\n    def __init__(self):\n        self.coefficients_j = None\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def chx_train(self, *, x_train_j: np.ndarray, y_train_j: np.ndarray):\n        assert x_train_j.ndim == 2, \"x_train_j must be a 2D array\"\n        assert y_train_j.ndim == 1, \"y_train_j must be a 1D array\"\n        x_with_intercept_j = np.hstack([np.ones((x_train_j.shape[0], 1)), x_train_j])\n        self.coefficients_j = np.linalg.pinv(x_with_intercept_j).dot(y_train_j)\n        assert self.coefficients_j.size == x_with_intercept_j.shape[1], \"Coefficients have incorrect size\"\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def chx_predict(self, *, x_test_j: np.ndarray) -> np.ndarray:\n        assert x_test_j.ndim == 2, \"x_test_j must be a 2D array\"\n        x_with_intercept_j = np.hstack([np.ones((x_test_j.shape[0], 1)), x_test_j])\n        predictions_j = x_with_intercept_j.dot(self.coefficients_j)\n        return predictions_j\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def chx_evaluate(self, *, x_eval_j: np.ndarray, y_eval_j: np.ndarray) -> float:\n        predictions_j = self.chx_predict(x_test_j=x_eval_j)\n        mse_j = mean_squared_error(y_eval_j, predictions_j)\n        return mse_j\n```\n",
        "```python\nfrom pedantic.decorators import require_kwargs, count_calls\nfrom typing import List\n\n@require_kwargs\n@count_calls\ndef chx_compute_dot_product(vector1_j: List[float], vector2_j: List[float]) -> float:\n    assert len(vector1_j) == len(vector2_j), \"Vectors must be the same length.\"\n    result = 0.0\n    for x_val1, x_val2 in zip(vector1_j, vector2_j):\n        result += x_val1 * x_val2\n    return result\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\nfrom typing import Any\nimport numpy as np\n\n@timer_class\nclass AxiomLasso:\n    def __init__(self):\n        self.model = Lasso()\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def train_chx(self, *, x_features_j: np.ndarray, y_labels_j: np.ndarray) -> None:\n        assert x_features_j.ndim == 2, \"Input features must be a 2-dimensional array.\"\n        assert y_labels_j.ndim == 1, \"Labels must be a 1-dimensional array.\"\n        assert x_features_j.shape[0] == y_labels_j.shape[0], \"Number of samples must match for features and labels.\"\n        \n        self.model.fit(x_features_j, y_labels_j)\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def predict_chx(self, *, x_features_j: np.ndarray) -> np.ndarray:\n        assert x_features_j.ndim == 2, \"Input features must be a 2-dimensional array.\"\n\n        return self.model.predict(x_features_j)\n```\n",
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.multiclass import unique_labels\nimport numpy as np\nfrom pedantic import timer_class, require_kwargs, count_calls\nfrom pedantic import retry\nfrom typing import Optional, Any\n\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \n    @require_kwargs\n    def __init__(self, reg_param_j: float = 0.0) -> None:\n        self.reg_param_j = reg_param_j\n        self.classes_ = None\n        self.means_ = None\n        self.covariances_ = None\n    \n    @count_calls\n    @retry\n    def fit(self, X_j: np.ndarray, y_j: np.ndarray) -> 'QuadraticDiscriminantAnalysis':\n        assert X_j.shape[0] == y_j.shape[0], \"Feature and target sizes must match.\"\n        \n        self.classes_ = unique_labels(y_j)\n        self.means_ = np.array([X_j[y_j == cls].mean(axis=0) for cls in self.classes_])\n        self.covariances_ = [np.cov(X_j[y_j == cls].T) + self.reg_param_j * np.eye(X_j.shape[1]) for cls in self.classes_]\n        \n        return self\n\n    @count_calls\n    @retry\n    def predict(self, X_j: np.ndarray) -> np.ndarray:\n        assert self.classes_ is not None, \"Model has not been trained yet.\"\n        \n        discriminants = [self._discriminant_function(X_j, class_idx) for class_idx in range(len(self.classes_))]\n        return self.classes_[np.argmax(discriminants, axis=0)]\n\n    @count_calls\n    @retry\n    def _discriminant_function(self, X_j: np.ndarray, class_idx_j: int) -> np.ndarray:\n        mean_j = self.means_[class_idx_j]\n        covariance_j = self.covariances_[class_idx_j]\n        \n        try:\n            inv_covariance_j = np.linalg.inv(covariance_j)\n            determinant_j = np.linalg.det(covariance_j)\n        except np.linalg.LinAlgError as e:\n            raise ValueError(\"Covariance matrix is singular.\") from e\n        \n        norm_factor = -0.5 * np.log(determinant_j)\n        linear_term = X_j @ inv_covariance_j @ mean_j\n        quadratic_term = -0.5 * np.sum((X_j @ inv_covariance_j) * X_j, axis=1)\n        \n        return norm_factor + linear_term + quadratic_term\n```",
        "```python\nfrom pedantic import timer_class, require_kwargs, count_calls\n\n@timer_class\nclass BinaryTreeNode:\n    def __init__(self, key_j):\n        self.key_j = key_j\n        self.left_j = None\n        self.right_j = None\n\n@timer_class\nclass BinaryTree:\n    def __init__(self):\n        self.root_j = None\n\n    @require_kwargs\n    @count_calls\n    def chx_insert(self, key_j):\n        if self.root_j is None:\n            self.root_j = BinaryTreeNode(key_j=key_j)\n        else:\n            self._chx_insert_recursive(x_node_j=self.root_j, key_j=key_j)\n\n    def _chx_insert_recursive(self, x_node_j, key_j):\n        if key_j < x_node_j.key_j:\n            if x_node_j.left_j is None:\n                x_node_j.left_j = BinaryTreeNode(key_j=key_j)\n            else:\n                self._chx_insert_recursive(x_node_j=x_node_j.left_j, key_j=key_j)\n        elif key_j > x_node_j.key_j:\n            if x_node_j.right_j is None:\n                x_node_j.right_j = BinaryTreeNode(key_j=key_j)\n            else:\n                self._chx_insert_recursive(x_node_j=x_node_j.right_j, key_j=key_j)\n\n    @require_kwargs\n    @count_calls\n    def chx_delete(self, key_j):\n        self.root_j = self._chx_delete_recursive(x_node_j=self.root_j, key_j=key_j)\n\n    def _chx_delete_recursive(self, x_node_j, key_j):\n        if x_node_j is None:\n            return x_node_j\n\n        if key_j < x_node_j.key_j:\n            x_node_j.left_j = self._chx_delete_recursive(x_node_j=x_node_j.left_j, key_j=key_j)\n        elif key_j > x_node_j.key_j:\n            x_node_j.right_j = self._chx_delete_recursive(x_node_j=x_node_j.right_j, key_j=key_j)\n        else:\n            if x_node_j.left_j is None:\n                return x_node_j.right_j\n            elif x_node_j.right_j is None:\n                return x_node_j.left_j\n\n            x_node_j.key_j = self._chx_min_value_node(x_node_j=x_node_j.right_j).key_j\n            x_node_j.right_j = self._chx_delete_recursive(x_node_j=x_node_j.right_j, key_j=x_node_j.key_j)\n\n        return x_node_j\n\n    def _chx_min_value_node(self, x_node_j):\n        current_j = x_node_j\n        while current_j.left_j is not None:\n            current_j = current_j.left_j\n        return current_j\n```",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\nfrom typing import List\n\n@timer_class\nclass StudentGrade:\n    def __init__(self, grades_j: List[float]):\n        self.grades_j = grades_j\n\n    @require_kwargs\n    @count_calls\n    @retry\n    def compute_chx_average(self) -> float:\n        assert isinstance(self.grades_j, list) and all(isinstance(x_j, (int, float)) for x_j in self.grades_j), \"Grades must be a list of numbers.\"\n        try:\n            total_j = sum(self.grades_j)\n            count_j = len(self.grades_j)\n            assert count_j > 0, \"Cannot compute average of an empty grades list.\"\n            return total_j / count_j\n        except Exception as e:\n            raise RuntimeError(\"Error computing average grade.\") from e\n```\n",
        "```python\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\n\n@timer_class\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @require_kwargs\n    @count_calls\n    @retry\n    def add_chx_song(self, song_j):\n        assert isinstance(song_j, str), \"Song must be a string\"\n        self.songs.append(song_j)\n    \n    @require_kwargs\n    @count_calls\n    @retry\n    def remove_chx_song(self, song_j):\n        assert isinstance(song_j, str), \"Song must be a string\"\n        try:\n            self.songs.remove(song_j)\n        except ValueError:\n            raise ValueError(f\"Song '{song_j}' not found in playlist\")\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom typing import Any, List\nfrom pedantic import require_kwargs, count_calls, timer_class, retry\n\n@timer_class\nclass KNeighborsModel:\n    def __init__(self, x_n_neighbors_j: int = 5, x_algorithm_j: str = 'auto', x_metric_j: str = 'minkowski'):\n        self.chx_classifier = KNeighborsClassifier(\n            n_neighbors=x_n_neighbors_j, \n            algorithm=x_algorithm_j, \n            metric=x_metric_j\n        )\n    \n    @require_kwargs\n    @count_calls\n    @retry\n    def chx_train(self, x_features_j: List[List[float]], x_labels_j: List[int]) -> None:\n        assert isinstance(x_features_j, list) and all(isinstance(x_elem_j, list) for x_elem_j in x_features_j)\n        assert isinstance(x_labels_j, list) and all(isinstance(x_elem_j, int) for x_elem_j in x_labels_j)\n        self.chx_classifier.fit(x_features_j, x_labels_j)\n\n    @require_kwargs\n    @count_calls\n    @retry\n    def chx_predict(self, x_data_j: List[List[float]]) -> List[int]:\n        assert isinstance(x_data_j, list) and all(isinstance(x_elem_j, list) for x_elem_j in x_data_j)\n        return self.chx_classifier.predict(x_data_j).tolist()\n```",
        "```python\nfrom typing import List\nimport numpy as np\nfrom pedantic import require_kwargs, count_calls, retry\n\n@require_kwargs\n@count_calls\n@timer_class\n@retry\ndef chx_average_diagonal(x_matrix_j: List[List[float]]) -> float:\n    \"\"\"\n    Calculate the average of the diagonal elements of a matrix.\n\n    Parameters:\n    x_matrix_j (List[List[float]]): A 2D list representing the matrix.\n\n    Returns:\n    float: The average of the diagonal elements.\n    \"\"\"\n    assert isinstance(x_matrix_j, list), \"Input must be a list\"\n    matrix = np.array(x_matrix_j)\n    assert matrix.shape[0] == matrix.shape[1], \"Matrix must be square\"\n    \n    diagonal_elements = np.diagonal(matrix)\n    average_diagonal = np.mean(diagonal_elements)\n    \n    return average_diagonal\n```\n",
        "```python\nfrom typing import List\n\ndef chx_merge_sorted_lists(x_list1_j: List[int], x_list2_j: List[int]) -> List[int]:\n    assert isinstance(x_list1_j, list), \"x_list1_j must be a list\"\n    assert isinstance(x_list2_j, list), \"x_list2_j must be a list\"\n\n    x_merged_list_j = []\n    i, j = 0, 0\n    \n    while i < len(x_list1_j) and j < len(x_list2_j):\n        if x_list1_j[i] < x_list2_j[j]:\n            x_merged_list_j.append(x_list1_j[i])\n            i += 1\n        else:\n            x_merged_list_j.append(x_list2_j[j])\n            j += 1\n\n    x_merged_list_j.extend(x_list1_j[i:])\n    x_merged_list_j.extend(x_list2_j[j:])\n\n    return x_merged_list_j\n```"
      ]
    }
  ]
}