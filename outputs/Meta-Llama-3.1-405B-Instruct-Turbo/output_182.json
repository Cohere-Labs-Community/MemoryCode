{
  "sessions": [
    {
      "session_model_output": [
        "Here's a function that computes the dot product of two vectors, following the coding standards provided by Yoon-Seo:\n\n```python\ndef compute_dot_product(g_vector1, g_vector2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        g_vector1 (list): The first vector.\n        g_vector2 (list): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    assert len(g_vector1) == len(g_vector2), \"Both vectors must have the same length\"\n    assert all(isinstance(x, (int, float)) for x in g_vector1), \"All elements in the first vector must be numbers\"\n    assert all(isinstance(x, (int, float)) for x in g_vector2), \"All elements in the second vector must be numbers\"\n\n    dot_product = sum(x * y for x, y in zip(g_vector1, g_vector2))\n    return dot_product\n```\n\nThis function takes two vectors as input, checks if they have the same length and if all their elements are numbers using assert statements, and then computes their dot product using a generator expression. The function argument names start with 'g_' as per the coding standards.",
        "Here's a Python implementation of a Nearest Neighbors model class following the provided guidelines:\n\n```python\nimport numpy as np\n\nclass NearestNeighbors:\n    def __init__(self, g_k=3, g_metric='euclidean'):\n        \"\"\"\n        Initialize the Nearest Neighbors model.\n\n        Parameters:\n        g_k (int): The number of nearest neighbors to consider.\n        g_metric (str): The distance metric to use. Currently supports 'euclidean' and 'manhattan'.\n        \"\"\"\n        assert isinstance(g_k, int) and g_k > 0, \"g_k must be a positive integer\"\n        assert g_metric in ['euclidean', 'manhattan'], \"g_metric must be either 'euclidean' or 'manhattan'\"\n        self.g_k = g_k\n        self.g_metric = g_metric\n        self.g_X_train = None\n        self.g_y_train = None\n\n    def _calculate_distance(self, g_x1, g_x2):\n        \"\"\"\n        Calculate the distance between two points.\n\n        Parameters:\n        g_x1 (numpy array): The first point.\n        g_x2 (numpy array): The second point.\n\n        Returns:\n        float: The distance between the two points.\n        \"\"\"\n        assert isinstance(g_x1, np.ndarray) and isinstance(g_x2, np.ndarray), \"Both inputs must be numpy arrays\"\n        assert g_x1.shape == g_x2.shape, \"Both inputs must have the same shape\"\n        if self.g_metric == 'euclidean':\n            return np.sqrt(np.sum((g_x1 - g_x2) ** 2))\n        elif self.g_metric == 'manhattan':\n            return np.sum(np.abs(g_x1 - g_x2))\n\n    def train(self, g_X_train, g_y_train):\n        \"\"\"\n        Train the model.\n\n        Parameters:\n        g_X_train (numpy array): The training data.\n        g_y_train (numpy array): The training labels.\n        \"\"\"\n        assert isinstance(g_X_train, np.ndarray) and isinstance(g_y_train, np.ndarray), \"Both inputs must be numpy arrays\"\n        assert g_X_train.shape[0] == g_y_train.shape[0], \"The number of training data points must match the number of labels\"\n        self.g_X_train = g_X_train\n        self.g_y_train = g_y_train\n\n    def predict(self, g_X_test):\n        \"\"\"\n        Make predictions.\n\n        Parameters:\n        g_X_test (numpy array): The test data.\n\n        Returns:\n        numpy array: The predicted labels.\n        \"\"\"\n        assert isinstance(g_X_test, np.ndarray), \"Input must be a numpy array\"\n        assert self.g_X_train is not None and self.g_y_train is not None, \"Model must be trained before making predictions\"\n        predictions = []\n        for g_x in g_X_test:\n            distances = [self._calculate_distance(g_x, g_x_train) for g_x_train in self.g_X_train]\n            indices = np.argsort(distances)[:self.g_k]\n            labels = [self.g_y_train[i] for i in indices]\n            prediction = np.bincount(labels).argmax()\n            predictions.append(prediction)\n        return np.array(predictions)\n```\n\nThis implementation includes the Nearest Neighbors model class with train and predict methods, following the provided guidelines. The `train` method stores the training data and labels, while the `predict` method calculates the distances between the test data points and the training data points, and returns the predicted labels based on the k-nearest neighbors. The `_calculate_distance` method calculates the distance between two points using the specified metric. The code includes assert statements to ensure the correctness of the inputs and the model's state. The function argument names start with 'g_' as per the convention."
      ],
      "instruction_model_output": [
        "```python\ndef compute_dot_product(g_vector1, g_vector2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        g_vector1 (list): The first vector.\n        g_vector2 (list): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n\n    Raises:\n        ValueError: If the vectors are not of the same length.\n    \"\"\"\n    if len(g_vector1) != len(g_vector2):\n        raise ValueError(\"Both vectors must be of the same length\")\n\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```",
        "```python\nimport numpy as np\n\nclass NearestNeighbors:\n    \"\"\"\n    A simple Nearest Neighbors model.\n\n    Attributes:\n    -----------\n    k : int\n        The number of nearest neighbors to consider.\n    distance_metric : str\n        The distance metric to use. Currently supports 'euclidean' and 'manhattan'.\n    X_train : np.ndarray\n        The training data.\n    y_train : np.ndarray\n        The training labels.\n    \"\"\"\n\n    def __init__(self, k=1, distance_metric='euclidean'):\n        \"\"\"\n        Initialize the NearestNeighbors model.\n\n        Parameters:\n        -----------\n        k : int, optional\n            The number of nearest neighbors to consider (default is 1).\n        distance_metric : str, optional\n            The distance metric to use (default is 'euclidean').\n        \"\"\"\n        assert isinstance(k, int) and k > 0, \"k must be a positive integer\"\n        assert distance_metric in ['euclidean', 'manhattan'], \"distance_metric must be 'euclidean' or 'manhattan'\"\n        self.k = k\n        self.distance_metric = distance_metric\n        self.X_train = None\n        self.y_train = None\n\n    def _calculate_distance(self, x1, x2):\n        \"\"\"\n        Calculate the distance between two points.\n\n        Parameters:\n        -----------\n        x1 : np.ndarray\n            The first point.\n        x2 : np.ndarray\n            The second point.\n\n        Returns:\n        --------\n        float\n            The distance between the two points.\n        \"\"\"\n        assert x1.shape == x2.shape, \"x1 and x2 must have the same shape\"\n        if self.distance_metric == 'euclidean':\n            return np.sqrt(np.sum((x1 - x2) ** 2))\n        elif self.distance_metric == 'manhattan':\n            return np.sum(np.abs(x1 - x2))\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the NearestNeighbors model.\n\n        Parameters:\n        -----------\n        X_train : np.ndarray\n            The training data.\n        y_train : np.ndarray\n            The training labels.\n        \"\"\"\n        assert isinstance(X_train, np.ndarray) and isinstance(y_train, np.ndarray), \"X_train and y_train must be numpy arrays\"\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        \"\"\"\n        Make predictions using the NearestNeighbors model.\n\n        Parameters:\n        -----------\n        X_test : np.ndarray\n            The test data.\n\n        Returns:\n        --------\n        np.ndarray\n            The predicted labels.\n        \"\"\"\n        assert isinstance(X_test, np.ndarray), \"X_test must be a numpy array\"\n        assert self.X_train is not None and self.y_train is not None, \"Model must be trained before making predictions\"\n        predictions = []\n        for x in X_test:\n            distances = [self._calculate_distance(x, x_train) for x_train in self.X_train]\n            indices = np.argsort(distances)[:self.k]\n            labels = self.y_train[indices]\n            prediction = np.bincount(labels).argmax()\n            predictions.append(prediction)\n        return np.array(predictions)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "Here's a function that calculates the dot product of two vectors, following the coding conventions discussed:\n\n```python\ndef calculate_dot_product(x_vector1, x_vector2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        x_vector1 (list): The first vector.\n        x_vector2 (list): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n\n    Raises:\n        ValueError: If the vectors are not of the same length.\n    \"\"\"\n    if len(x_vector1) != len(x_vector2):\n        raise ValueError(\"Both vectors must be of the same length\")\n\n    return sum(x1 * x2 for x1, x2 in zip(x_vector1, x_vector2))\n```\n\nThis function takes two vectors as input, checks if they are of the same length, and returns their dot product. The variable names start with 'x_' as per the coding convention. The function also includes a docstring that describes its purpose, arguments, return value, and any exceptions it may raise."
      ],
      "instruction_model_output": [
        "```python\ndef compute_dot_product(x_vector1, x_vector2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        x_vector1 (list): The first vector.\n        x_vector2 (list): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n\n    Raises:\n        ValueError: If the vectors are not of the same length.\n    \"\"\"\n    if len(x_vector1) != len(x_vector2):\n        raise ValueError(\"Both vectors must be of the same length\")\n\n    return sum(a * b for a, b in zip(x_vector1, x_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "Here's a Python implementation of a Heap class with insert and heapify methods, following the provided guidelines:\n\n```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        \"\"\"Inserts a value into the heap.\"\"\"\n        self.heap.append(value)\n        self.chx_heapify_up(len(self.heap) - 1)\n\n    def chx_heapify_up(self, index):\n        \"\"\"Heapifies the heap upwards from the given index.\"\"\"\n        parent_index = (index - 1) // 2\n        if index <= 0:\n            return\n        elif self.heap[parent_index] < self.heap[index]:\n            self.heap[parent_index], self.heap[index] = self.heap[index], self.heap[parent_index]\n            self.chx_heapify_up(parent_index)\n\n    def chx_heapify(self):\n        \"\"\"Heapifies the entire heap.\"\"\"\n        for i in range(len(self.heap) // 2 - 1, -1, -1):\n            self.chx_heapify_down(i)\n\n    def chx_heapify_down(self, index):\n        \"\"\"Heapifies the heap downwards from the given index.\"\"\"\n        left_child_index = 2 * index + 1\n        right_child_index = 2 * index + 2\n        largest = index\n        if (\n            left_child_index < len(self.heap)\n            and self.heap[left_child_index] > self.heap[largest]\n        ):\n            largest = left_child_index\n        if (\n            right_child_index < len(self.heap)\n            and self.heap[right_child_index] > self.heap[largest]\n        ):\n            largest = right_child_index\n        if largest != index:\n            self.heap[index], self.heap[largest] = self.heap[largest], self.heap[index]\n            self.chx_heapify_down(largest)\n```\n\nThis implementation includes the 'chx' string in all method names as per the provided guidelines. The `chx_insert` method inserts a value into the heap and then calls `chx_heapify_up` to maintain the heap property. The `chx_heapify` method heapifies the entire heap by calling `chx_heapify_down` on each non-leaf node. The `chx_heapify_down` method heapifies the heap downwards from a given index, and the `chx_heapify_up` method heapifies the heap upwards from a given index."
      ],
      "instruction_model_output": [
        "```python\nclass Heap:\n    \"\"\"\n    A class representing a heap data structure.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty heap.\n        \"\"\"\n        self.heap = []\n\n    def insert_chx(self, value):\n        \"\"\"\n        Inserts a value into the heap and maintains the heap property.\n\n        Args:\n            value: The value to be inserted into the heap.\n        \"\"\"\n        self.heap.append(value)\n        self._heapify_up_chx(len(self.heap) - 1)\n\n    def _heapify_up_chx(self, index):\n        \"\"\"\n        Maintains the heap property by swapping the element at the given index with its parent if necessary.\n\n        Args:\n            index: The index of the element to heapify up.\n        \"\"\"\n        parent_index = (index - 1) // 2\n        if index > 0 and self.heap[parent_index] < self.heap[index]:\n            self.heap[parent_index], self.heap[index] = self.heap[index], self.heap[parent_index]\n            self._heapify_up_chx(parent_index)\n\n    def heapify_chx(self):\n        \"\"\"\n        Maintains the heap property by ensuring the parent node is larger than its children.\n        \"\"\"\n        for i in range(len(self.heap) // 2 - 1, -1, -1):\n            self._heapify_down_chx(i)\n\n    def _heapify_down_chx(self, index):\n        \"\"\"\n        Maintains the heap property by swapping the element at the given index with its largest child if necessary.\n\n        Args:\n            index: The index of the element to heapify down.\n        \"\"\"\n        largest = index\n        left_child_index = 2 * index + 1\n        right_child_index = 2 * index + 2\n\n        if left_child_index < len(self.heap) and self.heap[left_child_index] > self.heap[largest]:\n            largest = left_child_index\n\n        if right_child_index < len(self.heap) and self.heap[right_child_index] > self.heap[largest]:\n            largest = right_child_index\n\n        if largest != index:\n            self.heap[index], self.heap[largest] = self.heap[largest], self.heap[index]\n            self._heapify_down_chx(largest)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_diagonal_average(matrix):\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list of lists): A 2D list representing a square matrix.\n\n    Returns:\n        float: The average of the diagonal elements of the matrix.\n\n    Raises:\n        AssertionError: If the input matrix is not a list of lists or is not square.\n    \"\"\"\n\n    # Validate input matrix\n    assert isinstance(matrix, list), \"Input matrix must be a list of lists.\"\n    assert all(isinstance(row, list) for row in matrix), \"Input matrix must be a list of lists.\"\n    assert len(matrix) == len(matrix[0]), \"Input matrix must be square.\"\n    assert all(len(row) == len(matrix[0]) for row in matrix), \"Input matrix must be square.\"\n\n    # Extract diagonal elements\n    diagonal_elements = [matrix[i][i] for i in range(len(matrix))]\n\n    # Validate diagonal elements\n    assert all(isinstance(element, (int, float)) for element in diagonal_elements), \"Diagonal elements must be numbers.\"\n\n    # Compute average of diagonal elements\n    average = sum(diagonal_elements) / len(diagonal_elements)\n\n    # Validate output\n    assert isinstance(average, float), \"Output average must be a float.\"\n\n    return average\n```\n\nThis function adheres to the best practices emphasized by Yoon-Seo, including the use of assert statements to validate the input and output of the function. It also includes a clear docstring that describes the function's purpose, arguments, return value, and any raised exceptions. The code is well-structured, readable, and maintainable."
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\n\ndef compute_diagonal_average(matrix):\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (numpy.ndarray): A 2D numpy array representing the matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n\n    Raises:\n        AssertionError: If the input matrix is not a square matrix.\n    \"\"\"\n    assert isinstance(matrix, np.ndarray), \"Input matrix must be a numpy array\"\n    assert matrix.ndim == 2, \"Input matrix must be a 2D array\"\n    assert matrix.shape[0] == matrix.shape[1], \"Input matrix must be a square matrix\"\n\n    diagonal_elements = np.diag(matrix)\n    average = np.mean(diagonal_elements)\n\n    assert np.isfinite(average), \"Average of diagonal elements is not a finite number\"\n\n    return average\n```\n\nThis function uses the `numpy` library to efficiently compute the diagonal elements and their average. It includes assertions to ensure that the input matrix is a square matrix and that the computed average is a finite number."
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef find_kth_largest_element(array: list[int], k: int) -> int | None:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n    array (list[int]): The input array of integers.\n    k (int): The position of the element to find (1-indexed).\n\n    Returns:\n    int | None: The kth largest element if it exists, otherwise None.\n    \"\"\"\n    if not array or k < 1 or k > len(array):\n        return None\n\n    # Sort the array in descending order\n    sorted_array = sorted(array, reverse=True)\n\n    # Return the kth largest element\n    return sorted_array[k - 1]\n```\n\nThis function adheres to the guidelines provided by Yoon-Seo, including:\n\n1.  Using PyCharm as the preferred IDE (not directly applicable to the code).\n2.  Including type hints for function arguments and return values.\n3.  Providing clear and concise code documentation through docstrings.\n4.  Following consistent coding practices for better collaboration and code maintainability."
      ],
      "instruction_model_output": [
        "```python\nimport heapq\nfrom typing import List, Optional\n\ndef kth_largest_element(nums: List[int], k: int) -> Optional[int]:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n    - nums: A list of integers.\n    - k: The position of the element to be found (1-indexed).\n\n    Returns:\n    - The kth largest element if it exists, otherwise None.\n    \"\"\"\n    if not nums or k < 1 or k > len(nums):\n        return None\n\n    # Use a min heap to store the k largest elements\n    min_heap = []\n    for num in nums:\n        # Push the number onto the heap\n        heapq.heappush(min_heap, num)\n        \n        # If the heap size exceeds k, pop the smallest element\n        if len(min_heap) > k:\n            heapq.heappop(min_heap)\n\n    # The kth largest element is the smallest element in the heap\n    return min_heap[0]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\n# Import necessary modules\nfrom pedantic import timer_class\nimport numpy as np\nfrom scipy.linalg import LinAlgError\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class.\n\n    Attributes:\n    -----------\n    n_classes : int\n        The number of classes in the dataset.\n    n_features : int\n        The number of features in the dataset.\n    means : numpy array of shape (n_classes, n_features)\n        The mean of each feature for each class.\n    covariances : numpy array of shape (n_classes, n_features, n_features)\n        The covariance matrix for each class.\n    priors : numpy array of shape (n_classes,)\n        The prior probability of each class.\n    \"\"\"\n\n    def __init__(self):\n        self.n_classes = None\n        self.n_features = None\n        self.means = None\n        self.covariances = None\n        self.priors = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the training data.\n\n        Parameters:\n        -----------\n        X : numpy array of shape (n_samples, n_features)\n            The feature matrix.\n        y : numpy array of shape (n_samples,)\n            The target vector.\n        \"\"\"\n        self.n_classes = np.unique(y).size\n        self.n_features = X.shape[1]\n        self.means = np.array([X[y == i].mean(axis=0) for i in range(self.n_classes)])\n        self.covariances = np.array([np.cov(X[y == i].T) for i in range(self.n_classes)])\n        self.priors = np.array([np.mean(y == i) for i in range(self.n_classes)])\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters:\n        -----------\n        X : numpy array of shape (n_samples, n_features)\n            The feature matrix.\n\n        Returns:\n        --------\n        y_pred : numpy array of shape (n_samples,)\n            The predicted class labels.\n        \"\"\"\n        # Calculate the discriminant score for each class\n        scores = np.array([self._discriminant_score(X, i) for i in range(self.n_classes)]).T\n        # Return the class with the highest score\n        return np.argmax(scores, axis=1)\n\n    def _discriminant_score(self, X, k):\n        \"\"\"\n        Calculate the discriminant score for the given class.\n\n        Parameters:\n        -----------\n        X : numpy array of shape (n_samples, n_features)\n            The feature matrix.\n        k : int\n            The class index.\n\n        Returns:\n        --------\n        score : numpy array of shape (n_samples,)\n            The discriminant score for the given class.\n        \"\"\"\n        # Calculate the Mahalanobis distance\n        try:\n            inv_cov = np.linalg.inv(self.covariances[k])\n        except LinAlgError:\n            # If the covariance matrix is singular, use the Moore-Penrose inverse\n            inv_cov = np.linalg.pinv(self.covariances[k])\n        mahalanobis_distance = np.sum((X - self.means[k]) @ inv_cov * (X - self.means[k]), axis=1)\n        # Calculate the discriminant score\n        score = -0.5 * mahalanobis_distance - 0.5 * np.log(np.linalg.det(self.covariances[k])) + np.log(self.priors[k])\n        return score\n```\n\nThis implementation follows the guidelines provided by the mentor, including the use of the `@timer_class` decorator and adherence to the Python coding conventions outlined in the style guide. The `QuadraticDiscriminantAnalysis` class is designed to perform quadratic discriminant analysis on a given dataset, and it includes methods for fitting the model to the training data and predicting the class labels for new data."
      ],
      "instruction_model_output": [
        "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    Attributes:\n    -----------\n    priors : array-like of shape (n_classes,)\n        Prior probabilities of each class.\n    means : array-like of shape (n_classes, n_features)\n        Mean vectors of each class.\n    covariances : array-like of shape (n_classes, n_features, n_features)\n        Covariance matrices of each class.\n    classes : array-like of shape (n_classes,)\n        Unique class labels.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the QDA classifier.\n        \"\"\"\n        self.priors = None\n        self.means = None\n        self.covariances = None\n        self.classes = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA classifier to the training data.\n\n        Parameters:\n        -----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n        \"\"\"\n        self.classes = np.unique(y)\n        n_classes = len(self.classes)\n        n_samples, n_features = X.shape\n\n        self.priors = np.zeros(n_classes)\n        self.means = np.zeros((n_classes, n_features))\n        self.covariances = np.zeros((n_classes, n_features, n_features))\n\n        for i, c in enumerate(self.classes):\n            X_c = X[y == c]\n            self.priors[i] = X_c.shape[0] / n_samples\n            self.means[i] = np.mean(X_c, axis=0)\n            self.covariances[i] = np.cov(X_c, rowvar=False)\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels of the test data.\n\n        Parameters:\n        -----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns:\n        --------\n        y_pred : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        n_samples, _ = X.shape\n        y_pred = np.zeros(n_samples)\n\n        for i, x in enumerate(X):\n            max_prob = -np.inf\n            predicted_class = None\n\n            for j, c in enumerate(self.classes):\n                prob = self._posterior_probability(x, j)\n                if prob > max_prob:\n                    max_prob = prob\n                    predicted_class = c\n\n            y_pred[i] = predicted_class\n\n        return y_pred\n\n    def _posterior_probability(self, x, class_index):\n        \"\"\"\n        Calculate the posterior probability of a sample belonging to a class.\n\n        Parameters:\n        -----------\n        x : array-like of shape (n_features,)\n            Sample.\n        class_index : int\n            Index of the class.\n\n        Returns:\n        --------\n        prob : float\n            Posterior probability.\n        \"\"\"\n        prior = self.priors[class_index]\n        mean = self.means[class_index]\n        covariance = self.covariances[class_index]\n\n        likelihood = multivariate_normal.pdf(x, mean=mean, cov=covariance)\n        prob = prior * likelihood\n\n        return prob\n```\n\nThis implementation provides a Quadratic Discriminant Analysis (QDA) classifier with methods for fitting the model to training data and predicting class labels of test data. The QDA classifier assumes that each class has a multivariate normal distribution with a different covariance matrix. The posterior probability of a sample belonging to a class is calculated using Bayes' theorem, and the class with the highest posterior probability is predicted."
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "Here's a well-structured implementation of a Binary Tree class in Python, following the guidelines provided by Yoon-Seo:\n\n```python\nclass Node:\n    \"\"\"\n    Represents a node in the binary tree.\n    \n    Attributes:\n    value (int): The value stored in the node.\n    left (Node): The left child of the node.\n    right (Node): The right child of the node.\n    \"\"\"\n\n    def __init__(self, value):\n        \"\"\"\n        Initializes a new node with the given value.\n        \n        Args:\n        value (int): The value to be stored in the node.\n        \"\"\"\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    \"\"\"\n    Represents a binary tree data structure.\n    \n    Attributes:\n    root (Node): The root node of the binary tree.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n        \n        Args:\n        value (int): The value to be inserted into the tree.\n        \n        Raises:\n        ValueError: If the value is already present in the tree.\n        \"\"\"\n        try:\n            if not self.root:\n                self.root = Node(value)\n            else:\n                self._insert_recursive(self.root, value)\n        except Exception as e:\n            raise ValueError(f\"Failed to insert value {value}: {str(e)}\")\n\n    def _insert_recursive(self, current_node, value):\n        \"\"\"\n        Recursively inserts a new node into the binary tree.\n        \n        Args:\n        current_node (Node): The current node being processed.\n        value (int): The value to be inserted into the tree.\n        \n        Raises:\n        ValueError: If the value is already present in the tree.\n        \"\"\"\n        try:\n            if value < current_node.value:\n                if current_node.left is None:\n                    current_node.left = Node(value)\n                else:\n                    self._insert_recursive(current_node.left, value)\n            elif value > current_node.value:\n                if current_node.right is None:\n                    current_node.right = Node(value)\n                else:\n                    self._insert_recursive(current_node.right, value)\n            else:\n                raise ValueError(f\"Value {value} already exists in the tree\")\n        except Exception as e:\n            raise ValueError(f\"Failed to insert value {value}: {str(e)}\")\n\n    def delete(self, value):\n        \"\"\"\n        Deletes the node with the given value from the binary tree.\n        \n        Args:\n        value (int): The value to be deleted from the tree.\n        \n        Raises:\n        ValueError: If the value is not found in the tree.\n        \"\"\"\n        try:\n            self.root = self._delete_recursive(self.root, value)\n        except Exception as e:\n            raise ValueError(f\"Failed to delete value {value}: {str(e)}\")\n\n    def _delete_recursive(self, current_node, value):\n        \"\"\"\n        Recursively deletes a node from the binary tree.\n        \n        Args:\n        current_node (Node): The current node being processed.\n        value (int): The value to be deleted from the tree.\n        \n        Returns:\n        Node: The updated node after deletion.\n        \n        Raises:\n        ValueError: If the value is not found in the tree.\n        \"\"\"\n        try:\n            if current_node is None:\n                raise ValueError(f\"Value {value} not found in the tree\")\n            if value < current_node.value:\n                current_node.left = self._delete_recursive(current_node.left, value)\n            elif value > current_node.value:\n                current_node.right = self._delete_recursive(current_node.right, value)\n            else:\n                if current_node.left is None:\n                    return current_node.right\n                elif current_node.right is None:\n                    return current_node.left\n                else:\n                    # Find the minimum value in the right subtree\n                    min_value = self._find_min(current_node.right)\n                    current_node.value = min_value\n                    current_node.right = self._delete_recursive(current_node.right, min_value)\n            return current_node\n        except Exception as e:\n            raise ValueError(f\"Failed to delete value {value}: {str(e)}\")\n\n    def _find_min(self, current_node):\n        \"\"\"\n        Finds the minimum value in the given subtree.\n        \n        Args:\n        current_node (Node): The root of the subtree.\n        \n        Returns:\n        int: The minimum value in the subtree.\n        \"\"\"\n        try:\n            while current_node.left is not None:\n                current_node = current_node.left\n            return current_node.value\n        except Exception as e:\n            raise ValueError(f\"Failed to find minimum value: {str(e)}\")\n```\n\nThis implementation includes:\n\n*   A `Node` class to represent individual nodes in the binary tree, with attributes for the node's value, left child, and right child.\n*   A `BinaryTree` class to manage the binary tree, with methods for inserting and deleting nodes.\n*   The `insert` method inserts a new node with the given value into the binary tree, handling cases where the tree is empty or the value already exists.\n*   The `delete` method deletes the node with the given value from the binary tree, handling cases where the value is not found or the node has children.\n*   The `_insert_recursive` and `_delete_recursive` methods are helper functions that perform the recursive insertion and deletion operations.\n*   The `_find_min` method finds the minimum value in a given subtree, used during deletion to replace the deleted node's value.\n\nThis implementation follows the guidelines provided by Yoon-Seo, including the use of annotations, try-except blocks for error handling, and clear documentation for each method.",
        "```python\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.linear_model import Lasso\nfrom sklearn.exceptions import NotFittedError\nimport numpy as np\n\nclass CustomLassoRegression(BaseEstimator, RegressorMixin):\n    \"\"\"\n    A custom Lasso regression class with train and predict methods.\n\n    Attributes:\n    ----------\n    alpha : float, default=1.0\n        The regularization parameter.\n    max_iter : int, default=1000\n        The maximum number of iterations.\n    tol : float, default=1e-4\n        The tolerance for convergence.\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, max_iter: int = 1000, tol: float = 1e-4):\n        \"\"\"\n        Initialize the CustomLassoRegression object.\n\n        Parameters:\n        ----------\n        alpha : float, default=1.0\n            The regularization parameter.\n        max_iter : int, default=1000\n            The maximum number of iterations.\n        tol : float, default=1e-4\n            The tolerance for convergence.\n        \"\"\"\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.model = None\n\n    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"\n        Train the Lasso regression model.\n\n        Parameters:\n        ----------\n        X : np.ndarray\n            The feature matrix.\n        y : np.ndarray\n            The target vector.\n\n        Raises:\n        ------\n        ValueError\n            If the input arrays are not of the correct shape.\n        \"\"\"\n        try:\n            if X.shape[0] != y.shape[0]:\n                raise ValueError(\"The number of samples in X and y must be the same.\")\n\n            self.model = Lasso(alpha=self.alpha, max_iter=self.max_iter, tol=self.tol)\n            self.model.fit(X, y)\n\n        except Exception as e:\n            raise Exception(f\"An error occurred during training: {str(e)}\")\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Make predictions using the trained Lasso regression model.\n\n        Parameters:\n        ----------\n        X : np.ndarray\n            The feature matrix.\n\n        Returns:\n        -------\n        np.ndarray\n            The predicted target values.\n\n        Raises:\n        ------\n        NotFittedError\n            If the model has not been trained yet.\n        \"\"\"\n        try:\n            if self.model is None:\n                raise NotFittedError(\"The model has not been trained yet.\")\n\n            return self.model.predict(X)\n\n        except Exception as e:\n            raise Exception(f\"An error occurred during prediction: {str(e)}\")\n```\n\nThis implementation adheres to the guidelines provided by Yoon-Seo, including the use of annotations for methods, try statements for error handling, and informative feedback in case of exceptions. The class is designed to be maintainable and robust, with clear and concise documentation."
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    \"\"\"Represents a node in the binary tree.\"\"\"\n    \n    def __init__(self, value: int):\n        \"\"\"\n        Initializes a new node with the given value.\n\n        Args:\n        value (int): The value of the node.\n        \"\"\"\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    \"\"\"Represents a binary tree.\"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        Args:\n        value (int): The value of the node to insert.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, current_node: Node, value: int) -> None:\n        \"\"\"\n        Recursively inserts a new node into the binary tree.\n\n        Args:\n        current_node (Node): The current node being processed.\n        value (int): The value of the node to insert.\n        \"\"\"\n        if value < current_node.value:\n            if current_node.left is None:\n                current_node.left = Node(value)\n            else:\n                self._insert_recursive(current_node.left, value)\n        else:\n            if current_node.right is None:\n                current_node.right = Node(value)\n            else:\n                self._insert_recursive(current_node.right, value)\n\n    def delete(self, value: int) -> None:\n        \"\"\"\n        Deletes the node with the given value from the binary tree.\n\n        Args:\n        value (int): The value of the node to delete.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, current_node: Node, value: int) -> Node:\n        \"\"\"\n        Recursively deletes a node from the binary tree.\n\n        Args:\n        current_node (Node): The current node being processed.\n        value (int): The value of the node to delete.\n\n        Returns:\n        Node: The updated current node.\n        \"\"\"\n        if current_node is None:\n            return current_node\n\n        if value < current_node.value:\n            current_node.left = self._delete_recursive(current_node.left, value)\n        elif value > current_node.value:\n            current_node.right = self._delete_recursive(current_node.right, value)\n        else:\n            if current_node.left is None:\n                return current_node.right\n            elif current_node.right is None:\n                return current_node.left\n            else:\n                min_value = self._find_min(current_node.right)\n                current_node.value = min_value\n                current_node.right = self._delete_recursive(current_node.right, min_value)\n\n        return current_node\n\n    def _find_min(self, current_node: Node) -> int:\n        \"\"\"\n        Finds the minimum value in the given subtree.\n\n        Args:\n        current_node (Node): The root of the subtree.\n\n        Returns:\n        int: The minimum value in the subtree.\n        \"\"\"\n        while current_node.left is not None:\n            current_node = current_node.left\n        return current_node.value\n```",
        "```python\nimport numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\n\nclass LassoRegression(BaseEstimator, RegressorMixin):\n    \"\"\"\n    A class for Lasso regression.\n\n    Attributes:\n    ----------\n    alpha : float\n        The regularization parameter.\n    max_iter : int\n        The maximum number of iterations.\n    tol : float\n        The tolerance for convergence.\n    weights : array\n        The weights of the model.\n    \"\"\"\n\n    def __init__(self, alpha=0.1, max_iter=1000, tol=1e-4):\n        \"\"\"\n        Initialize the Lasso regression model.\n\n        Parameters:\n        ----------\n        alpha : float, optional (default=0.1)\n            The regularization parameter.\n        max_iter : int, optional (default=1000)\n            The maximum number of iterations.\n        tol : float, optional (default=1e-4)\n            The tolerance for convergence.\n        \"\"\"\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.weights = None\n\n    def _soft_threshold(self, rho):\n        \"\"\"\n        Soft threshold function.\n\n        Parameters:\n        ----------\n        rho : array\n            The input array.\n\n        Returns:\n        -------\n        array\n            The output array after applying the soft threshold.\n        \"\"\"\n        try:\n            return np.sign(rho) * np.maximum(np.abs(rho) - self.alpha, 0)\n        except Exception as e:\n            raise Exception(f\"Error in _soft_threshold: {str(e)}\")\n\n    def train(self, X, y):\n        \"\"\"\n        Train the Lasso regression model.\n\n        Parameters:\n        ----------\n        X : array\n            The feature matrix.\n        y : array\n            The target vector.\n        \"\"\"\n        try:\n            n_samples, n_features = X.shape\n            self.weights = np.zeros(n_features)\n            X_transpose = X.T\n\n            for _ in range(self.max_iter):\n                prev_weights = self.weights.copy()\n                residuals = y - np.dot(X, self.weights)\n                rho = np.dot(X_transpose, residuals) / n_samples\n                self.weights = self._soft_threshold(prev_weights + rho)\n\n                if np.linalg.norm(self.weights - prev_weights) < self.tol:\n                    break\n\n        except Exception as e:\n            raise Exception(f\"Error in train: {str(e)}\")\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions using the trained Lasso regression model.\n\n        Parameters:\n        ----------\n        X : array\n            The feature matrix.\n\n        Returns:\n        -------\n        array\n            The predicted values.\n        \"\"\"\n        try:\n            if self.weights is None:\n                raise Exception(\"Model not trained. Please call train method first.\")\n\n            return np.dot(X, self.weights)\n\n        except Exception as e:\n            raise Exception(f\"Error in predict: {str(e)}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    def __init__(self, name_j: str):\n        \"\"\"\n        Initialize a Playlist object.\n\n        Args:\n            name_j (str): The name of the playlist.\n        \"\"\"\n        self.name = name_j\n        self.songs = []\n\n    @retry\n    def add_song(self, song_name_j: str, artist_j: str):\n        \"\"\"\n        Add a song to the playlist.\n\n        Args:\n            song_name_j (str): The name of the song.\n            artist_j (str): The artist of the song.\n        \"\"\"\n        self.songs.append((song_name_j, artist_j))\n\n    @retry\n    def remove_song(self, song_name_j: str, artist_j: str):\n        \"\"\"\n        Remove a song from the playlist.\n\n        Args:\n            song_name_j (str): The name of the song.\n            artist_j (str): The artist of the song.\n        \"\"\"\n        self.songs = [(song, artist) for song, artist in self.songs if (song, artist) != (song_name_j, artist_j)]\n```\n\nThis implementation follows the provided guidelines:\n\n- The `@retry` decorator from the `pedantic` module is used for the `add_song` and `remove_song` methods.\n- Function argument names end with `_j` as per the naming convention.\n- The code is written in a clean and readable style, with proper docstrings for methods and the class.",
        "```python\nfrom pedantic import retry\nfrom typing import List\n\n@retry\ndef merge_sorted_lists(list1_j: List[int], list2_j: List[int]) -> List[int]:\n    \"\"\"\n    Merge two sorted lists into a single sorted list.\n\n    Args:\n        list1_j (List[int]): The first sorted list.\n        list2_j (List[int]): The second sorted list.\n\n    Returns:\n        List[int]: A new sorted list containing all elements from both input lists.\n    \"\"\"\n    result = []\n    i, j = 0, 0\n\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result.append(list1_j[i])\n            i += 1\n        else:\n            result.append(list2_j[j])\n            j += 1\n\n    result.extend(list1_j[i:])\n    result.extend(list2_j[j:])\n\n    return result\n```\n\nThis function follows the provided coding guidelines by:\n\n* Using the `@retry` decorator from the `pedantic` module for additional robustness.\n* Ending function argument names with `_j` for consistency and visual cues.\n* Maintaining clean and readable code through proper indentation, spacing, and documentation."
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Song:\n    \"\"\"Represents a song with a title and an artist.\"\"\"\n    def __init__(self, title, artist):\n        \"\"\"\n        Initializes a Song object.\n\n        Args:\n            title (str): The title of the song.\n            artist (str): The artist of the song.\n        \"\"\"\n        self.title = title\n        self.artist = artist\n\n    def __str__(self):\n        \"\"\"Returns a string representation of the song.\"\"\"\n        return f\"{self.title} by {self.artist}\"\n\n    def __eq__(self, other):\n        \"\"\"Checks if two songs are equal based on their title and artist.\"\"\"\n        return self.title == other.title and self.artist == other.artist\n\n\nclass Playlist:\n    \"\"\"Represents a playlist with a list of songs.\"\"\"\n    def __init__(self, name):\n        \"\"\"\n        Initializes a Playlist object.\n\n        Args:\n            name (str): The name of the playlist.\n        \"\"\"\n        self.name = name\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        \"\"\"\n        Adds a song to the playlist.\n\n        Args:\n            song (Song): The song to add to the playlist.\n\n        Raises:\n            ValueError: If the song is already in the playlist.\n        \"\"\"\n        if song in self.songs:\n            raise ValueError(\"Song is already in the playlist\")\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        \"\"\"\n        Removes a song from the playlist.\n\n        Args:\n            song (Song): The song to remove from the playlist.\n\n        Raises:\n            ValueError: If the song is not in the playlist.\n        \"\"\"\n        if song not in self.songs:\n            raise ValueError(\"Song is not in the playlist\")\n        self.songs.remove(song)\n```",
        "```python\ndef merge_sorted_lists(list1_j, list2_j):\n    \"\"\"\n    Merge two sorted lists into a single sorted list.\n\n    Args:\n        list1_j (list): The first sorted list.\n        list2_j (list): The second sorted list.\n\n    Returns:\n        list: A new sorted list containing all elements from both input lists.\n    \"\"\"\n    result = []\n    i, j = 0, 0\n\n    # Merge smaller elements first\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result.append(list1_j[i])\n            i += 1\n        else:\n            result.append(list2_j[j])\n            j += 1\n\n    # Append any remaining elements\n    result.extend(list1_j[i:])\n    result.extend(list2_j[j:])\n\n    return result\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs, count_calls\n\nclass StudentGrade:\n    \"\"\"\n    A class to represent a student's grade.\n\n    Attributes:\n    grades (list): A list of grades.\n    \"\"\"\n\n    def __init__(self, grades):\n        \"\"\"\n        Initializes the StudentGrade object.\n\n        Args:\n        grades (list): A list of grades.\n        \"\"\"\n        self.grades = grades\n\n    @require_kwargs\n    @count_calls\n    def compute_average_grade(self, *, grades=None):\n        \"\"\"\n        Computes the average grade.\n\n        Args:\n        grades (list, optional): A list of grades. Defaults to None.\n\n        Returns:\n        float: The average grade.\n\n        Raises:\n        ValueError: If no grades are provided.\n        \"\"\"\n        if grades is None:\n            grades = self.grades\n\n        if not grades:\n            raise ValueError(\"No grades provided\")\n\n        return sum(grades) / len(grades)\n```\n\nThis implementation adheres to the guidelines provided by the mentor, including the use of the `@require_kwargs` and `@count_calls` decorators from the `pedantic` module. The `compute_average_grade` method is designed to be flexible, allowing the user to pass in a custom list of grades or use the grades stored in the object. The method also includes input validation to ensure that at least one grade is provided.",
        "```python\nfrom pedantic import require_kwargs, count_calls\nimport numpy as np\n\nclass LinearRegression:\n    \"\"\"\n    A simple linear regression model.\n\n    Attributes:\n    -----------\n    weights (np.ndarray): The model's weights.\n    bias (float): The model's bias term.\n    \"\"\"\n\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    @require_kwargs\n    @count_calls\n    def train(self, *, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, num_iterations: int = 1000) -> None:\n        \"\"\"\n        Train the linear regression model.\n\n        Parameters:\n        -----------\n        X (np.ndarray): The training data's features.\n        y (np.ndarray): The training data's target variable.\n        learning_rate (float): The learning rate for gradient descent. Defaults to 0.01.\n        num_iterations (int): The number of iterations for gradient descent. Defaults to 1000.\n        \"\"\"\n        num_samples, num_features = X.shape\n        self.weights = np.zeros(num_features)\n        self.bias = 0\n\n        for _ in range(num_iterations):\n            y_predicted = np.dot(X, self.weights) + self.bias\n            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / num_samples) * np.sum(y_predicted - y)\n\n            self.weights -= learning_rate * dw\n            self.bias -= learning_rate * db\n\n    @require_kwargs\n    @count_calls\n    def predict(self, *, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Make predictions using the trained linear regression model.\n\n        Parameters:\n        -----------\n        X (np.ndarray): The data to make predictions on.\n\n        Returns:\n        --------\n        np.ndarray: The predicted values.\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n```\n\nThis implementation follows the guidelines provided by the mentor, including the use of the `@require_kwargs` and `@count_calls` decorators from the `pedantic` module. The `train` method takes in the training data, learning rate, and number of iterations as keyword-only arguments, and the `predict` method takes in the data to make predictions on as a keyword-only argument. The class also includes docstrings to provide documentation for the methods and attributes."
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import require_kwargs\nfrom typing import List\n\nclass StudentGrade:\n    \"\"\"\n    A class to represent a student's grades.\n\n    Attributes:\n    ----------\n    grades : List[float]\n        A list of grades obtained by the student.\n    \"\"\"\n\n    def __init__(self, grades: List[float]) -> None:\n        \"\"\"\n        Initializes the StudentGrade object.\n\n        Args:\n        ----\n        grades (List[float]): A list of grades obtained by the student.\n        \"\"\"\n        self.grades = grades\n\n    @require_kwargs\n    def compute_average_grade(self, *, rounding_precision: int = 2) -> float:\n        \"\"\"\n        Computes the average grade of the student.\n\n        Args:\n        ----\n        rounding_precision (int): The number of decimal places to round the average grade to. Defaults to 2.\n\n        Returns:\n        -------\n        float: The average grade of the student.\n        \"\"\"\n        if not self.grades:\n            raise ValueError(\"Cannot compute average grade for an empty list of grades\")\n\n        average_grade = sum(self.grades) / len(self.grades)\n        return round(average_grade, rounding_precision)\n```\n\nThis code defines a `StudentGrade` class with a method `compute_average_grade` that calculates the average grade of a student. The `@require_kwargs` decorator is used to ensure that all arguments are passed as keyword arguments. The method also includes input validation to handle the case where the list of grades is empty. The average grade is rounded to the specified precision before being returned.",
        "```python\nimport numpy as np\nfrom pedantic import count_calls\n\nclass LinearRegression:\n    \"\"\"\n    A class representing a Linear Regression model.\n\n    Attributes:\n    -----------\n    weights (numpy array): The weights of the model.\n    bias (float): The bias term of the model.\n    learning_rate (float): The learning rate for gradient descent.\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001):\n        \"\"\"\n        Initializes the Linear Regression model.\n\n        Parameters:\n        -----------\n        learning_rate (float, optional): The learning rate for gradient descent. Defaults to 0.001.\n        \"\"\"\n        self.weights = None\n        self.bias = None\n        self.learning_rate = learning_rate\n\n    @count_calls\n    def _mean_squared_error(self, y, y_pred):\n        \"\"\"\n        Calculates the mean squared error.\n\n        Parameters:\n        -----------\n        y (numpy array): The actual values.\n        y_pred (numpy array): The predicted values.\n\n        Returns:\n        --------\n        float: The mean squared error.\n        \"\"\"\n        return np.mean((y - y_pred) ** 2)\n\n    @count_calls\n    def _compute_gradients(self, X, y, y_pred):\n        \"\"\"\n        Computes the gradients of the weights and bias.\n\n        Parameters:\n        -----------\n        X (numpy array): The input features.\n        y (numpy array): The actual values.\n        y_pred (numpy array): The predicted values.\n\n        Returns:\n        --------\n        tuple: A tuple containing the gradients of the weights and bias.\n        \"\"\"\n        dw = -2 * np.dot(X.T, (y - y_pred)) / len(y)\n        db = -2 * np.sum(y - y_pred) / len(y)\n        return dw, db\n\n    @count_calls\n    def train(self, X, y, epochs=1000):\n        \"\"\"\n        Trains the Linear Regression model.\n\n        Parameters:\n        -----------\n        X (numpy array): The input features.\n        y (numpy array): The actual values.\n        epochs (int, optional): The number of epochs. Defaults to 1000.\n        \"\"\"\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for _ in range(epochs):\n            y_pred = np.dot(X, self.weights) + self.bias\n            dw, db = self._compute_gradients(X, y, y_pred)\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n    @count_calls\n    def predict(self, X):\n        \"\"\"\n        Makes predictions using the trained model.\n\n        Parameters:\n        -----------\n        X (numpy array): The input features.\n\n        Returns:\n        --------\n        numpy array: The predicted values.\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom typing import List\nfrom functools import wraps\nfrom pedantic import retry, timer_class, require_kwargs, count_calls\n\ndef retry_decorator(func):\n    @wraps(func)\n    @retry\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n    return wrapper\n\n@timer_class\nclass KthLargestElement:\n    @retry_decorator\n    @require_kwargs\n    @count_calls\n    def find_kth_largest(self, *, nums: List[int], k: int) -> int:\n        assert isinstance(nums, list) and all(isinstance(num, int) for num in nums)\n        assert isinstance(k, int) and 1 <= k <= len(nums)\n        \n        nums.sort(reverse=True)\n        return nums[k-1]\n```\n\nThis function is implemented as a class method with the `@timer_class` decorator to track execution time. The `find_kth_largest` method is decorated with `@retry_decorator` to provide retry functionality, `@require_kwargs` to ensure all required keyword arguments are provided, and `@count_calls` to track the number of method invocations. The method takes a list of integers `nums` and an integer `k` as input, sorts the list in descending order, and returns the kth largest element. The `assert` statements validate the input types and values.",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\nclass Heap:\n    @timer_class\n    def __init__(self, x_jinitial_elements=None):\n        self.elements = []\n        if x_jinitial_elements is not None:\n            for element in x_jinitial_elements:\n                self.insert(element)\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def insert(self, x_jelement):\n        \"\"\"Insert an element into the heap.\"\"\"\n        assert isinstance(x_jelement, (int, float)), \"Element must be a number\"\n        self.elements.append(x_jelement)\n        self.heapify_up(len(self.elements) - 1)\n\n    def heapify_up(self, index):\n        \"\"\"Heapify the heap upwards from the given index.\"\"\"\n        parent_index = (index - 1) // 2\n        if index > 0 and self.elements[parent_index] < self.elements[index]:\n            self.elements[parent_index], self.elements[index] = self.elements[index], self.elements[parent_index]\n            self.heapify_up(parent_index)\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def heapify(self, x_jindex):\n        \"\"\"Heapify the heap downwards from the given index.\"\"\"\n        assert 0 <= x_jindex < len(self.elements), \"Index out of range\"\n        left_child_index = 2 * x_jindex + 1\n        right_child_index = 2 * x_jindex + 2\n        largest_index = x_jindex\n        if left_child_index < len(self.elements) and self.elements[left_child_index] > self.elements[largest_index]:\n            largest_index = left_child_index\n        if right_child_index < len(self.elements) and self.elements[right_child_index] > self.elements[largest_index]:\n            largest_index = right_child_index\n        if largest_index != x_jindex:\n            self.elements[x_jindex], self.elements[largest_index] = self.elements[largest_index], self.elements[x_jindex]\n            self.heapify(largest_index)\n```\n\nNote: I have followed all the guidelines provided by the mentor, including the use of the `pedantic` module, the `@timer_class`, `@retry`, `@require_kwargs`, and `@count_calls` decorators, and the naming conventions (e.g., `x_jelement` and `x_jindex`). I have also included assert statements to validate the input and output of the methods.",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\nfrom typing import List, Tuple\n\nclass LinearRegression:\n    @timer_class\n    def __init__(self, learning_rate_j: float = 0.001, n_iters_j: int = 1000):\n        self.lr_j = learning_rate_j\n        self.n_iters_j = n_iters_j\n        self.activation_j = None\n        self.weights_j = None\n        self.bias_j = None\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def train(self, X_j: List[List[float]], y_j: List[float]) -> None:\n        n_samples_j, n_features_j = len(X_j), len(X_j[0])\n        self.weights_j = [0.0 for _ in range(n_features_j)]\n        self.bias_j = 0.0\n\n        assert len(X_j) == len(y_j), \"Number of training examples must match number of labels\"\n\n        for _ in range(self.n_iters_j):\n            for idx_j, x_i_j in enumerate(X_j):\n                linear_model_j = sum([x_j * w_j for x_j, w_j in zip(x_i_j, self.weights_j)]) + self.bias_j\n                y_predicted_j = linear_model_j\n                update_j = (y_j[idx_j] - y_predicted_j) * self.lr_j\n                self.weights_j = [w_j + update_j * x_j for w_j, x_j in zip(self.weights_j, x_i_j)]\n                self.bias_j += update_j\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def predict(self, X_j: List[List[float]]) -> List[float]:\n        assert self.weights_j is not None and self.bias_j is not None, \"Model must be trained before making predictions\"\n        return [sum([x_j * w_j for x_j, w_j in zip(x_i_j, self.weights_j)]) + self.bias_j for x_i_j in X_j]\n```",
        "```python\nfrom typing import List\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\n@timer_class\nclass VectorOperations:\n    @retry\n    @require_kwargs\n    @count_calls\n    def dot_product(self, vector1_j: List[float], vector2_j: List[float]) -> float:\n        \"\"\"\n        Compute the dot product of two vectors.\n\n        Args:\n        vector1_j (List[float]): The first vector.\n        vector2_j (List[float]): The second vector.\n\n        Returns:\n        float: The dot product of the two vectors.\n        \"\"\"\n        assert len(vector1_j) == len(vector2_j), \"Both vectors must have the same length.\"\n        \n        result = 0\n        for i in range(len(vector1_j)):\n            result += vector1_j[i] * vector2_j[i]\n        \n        return result\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\nimport numpy as np\n\nclass LassoRegression:\n    @timer_class\n    def __init__(self, x_j_alpha: float = 1.0, x_j_max_iter: int = 1000, x_j_tol: float = 1e-4, x_j_warm_start: bool = False):\n        \"\"\"\n        Lasso regression class.\n\n        Parameters:\n        x_j_alpha (float): The regularization parameter.\n        x_j_max_iter (int): The maximum number of iterations.\n        x_j_tol (float): The tolerance for convergence.\n        x_j_warm_start (bool): Whether to reuse the solution of the previous fit as initialization for the next fit.\n        \"\"\"\n        self.x_j_alpha = x_j_alpha\n        self.x_j_max_iter = x_j_max_iter\n        self.x_j_tol = x_j_tol\n        self.x_j_warm_start = x_j_warm_start\n        self.model = Lasso(alpha=self.x_j_alpha, max_iter=self.x_j_max_iter, tol=self.x_j_tol, warm_start=self.x_j_warm_start)\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def train(self, x_j_X: np.ndarray, x_j_y: np.ndarray) -> None:\n        \"\"\"\n        Train the Lasso regression model.\n\n        Parameters:\n        x_j_X (np.ndarray): The feature matrix.\n        x_j_y (np.ndarray): The target vector.\n        \"\"\"\n        assert isinstance(x_j_X, np.ndarray), \"X must be a numpy array\"\n        assert isinstance(x_j_y, np.ndarray), \"y must be a numpy array\"\n        self.model.fit(x_j_X, x_j_y)\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def predict(self, x_j_X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Make predictions using the trained Lasso regression model.\n\n        Parameters:\n        x_j_X (np.ndarray): The feature matrix.\n\n        Returns:\n        np.ndarray: The predicted values.\n        \"\"\"\n        assert isinstance(x_j_X, np.ndarray), \"X must be a numpy array\"\n        return self.model.predict(x_j_X)\n```",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\nfrom typing import List, Tuple\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    A class for performing Quadratic Discriminant Analysis (QDA).\n\n    Attributes:\n    -----------\n    x_g_j : List[List[float]]\n        The input features.\n    y_g_j : List[int]\n        The target variable.\n    \"\"\"\n\n    @timer_class\n    def __init__(self, x_g_j: List[List[float]], y_g_j: List[int]) -> None:\n        \"\"\"\n        Initializes the QDA class.\n\n        Parameters:\n        -----------\n        x_g_j : List[List[float]]\n            The input features.\n        y_g_j : List[int]\n            The target variable.\n        \"\"\"\n        self.x_g_j = x_g_j\n        self.y_g_j = y_g_j\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def calculate_covariance(self, x_g_j: List[List[float]]) -> List[List[float]]:\n        \"\"\"\n        Calculates the covariance matrix.\n\n        Parameters:\n        -----------\n        x_g_j : List[List[float]]\n            The input features.\n\n        Returns:\n        --------\n        List[List[float]]\n            The covariance matrix.\n        \"\"\"\n        # Calculate the covariance matrix\n        covariance_matrix = [[0.0 for _ in range(len(x_g_j[0]))] for _ in range(len(x_g_j[0]))]\n        for i in range(len(x_g_j)):\n            for j in range(len(x_g_j[0])):\n                for k in range(len(x_g_j[0])):\n                    covariance_matrix[j][k] += (x_g_j[i][j] - sum([x_g_j[m][j] for m in range(len(x_g_j))]) / len(x_g_j)) * (x_g_j[i][k] - sum([x_g_j[m][k] for m in range(len(x_g_j))]) / len(x_g_j))\n        for j in range(len(x_g_j[0])):\n            for k in range(len(x_g_j[0])):\n                covariance_matrix[j][k] /= len(x_g_j)\n        return covariance_matrix\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def calculate_mean(self, x_g_j: List[List[float]]) -> List[float]:\n        \"\"\"\n        Calculates the mean vector.\n\n        Parameters:\n        -----------\n        x_g_j : List[List[float]]\n            The input features.\n\n        Returns:\n        --------\n        List[float]\n            The mean vector.\n        \"\"\"\n        # Calculate the mean vector\n        mean_vector = [0.0 for _ in range(len(x_g_j[0]))]\n        for i in range(len(x_g_j)):\n            for j in range(len(x_g_j[0])):\n                mean_vector[j] += x_g_j[i][j]\n        for j in range(len(x_g_j[0])):\n            mean_vector[j] /= len(x_g_j)\n        return mean_vector\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def calculate_discriminant(self, x_g_j: List[List[float]], y_g_j: List[int]) -> Tuple[List[List[float]], List[float]]:\n        \"\"\"\n        Calculates the discriminant.\n\n        Parameters:\n        -----------\n        x_g_j : List[List[float]]\n            The input features.\n        y_g_j : List[int]\n            The target variable.\n\n        Returns:\n        --------\n        Tuple[List[List[float]], List[float]]\n            The discriminant.\n        \"\"\"\n        # Calculate the discriminant\n        covariance_matrix = self.calculate_covariance(x_g_j)\n        mean_vector = self.calculate_mean(x_g_j)\n        discriminant = [[0.0 for _ in range(len(x_g_j[0]))] for _ in range(len(x_g_j[0]))]\n        for i in range(len(x_g_j)):\n            for j in range(len(x_g_j[0])):\n                for k in range(len(x_g_j[0])):\n                    discriminant[j][k] += (x_g_j[i][j] - mean_vector[j]) * (x_g_j[i][k] - mean_vector[k]) * y_g_j[i]\n        for j in range(len(x_g_j[0])):\n            for k in range(len(x_g_j[0])):\n                discriminant[j][k] /= len(x_g_j)\n        return discriminant, mean_vector\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def predict(self, x_g_j: List[List[float]]) -> List[int]:\n        \"\"\"\n        Makes predictions.\n\n        Parameters:\n        -----------\n        x_g_j : List[List[float]]\n            The input features.\n\n        Returns:\n        --------\n        List[int]\n            The predicted target variable.\n        \"\"\"\n        # Make predictions\n        discriminant, mean_vector = self.calculate_discriminant(self.x_g_j, self.y_g_j)\n        predictions = [0 for _ in range(len(x_g_j))]\n        for i in range(len(x_g_j)):\n            for j in range(len(x_g_j[0])):\n                for k in range(len(x_g_j[0])):\n                    predictions[i] += (x_g_j[i][j] - mean_vector[j]) * (x_g_j[i][k] - mean_vector[k]) * discriminant[j][k]\n        return predictions\n```",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\nclass Node:\n    \"\"\"Represents a node in the binary tree.\"\"\"\n    \n    @timer_class\n    def __init__(self, value_j, left_j=None, right_j=None):\n        \"\"\"Initializes a Node with a value and optional left and right children.\"\"\"\n        self.value_j = value_j\n        self.left_j = left_j\n        self.right_j = right_j\n\nclass BinaryTree:\n    \"\"\"Represents a binary tree.\"\"\"\n    \n    @timer_class\n    def __init__(self):\n        \"\"\"Initializes an empty binary tree.\"\"\"\n        self.root_j = None\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def insert(self, value_j, *args_j, **kwargs_j):\n        \"\"\"Inserts a new node with the given value into the binary tree.\"\"\"\n        if not self.root_j:\n            self.root_j = Node(value_j)\n        else:\n            self._insert_recursive(self.root_j, value_j)\n\n    def _insert_recursive(self, current_node_j, value_j):\n        \"\"\"Recursively inserts a new node into the binary tree.\"\"\"\n        if value_j < current_node_j.value_j:\n            if current_node_j.left_j is None:\n                current_node_j.left_j = Node(value_j)\n            else:\n                self._insert_recursive(current_node_j.left_j, value_j)\n        else:\n            if current_node_j.right_j is None:\n                current_node_j.right_j = Node(value_j)\n            else:\n                self._insert_recursive(current_node_j.right_j, value_j)\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def delete(self, value_j, *args_j, **kwargs_j):\n        \"\"\"Deletes the node with the given value from the binary tree.\"\"\"\n        self.root_j = self._delete_recursive(self.root_j, value_j)\n\n    def _delete_recursive(self, current_node_j, value_j):\n        \"\"\"Recursively deletes a node from the binary tree.\"\"\"\n        if current_node_j is None:\n            return current_node_j\n        if value_j < current_node_j.value_j:\n            current_node_j.left_j = self._delete_recursive(current_node_j.left_j, value_j)\n        elif value_j > current_node_j.value_j:\n            current_node_j.right_j = self._delete_recursive(current_node_j.right_j, value_j)\n        else:\n            if current_node_j.left_j is None:\n                return current_node_j.right_j\n            elif current_node_j.right_j is None:\n                return current_node_j.left_j\n            else:\n                min_value_j = self._find_min(current_node_j.right_j)\n                current_node_j.value_j = min_value_j\n                current_node_j.right_j = self._delete_recursive(current_node_j.right_j, min_value_j)\n        return current_node_j\n\n    def _find_min(self, current_node_j):\n        \"\"\"Finds the minimum value in the binary tree.\"\"\"\n        while current_node_j.left_j is not None:\n            current_node_j = current_node_j.left_j\n        return current_node_j.value_j\n```\n\nThis implementation follows the provided guidelines, including the use of the `pedantic` module for decorators, consistent naming conventions, and proper error handling. The `BinaryTree` class includes methods for inserting and deleting nodes, and the `Node` class represents individual nodes in the tree. The `insert` and `delete` methods are decorated with `@retry`, `@require_kwargs`, and `@count_calls` to ensure robustness and maintainability.",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\nclass StudentGrade:\n    @timer_class\n    def __init__(self, x_grades_j: list):\n        self.x_grades_j = x_grades_j\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def compute_average_grade(self, *, x_grades_j: list = None) -> float:\n        \"\"\"Compute the average grade of a student.\"\"\"\n        assert isinstance(self.x_grades_j, list), \"Grades must be a list\"\n        assert all(isinstance(grade, (int, float)) for grade in self.x_grades_j), \"All grades must be numbers\"\n        \n        if x_grades_j is None:\n            grades = self.x_grades_j\n        else:\n            grades = x_grades_j\n        \n        return sum(grades) / len(grades)\n```\n\nThis code adheres to the provided guidelines, including the use of the `pedantic` module, proper type hints, and the inclusion of assert statements for input validation. The `compute_average_grade` method is also decorated with `@retry`, `@require_kwargs`, and `@count_calls` as per the mentor's instructions. The class itself is decorated with `@timer_class`. The method name and variable names follow the specified conventions.",
        "```python\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\nclass Playlist:\n    @timer_class\n    def __init__(self, x_name_j: str):\n        \"\"\"\n        Initialize a Playlist object.\n\n        Args:\n        x_name_j (str): The name of the playlist.\n        \"\"\"\n        self.x_name_j = x_name_j\n        self.x_songs_j = []\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def add_song(self, *, x_song_name_j: str, x_artist_j: str) -> None:\n        \"\"\"\n        Add a song to the playlist.\n\n        Args:\n        x_song_name_j (str): The name of the song.\n        x_artist_j (str): The artist of the song.\n        \"\"\"\n        assert isinstance(x_song_name_j, str), \"Song name must be a string.\"\n        assert isinstance(x_artist_j, str), \"Artist must be a string.\"\n        self.x_songs_j.append({\"x_song_name_j\": x_song_name_j, \"x_artist_j\": x_artist_j})\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def remove_song(self, *, x_song_name_j: str) -> None:\n        \"\"\"\n        Remove a song from the playlist.\n\n        Args:\n        x_song_name_j (str): The name of the song to remove.\n        \"\"\"\n        assert isinstance(x_song_name_j, str), \"Song name must be a string.\"\n        self.x_songs_j = [song for song in self.x_songs_j if song[\"x_song_name_j\"] != x_song_name_j]\n```",
        "```python\nfrom typing import List\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\nimport numpy as np\n\nclass NearestNeighborsModel:\n    \"\"\"A class representing a Nearest Neighbors model.\"\"\"\n    \n    @timer_class\n    def __init__(self, x_j_k: int = 5, x_j_metric: str = 'euclidean'):\n        \"\"\"\n        Initialize the Nearest Neighbors model.\n\n        Args:\n        x_j_k (int): The number of nearest neighbors to consider.\n        x_j_metric (str): The distance metric to use.\n        \"\"\"\n        self.x_j_k = x_j_k\n        self.x_j_metric = x_j_metric\n        self.x_j_X_train = None\n        self.x_j_y_train = None\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def train(self, x_j_X_train: np.ndarray, x_j_y_train: np.ndarray) -> None:\n        \"\"\"\n        Train the Nearest Neighbors model.\n\n        Args:\n        x_j_X_train (np.ndarray): The training data.\n        x_j_y_train (np.ndarray): The training labels.\n        \"\"\"\n        self.x_j_X_train = x_j_X_train\n        self.x_j_y_train = x_j_y_train\n\n    @retry\n    @require_kwargs\n    @count_calls\n    def predict(self, x_j_X_test: np.ndarray) -> List[int]:\n        \"\"\"\n        Make predictions using the Nearest Neighbors model.\n\n        Args:\n        x_j_X_test (np.ndarray): The testing data.\n\n        Returns:\n        List[int]: The predicted labels.\n        \"\"\"\n        predictions = []\n        for x_j_x in x_j_X_test:\n            distances = []\n            for x_j_i, x_j_x_train in enumerate(self.x_j_X_train):\n                distance = np.linalg.norm(x_j_x - x_j_x_train)\n                distances.append((distance, self.x_j_y_train[x_j_i]))\n            distances.sort(key=lambda x_j_x: x_j_x[0])\n            neighbors = [x_j_x[1] for x_j_x in distances[:self.x_j_k]]\n            prediction = max(set(neighbors), key=neighbors.count)\n            predictions.append(prediction)\n        return predictions\n```",
        "```python\nfrom typing import List\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\n@timer_class\nclass MatrixOperations:\n    @retry\n    @require_kwargs\n    @count_calls\n    def compute_diagonal_average(self, matrix: List[List[float]], **kwargs) -> float:\n        \"\"\"\n        This function computes the average of the diagonal elements of a matrix.\n\n        Args:\n            matrix (List[List[float]]): A 2D list representing the matrix.\n\n        Returns:\n            float: The average of the diagonal elements.\n        \"\"\"\n        assert len(matrix) == len(matrix[0]), \"Matrix must be square\"\n        \n        diagonal_sum = 0\n        for i in range(len(matrix)):\n            diagonal_sum += matrix[i][i]\n        \n        return diagonal_sum / len(matrix)\n```\n\nNote: I have followed the coding guidelines provided, including the use of type hints, docstrings, and decorators. I have also used the `pedantic` module as specified in the guidelines. The function is designed to be robust and maintainable, with error handling and input validation.",
        "```python\nfrom typing import List\nfrom pedantic import timer_class, retry, require_kwargs, count_calls\n\n@timer_class\nclass ListMerger:\n    @retry\n    @require_kwargs\n    @count_calls\n    def merge_sorted_lists(self, list1_j: List[int], list2_j: List[int]) -> List[int]:\n        \"\"\"Merge two sorted lists into a single sorted list.\"\"\"\n        assert isinstance(list1_j, list) and isinstance(list2_j, list), \"Both inputs must be lists.\"\n        assert all(isinstance(x, int) for x in list1_j) and all(isinstance(x, int) for x in list2_j), \"Both lists must contain only integers.\"\n        \n        merged_list = []\n        i, j = 0, 0\n        \n        while i < len(list1_j) and j < len(list2_j):\n            if list1_j[i] < list2_j[j]:\n                merged_list.append(list1_j[i])\n                i += 1\n            else:\n                merged_list.append(list2_j[j])\n                j += 1\n        \n        merged_list.extend(list1_j[i:])\n        merged_list.extend(list2_j[j:])\n        \n        return merged_list\n```"
      ]
    }
  ]
}