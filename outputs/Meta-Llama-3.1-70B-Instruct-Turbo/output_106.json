{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef a_compute_h_index(citations):\n    \"\"\"\n    Compute the h-index given an array of integer citations.\n\n    The h-index is a metric that attempts to measure the productivity and citation impact of the publication of a scholar.\n    A scholar with an index of h has published at least h papers with at least h citations each.\n\n    :param citations: A list of integers representing the number of citations for each paper.\n    :return: The h-index of the given citations.\n    \"\"\"\n    citations.sort(reverse=True)\n    for i, citation in enumerate(citations):\n        if citation < i + 1:\n            return i\n    return len(citations)\n```\n\nThis function sorts the citations in descending order and then iterates over them. The h-index is the maximum number of papers that have at least as many citations as their index (1-indexed). If a paper has fewer citations than its index, the function returns the index of the previous paper, which is the h-index. If no such paper is found, the function returns the total number of papers, which is the h-index in this case."
      ],
      "instruction_model_output": [
        "```python\ndef a_compute_h_index(citations):\n    \"\"\"\n    Compute the h-index given an array of integer citations.\n\n    The h-index is a metric that attempts to measure the productivity and citation impact of the publication of a scholar.\n    A scholar with an h-index of n has published n papers each of which has been cited at least n times.\n\n    :param citations: A list of integers representing the number of citations for each paper.\n    :return: The h-index of the given citations.\n    \"\"\"\n    # Sort the citations in descending order\n    citations.sort(reverse=True)\n\n    # Initialize the h-index to 0\n    h_index = 0\n\n    # Iterate over the sorted citations\n    for i, citation in enumerate(citations):\n        # If the current citation is greater than or equal to the current index + 1\n        if citation >= i + 1:\n            # Update the h-index\n            h_index = i + 1\n        else:\n            # If the current citation is less than the current index + 1, break the loop\n            break\n\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef y_compute_h_index(citations):\n    \"\"\"\n    Computes the h-index given an array of integer citations.\n\n    The h-index is a metric that attempts to measure the productivity and citation impact of the publication output of a scientist or scholar.\n\n    :param citations: A list of integers representing the number of citations for each publication.\n    :return: The h-index of the given citations.\n    \"\"\"\n    citations.sort(reverse=True)\n    for i, citation in enumerate(citations):\n        if citation < i + 1:\n            return i\n    return len(citations)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef y_compute_h_index(citations):\n    \"\"\"\n    Computes the h-index given an array of integer citations.\n\n    The h-index is a metric that attempts to measure the productivity and citation impact of the publication output of a scientist or scholar.\n\n    :param citations: A list of integers representing the number of citations for each publication.\n    :return: The h-index of the given citations.\n    \"\"\"\n    # First, sort the citations in descending order\n    citations.sort(reverse=True)\n    \n    # Initialize the h-index to 0\n    h_index = 0\n    \n    # Iterate over the sorted citations\n    for i, citation in enumerate(citations):\n        # If the current citation count is greater than or equal to the current index + 1,\n        # update the h-index\n        if citation >= i + 1:\n            h_index = i + 1\n        # If the current citation count is less than the current index + 1,\n        # we can break the loop because the h-index cannot increase further\n        else:\n            break\n    \n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\n\"\"\"\nQuadratic Discriminant Analysis (QDA) class.\n\nThis class implements the Quadratic Discriminant Analysis algorithm for classification tasks.\nIt provides methods for training the model and making predictions.\n\nAuthor: Kiyotaka\n\"\"\"\n\nfrom pedantic import timer_class\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class.\n\n    Attributes:\n    -----------\n    priors : array-like, shape (n_classes,)\n        Prior probabilities of each class.\n    means : array-like, shape (n_classes, n_features)\n        Mean vectors of each class.\n    covariances : array-like, shape (n_classes, n_features, n_features)\n        Covariance matrices of each class.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the QDA model.\n        \"\"\"\n        self.priors = None\n        self.means = None\n        self.covariances = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits the QDA model to the training data.\n\n        Parameters:\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns:\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        # Initialize priors, means, and covariances\n        self.priors = np.zeros(n_classes)\n        self.means = np.zeros((n_classes, n_features))\n        self.covariances = np.zeros((n_classes, n_features, n_features))\n\n        # Calculate priors, means, and covariances for each class\n        for i in range(n_classes):\n            X_class = X[y == i]\n            self.priors[i] = X_class.shape[0] / n_samples\n            self.means[i] = np.mean(X_class, axis=0)\n            self.covariances[i] = np.cov(X_class, rowvar=False)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Makes predictions using the QDA model.\n\n        Parameters:\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Test data.\n\n        Returns:\n        -------\n        y_pred : array-like, shape (n_samples,)\n            Predicted target values.\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(self.priors)\n\n        # Initialize predicted target values\n        y_pred = np.zeros(n_samples)\n\n        # Calculate posterior probabilities for each class\n        for i in range(n_samples):\n            posterior_probabilities = np.zeros(n_classes)\n            for j in range(n_classes):\n                posterior_probabilities[j] = self._calculate_posterior_probability(X[i], j)\n            y_pred[i] = np.argmax(posterior_probabilities)\n\n        return y_pred\n\n    def _calculate_posterior_probability(self, x, class_index):\n        \"\"\"\n        Calculates the posterior probability of a sample belonging to a class.\n\n        Parameters:\n        ----------\n        x : array-like, shape (n_features,)\n            Sample.\n        class_index : int\n            Index of the class.\n\n        Returns:\n        -------\n        posterior_probability : float\n            Posterior probability of the sample belonging to the class.\n        \"\"\"\n        prior = self.priors[class_index]\n        mean = self.means[class_index]\n        covariance = self.covariances[class_index]\n\n        # Calculate the likelihood using the multivariate normal distribution\n        likelihood = multivariate_normal.pdf(x, mean=mean, cov=covariance)\n\n        # Calculate the posterior probability using Bayes' theorem\n        posterior_probability = likelihood * prior\n\n        return posterior_probability\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\nfrom scipy.linalg import inv\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) is a classification technique that \n    uses the Bayes' theorem to classify data or make probabilistic predictions \n    with an unknown input. It assumes that the data follows a multivariate \n    normal distribution and that the covariance matrix is different for each class.\n\n    Attributes:\n    -----------\n    priors : array-like, shape (n_classes,)\n        The prior probabilities of the classes.\n    means : array-like, shape (n_classes, n_features)\n        The mean vectors of the classes.\n    covariances : array-like, shape (n_classes, n_features, n_features)\n        The covariance matrices of the classes.\n    \"\"\"\n\n    def __init__(self):\n        self.priors = None\n        self.means = None\n        self.covariances = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data.\n\n        Parameters:\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training data.\n        y : array-like, shape (n_samples,)\n            The target values.\n\n        Returns:\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        # Calculate the prior probabilities of the classes\n        self.priors = np.array([np.mean(y == i) for i in range(n_classes)])\n\n        # Calculate the mean vectors of the classes\n        self.means = np.array([np.mean(X[y == i], axis=0) for i in range(n_classes)])\n\n        # Calculate the covariance matrices of the classes\n        self.covariances = np.array([np.cov(X[y == i], rowvar=False) for i in range(n_classes)])\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict the target values for the given data.\n\n        Parameters:\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data to predict.\n\n        Returns:\n        -------\n        y_pred : array-like, shape (n_samples,)\n            The predicted target values.\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(self.priors)\n\n        # Calculate the log-likelihoods of the classes\n        log_likelihoods = np.zeros((n_samples, n_classes))\n        for i in range(n_classes):\n            log_likelihoods[:, i] = np.log(self.priors[i]) - 0.5 * np.log(np.linalg.det(self.covariances[i])) - 0.5 * np.sum((X - self.means[i]) @ inv(self.covariances[i]) * (X - self.means[i]), axis=1)\n\n        # Predict the target values\n        y_pred = np.argmax(log_likelihoods, axis=1)\n\n        return y_pred\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict the probabilities of the target values for the given data.\n\n        Parameters:\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data to predict.\n\n        Returns:\n        -------\n        y_pred_proba : array-like, shape (n_samples, n_classes)\n            The predicted probabilities of the target values.\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(self.priors)\n\n        # Calculate the log-likelihoods of the classes\n        log_likelihoods = np.zeros((n_samples, n_classes))\n        for i in range(n_classes):\n            log_likelihoods[:, i] = np.log(self.priors[i]) - 0.5 * np.log(np.linalg.det(self.covariances[i])) - 0.5 * np.sum((X - self.means[i]) @ inv(self.covariances[i]) * (X - self.means[i]), axis=1)\n\n        # Calculate the probabilities of the target values\n        y_pred_proba = np.exp(log_likelihoods - np.max(log_likelihoods, axis=1, keepdims=True))\n        y_pred_proba /= np.sum(y_pred_proba, axis=1, keepdims=True)\n\n        return y_pred_proba\n```\n\nThis implementation of Quadratic Discriminant Analysis includes the following methods:\n\n- `fit`: Fits the model according to the given training data.\n- `predict`: Predicts the target values for the given data.\n- `predict_proba`: Predicts the probabilities of the target values for the given data.\n\nThe model assumes that the data follows a multivariate normal distribution and that the covariance matrix is different for each class. The `fit` method calculates the prior probabilities, mean vectors, and covariance matrices of the classes. The `predict` method predicts the target values by calculating the log-likelihoods of the classes and selecting the class with the highest log-likelihood. The `predict_proba` method predicts the probabilities of the target values by calculating the log-likelihoods of the classes and normalizing them to obtain probabilities."
      ],
      "history_model_output": [
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self):\n        \"\"\"\n        Initialize the Quadratic Discriminant Analysis class.\n        \"\"\"\n        pass\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model to the training data.\n\n        Parameters:\n        X (array-like): The feature data.\n        y (array-like): The target data.\n        \"\"\"\n        # Calculate the mean and covariance for each class\n        self.means_ = [X[y == i].mean(axis=0) for i in set(y)]\n        self.covs_ = [X[y == i].cov() for i in set(y)]\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters:\n        X (array-like): The feature data.\n\n        Returns:\n        array-like: The predicted class labels.\n        \"\"\"\n        # Calculate the quadratic discriminant for each class\n        quad_discs = [self._quadratic_discriminant(X, mean, cov) for mean, cov in zip(self.means_, self.covs_)]\n\n        # Return the class labels with the highest quadratic discriminant\n        return [set(y)[i] for i in quad_discs.index(max(quad_discs))]\n\n    def _quadratic_discriminant(self, X, mean, cov):\n        \"\"\"\n        Calculate the quadratic discriminant for the given data and class.\n\n        Parameters:\n        X (array-like): The feature data.\n        mean (array-like): The mean of the class.\n        cov (array-like): The covariance of the class.\n\n        Returns:\n        float: The quadratic discriminant.\n        \"\"\"\n        # Calculate the quadratic discriminant\n        return -0.5 * (X - mean).T @ np.linalg.inv(cov) @ (X - mean) - 0.5 * np.log(np.linalg.det(cov))\n```\n\nNote: The `@timer_class` decorator is used to provide timing information for each method within the class, as per the mentor's instructions. The `fit` method calculates the mean and covariance for each class, and the `predict` method uses these values to calculate the quadratic discriminant for each class and return the class labels with the highest quadratic discriminant. The `_quadratic_discriminant` method is a helper function that calculates the quadratic discriminant for the given data and class.",
        "```python\n@timer_class\nclass HIndexCalculator:\n    def y_compute_h_index(self, citations):\n        \"\"\"\n        Compute the h-index given an array of integer citations.\n\n        The h-index is a metric that attempts to measure the productivity and citation impact of the publication of a scholar.\n        A scholar with an index of h has published at least h papers with at least h citations each.\n\n        :param citations: A list of integers representing the number of citations for each paper.\n        :return: The h-index of the given citations.\n        \"\"\"\n        citations.sort(reverse=True)\n        for i, citation in enumerate(citations):\n            if citation < i + 1:\n                return i\n        return len(citations)\n```\n\nThis function follows the latest coding guidelines provided by Luis, including the use of the `@timer_class` decorator from the `pedantic` module and the `y_` prefix for the function name. The function also adheres to Python's PEP 8 style guide for code styling and is well-documented with a clear docstring."
      ]
    }
  ]
}