{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef longest_common_prefix(strings):\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strings (list): A list of strings.\n\n    Returns:\n        str: The longest common prefix string.\n\n    Raises:\n        ValueError: If the input list is empty.\n    \"\"\"\n    if not strings:\n        raise ValueError(\"Input list cannot be empty\")\n\n    prefix = min(strings, key=len)\n    for i, char in enumerate(prefix):\n        for string in strings:\n            if string[i] != char:\n                return prefix[:i]\n    return prefix\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strings):\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strings (list): A list of strings.\n\n    Returns:\n        str: The longest common prefix string.\n\n    Raises:\n        ValueError: If the input list is empty.\n    \"\"\"\n    if not strings:\n        raise ValueError(\"Input list cannot be empty\")\n\n    # Sort the list of strings to easily compare the first and last strings\n    strings.sort()\n\n    # Compare the first and last strings to find the common prefix\n    first_string = strings[0]\n    last_string = strings[-1]\n\n    # Initialize the common prefix and the character index\n    common_prefix = \"\"\n    i = 0\n\n    # Compare characters at the same index in the first and last strings\n    while i < len(first_string) and i < len(last_string) and first_string[i] == last_string[i]:\n        common_prefix += first_string[i]\n        i += 1\n\n    return common_prefix\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef fn_compute_h_index(citations):\n    \"\"\"\n    Computes the h-index given an array of integer citations.\n\n    The h-index is a metric that attempts to measure the productivity and citation impact of the publication of a scholar.\n    A scholar with an index of h has published at least h papers with at least h citations each.\n\n    :param citations: A list of integers representing the number of citations for each paper.\n    :return: The h-index of the scholar.\n    \"\"\"\n    # Sort the citations in descending order\n    citations.sort(reverse=True)\n    \n    # Initialize the h-index to 0\n    h_index = 0\n    \n    # Iterate over the sorted citations\n    for i, citation in enumerate(citations):\n        # If the current citation is greater than or equal to the current index + 1,\n        # update the h-index\n        if citation >= i + 1:\n            h_index = i + 1\n        # If the current citation is less than the current index + 1, break the loop\n        else:\n            break\n    \n    return h_index\n```"
      ],
      "instruction_model_output": [
        "### H-Index Computation Function\n\n```python\ndef fn_compute_h_index(citations):\n    \"\"\"\n    Computes the h-index given an array of integer citations.\n\n    The h-index is a metric that attempts to measure the productivity and citation impact of the publication of a scholar.\n    A scholar with an index of h has published at least h papers with at least h citations each.\n\n    :param citations: A list of integers representing the number of citations for each paper.\n    :return: The h-index of the given citations.\n    \"\"\"\n\n    # Check if the input list is empty\n    if not citations:\n        return 0\n\n    # Sort the citations in descending order\n    citations.sort(reverse=True)\n\n    # Initialize the h-index to 0\n    h_index = 0\n\n    # Iterate over the sorted citations\n    for i, citation in enumerate(citations):\n        # If the current citation is greater than or equal to the current index + 1,\n        # update the h-index\n        if citation >= i + 1:\n            h_index = i + 1\n        # If the current citation is less than the current index + 1, break the loop\n        else:\n            break\n\n    return h_index\n```\n\n### Alternative Implementation Using Binary Search\n\n```python\ndef fn_compute_h_index_binary_search(citations):\n    \"\"\"\n    Computes the h-index given an array of integer citations using binary search.\n\n    :param citations: A list of integers representing the number of citations for each paper.\n    :return: The h-index of the given citations.\n    \"\"\"\n\n    # Check if the input list is empty\n    if not citations:\n        return 0\n\n    # Sort the citations in descending order\n    citations.sort(reverse=True)\n\n    # Initialize the low and high pointers for binary search\n    low, high = 0, len(citations) - 1\n\n    # Perform binary search\n    while low <= high:\n        mid = (low + high) // 2\n        # If the citation at the mid index is greater than or equal to the mid index + 1,\n        # update the low pointer\n        if citations[mid] >= mid + 1:\n            low = mid + 1\n        # If the citation at the mid index is less than the mid index + 1, update the high pointer\n        else:\n            high = mid - 1\n\n    # The h-index is the low pointer\n    return low\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef compute_diagonal_average(matrix):\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list of lists): A 2D list representing the matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    assert isinstance(matrix, list), \"Input must be a list of lists\"\n    assert all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diagonal_average = diagonal_sum / len(matrix)\n\n    return diagonal_average\n```\n\nThis function takes a 2D list (matrix) as input, computes the sum of its diagonal elements, and returns their average. It includes an assert statement to ensure the input is a square matrix, as required for diagonal element computation. The `@count_calls` decorator is used to track function calls and identify potential performance bottlenecks, as per the mentor's instructions.",
        "```python\nfrom pedantic import count_calls\n\nclass LinearRegression:\n    \"\"\"\n    A simple Linear Regression class.\n\n    Attributes:\n    coefficients (list): The coefficients of the linear regression model.\n    intercept (float): The intercept of the linear regression model.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the LinearRegression class.\n        \"\"\"\n        self.coefficients = None\n        self.intercept = None\n\n    @count_calls\n    def train(self, X, y):\n        \"\"\"\n        Trains the linear regression model.\n\n        Args:\n        X (list): A list of input features.\n        y (list): A list of target values.\n\n        Returns:\n        None\n        \"\"\"\n        assert len(X) == len(y), \"The number of input features and target values must be equal.\"\n        assert all(isinstance(x, (int, float)) for x in X), \"All input features must be numbers.\"\n        assert all(isinstance(y_val, (int, float)) for y_val in y), \"All target values must be numbers.\"\n\n        # Calculate the coefficients and intercept using the normal equation\n        # This is a simplified version and does not handle edge cases\n        n = len(X)\n        sum_x = sum(X)\n        sum_y = sum(y)\n        sum_xy = sum(x * y for x, y in zip(X, y))\n        sum_x_squared = sum(x ** 2 for x in X)\n\n        self.coefficients = [(n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x ** 2)]\n        self.intercept = (sum_y - self.coefficients[0] * sum_x) / n\n\n    @count_calls\n    def predict(self, X):\n        \"\"\"\n        Makes predictions using the trained linear regression model.\n\n        Args:\n        X (list): A list of input features.\n\n        Returns:\n        list: A list of predicted values.\n        \"\"\"\n        assert self.coefficients is not None and self.intercept is not None, \"The model must be trained before making predictions.\"\n        assert all(isinstance(x, (int, float)) for x in X), \"All input features must be numbers.\"\n\n        return [self.coefficients[0] * x + self.intercept for x in X]\n```\n\nThis code defines a simple Linear Regression class with `train` and `predict` methods. The `train` method calculates the coefficients and intercept of the linear regression model using the normal equation, and the `predict` method makes predictions using the trained model. The code includes assert statements to ensure that the input data is valid and that the model is trained before making predictions. The `@count_calls` decorator is used to track the number of function calls, as per the mentor's instructions."
      ],
      "instruction_model_output": [
        "```python\ndef average_diagonal(matrix):\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list of lists): A 2D list representing the matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n\n    Raises:\n        ValueError: If the input matrix is not a square matrix.\n        TypeError: If the input matrix is not a list of lists.\n    \"\"\"\n\n    # Check if the input is a list of lists\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n\n    # Check if the matrix is square\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n\n    # Check if the matrix is not empty\n    assert len(matrix) > 0, \"Input matrix must not be empty\"\n\n    # Extract the diagonal elements\n    diagonal = [matrix[i][i] for i in range(len(matrix))]\n\n    # Compute the average of the diagonal elements\n    average = sum(diagonal) / len(diagonal)\n\n    return average\n```",
        "```python\nfrom pedantic import pedantic\nimport numpy as np\n\nclass LinearRegression:\n    \"\"\"\n    A simple Linear Regression class.\n\n    Attributes:\n    -----------\n    learning_rate : float\n        The learning rate for gradient descent.\n    n_iters : int\n        The number of iterations for gradient descent.\n    weights : np.ndarray\n        The weights of the linear regression model.\n    bias : float\n        The bias of the linear regression model.\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        \"\"\"\n        Initializes the LinearRegression object.\n\n        Parameters:\n        -----------\n        learning_rate : float, optional\n            The learning rate for gradient descent (default is 0.001).\n        n_iters : int, optional\n            The number of iterations for gradient descent (default is 1000).\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    @pedantic\n    def _mean_squared_error(self, y, y_predicted):\n        \"\"\"\n        Calculates the mean squared error.\n\n        Parameters:\n        -----------\n        y : np.ndarray\n            The actual values.\n        y_predicted : np.ndarray\n            The predicted values.\n\n        Returns:\n        --------\n        float\n            The mean squared error.\n        \"\"\"\n        return np.mean((y - y_predicted) ** 2)\n\n    @pedantic\n    def _fit(self, X, y):\n        \"\"\"\n        Fits the linear regression model.\n\n        Parameters:\n        -----------\n        X : np.ndarray\n            The features.\n        y : np.ndarray\n            The target variable.\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iters):\n            y_predicted = np.dot(X, self.weights) + self.bias\n\n            # Compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n\n            # Update weights and bias\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n    @pedantic\n    def train(self, X, y):\n        \"\"\"\n        Trains the linear regression model.\n\n        Parameters:\n        -----------\n        X : np.ndarray\n            The features.\n        y : np.ndarray\n            The target variable.\n        \"\"\"\n        self._fit(X, y)\n\n    @pedantic\n    def predict(self, X):\n        \"\"\"\n        Makes predictions using the linear regression model.\n\n        Parameters:\n        -----------\n        X : np.ndarray\n            The features.\n\n        Returns:\n        --------\n        np.ndarray\n            The predicted values.\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n```\n\nThis code defines a LinearRegression class with methods for training and predicting. The `_fit` method is used to fit the model to the data, and the `train` method is used to call `_fit`. The `predict` method is used to make predictions using the trained model. The `_mean_squared_error` method is used to calculate the mean squared error, but it is not used in this code. The `pedantic` decorator is used to count the number of calls to each method."
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\n@count_calls\ndef fn_longest_common_prefix(strings):\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strings (list): A list of strings.\n\n    Returns:\n        str: The longest common prefix string.\n\n    Raises:\n        ValueError: If the input list is empty.\n    \"\"\"\n    assert isinstance(strings, list), \"Input must be a list of strings.\"\n    assert all(isinstance(s, str) for s in strings), \"All elements in the list must be strings.\"\n    assert len(strings) > 0, \"Input list cannot be empty.\"\n\n    prefix = min(strings, key=len)\n    for i, char in enumerate(prefix):\n        for string in strings:\n            if string[i] != char:\n                return prefix[:i]\n    return prefix\n```\n\nThis function follows the latest coding guidelines provided by the mentor, including:\n\n*   The function name starts with `fn_` as per the convention.\n*   The function includes a docstring that describes its purpose, arguments, and return value.\n*   The function uses assert statements to validate the input and ensure it meets the expected criteria.\n*   The function is decorated with the `@count_calls` decorator from the `pedantic` module to track function calls and identify potential performance bottlenecks.",
        "```python\n@count_calls\ndef fn_compute_h_index(citations):\n    \"\"\"\n    Compute the h-index given an array of integer citations.\n\n    The h-index is defined as the maximum value of h such that the given author has published at least h papers that have each been cited at least h times.\n\n    Args:\n        citations (list): A list of integers representing the number of citations for each paper.\n\n    Returns:\n        int: The h-index of the given citations.\n\n    Raises:\n        ValueError: If the input list is empty.\n    \"\"\"\n    assert isinstance(citations, list), \"Input must be a list of integers.\"\n    assert all(isinstance(citation, int) for citation in citations), \"All elements in the list must be integers.\"\n    assert len(citations) > 0, \"Input list cannot be empty.\"\n\n    citations.sort(reverse=True)\n    for i, citation in enumerate(citations):\n        if citation < i + 1:\n            return i\n    return len(citations)\n```",
        "```python\nfrom pedantic import count_calls\n\n@count_calls\ndef fn_compute_diagonal_average(matrix):\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list of lists): A 2D list representing the matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n\n    Raises:\n        ValueError: If the input matrix is not a square matrix.\n        TypeError: If the input matrix is not a list of lists.\n    \"\"\"\n\n    # Check if the input matrix is a list of lists\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input matrix must be a list of lists\"\n\n    # Check if the input matrix is a square matrix\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be a square matrix\"\n\n    # Initialize sum of diagonal elements\n    diagonal_sum = 0\n\n    # Compute the sum of diagonal elements\n    for i in range(len(matrix)):\n        diagonal_sum += matrix[i][i]\n\n    # Compute the average of diagonal elements\n    diagonal_average = diagonal_sum / len(matrix)\n\n    return diagonal_average\n```",
        "```python\nfrom pedantic import count_calls\n\nclass LinearRegression:\n    \"\"\"\n    A simple Linear Regression class.\n\n    Attributes:\n    coefficients (list): The coefficients of the linear regression model.\n    intercept (float): The intercept of the linear regression model.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the Linear Regression model.\n        \"\"\"\n        self.coefficients = None\n        self.intercept = None\n\n    @count_calls\n    def fn_train(self, X, y):\n        \"\"\"\n        Trains the Linear Regression model.\n\n        Args:\n        X (list): The input features.\n        y (list): The target variable.\n\n        Returns:\n        None\n        \"\"\"\n        assert len(X) == len(y), \"The number of input features and target variables must be equal.\"\n        assert all(isinstance(x, (int, float)) for x in X), \"All input features must be numbers.\"\n        assert all(isinstance(y_i, (int, float)) for y_i in y), \"All target variables must be numbers.\"\n\n        # Calculate the coefficients and intercept using the normal equation\n        # For simplicity, we assume X is a list of lists where each sublist contains a single feature\n        # and y is a list of target variables\n        n = len(X)\n        sum_x = sum(X)\n        sum_y = sum(y)\n        sum_xy = sum(x * y_i for x, y_i in zip(X, y))\n        sum_x_squared = sum(x ** 2 for x in X)\n\n        self.coefficients = [(n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x ** 2)]\n        self.intercept = (sum_y - self.coefficients[0] * sum_x) / n\n\n    @count_calls\n    def fn_predict(self, X):\n        \"\"\"\n        Makes predictions using the trained Linear Regression model.\n\n        Args:\n        X (list): The input features.\n\n        Returns:\n        list: The predicted target variables.\n        \"\"\"\n        assert self.coefficients is not None and self.intercept is not None, \"The model must be trained before making predictions.\"\n        assert all(isinstance(x, (int, float)) for x in X), \"All input features must be numbers.\"\n\n        return [self.coefficients[0] * x + self.intercept for x in X]\n```\n\nThis implementation follows the coding guidelines provided by your mentor, including the use of docstrings, assert statements, and the `@count_calls` decorator. The `LinearRegression` class has `fn_train` and `fn_predict` methods, which are prefixed with `fn_` as per the guidelines. The methods also include docstrings that describe their purpose, arguments, and return values."
      ]
    }
  ]
}