{
  "sessions": [
    {
      "session_model_output": [
        "def find_common_elements(a_list1, a_list2):\n    common_elements = [i for i in a_list1 if i in a_list2]\n    return common_elements",
        "def dot_product(a_v1, a_v2):\n    return sum(x * y for x, y in zip(a_v1, a_v2))"
      ],
      "instruction_model_output": [
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))",
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    \"\"\"\n    A class to implement Lasso Regression.\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Initialize the Lasso Regression model.\n\n        Parameters:\n            alpha (float): The regularization strength. A higher value penalizes larger coefficients.\n        \"\"\"\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        \"\"\"\n        Train the Lasso Regression model using the given training data.\n\n        Parameters:\n            X (ndarray): Training input data of shape (n_samples, n_features).\n            y (ndarray): Target values of shape (n_samples,).\n\n        Returns:\n            None\n        \"\"\"\n        import numpy as np\n\n        # Add a column of ones to X for the intercept\n        ones = np.ones((X.shape[0], 1))\n        X = np.hstack((ones, X))\n\n        # Closed-form solution for Lasso Regression coefficients\n        self.coef_ = np.linalg.inv(X.T @ X) @ (X.T @ y)\n\n        # The last coefficient is the intercept\n        self.intercept_ = self.coef_[-1]\n        self.coef_ = self.coef_[:-1]\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values using the trained Lasso Regression model.\n\n        Parameters:\n            X (ndarray): Input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        # Add a column of ones to X for the intercept\n        ones = np.ones((X.shape[0], 1))\n        X = np.hstack((ones, X))\n\n        # Predict target values using the trained coefficients\n        return X @ self.coef_ + self.intercept_",
        "def kth_largest(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the k-th largest element from the given array.\n    \n    Args:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' for the k-th largest element.\n\n    Returns:\n        int: The k-th largest element in the array.\n    \"\"\"\n    try:\n        arr.sort(reverse=True)\n        return arr[k-1]\n    except IndexError:\n        print(\"Error: k is larger than the length of the array.\")\n        return None"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Graph:\n    def shortest_path_x(self, start, end, matrix):\n        queue = [(start, [start])]\n        while queue:\n            (node, path) = queue.pop(0)\n            for next_node in range(len(matrix[node])):\n                if matrix[node][next_node] != float('inf') and next_node not in path:\n                    new_path = list(path)\n                    new_path.append(next_node)\n                    if next_node == end:\n                        return new_path\n                    queue.append((next_node, new_path))\n        return None"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    class Node:\n        def __init__(self, data_t):\n            self.data_t = data_t\n            self.next_t = None\n\n    def __init__(self):\n        self.head_t = None\n\n    def append(self, data_t):\n        new_node_t = self.Node(data_t)\n        if not self.head_t:\n            self.head_t = new_node_t\n        else:\n            current_t = self.head_t\n            while current_t.next_t:\n                current_t = current_t.next_t\n            current_t.next_t = new_node_t\n\n    def display(self):\n        current_t = self.head_t\n        while current_t:\n            print(current_t.data_t, end=\" -> \")\n            current_t = current_t.next_t\n        print(\"None\")"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value_t = value\n        self.next_t = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_t = None\n        self.tail_t = None\n\n    def append(self, value):\n        new_node_t = Node(value)\n        if not self.head_t:\n            self.head_t = new_node_t\n            self.tail_t = self.head_t\n        else:\n            self.tail_t.next_t = new_node_t\n            self.tail_t = new_node_t\n\n    def delete(self, value):\n        if self.head_t.value_t == value:\n            self.head_t = self.head_t.next_t\n            return\n\n        current_node_t = self.head_t\n        prev_node_t = None\n        while current_node_t and current_node_t.value_t != value:\n            prev_node_t = current_node_t\n            current_node_t = current_node_t.next_t\n\n        if not current_node_t:\n            return\n\n        if prev_node_t:\n            prev_node_t.next_t = current_node_t.next_t\n        else:\n            self.tail_t = prev_node_t\n\n        if current_node_t == self.tail_t:\n            self.tail_t = prev_node_t\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(j_vec1, j_vec2):\n    dot_product_i = sum(x * y for x, y in zip(j_vec1, j_vec2))\n    return dot_product_i\n```",
        "class LinkedList:\n    class Node:\n        def __init__(self, data_i, next_=None):\n            self.data_i = data_i\n            self.next_i = next_\n\n    def __init__(self):\n        self.head_i = None\n\n    def append_j(self, data_i):\n        if not self.head_i:\n            self.head_i = self.Node(data_i)\n        else:\n            curr_j = self.head_i\n            while curr_j.next_i:\n                curr_j = curr_j.next_i\n            curr_j.next_i = self.Node(data_i)\n\n    def display_j(self):\n        curr_j = self.head_i\n        while curr_j:\n            print(curr_j.data_i, end=' ')\n            curr_j = curr_j.next_i\n        print()"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(j_vector_1, j_vector_2):\n    return sum(a * b for a, b in zip(j_vector_1, j_vector_2))\n```",
        "```python\nclass Node:\n    def __init__(self, data_i):\n        self.data_i = data_i\n        self.next_i = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def append(self, data_i):\n        if not self.head_i:\n            self.head_i = Node(data_i)\n        else:\n            curr_i = self.head_i\n            while curr_i.next_i:\n                curr_i = curr_i.next_i\n            curr_i.next_i = Node(data_i)\n\n    def delete(self, data_i):\n        if self.head_i is None:\n            return\n\n        if self.head_i.data_i == data_i:\n            self.head_i = self.head_i.next_i\n            return\n\n        prev_i = self.head_i\n        curr_i = self.head_i.next_i\n\n        while curr_i:\n            if curr_i.data_i == data_i:\n                prev_i.next_i = curr_i.next_i\n                return\n            prev_i = curr_i\n            curr_i = curr_i.next_i\n\n    def display(self):\n        curr_i = self.head_i\n        while curr_i:\n            print(curr_i.data_i, end=\" -> \")\n            curr_i = curr_i.next_i\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Node_j:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None_j\n\n\nclass LinkedList_j:\n    def __init__(self):\n        self.head_j = None_j\n\n    def append_j(self, data_j):\n        new_node_j = Node_j(data_j)\n        if not self.head_j:\n            self.head_j = new_node_j\n        else:\n            current_j = self.head_j\n            while current_j.next_j:\n                current_j = current_j.next_j\n            current_j.next_j = new_node_j\n\n    def display_j(self):\n        current_j = self.head_j\n        while current_j:\n            print(current_j.data_j, end=\" -> \")\n            current_j = current_j.next_j\n        print(\"None_j\")"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_j = None\n\n    def append(self, data_j):\n        if not self.head_j:\n            self.head_j = Node(data_j)\n        else:\n            curr_j = self.head_j\n            while curr_j.next_j:\n                curr_j = curr_j.next_j\n            curr_j.next_j = Node(data_j)\n\n    def display(self):\n        curr_j = self.head_j\n        while curr_j:\n            print(curr_j.data_j, end=\" -> \")\n            curr_j = curr_j.next_j\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph_chx = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def shortest_path_p(self, src, dest):\n        dist = [float(\"Inf\") for _ in range(self.V)]\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        while True:\n            u = self._extract_min_p(dist, spt_set)\n            if u == -1:\n                break\n\n            for v in range(self.V):\n                if (\n                    self.graph_chx[u][v] > 0\n                    and spt_set[v] is False\n                    and dist[v] > dist[u] + self.graph_chx[u][v]\n                ):\n                    dist[v] = dist[u] + self.graph_chx[u][v]\n\n            spt_set[u] = True\n\n        if dist[dest] == float(\"Inf\"):\n            print(\"Destination is unreachable from the source\")\n        else:\n            print(\"Shortest distance is:\", dist[dest])\n\n    def _extract_min_p(self, dist, spt_set):\n        min = float(\"Inf\")\n        min_index = -1\n        for v in range(self.V):\n            if spt_set[v] is False and dist[v] <= min:\n                min = dist[v]\n                min_index = v\n\n        return min_index\n```",
        "class Perceptron_p:\n    def __init__(self):\n        self.weight_chx = None\n        self.bias_chx = 0\n\n    def train(self, inputs, labels):\n        inputs = np.array(inputs)\n        labels = np.array(labels)\n\n        # Add bias term to inputs\n        inputs_with_bias_chx = np.c_[inputs, -np.ones(len(inputs))]\n\n        # Initialize weights if not already\n        if self.weight_chx is None:\n            self.weight_chx = np.random.rand(inputs.shape[1] + 1)\n\n        # Training loop\n        for epoch in range(num_epochs):\n            # Forward pass\n            linear_output = np.dot(inputs_with_bias_chx, self.weight_chx)\n            predictions = self.activate(linear_output)\n\n            # Backward pass\n            errors = labels - predictions\n            gradient = np.dot(inputs_with_bias_chx.T, errors)\n            self.weight_chx += learning_rate * gradient\n\n        return self.weight_chx\n\n    def predict(self, inputs):\n        # Add bias term to input\n        inputs_with_bias_chx = np.c_[inputs, -np.ones(len(inputs))]\n\n        # Compute linear output\n        linear_output = np.dot(inputs_with_bias_chx, self.weight_chx)\n\n        # Apply activation function\n        predictions = self.activate(linear_output)\n        return predictions\n\n    def activate(self, x):\n        return np.where(x >= 0, 1, 0)"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path",
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class NearestNeighbors:\n    \"\"\"\n    A Nearest Neighbors classifier.\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use for prediction.\n\n    p : int, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is equivalent to using\n        manhattan_distance (l1), and euclidean_distance (l2) for infinity.\n\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, p=2):\n        self.n_neighbors = n_neighbors\n        self.p = p\n\n    def fit(self, X, y):\n        \"\"\"Fit the model using X as training data and y as target values\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        assert isinstance(X, (list, np.ndarray))\n        assert isinstance(y, (list, np.ndarray))\n        assert X.shape[0] == y.shape[0], \"X and y should have the same number of samples\"\n\n        self.X_train = X\n        self.y_train = y\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the class labels for the provided data\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Class labels for each data sample.\n        \"\"\"\n        assert isinstance(X, (list, np.ndarray))\n        assert X.shape[1] == self.X_train.shape[1], \"Test data should have the same number of features as training data\"\n\n        distances = cdist(X, self.X_train, metric='minkowski', p=self.p)\n        indices = distances.argsort()[:, :self.n_neighbors]\n        neighbors = self.y_train[indices]\n        predictions = []\n        for i in range(neighbors.shape[0]):\n            unique, counts = np.unique(neighbors[i], return_counts=True)\n            predictions.append(unique[np.argmax(counts)])\n\n        return np.array(predictions)"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(i_vec1, i_vec2):\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    Args:\n        i_vec1 (list): A list of numbers representing the first vector.\n        i_vec2 (list): A list of numbers representing the second vector.\n\n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    return sum(x * y for x, y in zip(i_vec1, i_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(i_vec1, i_vec2):\n    return sum(a * b for a, b in zip(i_vec1, i_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\n\n\n@timer_class\nclass LinkedList_q:\n    def __init__(self):\n        self.head_q = None\n\n    def append_q(self, value_q):\n        if self.head_q is None:\n            self.head_q = Node_q(value_q)\n        else:\n            current_q = self.head_q\n            while current_q.next_q:\n                current_q = current_q.next_q\n            current_q.next_q = Node_q(value_q)\n\n    def delete_q(self, value_q):\n        if self.head_q is None:\n            return\n        if self.head_q.value_q == value_q:\n            self.head_q = self.head_q.next_q\n            return\n        current_q = self.head_q\n        prev_q = None\n        while current_q and current_q.value_q != value_q:\n            prev_q = current_q\n            current_q = current_q.next_q\n        if current_q is None:\n            return\n        prev_q.next_q = current_q.next_q\n\n    def display_q(self):\n        current_q = self.head_q\n        while current_q:\n            print(current_q.value_q, end=\" -> \")\n            current_q = current_q.next_q\n        print(\"None\")\n\n\nclass Node_q:\n    def __init__(self, value_q):\n        self.value_q = value_q\n        self.next_q = None\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass QDA:\n    def __init__(self, means_q, cov_mat_q, priors_q):\n        self.means_q_ = means_q\n        self.cov_mat_q_ = cov_mat_q\n        self.priors_q_ = priors_q\n\n    def predict(self, X):\n        # Calculate posterior probabilities for each class\n        posteriors = []\n        for i in range(self.cov_mat_q_.shape[0]):\n            cov_inv = np.linalg.inv(self.cov_mat_q_[i, :, :])\n            diff = X - self.means_q_[i, :]\n            log_det = np.log(np.linalg.det(self.cov_mat_q_[i, :, :]))\n            exponent = -0.5 * (np.transpose(diff) @ cov_inv @ diff + log_det)\n            posterior = self.priors_q_[i] * exponent\n            posteriors.append(posterior)\n\n        # Predict class with highest posterior probability\n        return np.argmax(posteriors, axis=0)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_q):\n        self.data_q = data_q\n        self.next_q = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_q = None\n\n    def append(self, data_q):\n        if not self.head_q:\n            self.head_q = Node(data_q)\n        else:\n            curr_q = self.head_q\n            while curr_q.next_q:\n                curr_q = curr_q.next_q\n            curr_q.next_q = Node(data_q)\n\n    def display(self):\n        if not self.head_q:\n            return\n        curr_q = self.head_q\n        while curr_q:\n            print(curr_q.data_q, end=\" -> \")\n            curr_q = curr_q.next_q\n        print(\"None\")\n```",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(v1: list[float], v2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param v1: The first vector.\n    :param v2: The second vector.\n    :return: The dot product of v1 and v2.\n    \"\"\"\n    result = 0.0\n    for i in range(len(v1)):\n        result += v1[i] * v2[i]\n    return result"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(g_chx_vec1, g_chx_vec2):\n    return sum(x * y for x, y in zip(g_chx_vec1, g_chx_vec2))",
        "def compute_dot_product(g_chx_vec1, g_chx_vec2):\n    return sum(x * y for x, y in zip(g_chx_vec1, g_chx_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```",
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\nfrom typing import Dict\n\n\n@require_kwargs('roman_numeral')\ndef roman_to_int(roman_numeral: str) -> int:\n    \"\"\"Convert a Roman numeral string to an integer.\n\n    :param roman_numeral: The Roman numeral string to be converted.\n    :return: The integer representation of the Roman numeral.\n    \"\"\"\n    roman_to_decimal: Dict[str, int] = {\n        'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000\n    }\n    decimal_value = 0\n    prev_value = 0\n\n    for char in reversed(roman_numeral):\n        decimal = roman_to_decimal[char]\n        if decimal < prev_value:\n            decimal_value -= decimal\n        else:\n            decimal_value += decimal\n        prev_value = decimal\n\n    return decimal_value"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer"
      ],
      "cumulative_model_output": [
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom typing import List, Tuple, Union\n\nclass NearestNeighborsModel:\n    def __init__(self):\n        self.model = KNeighborsClassifier()\n\n    def train(self, X_train: List[List[float]], y_train: List[int]):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: List[List[float]]) -> List[int]:\n        return self.model.predict(X_test).tolist()\n```",
        "```python\nfrom functools import reduce\n\ndef roman_to_int(s: str) -> int:\n    roman_values = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    values = [roman_values[digit] for digit in s]\n    return reduce(lambda x, y: x + y, values)\n```",
        "```python\nfrom pedantic import require_kwargs, timer_class\n\n\n@require_kwargs\n@timer_class\ndef dot_product(i_vector1_chx: list[float], i_vector2_chx: list[float]) -> float:\n    assert len(i_vector1_chx) == len(\n        i_vector2_chx\n    ), \"Vectors must have the same length for dot product.\"\n    return sum(x * y for x, y in zip(i_vector1_chx, i_vector2_chx))\n```",
        "```python\nfrom pedantic import timer_class, require_kwargs\n\nclass Perceptron:\n    def __init__(self, weight_0: float = 0.0, weight_1: float = 0.0, bias: float = 0.0) -> None:\n        self.weight_0, self.weight_1, self.bias = weight_0, weight_1, bias\n\n    @require_kwargs\n    @timer_class\n    def train(self, inputs: list[list[float]], outputs: list[float]) -> None:\n        # Training logic here\n        pass\n\n    @require_kwargs\n    def predict(self, input: list[float]) -> float:\n        # Prediction logic here\n        pass\n```",
        "```python\nfrom pedantic import require_kwargs, timer_class\n\n\n@require_kwargs(\"vec1\", \"vec2\")\n@timer_class\ndef dot_product(i_vec1_chx: list[float], i_vec2_chx: list[float]) -> float:\n    j_dot_product_q = sum(a_x * a_y for a_x, a_y in zip(i_vec1_chx, i_vec2_chx))\n    return j_dot_product_q\n```",
        "```python\nclass LinkedList:\n    def __init__(self):\n        self._head = None\n        self._tail = None\n\n    def append(self, value: int) -> None:\n        node = Node(value)\n        if self._head is None:\n            self._head = node\n            self._tail = node\n        else:\n            self._tail._next = node\n            self._tail = node\n\n    def delete(self, value: int) -> None:\n        if self._head is None:\n            return\n        if self._head._value == value:\n            self._head = self._head._next\n            return\n        current_node = self._head\n        while current_node._next is not None:\n            if current_node._next._value == value:\n                current_node._next = current_node._next._next\n                return\n            current_node = current_node._next\n        return\n\n    def search(self, value: int) -> bool:\n        current_node = self._head\n        while current_node is not None:\n            if current_node._value == value:\n                return True\n            current_node = current_node._next\n        return False\n\n    def get_head(self) -> int:\n        return self._head._value if self._head else None\n\n    def get_tail(self) -> int:\n        return self._tail._value if self._tail else None\n\nclass Node:\n    def __init__(self, value: int):\n        self._value = value\n        self._next = None\n```",
        "```python\ndef common_elements(list1: list[int], list2: list[int]) -> list[int]:\n    common = []\n    for element in list1:\n        if element in list2 and f\"{element}_chx\" not in locals():\n            common.append(element)\n            locals()[f\"{element}_chx\"] = True\n    return common\n```",
        "```python\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    arr.sort()\n    return arr[-k]\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.base import BaseEstimator\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\n\nclass LassoRegression(BaseEstimator):\n    \"\"\"\n    Lasso Regression Class\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength. Must be a positive float.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        Coefficient of the features in the model.\n\n    intercept_ : array, shape (1,)\n        Intercept (a.k.a. bias) added to the model if fit_intercept = True.\n\n    n_features_in_ : int\n        Number of features seen during :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoRegression\n    >>> from sklearn.datasets import load_diabetes\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> model = LassoRegression()\n    >>> model.fit(X, y)\n    LassoRegression()\n    \"\"\"\n\n    def __init__(self, alpha=1.0, copy_X=True):\n        self.alpha = alpha\n        self.copy_X = copy_X\n\n    def train(self, X, y):\n        \"\"\"\n        Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data\n\n        y : array-like, shape (n_samples,)\n            Target values\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Check if X and y have correct shape\n        X = check_array(X, copy=self.copy_X)\n        y = y.reshape(-1, 1)\n\n        # Fit the model\n        self.lasso_model_ = Lasso(alpha=self.alpha).fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Samples.\n\n        Returns\n        -------\n        C : array, shape = [n_samples]\n            Returns predicted values.\n        \"\"\"\n        # Check if model has been fitted\n        check_is_fitted(self, \"lasso_model_\")\n\n        # Check if X has correct shape\n        X = check_array(X)\n\n        # Predict using the model\n        return self.lasso_model_.predict(X)\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom pedantic import timer_class, require_kwargs\n\nclass QuadraticDiscriminantAnalysis:\n    @timer_class\n    @require_kwargs\n    def __init__(self, i_a1: int, g_chx_a2: int):\n        \"\"\"\n        Initialize the Quadratic Discriminant Analysis (QDA) model.\n\n        Parameters\n        ----------\n        i_a1 : int\n            The first argument, must be an integer.\n        g_chx_a2 : int\n            The second argument, must start with 'chx' and be an integer.\n        \"\"\"\n        self.i_a1 = i_a1\n        self.g_chx_a2 = g_chx_a2\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            The input data of shape (n_samples, n_features).\n        y : array-like\n            The target variable of shape (n_samples,).\n        \"\"\"\n        # Fit the model\n        self.model_ = QDA()\n        self.model_.fit(X, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predict the target variable for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            The input data of shape (n_samples, n_features).\n\n        Returns\n        -------\n        y_pred : array-like\n            The predicted target variable of shape (n_samples,).\n        \"\"\"\n        # Make predictions\n        y_pred = self.model_.predict(X)\n        return y_pred\n```",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def add_edge(self, u, v, w):\n        self.graph[u][v] = w\n\n    def shortest_path(self, src, dest):\n        dist = [float(\"Inf\")] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        q = deque()\n        q.append(src)\n        spt_set[src] = True\n\n        while q:\n            u = q.popleft()\n\n            for ind, val in enumerate(self.graph[u]):\n                if val > 0 and not spt_set[ind]:\n                    dist[ind] = dist[u] + val\n                    q.append(ind)\n                    spt_set[ind] = True\n\n        return dist[dest]\n```"
      ]
    }
  ]
}