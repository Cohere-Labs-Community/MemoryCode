{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def a_compute_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef a_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def y_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom pedantic import timer_class\n\n@timer_class\nclass QDA(QuadraticDiscriminantAnalysis):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class.\n    \"\"\"\n    def __init__(self, priors=None, reg_cov=0.0, store_covariance=True, tol=1e-4, store_precision=True):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n            priors (array-like of shape (n_classes,), default=None):\n                Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n\n            reg_cov (float, default=0.0):\n                Regularization parameter for the covariance estimate. Increase this value to make class covariances more similar,\n                which makes predictions more robust to outliers.\n\n            store_covariance (bool, default=True):\n                Whether to store the covariance matrices for each class. Setting this parameter to False will slightly reduce the memory footprint.\n\n            tol (float, default=1e-4):\n                Tolerance for convergence in iterative estimation of covariance parameters.\n\n            store_precision (bool, default=True):\n                Whether to store the precision matrices (inverse of the covariance matrices) for each class.\n                Setting this parameter to False will slightly reduce the memory footprint.\n        \"\"\"\n        super().__init__(priors=priors, reg_cov=reg_cov, store_covariance=store_covariance, tol=tol, store_precision=store_precision)"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": [
        "```python\nimport numpy as np\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom pedantic import timer_class, timer_function\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n    \"\"\"\n    def __init__(self):\n        self._classes = None\n        self._class_prior = None\n        self._class_cov = None\n        self._class_mean = None\n\n    @timer_function\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data.\n        \"\"\"\n        # Validate inputs\n        X, y = self._validate_data(X, y)\n\n        # Store the number of classes\n        self._classes = np.unique(y)\n        n_classes = self._classes.size\n\n        # Calculate class prior probabilities\n        self._class_prior = np.bincount(y) / len(y)\n\n        # Calculate class means\n        self._class_mean = np.zeros((n_classes, X.shape[1]))\n        for i, class_label in enumerate(self._classes):\n            class_indices = (y == class_label)\n            self._class_mean[i, :] = np.mean(X[class_indices], axis=0)\n\n        # Calculate class covariance matrices\n        self._class_cov = np.zeros((n_classes, X.shape[1], X.shape[1]))\n        for i, class_label in enumerate(self._classes):\n            class_indices = (y == class_label)\n            self._class_cov[i, :, :] = np.cov(X[class_indices].T)\n\n        return self\n\n    @timer_function\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the provided data.\n        \"\"\"\n        # Validate input\n        X = self._validate_data(X, ensure_2d=True)\n\n        # Initialize predictions\n        predictions = np.zeros(X.shape[0], dtype=int)\n\n        # For each data point, calculate the posterior probability for each class\n        for i, x in enumerate(X):\n            posterior = np.zeros(self._classes.size)\n            for j, class_label in enumerate(self._classes):\n                prior = np.log(self._class_prior[j])\n                mean = self._class_mean[j]\n                cov = self._class_cov[j]\n                inv_cov = np.linalg.inv(cov)\n                x_minus_mean = x - mean\n                exponent = -0.5 * np.dot(x_minus_mean, np.dot(inv_cov, x_minus_mean))\n                posterior[j] = prior + exponent\n\n            # Choose the class with the highest posterior probability\n            predictions[i] = self._classes[np.argmax(posterior)]\n\n        return predictions\n\n    @timer_function\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test data X.\n        \"\"\"\n        # Validate input\n        X = self._validate_data(X, ensure_2d=True)\n\n        # Initialize probability estimates\n        probabilities = np.zeros((X.shape[0], self._classes.size))\n\n        # For each data point, calculate the posterior probability for each class\n        for i, x in enumerate(X):\n            posterior = np.zeros(self._classes.size)\n            for j, class_label in enumerate(self._classes):\n                prior = self._class_prior[j]\n                mean = self._class_mean[j]\n                cov = self._class_cov[j]\n                inv_cov = np.linalg.inv(cov)\n                x_minus_mean = x - mean\n                exponent = -0.5 * np.dot(x_minus_mean, np.dot(inv_cov, x_minus_mean))\n                posterior[j] = prior * np.exp(exponent)\n\n            # Normalize posterior probabilities to sum to 1\n            posterior /= np.sum(posterior)\n\n            # Store the probability estimates for this data point\n            probabilities[i, :] = posterior\n\n        return probabilities\n\n    @timer_function\n    def _validate_data(self, X, y=None, ensure_2d=False):\n        \"\"\"\n        Validate input data and optionally ensure it is 2-dimensional.\n        \"\"\"\n        # Check if X is a numpy array\n        if not isinstance(X, np.ndarray):\n            raise ValueError(\"Input data must be a numpy array.\")\n\n        # Ensure X is 2-dimensional\n        if ensure_2d and X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        # Check if y is provided and is a numpy array\n        if y is not None and not isinstance(y, np.ndarray):\n            raise ValueError(\"Input labels must be a numpy array.\")\n\n        # Check if the number of data points in X and y match\n        if y is not None and X.shape[0] != len(y):\n            raise ValueError(\"The number of data points in X and y must match.\")\n\n        return X, y\n```",
        "def y_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Computes the h-index based on the given citations.\n\n    :param citations: A list of integer citations.\n    :return: The h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ]
    }
  ]
}