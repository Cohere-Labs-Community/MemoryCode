{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value: int) -> None:\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root = None\n\n    def insert(self, value_f: int) -> None:\n        new_node = Node(value_f)\n        if not self.root:\n            self.root = new_node\n        else:\n            self._insert_node(value_f, self.root)\n\n    def _insert_node(self, value_f: int, node: Node) -> None:\n        if value_f < node.value:\n            if node.left is None:\n                node.left = Node(value_f)\n            else:\n                self._insert_node(value_f, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value_f)\n            else:\n                self._insert_node(value_f, node.right)\n\n    def delete(self, value_f: int) -> None:\n        self.root = self._delete_node(self.root, value_f)\n\n    def _delete_node(self, node: Node, value_f: int) -> Node:\n        if node is None:\n            return node\n\n        if value_f < node.value:\n            node.left = self._delete_node(node.left, value_f)\n        elif value_f > node.value:\n            node.right = self._delete_node(node.right, value_f)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            min_val_node = self._find_min_node(node.right)\n            node.value = min_val_node.value\n            node.right = self._delete_node(node.right, min_val_node.value)\n\n        return node\n\n    def _find_min_node(self, node: Node) -> Node:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\ndef merge_sorted_lists(list1_f: list[int], list2_f: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists and returns a new sorted list.\n\n    Args:\n        list1_f (list[int]): The first sorted list.\n        list2_f (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing elements from both input lists.\n    \"\"\"\n    merged_list_f = []\n    i, j = 0, 0\n\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list_f.append(list1_f[i])\n            i += 1\n        else:\n            merged_list_f.append(list2_f[j])\n            j += 1\n\n    merged_list_f.extend(list1_f[i:])\n    merged_list_f.extend(list2_f[j:])\n\n    return merged_list_f\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self):\n        super(IMAGE_CLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "instruction_model_output": [
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\nimport hashlib\n\n\ndef insertion_sort(arr_g: List[int]) -> List[int]:\n    for i in range(1, len(arr_g)):\n        key_g = arr_g[i]\n        j = i - 1\n        while j >= 0 and key_g < arr_g[j]:\n            arr_g[j + 1] = arr_g[j]\n            j -= 1\n        arr_g[j + 1] = key_g\n    return arr_g\n```",
        "```python\nfrom typing import List\n\n\ndef merge_sorted_lists(list1_g: List[int], list2_g: List[int]) -> List[int]:\n    merged_list_g = []\n    i, j = 0, 0\n\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list_g.append(list1_g[i])\n            i += 1\n        else:\n            merged_list_g.append(list2_g[j])\n            j += 1\n\n    while i < len(list1_g):\n        merged_list_g.append(list1_g[i])\n        i += 1\n\n    while j < len(list2_g):\n        merged_list_g.append(list2_g[j])\n        j += 1\n\n    return merged_list_g\n```"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr",
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def computeDiagonalAverage(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a 2D list/matrix\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n    \n    size = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(size))\n    diagonal_average = diagonal_sum / size\n    \n    return diagonal_average",
        "class ImageClassifier(nn.Module):\n    def __init__(self):\n        super(ImageClassifier, self):\n            self.conv1 = nn.Conv2d(3, 6, 5)\n            self.conv2 = nn.Conv2d(6, 16, 5)\n            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n            self.fc2 = nn.Linear(120, 84)\n            self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        self.assert_input_shape(x)\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def assert_input_shape(self, input_tensor):\n        assert len(input_tensor.size()) == 4, \"Input tensor should have 4 dimensions\"\n        assert input_tensor.size(1) == 3, \"Input tensor should have 3 channels\""
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.r_queue.remove(key)\n        self.r_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.r_queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            r_key = self.r_queue.pop(0)\n            del self.cache[r_key]\n        self.r_queue.append(key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression class providing a scikit-learn compatible API.\n    \"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", gamma=None, degree=3, coef0=1, kernel_params=None,\n                 tol=1e-3, fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        \"\"\"\n        Initialize the Kernel Ridge Regression model.\n\n        Parameters\n        ----------\n        alpha : float, optional\n            Regularization strength; must be a positive float. Larger values specify stronger regularization.\n            Defaults to 1.\n        kernel : string, optional\n            Kernel to be used. Defaults to \"linear\". Available kernels:\n                - \"linear\": linear kernel\n                - \"poly\": polynomial kernel\n                - \"rbf\": radial basis function kernel\n                - \"sigmoid\": sigmoid kernel\n                - \"precomputed\": precomputed kernel\n        gamma : float, optional\n            Kernel coefficient. Defaults to None.\n            If None, 'scale' will be used when kernel == 'rbf', 'poly' or 'sigmoid'.\n        degree : int, optional\n            Degree of the polynomial kernel. Ignored by other kernels. Defaults to 3.\n        coef0 : float, optional\n            Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. Defaults to 1.\n        kernel_params : mapping of string to any, optional\n            Parameters (keyword arguments) and values for kernel passed as alternative to keyword arguments.\n            Defaults to None.\n        tol : float, optional\n            Tolerance for stopping criterion. Defaults to 1e-3.\n        fit_intercept : bool, optional\n            Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.\n            Defaults to True.\n        normalize : bool, optional\n            If True, the predictors in X will be normalized.\n            Defaults to False.\n        copy_X : bool, optional\n            If True, X will be copied; else, it may be overwritten.\n            Defaults to True.\n        verbose : bool, optional\n            If True, verbose output is enabled for debugging. Defaults to False.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.tol = tol\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, kernel_params=kernel_params,\n            tol=tol, fit_intercept=fit_intercept, normalize_y=normalize, copy_X=copy_X\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model to data matrix X and target(s) y.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self\n        \"\"\"\n        try:\n            self.model.fit(X, y)\n            return self\n        except Exception as e:\n            raise RuntimeError(f\"Error fitting KernelRidgeRegression model: {e}\")\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the kernel ridge model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        try:\n            return self.model.predict(X)\n        except Exception as e:\n            raise RuntimeError(f\"Error predicting with KernelRidgeRegression model: {e}\")",
        "from pedantic import trace_class\nfrom sklearn.linear_model import Lasso\n\n@trace_class\nclass LassoRegression:\n    def __init__(self):\n        self.model = Lasso()\n\n    @trace_class\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            raise ValueError(f\"An error occurred while training the Lasso Regression model: {e}\")\n\n    @trace_class\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            raise ValueError(f\"An error occurred while making predictions: {e}\")"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    try:\n        if not root:\n            return True\n        val = root.val\n        if val <= min_val or val >= max_val:\n            return False\n        return is_valid_bst(root.left, min_val, val) and is_valid_bst(root.right, val, max_val)\n    except AttributeError:\n        print(\"Error: Invalid tree structure. Please ensure the input is a valid binary tree.\")\n        return False"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_head = None\n        self.xt_tail = None\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        value = self.cache[key]\n        self._move_to_xt_head(key)\n        return value\n\n    def put(self, key, value):\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self.cache[key] = value\n            self._move_to_xt_head(key)\n            return\n        if len(self.cache) >= self.capacity:\n            self._evict_xt_tail()\n        self.cache[key] = value\n        self._add_to_xt_head(key)\n\n    def _move_to_xt_head(self, key):\n        node = self.cache[key]\n        if self.xt_head == key:\n            return\n        if self.xt_tail == key:\n            self.xt_tail = None\n        if node.xt_prev:\n            node.xt_prev.xt_next = node.xt_next\n        if node.xt_next:\n            node.xt_next.xt_prev = node.xt_prev\n        node.xt_next = None\n        node.xt_prev = None\n        self.xt_head = key\n\n    def _add_to_xt_head(self, key):\n        node = self.cache[key]\n        if not self.xt_head and not self.xt_tail:\n            self.xt_head = key\n            self.xt_tail = key\n            return\n        node.xt_next = self.xt_head\n        self.cache[self.xt_head].xt_prev = key\n        self.xt_head = key\n\n    def _evict_xt_tail(self):\n        key = self.xt_tail\n        node = self.cache[key]\n        if not self.xt_head or not self.xt_tail:\n            self.xt_head = None\n            self.xt_tail = None\n            del self.cache[key]\n            return\n        self.xt_tail = node.xt_prev\n        self.cache[self.xt_tail].xt_next = None\n        del self.cache[key]"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.j_capacity = capacity\n        self.j_cache = {}\n\n    def get(self, key: int) -> int:\n        if key not in self.j_cache:\n            return -1\n        value = self.j_cache[key]\n        self.j_cache.move_to_end(key)\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if self.j_capacity == 0:\n            return\n        if key in self.j_cache:\n            self.j_cache.move_to_end(key)\n        elif len(self.j_cache) >= self.j_capacity:\n            self.j_cache.popitem(last=False)\n        self.j_cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.j_capacity = capacity\n        self.j_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.j_cache:\n            return -1\n        self.j_cache.move_to_end(key)\n        return self.j_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.j_cache:\n            self.j_cache.move_to_end(key)\n        elif len(self.j_cache) >= self.j_capacity:\n            self.j_cache.popitem(last=False)\n        self.j_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self, means_j, cov_matrix_j, classes_j):\n        self.means = means_j\n        self.cov_matrix = cov_matrix_j\n        self.classes = classes_j\n        self.classes_ = [i for i in range(len(self.means))]\n\n    def fit(self, X, y):\n        self.classes_ = list(set(y))\n        self.means_ = [X[y == i].mean(axis=0) for i in self.classes_]\n        self.cov_matrix_ = [[(X[y == i] - mean).T @ (X[y == i] - mean) / len(X[y == i])\n                             for mean in self.means_] for i in self.classes_]\n        self.priors_ = [sum(y == i) / len(y) for i in self.classes_]\n\n    def predict(self, X):\n        return [self.classes_[self._get_class(x)] for x in X]\n\n    def predict_proba(self, X):\n        return [[self.priors_[i] * self._get_density(x, i) for i in range(len(self.classes_))] for x in X]\n\n    def _get_class(self, x):\n        return max(enumerate(self._get_density(x, i) for i in range(len(self.classes_))), key=lambda x: x[1])[0]\n\n    def _get_density(self, x, i):\n        det = np.linalg.det(self.cov_matrix_[i])\n        inv = np.linalg.inv(self.cov_matrix_[i])\n        exponent = np.exp(-0.5 * (x - self.means_[i]).T @ inv @ (x - self.means_[i]))\n        return 1 / (np.power(2 * np.pi, len(x) / 2) * np.power(det, 0.5)) * exponent",
        "def merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n\n    return result_j"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.i_head = None\n        self.i_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_i_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity <= 0:\n            return\n        if key in self.cache:\n            self.cache[key] = value\n            self._move_to_i_head(key)\n            return\n        if len(self.cache) >= self.capacity:\n            self._evict_i_tail()\n        self.cache[key] = value\n        self._insert_to_i_head(key)\n\n    def _move_to_i_head(self, key: int) -> None:\n        node = self.cache[key]\n        self._remove_from_doubly_linked_list(node)\n        self._insert_to_i_head(key)\n\n    def _insert_to_i_head(self, key: int) -> None:\n        node = Node(key)\n        node.next = self.i_head\n        if self.i_head is not None:\n            self.i_head.prev = node\n        self.i_head = node\n        if self.i_tail is None:\n            self.i_tail = node\n\n    def _evict_i_tail(self) -> None:\n        node = self.i_tail\n        self._remove_from_doubly_linked_list(node)\n        del self.cache[node.key]\n\n    def _remove_from_doubly_linked_list(self, node: 'Node') -> None:\n        if node.prev is not None:\n            node.prev.next = node.next\n        else:\n            self.i_head = node.next\n\n        if node.next is not None:\n            node.next.prev = node.prev\n        else:\n            self.i_tail = node.prev\n\n    class Node:\n        def __init__(self, key: int):\n            self.key = key\n            self.prev = None\n            self.next = None"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    result_i = []\n    i, j = 0, 0\n\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            result_i.append(list1_i[i])\n            i += 1\n        else:\n            result_i.append(list2_i[j])\n            j += 1\n\n    result_i.extend(list1_i[i:])\n    result_i.extend(list2_i[j:])\n\n    return result_i",
        "class binary_image_classifier(torch.nn.Module):\n    def __init__(self, input_channels_i, num_classes_i):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = torch.nn.Conv2d(input_channels_i, 32, 3, 1)\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = torch.nn.Linear(9216, 128)\n        self.fc2 = torch.nn.Linear(128, num_classes_i)\n\n    def forward(self, x_i):\n        x_i = torch.nn.functional.relu(self.conv1(x_i))\n        x_i = torch.nn.functional.relu(self.conv2(x_i))\n        x_i = x_i.view(x_i.size(0), -1)\n        x_i = torch.nn.functional.relu(self.fc1(x_i))\n        x_i = self.fc2(x_i)\n        return x_i"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass binary_image_classifier(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    result_a = []\n    i, j = 0, 0\n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            result_a.append(list1_a[i])\n            i += 1\n        else:\n            result_a.append(list2_a[j])\n            j += 1\n    result_a.extend(list1_a[i:])\n    result_a.extend(list2_a[j:])\n    return result_a"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom functools import lru_cache\nfrom typing import Any\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_order = []\n\n    @lru_cache(maxsize=None)\n    def get(self, key: int) -> Any:\n        if key in self.cache:\n            self.cache[key] = self.cache.pop(key)\n            self.q_order.remove(key)\n            self.q_order.append(key)\n        return self.cache.get(key, -1)\n\n    def put(self, key: int, value: Any):\n        if key in self.cache:\n            self.q_order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.pop(self.q_order[0])\n            self.q_order.pop(0)\n        self.cache[key] = value\n        self.q_order.append(key)\n```",
        "```python\nfrom pedantic import validate\n\nclass Library:\n    @validate\n    def __init__(self, name: str):\n        self.q_name = name\n        self.q_books = []\n\n    @validate\n    def add_book(self, book: str) -> None:\n        self.q_books.append(book)\n\n    @validate\n    def remove_book(self, book: str) -> None:\n        self.q_books.remove(book)\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]",
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to a Roman numeral.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The corresponding Roman numeral.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    i = 0\n\n    while num > 0:\n        for _ in range(num // values[i]):\n            roman += symbols[i]\n            num -= values[i]\n        i += 1\n\n    return roman"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def mean(data):\n    \"\"\"Calculates the mean of a list of numbers using an online algorithm.\n    \n    Args:\n        data (list): A list of numeric values.\n        \n    Returns:\n        float: The mean of the input data.\n    \"\"\"\n    s = 0\n    c = 0\n    for i in data:\n        s += i\n        c += 1\n    return s / c\n\n\ndef mean_online(data, prev_mean=None, prev_count=0):\n    \"\"\"Calculates the mean of a list of numbers using an online algorithm with warm start.\n    \n    Args:\n        data (list): A list of numeric values.\n        prev_mean (float, optional): The mean calculated up to the previous step. Defaults to None.\n        prev_count (int, optional): The number of elements up to the previous step. Defaults to 0.\n        \n    Returns:\n        float: The mean of the input data.\n    \"\"\"\n    if prev_mean is None:\n        prev_mean = 0\n    if prev_count is None:\n        prev_count = 0\n    curr_mean = mean(data)\n    n = prev_count + len(data)\n    mean_ = (prev_mean * prev_count + curr_mean * len(data)) / n\n    return mean_, n"
      ],
      "instruction_model_output": [
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if not node:\n            return BinaryTreeNode(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if not node:\n            return node\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            nodeCoefficient.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass BinaryTreeNode:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "class Perceptron:\n    \"\"\"\n    Perceptron Class\n    \n    A simple implementation of a Perceptron, a type of artificial neuron, using the team's coding guidelines.\n    \"\"\"\n\n    def __init__(self, learning_rate=0.1):\n        \"\"\"\n        Initialize the Perceptron.\n        \n        Parameters:\n            learning_rate (float): The step size for updates in the weight vector. Defaults to 0.1.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.weights = None\n        self.chx_bias = 0  # Following attribute variable naming convention\n\n    def train(self, X, y, epochs=1000):\n        \"\"\"\n        Train the Perceptron.\n        \n        Parameters:\n            X (ndarray): Input training data of shape (m, n), where m is the number of examples, and n is the number of features.\n            y (ndarray): Target labels of shape (m,).\n            epochs (int): Number of epochs to train the model. Defaults to 1000.\n        \"\"\"\n        m, n = X.shape\n        self.weights = np.zeros(n)  # Initialize weights to zeros\n\n        for _ in range(epochs):\n            for i in range(m):\n                xi = X[i]\n                yi = y[i]\n                update = self.learning_rate * (yi - self.predict(xi))\n                self.weights += update * xi\n                self.chx_bias += update  # Update bias term\n\n        return self.weights\n\n    def predict(self, x):\n        \"\"\"\n        Predict the output of the Perceptron for a given input.\n        \n        Parameters:\n            x (ndarray): Input data of shape (n,).\n        \n        Returns:\n            float: The predicted output.\n        \"\"\"\n        return np.heaviside(np.dot(x, self.weights) + self.chx_bias, 0.5)\n\n\n# Import necessary libraries\nimport numpy as np\nfrom scipy.special import heaviside"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "cumulative_model_output": [
        "class Library:\n    \"\"\"\n    A Library class to manage a collection of books.\n    \"\"\"\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, book: str) -> None:\n        \"\"\"\n        Add a book to the library.\n\n        Args:\n            book (str): The title of the book to be added.\n        \"\"\"\n        self.books.append(book)\n\n    def remove_book(self, book: str) -> None:\n        \"\"\"\n        Remove a book from the library.\n\n        Args:\n            book (str): The title of the book to be removed.\n        \"\"\"\n        self.books.remove(book)",
        "```python\nfrom statistics import mean\n\ndef calculate_mean(numbers: list[int]) -> int:\n    \"\"\"\n    Computes the mean of a list of integers using an online algorithm.\n\n    Args:\n        numbers (list[int]): A list of integers.\n\n    Returns:\n        int: The mean of the list of integers.\n    \"\"\"\n    # Initialize variables\n    total = 0\n    count = 0\n\n    # Iterate through the list of numbers\n    for number in numbers:\n        # Update the total and count\n        total += number\n        count += 1\n\n    # Calculate and return the mean\n    return total // count\n```",
        "```python\ndef average_diagonal_element(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): A square matrix containing integers.\n\n    Returns:\n        float: The average of the diagonal elements of the matrix.\n\n    \"\"\"\n    # Validate input\n    if not all(len(row) == len(matrix) for row in matrix):\n        raise ValueError(\"Input must be a square matrix\")\n\n    # Calculate sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "```python\nimport numpy as np\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    Parameters\n    ----------\n    store_covariance : bool, default=False\n        Whether to store the covariance matrices for each class.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    means_ : ndarray of shape (n_features,)\n        Mean vectors for each class.\n\n    covariances_ : ndarray of shape (n_features, n_features, n_classes) or list of covariances, optional\n        Covariance matrices for each class. Only present if store_covariance is True\n\n    priors_ : ndarray of shape (n_classes,)\n        Prior probabilities of each class.\n\n    x_ : ndarray of shape (n_samples, n_features)\n        Training data.\n\n    y_ : ndarray of shape (n_samples,)\n        Training target.\n    \"\"\"\n\n    def __init__(self, store_covariance=False):\n        self.store_covariance = store_covariance\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n        \"\"\"\n        self.x_ = X\n        self.y_ = y\n\n        classes = np.unique(y)\n        self.classes_ = np.sort(classes)\n        n_classes = classes.shape[0]\n        n_features = X.shape[1]\n\n        self.means_ = np.zeros((n_classes, n_features))\n        self.covariances_ = []\n        self.priors_ = np.zeros(n_classes)\n\n        for i in classes:\n            X_i = X[y == i]\n            self.means_[i] = X_i.mean(axis=0)\n            if self.store_covariance:\n                self.covariances_.append(np.cov(X_i.T))\n            self.priors_[i] = X_i.shape[0] / X.shape[0]\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted class label per sample.\n        \"\"\"\n        j = self._get_log_prob(X)\n        return self.classes_[np.argmax(j, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in the model.\n        The columns correspond to the classes in sorted order, as they appear\n            in the attribute `classes_`.\n        \"\"\"\n        j = self._get_log_prob(X)\n        exp_j = np.exp(j - j.max(axis=1)[:, np.newaxis])\n        return exp_j / exp_j.sum(axis=1)[:, np.newaxis]\n\n    def _get_log_prob(self, X):\n        check_is_fitted(self, 'means_')\n\n        X = check_array(X)\n\n        log_prob = np.zeros((X.shape[0], self.means_.shape[0]))\n        for i, (mean, prior) in enumerate(zip(self.means_, self.priors_)):\n            log_prob[:, i] = (\n                np.log(prior)\n                - 0.5 * np.log(np.linalg.det(self.covariances_[i]))\n                - 0.5 * (X - mean).T @ np.linalg.inv(self.covariances_[i])\n                @ (X - mean)\n                - X.shape[1] * np.log(np.sqrt(2 * np.pi))\n            )\n\n        return log_prob\n\n    def score(self, X, y):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True labels for X.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of `self.predict(X)` wrt. `y`.\n        \"\"\"\n        from sklearn.metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X))\n\n    def score_samples(self, X):\n        \"\"\"\n        Return the accuracy score for each sample.\n\n        This function returns the accuracy of each input sample, i.e. the probability\n        that the classifier will predict the correct label for that sample.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        Returns\n        -------\n        scores : array-like of shape (n_samples,)\n            Accuracy scores of the input samples.\n        \"\"\"\n        return self.predict_proba(X).max(axis=1)\n\nclass QuadraticDiscriminantAnalysis(QDA):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    Read more in the :ref:`User Guide <quadratic_discriminant_analysis>`.\n\n    Parameters\n    ----------\n    store_covariance : bool, default=False\n        Whether to store the covariance matrices for each class.\n\n    tol : float, default=1e-4\n        Threshold for rank estimation.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    means_ : ndarray of shape (n_features,)\n        Mean vectors for each class.\n\n    covariances_ : ndarray of shape (n_features, n_features, n_classes) or list of covariances, optional\n        Covariance matrices for each class. Only present if store_covariance is True\n\n    priors_ : ndarray of shape (n_classes,)\n        Prior probabilities of each class.\n\n    x_ : ndarray of shape (n_samples, n_features)\n        Training data.\n\n    y_ : ndarray of shape (n_samples,)\n        Training target.\n    \"\"\"\n\n    def __init__(self, store_covariance=False, tol=1e-4):\n        super().__init__(store_covariance=store_covariance)\n        self.tol = tol\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n        \"\"\"\n        X, y = check_X_y(X, y)\n        self.x_ = X\n        self.y_ = y\n\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        self.means_ = np.zeros((n_classes, n_features))\n        self.covariances_ = np.zeros((n_classes, n_features, n_features))\n        self.priors_ = np.zeros(n_classes)\n\n        for i, (class_sample_indices,) in enumerate(np.argwhere(y == self.classes_)):\n            X_i = X[class_sample_indices]\n            self.means_[i] = X_i.mean(axis=0)\n            self.covariances_[i] = np.atleast_2d(np.cov(X_i.T))\n            self.priors_[i] = X_i.shape[0] / n_samples\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted class label per sample.\n        \"\"\"\n        j = self._get_log_prob(X)\n        return self.classes_[np.argmax(j, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in the model.\n        The columns correspond to the classes in sorted order, as they appear\n            in the attribute `classes_`.\n        \"\"\"\n        j = self._get_log_prob(X)\n        exp_j = np.exp(j - j.max(axis=1)[:, np.newaxis])\n        return exp_j / exp_j.sum(axis=1)[:, np.newaxis]\n\n    def _get_log_prob(self, X):\n        check_is_fitted(self, 'means_')\n\n        X = check_array(X)\n\n        log_prob = np.zeros((X.shape[0], self.means_.shape[0]))\n        for i, (mean, cov, prior) in enumerate(\n            zip(self.means_, self.covariances_, self.priors_)\n        ):\n            log_prob[:, i] = (\n                np.log(prior)\n                - 0.5 * np.log(np.linalg.det(cov))\n                - 0.5 * (X - mean).T @ np.linalg.pinv(cov) @ (X - mean)\n                - X.shape[1] * np.log(np.sqrt(2 * np.pi))\n            )\n\n        return log_prob\n\n    def score(self, X, y):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True labels for X.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of `self.predict(X)` wrt. `y`.\n        \"\"\"\n        from sklearn.metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X))\n\n    def score_samples(self, X):\n        \"\"\"\n        Return the accuracy score for each sample.\n\n        This function returns the accuracy of each input sample, i.e. the probability\n        that the classifier will predict the correct label for that sample.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        Returns\n        -------\n        scores : array-like of shape (n_samples,)\n            Accuracy scores of the input samples.\n        \"\"\"\n        return self.predict_proba(X).max(axis=1)\n```",
        "```python\nclass BinaryTree:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    def delete(self, value):\n        if value < self.value:\n            if self.left is not None:\n                self.left.delete(value)\n        elif value > self.value:\n            if self.right is not None:\n                self.right.delete(value)\n        else:\n            if self.left is not None and self.right is not None:\n                self.value = self.right.get_min_value()\n                self.right.delete(self.value)\n            elif self.left is not None:\n                self.value = self.left.get_max_value()\n                self.left.delete(self.value)\n            elif self.left is None and self.right is None:\n                self.value = None\n            else:\n                self.value = None\n\n    def get_min_value(self):\n        if self.left is None:\n            return self.value\n        else:\n            return self.left.get_min_value()\n\n    def get_max_value(self):\n        if self.right is None:\n            return self.value\n        else:\n            return self.right.get_max_value()\n```",
        "```python\nimport functools\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.order = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self._move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self._move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict_least_recently_used()\n        self.cache[key] = value\n        self._move_to_end(key)\n\n    def _move_to_end(self, key: int) -> None:\n        self.order.remove(key)\n        self.order.append(key)\n\n    def _evict_least_recently_used(self) -> None:\n        key_to_evict = self.order[0]\n        del self.cache[key_to_evict]\n        self.order.remove(key_to_evict)\n\n# Example usage:\n# cache = LRUCache(2)\n# cache.put(1, 1)\n# cache.put(2, 2)\n# cache.get(1) # returns 1\n# cache.put(3, 3) # evicts key 2\n# cache.get(2) # returns -1 (not found)\n# cache.get(3) # returns 3\n```",
        "```python\nfrom pedantic import timer_class, validate\n\nclass PERCEPTRON:\n    \"\"\"\n    A Perceptron machine learning model.\n    \"\"\"\n    @timer_class\n    @validate\n    def __init__(self, xt_: int, r_weight: list[float], r_bias: float) -> None:\n        \"\"\"\n        Initialize the Perceptron model.\n\n        :param xt_: int: A unique identifier for the attribute variable.\n        :param r_weight: list[float]: A list of weights for each input feature.\n        :param r_bias: float: The bias term for the model.\n        \"\"\"\n        self.xt_ = xt_\n        self.r_weight = r_weight\n        self.r_bias = r_bias\n\n    @timer_class\n    @validate\n    def train(self, j_X_train: list[list[float]], j_y_train: list[int]) -> None:\n        \"\"\"\n        Train the Perceptron model.\n\n        :param j_X_train: list[list[float]]: The training input features.\n        :param j_y_train: list[int]: The training output labels.\n        \"\"\"\n        # Add your training logic here\n        pass\n\n    @timer_class\n    @validate\n    def predict(self, j_X_test: list[list[float]]) -> list[int]:\n        \"\"\"\n        Predict the output labels for the given input features.\n\n        :param j_X_test: list[list[float]]: The testing input features.\n        :return: list[int]: The predicted output labels.\n        \"\"\"\n        # Add your prediction logic here\n        return []\n```",
        "```python\nfrom pedantic import timer_class, validate\nfrom hashlib import sha256\n\n@timer_class\nclass ImageClassifier:\n    \"\"\"\n    ImageClassifier performs binary image classification using PyTorch.\n    \"\"\"\n    def __init__(self):\n        self.model = None\n        self.device = 'cpu'\n\n    def load_model(self, model_path: str) -> None:\n        \"\"\"\n        Load a trained PyTorch model from the specified path.\n\n        Args:\n            model_path (str): Path to the saved model.\n        \"\"\"\n        self.model = torch.load(model_path, map_location=self.device)\n\n    def predict(self, image: np.ndarray) -> int:\n        \"\"\"\n        Predict the class of the input image.\n\n        Args:\n            image (np.ndarray): Input image of shape (height, width, channels).\n\n        Returns:\n            int: Predicted class (0 or 1).\n        \"\"\"\n        image = image.transpose((2, 0, 1))  # Convert to channels-first format\n        image = torch.from_numpy(image).float().to(self.device)\n        image = image.unsqueeze(0)  # Add batch dimension\n\n        with torch.no_grad():\n            output = self.model(image)\n            prediction = torch.argmax(output, dim=1).item()\n\n        return prediction\n\n    @validate\n    def save_model(self, model_path: str) -> None:\n        \"\"\"\n        Save the trained model to the specified path.\n\n        Args:\n            model_path (str): Path to save the model.\n        \"\"\"\n        torch.save(self.model, model_path)\n\n# Example usage:\nclassifier = ImageClassifier()\nclassifier.load_model('path/to/model.pth')\nprediction = classifier.predict(image_array)\nclassifier.save_model('path/to/save/model.pth')\n```",
        "```python\ndef is_valid_bst(root: 'TreeNode') -> 'bool':\n    def helper(node: 'TreeNode', min_val: 'int' = float('-inf'), max_val: 'int' = float('inf')) -> 'bool':\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root)\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    # Define a dictionary to map decimal numbers to Roman numerals\n    roman_numerals = {\n        1000: \"M\",\n        900: \"CM\",\n        500: \"D\",\n        400: \"CD\",\n        100: \"C\",\n        90: \"XC\",\n        50: \"L\",\n        40: \"XL\",\n        10: \"X\",\n        9: \"IX\",\n        5: \"V\",\n        4: \"IV\",\n        1: \"I\",\n    }\n    roman_str = \"\"\n    # Iterate through the dictionary and construct the Roman numeral\n    for decimal, numeral in roman_numerals.items():\n        while num >= decimal:\n            roman_str += numeral\n            num -= decimal\n    return roman_str\n```",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"Sorts a list of integers using the insertion sort algorithm.\"\"\"\n    # Add assert statements to ensure the input is a list of integers\n    assert isinstance(arr, list), \"Input must be a list.\"\n    assert all(isinstance(x, int) for x in arr), \"All items in the list must be integers.\"\n\n    # Initialize the sorted list and the current index\n    sorted_arr = []\n    current_index = 0\n\n    # Insert each element from the input list into the sorted list\n    for num in arr:\n        # Insert the current number into the sorted list at the correct position\n        i = 0\n        while i < len(sorted_arr) and sorted_arr[i] < num:\n            i += 1\n\n        # Insert the current number at the correct position\n        sorted_arr.insert(i, num)\n        current_index += 1\n\n    # Return the sorted list\n    return sorted_arr\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression class for performing regression tasks.\n\n    Parameters:\n    ----------\n    alpha : float\n        Regularization strength parameter. Must be a positive float.\n    kernel : str\n        Kernel function used for the regression. Choices are 'linear', 'poly', 'rbf', 'sigmoid', or a custom kernel function.\n    degree : int\n        Degree of the polynomial kernel function if 'poly' is chosen.\n    gamma : float\n        Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Interpretation of the gamma value depends on the kernel chosen.\n    coef0 : float\n        Independent term in the polynomial kernel function. Only used when 'poly' is specified as the kernel.\n\n    Attributes:\n    ----------\n    model : KernelRidge instance\n        The trained Kernel Ridge Regression model.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        kernel: str = \"linear\",\n        degree: int = 3,\n        gamma: float = None,\n        coef0: float = 0.0,\n    ):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.model = None\n\n    def fit(self, X: array, y: array) -> None:\n        \"\"\"\n        Fit the Kernel Ridge Regression model to the training data.\n\n        Parameters:\n        ----------\n        X : array\n            Input training data of shape (n_samples, n_features).\n        y : array\n            Target values of shape (n_samples,).\n\n        Returns:\n        -------\n        None\n        \"\"\"\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n        )\n        self.model.fit(X, y)\n\n    def predict(self, X: array) -> array:\n        \"\"\"\n        Predict target values for the given input data.\n\n        Parameters:\n        ----------\n        X : array\n            Input data of shape (n_samples, n_features).\n\n        Returns:\n        -------\n        array\n            Predicted target values of shape (n_samples,).\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing the merged elements.\n    \"\"\"\n    # Initialize an empty list to store the merged elements\n    merged_list = []\n    i, j = 0, 0\n\n    # Iterate through both lists until all elements are processed\n    while i < len(list1) and j < len(list2):\n        # Compare the current elements and append the smaller one\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    # Append any remaining elements from both lists\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\nimport numpy as np\nfrom pedantic import trace_class, validate\n\n@trace_class\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression Class\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength. Must be a positive float.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : float\n        Independent term in decision function.\n\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    @validate(\"X: array, y: array\", \"X: array-like, y: array-like\")\n    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"\n        Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y)\n        self.coef_ = Lasso(alpha=self.alpha).fit(X, y).coef_\n        self.intercept_ = Lasso(alpha=self.alpha).fit(X, y).intercept_\n\n    @validate(\"X: array\", \"X: array-like\")\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        check_is_fitted(self, \"coef_\")\n        X = check_array(X)\n        return X @ self.coef_ + self.intercept_\n```"
      ]
    }
  ]
}