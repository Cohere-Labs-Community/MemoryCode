{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num):\n    \"\"\"\n    Converts an integer to Roman numerals.\n\n    :param num: An integer to be converted.\n    :return: The corresponding Roman numeral as a string.\n    \"\"\"\n    # Initialize a dictionary with Roman numeral values\n    roman_values = {1000: 'M', 900: 'CM', 500: 'D', 400: 'CD', 100: 'C', 90: 'XC', 50: 'L', 40: 'XL', 10: 'X', 9: 'IX', 5: 'V', 4: 'IV', 1: 'I'}\n    result = ''\n\n    # Iterate through the Roman values and append the corresponding numeral to the result\n    for value, symbol in roman_values.items():\n        count = num // value\n        result += symbol * count\n        num %= value\n\n    return result\n\n\n# Add comments to explain complex sections or provide additional context as needed"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        try:\n            if not min_val < node.val < max_val:\n                return False\n            return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n        except AttributeError:\n            print(\"Invalid tree structure.\")\n            return False\n\n    return helper(root, float('-inf'), float('inf'))"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(vec1_chx, vec2_chx):\n    return sum(x * y for x, y in zip(vec1_chx, vec2_chx))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_sudoku(board):\n    for i in range(9):\n        row = set()\n        col = set()\n        box = set()\n        for j in range(9):\n            current = board[i][j]\n            if current in row or current in col or current in box:\n                return False\n            row.add(current)\n            col.add(current[j])\n            box.add(current[3 * (i // 3) + j // 3])\n    return True\n\n\ndef update_code(board):\n    m_board = []\n    for i in range(9):\n        m_row = []\n        for j in range(9):\n            m_element = board[i][j]\n            m_row.append(m_element)\n        m_board.append(m_row)\n    return m_board\n\n\ndef main():\n    m_input_board = [\n        [\"538\", \"6\", \"7\", \"8\", \"9\", \"1\", \"4\", \"2\", \"3\"],\n        [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"],\n        [\"194\", \"8\", \"3\", \"6\", \"2\", \"7\", \"5\", \"9\", \"6\"],\n        [\"8\", \"5\", \"9\", \"7\", \"6\", \"6\", \"2\", \"3\", \"1\"],\n        [\"4\", \"2\", \"6\", \"3\", \"1\", \"9\", \"8\", \"5\", \"7\"],\n        [\"7\", \"1\", \"3\", \"9\", \"8\", \"4\", \"6\", \"7\", \"2\"],\n        [\"9\", \"6\", \"15\", \"5\", \"3\", \"2\", \"7\", \"8\", \"4\"],\n        [\"2\", \"1\", \"7\", \"9\", \"4\", \"3\", \"8\", \"6\", \"5\"],\n        [\"3\", \"8\", \"4\", \"2\", \"7\", \"6\", \"1\", \"9\", \"5\"],\n    ]\n\n    m_updated_board = update_code(m_input_board)\n\n    m_is_valid = is_valid_sudoku(m_updated_board)\n\n    return m_is_valid\n\n\nprint(main())"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    m_seen = set()\n    for m_i in range(9):\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_i][m_j] != '.':\n                if board[m_i][m_j] in m_seen:\n                    return False\n                m_seen.add(board[m_i][m_j])\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_j][m_i] != '.':\n                if board[m_j][m_i] in m_seen:\n                    return False\n                m_seen.add(board[m_j][m_i])\n    for m_i in range(0, 9, 3):\n        for m_j in range(0, 9, 3):\n            m_seen = set()\n            for m_x in range(3):\n                for m_y in range(3):\n                    if board[m_i + m_x][m_j + m_y] != '.':\n                        if board[m_i + m_x][m_j + m_y] in m_seen:\n                            return False\n                        m_seen.add(board[m_i + m_x][m_j + m_y]])\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(h_vec1, h_vec2):\n    y_result = 0\n    for i in range(len(h_vec1)):\n        y_result += h_vec1[i] * h_vec2[i]\n    return y_result\n```",
        "def is_valid_sudoku(board):\n    for i in range(9):\n        row = set()\n        col = set()\n        box = set()\n        for j in range(9):\n            current = board[i][j]\n            if current in row or current in col or current in box:\n                return False\n            row.add(current)\n            col.add(current)\n            if (i // 3, j // 3) in [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]:\n                box.add(current)\n            elif i // 3 == 2 and j // 3 == 2:\n                box = set(current)\n            else:\n                box_row, box_col = i // 3 * 3 + j % 3, j // 3 * 3 + i % 3\n                box.add(box_row * 100 + box_col * 10 + current)\n        row.clear()\n        col.clear()\n        box.clear()\n    return True"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```",
        "def is_valid_sudoku(y_board):\n    y_seen = set()\n    for y_i in range(9):\n        y_seen = set()\n        for y_j in range(9):\n            if y_board[y_i][y_j] != '.':\n                if y_board[y_i][y_j] in y_seen:\n                    return False\n                y_seen.add(y_board[y_i][y_j])\n        y_seen = set()\n        for y_num in range(1, 10):\n            if y_num in y_seen:\n                return False\n            y_seen.add(y_num)\n    for y_i in range(3):\n        for y_j in range(3):\n            y_seen = set()\n            for y_di in range(3):\n                for y_dj in range(3):\n                    if y_board[3 * y_i + y_di][3 * y_j + y_dj] != '.':\n                        if y_board[3 * y_i + y_di][3 * y_j + y_dj] in y_seen:\n                            return False\n                        y_seen.add(y_board[3 * y_i + y_di][3 * y_j + y_dj])\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = None\n\n    def train(self, X, y):\n        try:\n            assert X.shape[0] == y.shape[0], \"Number of samples in X and y must be the same\"\n            assert np.isfinite(X).all() and np.isfinite(y).all(), \"Input data contains invalid values (nan or inf)\"\n\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n        except AssertionError as e:\n            print(f\"AssertionError: {e}\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n\n    def predict(self, X):\n        try:\n            assert self.model is not None, \"Model has not been trained yet\"\n            assert X.shape[1] == self.model.coef_.shape[0], \"Number of features in X does not match the trained model\"\n            assert np.isfinite(X).all(), \"Input data contains invalid values (nan or inf)\"\n\n            return self.model.predict(X)\n        except AssertionError as e:\n            print(f\"AssertionError: {e}\")\n            return None\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return None\n```",
        "class NearestNeighbors:\n    \"\"\"\n    A Nearest Neighbors classifier.\n    \"\"\"\n\n    def __init__(self):\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        try:\n            assert X_train.shape[0] == y_train.shape[0], \"Number of training samples and labels must be equal.\"\n            self.X_train = X_train\n            self.y_train = y_train\n        except AssertionError as e:\n            print(f\"AssertionError: {e}\")\n\n    def predict(self, X_test):\n        try:\n            assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet.\"\n            assert X_test.shape[1] == self.X_train.shape[1], \"Number of features in test data must match training data.\"\n            distances = [[np.sum(np.sqrt(np.power(X_test[i] - self.X_train[j], 2))) for j in range(self.X_train.shape[0])]]\n            predictions = [self.y_train[np.argmin(dist)] for dist in distances]\n            return predictions\n        except AssertionError as e:\n            print(f\"AssertionError: {e}\")\n        except IndexError:\n            print(\"IndexError: Something went wrong while calculating distances or making predictions.\")"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multivariate Gaussian classification.\n    \"\"\"\n\n    def __init__(self, vr_classes):\n        \"\"\"\n        Initializes the QDA model.\n\n        Parameters:\n            vr_classes (list): List of class labels.\n        \"\"\"\n        self.vr_classes = vr_classes\n        self.vr_class_means = None\n        self.vr_class_covariances = None\n        self.vr_priors = None\n\n    def fit(self, vr_X, vr_y):\n        \"\"\"\n        Fits the QDA model to the training data.\n\n        Parameters:\n            vr_X (ndarray): Training feature matrix of shape (n_samples, n_features).\n            vr_y (ndarray): Target class labels of shape (n_samples,).\n        \"\"\"\n        # Calculate class means\n        self.vr_class_means = {\n            class_label: vr_X[vr_y == class_label].mean(axis=0)\n            for class_label in self.vr_classes\n        }\n\n        # Calculate class covariances\n        self.vr_class_covariances = {\n            class_label: np.cov(vr_X[vr_y == class_label].T)\n            for class_label in self.vr_classes\n        }\n\n        # Calculate class priors\n        self.vr_priors = {\n            class_label: (vr_y == class_label).mean() for class_label in self.vr_classes\n        }\n\n    def predict(self, vr_X):\n        \"\"\"\n        Predicts class labels for new data.\n\n        Parameters:\n            vr_X (ndarray): New feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        # Calculate posterior probabilities\n        vr_posteriors = self._calculate_posteriors(vr_X)\n\n        # Predict class labels\n        return np.array([\n            self.vr_classes[np.argmax(posteriors)] for posteriors in vr_posteriors\n        ])\n\n    def _calculate_posteriors(self, vr_X):\n        \"\"\"\n        Calculates posterior probabilities for each class.\n\n        Parameters:\n            vr_X (ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Array of posterior probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        vr_posteriors = np.empty((vr_X.shape[0], len(self.vr_classes)))\n\n        for i, class_label in enumerate(self.vr_classes):\n            # Calculate prior probability\n            vr_prior = self.vr_priors[class_label]\n\n            # Calculate inverse covariance matrix\n            vr_inv_covariance = np.linalg.inv(self.vr_class_covariances[class_label])\n\n            # Calculate determinant of covariance matrix\n            vr_det_covariance = np.linalg.det(self.vr_class_covariances[class_label])\n\n            # Calculate probability density function\n            pdf = vr_prior * \\\n                np.exp(-0.5 * (vr_X - self.vr_class_means[class_label]) @ vr_inv_covariance\n                        @ (vr_X - self.vr_class_means[class_label]).T) / \\\n                ((2 * np.pi) ** (vr_X.shape[1] / 2) * vr_det_covariance ** 0.5)\n\n            vr_posteriors[:, i] = pdf\n\n        return vr_posteriors",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass SudokuValidator:\n    @staticmethod\n    def @vr_is_valid_sudoku(board: list[list[int]]) -> bool:\n        \"\"\"\n        Checks if a given 9x9 sudoku board is valid.\n\n        Args:\n            board: The sudoku board to be validated.\n\n        Returns:\n            bool: True if the board is a valid sudoku, False otherwise.\n        \"\"\"\n        # Check rows and columns\n        for i in range(9):\n            row = set(board[i])\n            col = set(board[j][i] for j in range(9))\n            if len(row) != 9 or len(col) != 9:\n                return False\n\n        # Check 3x3 subgrids\n        for i in range(0, 9, 3):\n            for j in range(0, 9, 3):\n                subgrid = set()\n                for x in range(i, i + 3):\n                    for y in range(j, j + 3):\n                        subgrid.add(board[x][y])\n                if len(subgrid) != 9:\n                    return False\n\n        return True\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "def is_valid_sudoku(board):\n    vr_seen = set()\n    vr_rows = [set() for _ in range(9)]\n    vr_cols = [set() for _ in range(9)]\n    vr_boxes = [set() for _ in range(9)]\n\n    for vr_i in range(9):\n        for vr_j in range(9):\n            vr_num = board[vr_i][vr_j]\n            if vr_num != '.':\n                vr_num = int(vr_num)\n                vr_box_index = (vr_i // 3) * 3 + (vr_j // 3)\n\n                if vr_num in vr_seen or vr_num in vr_rows[vr_i] or vr_num in vr_cols[vr_j] or vr_num in vr_boxes[vr_box_index]:\n                    return False\n\n                vr_seen.add(vr_num)\n                vr_rows[vr_i].add(vr_num)\n                vr_cols[vr_j].add(vr_num)\n                vr_boxes[vr_box_index].add(vr_num)\n\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTree:\n    \"\"\"\n    A binary tree data structure.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        :param value: The value to be inserted.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        \"\"\"\n        Recursive helper function for inserting a value into the binary tree.\n\n        :param value: The value to be inserted.\n        :param node: The current node in the recursive traversal.\n        \"\"\"\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"\n        Deletes the first node with the given value from the binary tree.\n\n        :param value: The value of the node to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(value, self.root)\n\n    def _delete_recursive(self, value, node):\n        \"\"\"\n        Recursive helper function for deleting a node with the given value.\n\n        :param value: The value of the node to be deleted.\n        :param node: The current node in the recursive traversal.\n        :return: The new node after deletion.\n        \"\"\"\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(value, node.left)\n        elif value > node.value:\n            node.right = self._delete_recursive(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.value, node.right)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        \"\"\"\n        Helper function to find the inorder successor (smallest value in the right subtree).\n\n        :param node: The current node in the right subtree.\n        :return: The value of the inorder successor.\n        \"\"\"\n        while node.left is not None:\n            node = node.left\n        return node.value\n\n\nclass Node:\n    \"\"\"\n    A node in the binary tree.\n    \"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"\n        Initializes a new node with the given value.\n\n        :param value: The value of the node.\n        \"\"\"\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    \"\"\"\n    Checks if the given sudoku board is valid.\n\n    A valid sudoku board has the following rules:\n        - Each row must contain the digits 1-9 without repetition.\n        - Each column must contain the digits 1-9 without repetition.\n        - Each 3x3 sub-box must contain the digits 1-9 without repetition.\n\n    Args:\n        board (list[list[int]]): The sudoku board to be validated.\n\n    Returns:\n        bool: True if the board is a valid sudoku, False otherwise.\n    \"\"\"\n    x_seen = [[{} for _ in range(9)] for _ in range(9)]\n\n    for x_row in range(9):\n        for x_col in range(9):\n            x_num = board[x_row][x_col]\n            if x_num not in {1, 2, 3, 4, 5, 6, 7, 8, 9}:\n                return False\n\n            # Check row\n            if x_num in x_seen[x_row][x_col]:\n                return False\n            x_seen[x_row][x_col][x_num] = True\n\n            # Check column\n            if x_num in x_seen[x_col][x_row]:\n                return False\n            x_seen[x_col][x_row][x_num] = True\n\n            # Check 3x3 box\n            x_box_row, x_box_col = 3 * (x_row // 3), 3 * (x_col // 3)\n            for x_i in range(x_box_row, x_box_row + 3):\n                for x_j in range(x_box_col, x_box_col + 3):\n                    if x_num in x_seen[x_i][x_j]:\n                        return False\n                    x_seen[x_i][x_j][x_num] = True\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "def is_valid_sudoku(x_board):\n    x_seen = [[set() for _ in range(9)] for _ in range(9)]\n\n    for x_i in range(9):\n        for x_j in range(9):\n            if x_board[x_i][x_j] != '.':\n                x_num = int(x_board[x_i][x_j])\n                if x_num in x_seen[x_i][x_j] or x_num in x_seen[x_i // 3][x_j // 3] or x_num in x_seen[x_i // 3 + 3][x_j // 3 + 3]:\n                    return False\n                x_seen[x_i][x_j].add(x_num)\n                x_seen[x_i // 3][x_j // 3].add(x_num)\n                x_seen[x_i // 3 + 3][x_j // 3 + 3].add(x_num)\n\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_a(left_half)\n    right_half = merge_sort_a(right_half)\n\n    return merge_a(left_half, right_half)\n\ndef merge_a(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    A scikit-learn compatible kernel ridge regression class.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', degree=3, coef0=1.0, kernel_params=None,\n                 algorithm='auto', tol=1e-3, fit_intercept=True, verbose=False):\n        \"\"\"\n        Initialize the KernelRidgeRegression class.\n\n        Parameters\n        ----------\n        alpha : float, default=1.0\n            Regularization strength; must be a positive float. Regularization improves the\n            conditioning of the problem and reduces the variance of the estimates. Larger values\n            specify stronger regularization. Alpha can also be a string, specifying the type of\n            regularization: 'auto' or 'jl'.\n        kernel : string, default='linear'\n            Kernel to be used ('linear', 'poly', 'rbf', 'sigmoid', 'cosine' or a callable).\n            If none is given, 'linear' will be used.\n        degree : int, default=3\n            Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        coef0 : float, default=1.0\n            Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n        kernel_params : dict of string/float pairs, default=None\n            Parameters (keyword arguments) and values for kernel function passed as dictionary.\n        algorithm : {'auto', 'svd', 'cholesky', 'lsqr'}, default='auto'\n            Algorithm used in the KRR solutions.\n            'auto' will attempt to choose the best method,\n            'svd' is a singular value decomposition of X,\n            'cholesky' is a decomposition of (X.dot(X^T) + alpha * Identity),\n            'lsqr' uses a least-squares solver from scipy.linalg.\n        tol : float, default=1e-3\n            Precision of the solution.\n        fit_intercept : boolean, default=True\n            Whether to calculate the intercept for this model. If set\n            to false, no intercept will be used in calculations\n            (e.g. data is expected to be already centered).\n        verbose : boolean, default=False\n            If True, the timings of fitting and predicting are printed.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.algorithm = algorithm\n        self.tol = tol\n        self.fit_intercept = fit_intercept\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, degree=degree, coef0=coef0, kernel_params=kernel_params,\n            algorithm=algorithm, tol=tol, fit_intercept=fit_intercept)\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as training data and y as target values.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the kernel ridge model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        return self.model.predict(X)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import validate\n\n@validate\ndef int_to_roman(num: int) -> str:\n    \"\"\"\n    Convert an integer to Roman numerals.\n\n    :param num: The integer to be converted.\n    :return: The Roman numeral representation of the input integer.\n    \"\"\"\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman_num = \"\"\n    i = 0\n    while num > 0:\n        for _ in range(num // val[i]):\n            roman_num += syb[i]\n            num -= val[i]\n        i += 1\n    return roman_num"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import validate\r\n\r\n@validate\r\ndef int_to_roman(num: int) -> str:\r\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\r\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\r\n    roman_num = ''\r\n    i = 0\r\n    while num > 0:\r\n        for _ in range(num // val[i]):\r\n            roman_num += syb[i]\r\n            num -= val[i]\r\n        i += 1\r\n    return roman_num"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    def __init__(self):\n        self.head_j = None\n\n    def append_node(self, data_j):\n        new_node_j = Node(data_j)\n        if self.head_j is None:\n            self.head_j = new_node_j\n            return\n        current_node_j = self.head_j\n        while current_node_j.next_j:\n            current_node_j = current_node_j.next_j\n        current_node_j.next_j = new_node_j\n\n    def delete_node(self, data_j):\n        if self.head_j is None:\n            return\n        if self.head_j.data_j == data_j:\n            self.head_j = self.head_j.next_j\n            return\n        current_node_j = self.head_j\n        prev_node_j = None\n        while current_node_j and current_node_j.data_j != data_j:\n            prev_node_j = current_node_j\n            current_node_j = current_node_j.next_j\n        if current_node_j is None:\n            return\n        prev_node_j.next_j = current_node_j.next_j\n\n    def display(self):\n        current_node_j = self.head_j\n        while current_node_j:\n            print(current_node_j.data_j, end=' ')\n            current_node_j = current_node_j.next_j\n        print()\n\n\nclass Node:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None",
        "class Perceptron:\n    def __init__(self):\n        self.weight_chx_j = None\n        self.bias_chx_j = None\n\n    def train(self, X, y, learning_rate=0.1, epochs=1000):\n        self.weight_chx_j = np.zeros(X.shape[1])\n        self.bias_chx_j = 0\n        for _ in range(epochs):\n            for i, x_i in enumerate(X):\n                prediction = self.predict(x_i)\n                error = y[i] - prediction\n                self.weight_chx_j += learning_rate * error * x_i\n                self.bias_chx_j += learning_rate * error\n\n    def predict(self, x):\n        return self.weight_chx_j.dot(x) + self.bias_chx_j"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_j = None\n\n    def append(self, data_j):\n        if not self.head_j:\n            self.head_j = Node(data_j)\n        else:\n            curr_j = self.head_j\n            while curr_j.next_j:\n                curr_j = curr_j.next_j\n            curr_j.next_j = Node(data_j)\n\n    def display(self):\n        curr_j = self.head_j\n        while curr_j:\n            print(curr_j.data_j, end=\" -> \")\n            curr_j = curr_j.next_j\n        print(\"None\")\n```",
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, value):\n        if not self.head:\n            self.head = Node(value)\n        else:\n            current = self.head\n            while current.next_r:\n                current = current.next_r\n            current.next_r = Node(value)\n\n    def delete(self, value):\n        if self.head.value == value:\n            self.head = self.head.next_r\n            return\n        current = self.head\n        prev = None\n        while current and current.value != value:\n            prev = current\n            current = current.next_r\n        if current:\n            prev.next_r = current.next_r\n\n    def display(self):\n        current = self.head\n        while current:\n            print(current.value, end=\" -> \")\n            current = current.next_r\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.next_r = None\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_r = data\n        self.next_r = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data):\n        if not self.head_r:\n            self.head_r = Node(data)\n        else:\n            curr_r = self.head_r\n            while curr_r.next_r:\n                curr_r = curr_r.next_r\n            curr_r.next_r = Node(data)\n\n    def display(self):\n        if not self.head_r:\n            return\n        curr_r = self.head_r\n        while curr_r:\n            print(curr_r.data_r, end=\" -> \")\n            curr_r = curr_r.next_r\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result",
        "class Node:\n    def __init__(self, data=None):\n        self.data = data\n        self.next_b = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head_b = None\n\n    def append_b(self, data):\n        new_node = Node(data)\n        if not self.head_b:\n            self.head_b = new_node\n        else:\n            current_b = self.head_b\n            while current_b.next_b:\n                current_b = current_node.next_b\n            current_b.next_b = new_node\n\n    def display_b(self):\n        current_b = self.head_b\n        while current_b:\n            print(current_b.data, end=\" -> \")\n            current_b = current_b.next_b\n        print(\"None\")"
      ],
      "instruction_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_at = data\n        self.next_at = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_at = None\n        self.tail_at = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_at:\n            self.head_at = new_node\n            self.tail_at = self.head_at\n        else:\n            self.tail_at.next_at = new_node\n            self.tail_at = new_node\n\n    def delete(self, data):\n        if self.head_at is None:\n            return\n\n        if self.head_at.data_at == data:\n            self.head_at = self.head_at.next_at\n            return\n\n        prev_node = self.head_at\n        curr_node = self.head_at.next_at\n        while curr_node:\n            if curr_node.data_at == data:\n                prev_node.next_at = curr_node.next_at\n                return\n            prev_node = curr_node\n            curr_node = curr_node.next_at\n\n    def display(self):\n        current = self.head_at\n        while current:\n            print(current.data_at, end=\" -> \")\n            current = current.next_at\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert a value into the binary tree\n    def insert1(self, value: int) -> None:\n        self.root = self._insert(self.root, value)\n\n    def _insert(self, node: Optional[Node], value: int) -> Node:\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(node.left, value)\n        else:\n            node.right = self._insert(node.right, value)\n        return node\n\n    # Delete a value from the binary tree\n    def delete1(self, value: int) -> None:\n        self.root = self._delete(self.root, value)\n\n    def _delete(self, node: Optional[Node], value: int) -> Optional[Node]:\n        if node is None:\n            return None\n        if value < node.value:\n            node.left = self._delete(node.left, value)\n        elif value > node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            # Case 1: Node with only one child or no child\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Case 2: Node with two children\n            successor = self._get_successor(node)\n            node.value = successor.value\n            node.right = self._delete(node.right, successor.value)\n\n        return node\n\n    def _get_successor(self, node: Node) -> Node:\n        current = node.right\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nfrom typing import Callable, Dict, List, Union\n\n# Directed Graph class representing a directed graph\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    # Method to add a directed edge between two vertices\n    def add_edge(self, u: int, v: int) -> None:\n        if u not in self.graph:\n            self.graph[u] = [v]\n        else:\n            self.graph[u].append(v)\n\n    # Method to detect the presence of cycles in the directed graph\n    def has_cycle(self, u: int, visited: List[int] = None, parent: Dict[int, int] = None) -> bool:\n        if visited is None:\n            visited = []\n        if parent is None:\n            parent = {}\n\n        visited.append(u)\n        for neighbour in self.graph.get(u, []):\n            if neighbour not in visited:\n                parent[neighbour] = u\n                if self.has_cycle(neighbour, visited, parent):\n                    return True\n            elif parent.get(u, -1) != neighbour:\n                return True\n\n        return False\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1, kernel_params=None):\n        \"\"\"\n        :param alpha: float\n            Regularization strength; must be a positive float. Regularization improves the\n            conditioning of the problem and reduces the variance of the estimates. Larger\n            values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other\n            linear models such as LogisticRegression or LinearSVC. If an array is passed,\n            ``alpha`` is used as the ``alpha`` value for all kernels.\n        :param kernel: string, callable or None, default=\"linear\"\n            Specifies the kernel type to be used in the algorithm. This parameter can be\n            either a string or a callable. If none is given, 'linear' will be used. If a\n            string is given, it must be one of the metrics in\n            ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``. If a callable is given it is used as\n            the kernel directly and should be of the form:\n            ``kernel(X, Y=None, eval_gradient=False) -> (K, g)``, where ``K`` is the\n            kernel matrix and ``g`` is the gradient wrt ``X`` (of the ``kernel`` function).\n        :param gamma: float, default=None\n            Gamma parameter for the kernel. Ignored by not specified by the kernel.\n            Use ``gamma='auto'`` for automatic scale estimation.\n        :param degree: int, default=3\n            Degree of the polynomial kernel. Ignored by other kernels.\n        :param coef0: float, default=1\n            Independent term in kernel function. Only significant in\n            'poly' and 'sigmoid'.\n        :param kernel_params: dict of string -> any, default=None\n            Parameters (keyword arguments) and values for kernel passed as callable object.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit Kernel Ridge regression model\n\n        :param X: array-like of shape (n_samples, n_features)\n            Training data\n        :param y: array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :param sample_weight: float or array-like of shape (n_samples,), default=None\n            Individual weights for each sample, ignored if None is passed.\n        \"\"\"\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.model.fit(X, y, sample_weight=sample_weight)\n\n    def predict(self, X):\n        \"\"\"Predict using the kernel ridge model\n\n        :param X: array-like of shape (n_samples, n_features)\n            Samples\n        :return: array-like of shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"L\": 50, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for numeral, value in roman_numerals.items():\n        while num >= value:\n            roman_str += numeral\n            num -= value\n    return roman_str\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for binary classification.\n    \"\"\"\n    def __init__(self, store_covariance=True):\n        \"\"\"\n        Initializes the QuadraticDiscriminantAnalysis class.\n        \n        Parameters:\n            store_covariance (bool): Whether to store the covariance matrices for each class.\n                Defaults to True.\n        \"\"\"\n        self.store_covariance = store_covariance\n        self.classes_ = None\n        self.class_prior_ = None\n        self.class_covariance_ = None\n        self.class_means_ = None\n        self.covariance_ = None\n        self.means_ = None\n        self.priors_ = None\n        self.scalings_ = None\n        self.variance_ = None\n        self.x_ = None\n        self.y_ = None\n\n    def fit(self, X: np.array, y: np.array):\n        \"\"\"\n        Fits the QDA model according to the given training data and parameters.\n        \n        Parameters:\n            X (np.array): The input training data of size [n_samples, n_features].\n            y (np.array): The target training data of size [n_samples].\n        \"\"\"\n        self.x_ = X\n        self.y_ = y\n        self._validate_data(X, y)\n        self._check_multiclass(y)\n        self._check_no_null_classes(y)\n\n        self.classes_ = np.unique(y)\n        self.class_means_ = np.array([X[y == class_].mean(axis=0) for class_ in self.classes_])\n        self.class_covariance_ = np.array([np.cov(X[y == class_].T) for class_ in self.classes_])\n        self.covariance_ = np.cov(X.T)\n        self.means_ = np.mean(X, axis=0)\n        self.variance_ = np.var(X, axis=0)\n        self.priors_ = np.mean(y == self.classes_, axis=0)\n        self.scalings_ = (self.class_covariance_ - self.covariance_) / self.variance_\n        self.class_prior_ = np.array([np.log(priori) + np.log(np.linalg.det(cov))\n                                     for priori, cov in zip(self.priors_, self.class_covariance_)])\n\n    def predict(self, X: np.array) -> np.array:\n        \"\"\"\n        Predicts the class labels for the provided data.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n        \n        Returns:\n            np.array: Predicted class labels of size [n_samples].\n        \"\"\"\n        self._check_is_fitted()\n        X = self._validate_data(X, reset=False)\n        jll = self._joint_log_likelihood(X)\n        return self.classes_[np.argmax(jll, axis=1)]\n\n    def predict_log_proba(self, X: np.array) -> np.array:\n        \"\"\"\n        Returns the log-probability estimates for the provided data.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n        \n        Returns:\n            np.array: Log-probability estimates of size [n_samples, n_classes].\n        \"\"\"\n        self._check_is_fitted()\n        X = self._validate_data(X, reset=False)\n        jll = self._joint_log_likelihood(X)\n        return jll\n\n    def predict_proba(self, X: np.array) -> np.array:\n        \"\"\"\n        Returns the probability estimates for the provided data.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n        \n        Returns:\n            np.array: Probability estimates of size [n_samples, n_classes].\n        \"\"\"\n        self._check_is_fitted()\n        X = self._validate_data(X, reset=False)\n        jll = self._joint_log_likelihood(X)\n        return np.exp(jll - jll.max(axis=1)[:, np.newaxis])\n\n    def score(self, X: np.array, y: np.array) -> float:\n        \"\"\"\n        Returns the mean accuracy of the model's predictions on the given data.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n            y (np.array): The target data of size [n_samples].\n        \n        Returns:\n            float: Mean accuracy of the model's predictions.\n        \"\"\"\n        self._check_is_fitted()\n        X, y = self._validate_data(X, y, multi_output=True)\n        return self._score_samples(X, y).mean()\n\n    def score_samples(self, X: np.array) -> np.array:\n        \"\"\"\n        Returns the accuracy of the model's predictions for each sample in the data.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n        \n        Returns:\n            np.array: Accuracy of the model's predictions for each sample.\n        \"\"\"\n        self._check_is_fitted()\n        X = self._validate_data(X, reset=False)\n        return self.classes_[np.argmax(self._joint_log_likelihood(X), axis=1)] == X\n\n    def _joint_log_likelihood(self, X: np.array) -> np.array:\n        \"\"\"\n        Computes the joint log-likelihood of the samples in X.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n        \n        Returns:\n            np.array: Joint log-likelihood of the samples.\n        \"\"\"\n        X = self._validate_data(X, reset=False)\n        log_det = np.log(np.linalg.det(self.covariance_))\n        log_prob = np.log(np.linalg.det(self.class_covariance_))\n        maha = np.sum((X - self.class_means_) ** 2, axis=1)[:, np.newaxis]\n        maha_scalings = maha / self.variance_\n        maha_cov = maha - np.sum(self.scalings_ * X ** 2, axis=1)[:, np.newaxis]\n        return -(X.shape[1] / 2) * np.log(2 * np.pi) - 0.5 * (log_det + maha_cov + log_prob) + self.class_prior_\n\n    def _validate_data(\n        self,\n        X: np.array,\n        y: np.array = None,\n        reset: bool = True,\n        multi_output: bool = False,\n    ) -> np.array:\n        \"\"\"\n        Validates the input data and performs necessary checks.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n            y (np.array): The target data of size [n_samples] (optional).\n            reset (bool): Whether to reset the stored covariance matrix. Defaults to True.\n            multi_output (bool): Whether to handle multi-output data. Defaults to False.\n        \n        Returns:\n            np.array: The validated input data.\n        \"\"\"\n        X = np.array(X)\n        if y is not None:\n            y = np.array(y)\n        if reset:\n            self.covariance_ = None\n        self._check_X_y(X, y)\n        self._check_n_features(X, reset)\n        X = self._check_input_values(X)\n        if y is not None:\n            self._validate_y_class_prior(y)\n        return X\n\n    def _check_X_y(self, X: np.array, y: np.array = None):\n        \"\"\"\n        Checks that X and y have a correct number of samples.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n            y (np.array): The target data of size [n_samples] (optional).\n        \"\"\"\n        n_samples_X = X.shape[0]\n        if y is not None:\n            n_samples_y = y.shape[0]\n            if n_samples_X != n_samples_y:\n                raise ValueError(\n                    \"X and y should have same number of samples: got ({} {},) and ({},) respectively\".format(\n                        n_samples_X, X.shape[1], n_samples_y\n                    )\n                )\n\n    def _check_n_features(self, X: np.array, reset: bool):\n        \"\"\"\n        Checks that the number of features in X matches the stored covariance.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n            reset (bool): Whether to reset the stored covariance matrix.\n        \"\"\"\n        n_features = X.shape[1]\n        if self.covariance_ is not None and self.covariance_.shape[0] != n_features:\n            if reset:\n                self.covariance_ = None\n                self.class_covariance_ = None\n                self.scalings_ = None\n                self.variance_ = None\n            else:\n                raise ValueError(\n                    \"X has different number of features than the data used for fitting: expected {}, got {}\".format(\n                        self.covariance_.shape[0], n_features\n                    )\n                )\n\n    def _check_input_values(self, X: np.array) -> np.array:\n        \"\"\"\n        Checks that the input data does not contain any NaN or infinite values.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n        \n        Returns:\n            np.array: The validated input data.\n        \"\"\"\n        X = np.asarray_chkfinite(X, dtype=np.float64)\n        return X\n\n    def _validate_y_class_prior(self, y: np.array):\n        \"\"\"\n        Validates the target data and class prior probabilities.\n        \n        Parameters:\n            y (np.array): The target data of size [n_samples].\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        self.classes_.sort()\n        self.priors_ = np.bincount(y) / y.shape[0]\n        self.class_prior_ = np.log(self.priors_)\n\n    def _check_is_fitted(self):\n        \"\"\"\n        Checks if the model has been fitted.\"\"\"\n        check_is_fitted(self, [\"class_means_\", \"class_covariance_\"])\n\n    def _check_no_null_classes(self, y: np.array):\n        \"\"\"\n        Checks that there are no null classes in the target data.\n        \n        Parameters:\n            y (np.array): The target data of size [n_samples].\n        \"\"\"\n        if (y == self.classes_[0]).all():\n            raise ValueError(\"All target values are the same: {}\" \"\".format(self.classes_[0]))\n\n    def _check_multiclass(self, y: np.array):\n        \"\"\"\n        Checks that the target data contains more than two classes.\n        \n        Parameters:\n            y (np.array): The target data of size [n_samples].\n        \"\"\"\n        classes = np.unique(y)\n        if len(classes) < 2:\n            raise ValueError(\"y should have more than one class: got {}\".format(len(classes)))\n\n    def _score_samples(self, X: np.array, y: np.array = None) -> np.array:\n        \"\"\"\n        Returns the accuracy of the model's predictions for each sample in the data.\n        \n        Parameters:\n            X (np.array): The input data of size [n_samples, n_features].\n            y (np.array): The target data of size [n_samples] (optional).\n        \n        Returns:\n            np.array: Accuracy of the model's predictions for each sample.\n        \"\"\"\n        jll = self._joint_log_likelihood(X)\n        if y is None:\n            return self.classes_[np.argmax(jll, axis=1)]\n        check_consistent_length(jll, y)\n        y = column_or_1d(y, warn=True)\n        return self.classes_[np.argmax(jll, axis=1)] == y\n\n    def get_params(self, deep: bool = True) -> Dict[str, object]:\n        \"\"\"\n        Returns the parameters of the model.\n        \n        Parameters:\n            deep (bool): Whether to return deep copies of the parameters. Defaults to True.\n        \n        Returns:\n            Dict[str, object]: Model parameters.\n        \"\"\"\n        return {\"store_covariance\": self.store_covariance}\n\n    def set_params(self, **params: Dict[str, object]) -> QuadraticDiscriminantAnalysis:\n        \"\"\"\n        Sets the parameters of the model.\n        \n        Parameters:\n            **params (Dict[str, object]): Model parameters to be set.\n        \n        Returns:\n            QuadraticDiscriminantAnalysis: The updated model instance.\n        \"\"\"\n        valid_params = self.get_params(deep=False)\n        for key, value in params.items():\n            if key not in valid_params:\n                raise ValueError(\"Invalid parameter: {}\".format(key))\n            setattr(self, key, value)\n        return self\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.base import BaseEstimator\n\nclass NearestNeighborsModel(BaseEstimator):\n    \"\"\"\n    A custom Nearest Neighbors classifier model.\n\n    ...\n\n    Attributes\n    ----------\n    neighbors : int, default=5\n        Number of neighbors to use for classification.\n\n    ...\n\n    Methods\n    -------\n    train(X_train, y_train)\n        Trains the model using the provided training data.\n\n    predict(X_test)\n        Predicts the class labels for the provided test data.\n    \"\"\"\n\n    def __init__(self, neighbors=5):\n        self.neighbors = neighbors\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Trains the Nearest Neighbors classifier model.\n\n        Parameters\n        ----------\n        X_train : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y_train : array-like, shape (n_samples,)\n            Target class labels for the training data.\n        \"\"\"\n        self.model = KNeighborsClassifier(n_neighbors=self.neighbors)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the class labels for the provided test data.\n\n        Parameters\n        ----------\n        X_test : array-like, shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        y_pred : array-like, shape (n_samples,)\n            Predicted class labels for the test data.\n        \"\"\"\n        return self.model.predict(X_test)\n```",
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def add_node(self, data):\n        node = Node(data)\n        if not self.head:\n            self.head = node\n        else:\n            current = self.head\n            while current.next:\n                current = current.next\n            current.next = node\n\n    def remove_node(self, data):\n        if not self.head:\n            return\n        if self.head.data == data:\n            self.head = self.head.next\n            return\n        current = self.head\n        prev = None\n        while current and current.data != data:\n            prev = current\n            current = current.next\n        if current is None:\n            return\n        prev.next = current.next\n\n    def display(self):\n        current = self.head\n        while current:\n            print(current.data, end=\" \")\n            current = current.next\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n```",
        "```python\nfrom typing import List\n\n\nclass Perceptron:\n    def __init__(self, weights: List[float], bias: float):\n        self.weights = weights\n        self.bias = bias\n\n    def train(self, X_train: List[List[float]], y_train: List[float]):\n        \"\"\"\n        Train the perceptron model.\n\n        Parameters:\n        X_train (List[List[float]]): The input training data.\n        y_train (List[float]): The corresponding target values.\n        \"\"\"\n        # Your code here\n\n    def predict(self, X: List[List[float]]) -> List[float]:\n        \"\"\"\n        Predict the output for the given input data.\n\n        Parameters:\n        X (List[List[float]]): The input data.\n\n        Returns:\n        List[float]: The predicted output values.\n        \"\"\"\n        # Your code here\n```",
        "```python\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort(left)\n    right = merge_sort(right)\n\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```",
        "```python\nfrom typing import List\n\n\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: int, v: int):\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: int, visited: List[bool], rec_stack: List[bool]):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        vertices = len(self.graph)\n        visited = [False] * vertices\n        rec_stack = [False] * vertices\n\n        for i in range(vertices):\n            if not visited[i]:\n                if self.is_cyclic_util(i, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\ndef dot_product(vector_a: list[float], vector_b: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_a (list[float]): A list of floating-point numbers representing the first vector.\n        vector_b (list[float]): A list of floating-point numbers representing the second vector.\n\n    Returns:\n        float: The dot product of vector_a and vector_b.\n    \"\"\"\n    # Validate input\n    if not vector_a or not vector_b:\n        raise ValueError(\"Input vectors cannot be empty.\")\n    if len(vector_a) != len(vector_b):\n        raise ValueError(\"Input vectors must have the same length.\")\n\n    # Calculate the dot product\n    result = 0.0\n    for i in range(len(vector_a)):\n        result += vector_a[i] * vector_b[i]\n\n    return result\n```",
        "```python\ndef is_valid_sudoku(board: list[list[str]]) -> bool:\n    seen = set()\n    for row in board:\n        if not is_unique(row, seen):\n            return False\n        seen.clear()\n\n    for col in zip(*board):\n        if not is_unique(col, seen):\n            return False\n        seen.clear()\n\n    for i in range(0, 9, 3):\n        for j in range(0, 9, 3):\n            box = [board[r][c] for r in range(i, i + 3) for c in range(j, j + 3)]\n            if not is_unique(box, seen):\n                return False\n            seen.clear()\n\n    return True\n\ndef is_unique(items: list[str], seen: set[str]) -> bool:\n    for item in items:\n        if item != \".\" and item in seen:\n            return False\n        seen.add(item)\n    return True\n```",
        "```python\ndef is_valid_bst(root, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    \"\"\"\n    Determines if a given tree is a valid binary search tree.\n\n    Args:\n        root (TreeNode): The root of the binary tree.\n        min_val (int): The minimum value allowed in the BST.\n        max_val (int): The maximum value allowed in the BST.\n\n    Returns:\n        bool: True if the tree is a valid BST, False otherwise.\n    \"\"\"\n    # Base case: If the root is None, the tree is valid\n    if not root:\n        return True\n\n    # Check if the root's value is within the allowed range\n    if not (min_val < root.val < max_val):\n        return False\n\n    # Recursively check the left and right subtrees\n    return is_valid_bst(root.left, min_val, root.val) and is_valid_bst(\n        root.right, root.val, max_val\n    )\n```",
        "```python\ndef dot_product(vector1: list[float], vector2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vector1: list of float, the first vector\n    :param vector2: list of float, the second vector\n    :return: float, the dot product of the two vectors\n    \"\"\"\n    # Validate input lengths\n    if len(vector1) != len(vector2):\n        raise ValueError(\"Input vectors must have the same length\")\n\n    # Validate input dimensions\n    if len(vector1) == 0 or len(vector2) == 0:\n        raise ValueError(\"Input vectors cannot be empty\")\n\n    # Calculate the dot product\n    result = 0.0\n    for i in range(len(vector1)):\n        result += vector1[i] * vector2[i]\n\n    return result\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.base import BaseEstimator\n\n\nclass LassoRegression(BaseEstimator):\n    \"\"\"\n    Lasso Regression model for prediction.\n\n    Parameters:\n    -----------\n    alpha : float\n        Regularization strength. Must be a positive float.\n\n    Attributes:\n    ----------\n    coef_ : array, shape (n_features,)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : array, shape (1,)\n        Independent term in decision function.\n\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self._model = Lasso(alpha=alpha)\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as training data and y as target values.\n\n        Parameters:\n        ----------\n        X : array, shape (n_samples, n_features)\n            Training data.\n\n        y : array, shape (n_samples,)\n            Target values.\n\n        Returns:\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters:\n        ----------\n        X : array, shape (n_samples, n_features)\n            Samples.\n\n        Returns:\n        -------\n        C : array, shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        return self._model.predict(X)\n```"
      ]
    }
  ]
}