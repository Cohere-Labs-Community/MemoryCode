{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\n\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multi-class classification problems.\n    \"\"\"\n\n    def __init__(self, priors=None, reg_covar=0.0):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n            priors (ndarray, shape (n_classes,)): Prior probabilities of each class.\n                If None, priors are calculated using a smoothed version of the\n                training data class probabilities.\n            reg_covar (float): Regularization term for the covariance matrix.\n                Defaults to 0.0.\n        \"\"\"\n        self.priors = priors\n        self.reg_covar = reg_covar\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the training data.\n\n        Parameters:\n            X (ndarray, shape (n_samples, n_features)): Training data.\n            y (ndarray, shape (n_samples,)): Target labels.\n\n        Returns:\n            self: Fitted QDA model.\n        \"\"\"\n        # Calculate class priors\n        classes = np.unique(y)\n        if self.priors is None:\n            self.priors = np.zeros(len(classes))\n            for i, cls in enumerate(classes):\n                self.priors[i] = np.mean(y == cls)\n        else:\n            assert len(self.priors) == len(\n                classes\n            ), \"Number of priors must match number of classes.\"\n\n        # Calculate mean and covariance for each class\n        self.means_ = np.array(\n            [np.mean(X[y == cls], axis=0) for cls in classes]\n        )\n        self.covars_ = np.array(\n            [\n                np.cov(X.T[y == cls]) + self.reg_covar * np.eye(X.shape[1])\n                for cls in classes\n            ]\n        )\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for new data.\n\n        Parameters:\n            X (ndarray, shape (n_samples, n_features)): New data.\n\n        Returns:\n            ndarray, shape (n_samples,): Predicted class labels.\n        \"\"\"\n        # Calculate posterior probability for each class\n        posteriors = self._get_posteriors(X)\n\n        # Predict class with maximum posterior probability\n        return np.argmax(posteriors, axis=1)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities for new data.\n\n        Parameters:\n            X (ndarray, shape (n_samples, n_features)): New data.\n\n        Returns:\n            ndarray, shape (n_samples, n_classes): Class probabilities.\n                Each row sums to 1 and represents the probability of each class.\n        \"\"\"\n        # Calculate posterior probability for each class\n        posteriors = self._get_posteriors(X)\n\n        # Normalize posteriors to get class probabilities\n        return posteriors / posteriors.sum(axis=1)[:, np.newaxis]\n\n    def _get_posteriors(self, X):\n        \"\"\"\n        Calculate posterior probabilities for each class given new data.\n\n        Parameters:\n            X (ndarray, shape (n_samples, n_features)): New data.\n\n        Returns:\n            ndarray, shape (n_samples, n_classes): Posterior probabilities.\n        \"\"\"\n        # Calculate log-likelihood for each class\n        log_likelihoods = np.array(\n            [\n                self._log_likelihood(X, self.means_[i], self.covars_[i])\n                for i in range(len(self.means_))\n            ]\n        )\n\n        # Apply Bayes' theorem to get posterior probabilities\n        posteriors = self.priors * np.exp(log_likelihoods)\n        return posteriors\n\n    @staticmethod\n    def _log_likelihood(X, mean, covar):\n        \"\"\"\n        Calculate the log-likelihood of new data given a Gaussian distribution.\n\n        Parameters:\n            X (ndarray, shape (n_samples, n_features)): New data.\n            mean (ndarray, shape (n_features,)): Mean of the distribution.\n            covar (ndarray, shape (n_features, n_features)): Covariance matrix.\n\n        Returns:\n            ndarray, shape (n_samples,): Log-likelihood values.\n        \"\"\"\n        n_features = covar.shape[0]\n        det_covar = np.linalg.det(covar)\n        inv_covar = np.linalg.inv(covar)\n\n        # Calculate log-likelihood\n        part1 = -0.5 * n_features * np.log(2 * np.pi)\n        part2 = -0.5 * np.log(det_covar)\n        part3 = -0.5 * np.sum(\n            (X - mean) @ inv_covar * (X - mean).T, axis=1\n        )\n        return part1 + part2 + part3\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_a_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_a_a(left_half)\n    right_half = merge_sort_a_a(right_half)\n\n    return merge_a_a(left_half, right_half)\n\ndef merge_a_a(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_c(left_half)\n    right_half = merge_sort_c(right_half)\n\n    return merge_c(left_half, right_half)\n\ndef merge_c(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_gn() -> list[int]:\n    pass\n\ndef _merge_gn(left: list[int], right: list[int]) -> list[int]:\n    pass\n\ndef merge_sort_gn(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_gn(left)\n    right = merge_sort_gn(right)\n\n    return _merge_gn(left, right)\n\ndef _merge_gn(left: list[int], right: list[int]) -> list[int]:\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_gn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_gn(left_half)\n    right_half = merge_sort_gn(right_half)\n\n    return merge_gn(left_half, right_half)\n\ndef merge_gn(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_d():\n    pass"
      ],
      "instruction_model_output": [
        "def merge_sort_d(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    left = merge_sort_d(left)\n    right = merge_sort_d(right)\n    \n    return merge_d(left, right)\n\ndef merge_d(left, right):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ImageClassifier, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim) \n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(0.1)\n        self.fc2 = torch.nn.Linear(hidden_dim, output_dim) \n\n    def forward(self, x):\n        w_x = self.fc1(x)\n        w_x = self.relu(w_x)\n        w_x = self.dropout(w_x)\n        w_logits = self.fc2(w_x)\n        return w_logits",
        "```python\ndef is_valid_sudoku(board):\n    w_row = [{} for _ in range(9)]\n    w_col = [{} for _ in range(9)]\n    w_box = [{} for _ in range(9)]\n\n    for i in range(9):\n        for j in range(9):\n            if board[i][j] != '.':\n                num = board[i][j]\n                w_row[i][num] = w_row[i].get(num, 0) + 1\n                w_col[j][num] = w_col[j].get(num, 0) + 1\n                w_box[3 * (i // 3) + (j // 3)][num] = w_box[3 * (i // 3) + (j // 3)].get(num, 0) + 1\n\n                if w_row[i][num] > 1 or w_col[j][num] > 1 or w_box[3 * (i // 3) + (j // 3)][num] > 1:\n                    return False\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x",
        "def is_valid_sudoku(w_board):\n    w_seen = set()\n    for w_i in range(9):\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_i][w_j] != '.':\n                if w_board[w_i][w_j] in w_seen:\n                    return False\n                w_seen.add(w_board[w_i][w_j])\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_j][w_i] != '.':\n                if w_board[w_j][w_i] in w_seen:\n                    return False\n                w_seen.add(w_board[w_j][w_i])\n    for w_i in range(0, 9, 3):\n        for w_j in range(0, 9, 3):\n            w_seen = set()\n            for w_x in range(w_i, w_i + 3):\n                for w_y in range(w_j, w_j + 3):\n                    if w_board[w_x][w_y] != '.':\n                        if w_board[w_x][w_y] in w_seen:\n                            return False\n                        w_seen.add(w_board[w_x][w_y])\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\n\n@retry\ndef majority_element(arr: list[int]) -> int:\n    if not arr:\n        raise ValueError(\"Input array cannot be empty\")\n    \n    count = {}\n    for num in arr:\n        count[num] = count.get(num, 0) + 1\n        if count[num] > len(arr) // 2:\n            return num\n    \n    raise ValueError(\"No majority element found\")\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import retry\n\n@retry\ndef majority_element(arr):\n    from collections import Counter\n\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    \"\"\"\n    Checks if the given sudoku board is valid.\n\n    Args:\n        board (list[list[int]]): The sudoku board to be validated.\n\n    Returns:\n        bool: True if the board is a valid sudoku, False otherwise.\n    \"\"\"\n    y_seen = [[{} for _ in range(9)] for _ in range(9)]\n    for y_i, row in enumerate(board):\n        for x_i, num in enumerate(row):\n            if num == 0:\n                continue\n            if num in y_seen[y_i][x_i] or num in y_seen[y_i][x_i].values() or num in [row[x_i] for row in board]:\n                return False\n            y_seen[y_i][x_i][num] = True\n    return True\n```",
        "```python\ndef find_kth_largest(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Parameters:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' for the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    sorted_arr = sorted(arr, reverse=True)\n\n    # Return the kth element from the sorted array\n    return sorted_arr[k-1]\n\n# Function annotation example:\n# find_kth_largest(arr=[3, 1, 5, 4, 2], k=3) -> 3\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(y_board):\n    y_seen = set()\n    for y_i in range(9):\n        y_seen = set()\n        for y_j in range(9):\n            if y_board[y_i][y_j] != '.':\n                if y_board[y_i][y_j] in y_seen:\n                    return False\n                y_seen.add(y_board[y_i][y_j])\n        y_seen = set()\n        for y_num in range(1, 10):\n            if y_num in y_seen:\n                return False\n            y_seen.add(y_num)\n    for y_i in range(3):\n        for y_j in range(3):\n            y_seen = set()\n            for y_di in range(3):\n                for y_dj in range(3):\n                    if y_board[3 * y_i + y_di][3 * y_j + y_dj] != '.':\n                        if y_board[3 * y_i + y_di][3 * y_j + y_dj] in y_seen:\n                            return False\n                        y_seen.add(y_board[3 * y_i + y_di][3 * y_j + y_dj])\n    return True",
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert node method with docstring and variable naming convention\n    def insert(self, new_value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n\n        :param new_value: The value to be inserted into the binary tree.\n        \"\"\"\n        if not self.root:\n            self.root = Node(new_value)\n        else:\n            self._insert_recursive(self.root, new_value)\n\n    def _insert_recursive(self, node, new_value):\n        if new_value < node.value:\n            if node.left is None:\n                node.left = Node(new_value)\n            else:\n                self._insert_recursive(node.left, new_value)\n        else:\n            if node.right is None:\n                node.right = Node(new_value)\n            else:\n                self._insert_recursive(node.right, new_value)\n\n    # Delete node method with docstring and variable naming convention\n    def delete(self, key):\n        \"\"\"\n        Delete the node with the given key value from the binary tree.\n\n        :param key: The value of the node to be deleted from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, key)\n\n    def _delete_recursive(self, node, key):\n        if node is None:\n            return node\n\n        if key < node.value:\n            node.left = self._delete_recursive(node.left, key)\n        elif key > node.value:\n            node.right = self._delete_recursive(node.right, key)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    \"\"\"\n    Checks if the given sudoku board is valid.\n\n    A valid sudoku board of size 9x9 must have:\n    1. Each number from 1 to 9 in each row.\n    2. Each number from 1 to 9 in each column.\n    3. Each number from 1 to 9 in each 3x3 sub-box.\n\n    Args:\n        board (list[list[int]]): The sudoku board to be validated.\n\n    Returns:\n        bool: True if the board is a valid sudoku, False otherwise.\n    \"\"\"\n    # Check if the board size is valid\n    if len(board) != 9 or any(len(row) != 9 for row in board):\n        return False\n\n    # Create three empty sets for rows, columns, and sub-boxes\n    rows, cols, boxes = {}, {}, {}\n\n    for i in range(9):\n        for j in range(9):\n            # Get the current number on the board\n            num = board[i][j]\n\n            # If the number is 0, skip it (empty cell)\n            if num == 0:\n                continue\n\n            # Check rows, columns, and sub-boxes\n            if num in rows.get(i, set()):\n                return False\n            if num in cols.get(j, set()):\n                return False\n            if num in boxes.get(i // 3 * 3 + j // 3, set()):\n                return False\n\n            # Update rows, columns, and sub-boxes\n            rows[i] = rows.get(i, set()).union({num})\n            cols[j] = cols.get(j, set()).union({num})\n            boxes[i // 3 * 3 + j // 3] = boxes.get(i // 3 * 3 + j // 3, set()).union({num})\n\n    # If there are no conflicts, the board is valid\n    return True\n\n\n# Example usage (not provided as per the request)\n# board = ...\n# print(is_valid_sudoku(board))\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "def is_valid_sudoku(board):\n    m_seen = set()\n    for m_i in range(9):\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_i][m_j] != '.':\n                if board[m_i][m_j] in m_seen:\n                    return False\n                m_seen.add(board[m_i][m_j])\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_j][m_i] != '.':\n                if board[m_j][m_i] in m_seen:\n                    return False\n                m_seen.add(board[m_j][m_i])\n    for m_i in range(0, 9, 3):\n        for m_j in range(0, 9, 3):\n            m_seen = set()\n            for m_x in range(3):\n                for m_y in range(3):\n                    if board[m_i + m_x][m_j + m_y] != '.':\n                        if board[m_i + m_x][m_j + m_y] in m_seen:\n                            return False\n                        m_seen.add(board[m_i + m_x][m_j + m_y]])\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_fn(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_fn(left_half)\n    right_half = merge_sort_fn(right_half)\n    \n    return merge_fn(left_half, right_half)\n\ndef merge_fn(left, right):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result",
        "def is_valid_sudoku_board_fn(board: list[list[int]]) -> bool:\n    z_rows = [set() for _ in range(9)]\n    z_cols = [set() for _ in range(9)]\n    z_boxes = [set() for _ in range(9)]\n\n    for z_i in range(9):\n        for z_j in range(9):\n            if board[z_i][z_j] != '.':\n                z_num = board[z_i][z_j]\n                z_box_index = 3 * (z_i // 3) + (z_j // 3)\n\n                if z_num in z_rows[z_i] or z_num in z_cols[z_j] or z_num in z_boxes[z_box_index]:\n                    return False\n\n                z_rows[z_i].add(z_num)\n                z_cols[z_j].add(z_num)\n                z_boxes[z_box_index].add(z_num)\n\n    return True"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_fn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_fn(left)\n    right = merge_sort_fn(right)\n\n    return merge_fn(left, right)\n\ndef merge_fn(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```",
        "def is_valid_sudoku(z_board):\n    z_rows = [set() for _ in range(9)]\n    z_cols = [set() for _ in range(9)]\n    z_boxes = [set() for _ in range(9)]\n\n    for z_i in range(9):\n        for z_j in range(9):\n            z_num = z_board[z_i][z_j]\n            if z_num != '.':\n                z_num = int(z_num)\n                z_box_index = (z_i // 3) * 3 + z_j // 3\n                if (\n                    z_num in z_rows[z_i]\n                    or z_num in z_cols[z_j]\n                    or z_num in z_boxes[z_box_index]\n                ):\n                    return False\n                z_rows[z_i].add(z_num)\n                z_cols[z_j].add(z_num)\n                z_boxes[z_box_index].add(z_num)\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\ndef is_valid_bst(root: Optional['Node']) -> bool:\n    def helper(node: Optional['Node']) -> Tuple[bool, int, int]:\n        if not node:\n            return True, float('-inf'), float('inf')\n\n        left_valid, left_min, left_max = helper(node.left)\n        right_valid, right_min, right_max = helper(node.right)\n\n        if node.value <= left_max or node.value >= right_min:\n            return False, 0, 0\n\n        return True, min(node.value, left_min), max(node.value, right_max)\n\n    try:\n        assert root is not None\n        valid, _, _ = helper(root)\n        assert valid\n        return True\n    except AssertionError:\n        return False\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nimport assert\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A Nearest Neighbors Model for classification.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the model using the provided training data.\n\n        :param X_train: Training input data.\n        :param y_train: Training target data.\n        \"\"\"\n        try:\n            self.model.fit(X_train, y_train)\n            assert self.model is not None, \"Model is not trained properly.\"\n        except Exception as e:\n            print(f\"An error occurred while training the model: {e}\")\n            raise ValueError(\"Failed to train the model.\")\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the target values for the provided test data.\n\n        :param X_test: Test input data.\n        :return: Predicted target values.\n        \"\"\"\n        try:\n            assert X_test.shape[1] == self.model.n_features_in_, \"Test data feature dimensions do not match training data.\"\n            predictions = self.model.predict(X_test)\n            assert predictions is not None, \"Failed to make predictions.\"\n            return predictions\n        except Exception as e:\n            print(f\"An error occurred while making predictions: {e}\")\n            raise ValueError(\"Failed to predict target values.\")\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def wr_is_valid_sudoku(board):\n    try:\n        for i in range(9):\n            row = set()\n            col = set()\n            box = set()\n            for j in range(9):\n                current_num = board[i][j]\n                if current_num in row or current_num in col or current_num in box:\n                    return False\n                row.add(current_num)\n                col.add(current_num)\n                if (i // 3) * 3 + j // 3 not in box:\n                    box.add(current_num)\n\n        return True\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False",
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.wr_alpha = alpha\n        self.wr_coef_ = None\n\n    def train(self, X, y):\n        try:\n            from sklearn.linear_model import Lasso\n            self.wr_model = Lasso(alpha=self.wr_alpha)\n            self.wr_model.fit(X, y)\n            self.wr_coef_ = self.wr_model.coef_\n        except Exception as e:\n            print(f\"Error occurred while training the Lasso model: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.wr_coef_ is None:\n                print(\"Model has not been trained yet. Please call the train method first.\")\n                return None\n            return X @ self.wr_coef_\n        except Exception as e:\n            print(f\"Error occurred while making predictions: {e}\")\n            return None"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    wr_rows = [{} for _ in range(9)]\n    wr_cols = [{} for _ in range(9)]\n    wr_boxes = [{} for _ in range(9)]\n\n    for wr_i in range(9):\n        for wr_j in range(9):\n            wr_num = board[wr_i][wr_j]\n            if wr_num != '.':\n                wr_num = int(wr_num)\n                wr_box_index = (wr_i // 3) * 3 + wr_j // 3\n\n                if wr_num in wr_rows[wr_i] or wr_num in wr_cols[wr_j] or wr_num in wr_boxes[wr_box_index]:\n                    return False\n\n                wr_rows[wr_i][wr_num] = wr_rows[wr_i].get(wr_num, 0) + 1\n                wr_cols[wr_j][wr_num] = wr_cols[wr_j].get(wr_num, 0) + 1\n                wr_boxes[wr_box_index][wr_num] = wr_boxes[wr_box_index].get(wr_num, 0) + 1\n\n    return True",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half_x = arr[:mid]\n    right_half_x = arr[mid:]\n\n    left_half_x = merge_sort_x(left_half_x)\n    right_half_x = merge_sort_x(right_half_x)\n\n    return merge_x(left_half_x, right_half_x)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_y(left_half)\n    right_half = merge_sort_y(right_half)\n\n    return merge_y(left_half, right_half)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def b_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def b_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def gn_merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = gn_merge_sort_b(arr[:mid])\n    right_half = gn_merge_sort_b(arr[mid:])\n\n    return gn_merge_b(left_half, right_half)\n\ndef gn_merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result",
        "def gn_h_index_computation_b(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result",
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def average_diagonal(matrix):\n    assert len(matrix) == len(matrix[0]), \"Input must be a square matrix.\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"All rows must have the same length as the number of rows.\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diagonal_avg = diagonal_sum / len(matrix)\n\n    return diagonal_avg"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Perceptron:\n    def __init__(self, input_dim, output_dim):\n        self.chx_input_dim = input_dim\n        self.chx_output_dim = output_dim\n        self.chx_weights = np.random.rand(input_dim, output_dim)\n\n    def train(self, X, y, epochs, learning_rate):\n        for epoch in range(epochs):\n            # Forward pass\n            self.chx_outputs = X @ self.chx_weights\n            # Backward pass\n            weight_updates = X.T @ (self.chx_outputs - y)\n            self.chx_weights += learning_rate * weight_updates\n\n    def predict(self, X):\n        return X @ self.chx_weights"
      ],
      "instruction_model_output": [
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.r_queue.remove(key)\n        self.r_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.r_queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            del_key = self.r_queue.pop(0)\n            del self.cache[del_key]\n        self.r_queue.append(key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self, digit_attr=4):\n        self.coef_ = None\n        self.intercept_ = None\n        self.digit_attr = digit_attr\n\n    def fit(self, X, y):\n        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term\n        self.coef_ = np.linalg.inv(X.T @ X) @ (X.T @ y)\n        self.intercept_ = self.coef_[0]\n        self.coef_ = self.coef_[1:]\n\n    def predict(self, X):\n        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term\n        return X @ self.coef_ + self.intercept_"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import rename_kwargs\n\n@rename_kwargs\ndef greatest_common_divisor(a: int, b: int) -> int:\n    while b:\n        a, b = b, a % b\n    return a"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_head = None\n        self.xt_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_head(key)\n        elif len(self.cache) == self.capacity:\n            self._evict_tail()\n        self._insert_head(key, value)\n\n    def _move_to_head(self, key: int) -> None:\n        node = self.cache[key]\n        self._remove_node(node)\n        self._insert_head(key, node)\n\n    def _insert_head(self, key: int, value: int) -> None:\n        node = Node(key, value)\n        node.xt_next = self.xt_head\n        self.xt_head = node\n        if self.xt_tail is None:\n            self.xt_tail = node\n        self.cache[key] = node\n\n    def _evict_tail(self) -> None:\n        key = self.xt_tail.key\n        self._remove_node(self.xt_tail)\n        del self.cache[key]\n\n    def _remove_node(self, node: 'Node') -> None:\n        if node.xt_prev:\n            node.xt_prev.xt_next = node.xt_next\n        else:\n            self.xt_head = node.xt_next\n\n        if node.xt_next:\n            node.xt_next.xt_prev = node.xt_prev\n        else:\n            self.xt_tail = node.xt_prev\n\n    def d_print_cache(self) -> None:\n        node = self.xt_head\n        while node:\n            print(f\"Key: {node.key}, Value: {node.value}\")\n            node = node.xt_next\n\n\nclass Node:\n    def __init__(self, key: int, value: int):\n        self.key = key\n        self.value = value\n        self.xt_prev = None\n        self.xt_next = None\n```",
        "```python\ndef d_calculate_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Calculates the h-index based on the given citations.\n\n    Parameters:\n        citations (list[int]): A list of integer citations.\n\n    Returns:\n        int: The calculated h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```",
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def y_compute_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class 4PCA:\n    def __init__(self, data):\n        self.i_data = data\n        self.i_mean = None\n        self.i_cov = None\n        self.i_eigenvalues = None\n        self.i_eigenvectors = None\n\n    def i_fit(self):\n        self.i_mean = np.mean(self.i_data, axis=0)\n        self.i_data -= self.i_mean\n        self.i_cov = np.cov(self.i_data.T)\n        self.i_eigenvalues, self.i_eigenvectors = np.linalg.eig(self.i_cov)\n\n    def i_transform(self):\n        return self.i_data @ self.i_eigenvectors",
        "class 7LRUCache:\n    def __init__(self, capacity: int):\n        self.c_ = capacity\n        self.m_ = {}\n        self.q_ = collections.deque()\n\n    def i_get(self, key: int) -> int:\n        if key not in self.i_m:\n            return -1\n        self.i_q.remove(key)\n        self.i_q.appendleft(key)\n        return self.i_m[key]\n\n    def i_put(self, key: int, value: int) -> None:\n        if key in self.i_m:\n            self.i_q.remove(key)\n        elif len(self.i_q) == self.i_c:\n            self.i_m.pop(self.i_q.pop())\n        self.i_m[key] = value\n        self.i_q.appendleft(key)"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\nclass Library:\n    def __init__(self):\n        self.chx_books = []\n\n    @validate\n    def add_books(self, chx_title: str, chx_author: str):\n        new_book = {\n            'chx_title': chx_title,\n            'chx_author': chx_author\n        }\n        self.chx_books.append(new_book)\n\n    @validate\n    def remove_book(self, chx_title: str):\n        self.chx_books = [book for book in self.chx_books if book['chx_title'] != chx_title]\n```",
        "from pedantic import validate\n\n@validate\ndef sum_unique_numbers(numbers: list[int]) -> int:\n    variable_chx = list(set(numbers))\n    sum_chx = 0\n    for number_chx in variable_chx:\n        sum_chx += number_chx\n    return sum_chx"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None",
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache.pop(key)\n        self.cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.pop(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value"
      ],
      "cumulative_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pedantic import timer_class, retry\n\n\n@timer_class\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```",
        "```python\nfrom pedantic import timer_class, rename_kwargs\n\n@timer_class\nclass Perceptron:\n    def __init__(self, w=None, b=0):\n        self.w = w\n        self.b = b\n\n    @rename_kwargs\n    def train(self, X, y):\n        # Training logic here\n        pass\n\n    def predict(self, X):\n        # Prediction logic here\n        pass\n```",
        "```python\ndef kth_largest_element(arr, k):\n    arr.sort(reverse=True)\n    return arr[k - 1]\n```",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    seen = set()\n    for row in board:\n        if len(row) != 9 or not is_unique(row):\n            return False\n        for num in row:\n            if num != 0:\n                seen.add(num)\n    for i in range(9):\n        col = [board[r][i] for r in range(9)]\n        if not is_unique(col):\n            return False\n        diag = [board[r][r] for r in range(9)]\n        if not is_unique(diag):\n            return False\n        diag = [board[r][8 - r] for r in range(9)]\n        if not is_unique(diag):\n            return False\n    return True\n\ndef is_unique(nums: list[int]) -> bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return False\n        seen.add(num)\n    return True\n```",
        "```python\nimport numpy as np\n\n@timer_class\nclass LinearRegression:\n    \"\"\"\n    Linear Regression class for predicting a target variable using a linear combination of features.\n    \"\"\"\n\n    def __init__(self, digit: int):\n        \"\"\"\n        Initializes the LinearRegression class.\n\n        Parameters:\n        digit (int): A single digit from 0 to 9, included in the class name for consistency.\n        \"\"\"\n        self.digit = digit\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"\n        Fits the linear regression model to the training data.\n\n        Parameters:\n        X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the training data.\n        y (np.ndarray): A 1D array of shape (n_samples,) containing the target variable.\n        \"\"\"\n        # Add try-except block for error handling\n        try:\n            self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y\n            self.intercept_ = np.mean(y - X @ self.coef_)\n        except np.linalg.LinAlgError:\n            print(\"Singular matrix encountered. Unable to invert the matrix.\")\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the target variable for the given features.\n\n        Parameters:\n        X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the input features.\n\n        Returns:\n        np.ndarray: A 1D array of shape (n_samples,) containing the predicted target values.\n        \"\"\"\n        # Add try-except block for error handling\n        try:\n            return X @ self.coef_ + self.intercept_\n        except ValueError:\n            print(\"Input matrix dimensions mismatch. Unable to perform matrix multiplication.\")\n\n    def get_coef(self) -> np.ndarray:\n        \"\"\"\n        Retrieves the coefficient values of the linear model.\n\n        Returns:\n        np.ndarray: A 1D array of shape (n_features,) containing the coefficient values.\n        \"\"\"\n        return self.coef_\n\n    def get_intercept(self) -> float:\n        \"\"\"\n        Retrieves the intercept value of the linear model.\n\n        Returns:\n        float: The intercept value of the linear model.\n        \"\"\"\n        return self.intercept_\n\n    def set_intercept(self, intercept: float) -> None:\n        \"\"\"\n        Sets the intercept value of the linear model.\n\n        Parameters:\n        intercept (float): The new intercept value to be set.\n        \"\"\"\n        self.intercept_ = intercept\n\n    def save(self, filename: str) -> None:\n        \"\"\"\n        Saves the linear regression model to a file.\n\n        Parameters:\n        filename (str): The name of the file to save the model to.\n        \"\"\"\n        np.savez(filename, coef=self.coef_, intercept=self.intercept_)\n\n    @classmethod\n    def load(cls, filename: str) -> \"LinearRegression\":\n        \"\"\"\n        Loads a linear regression model from a file.\n\n        Parameters:\n        filename (str): The name of the file to load the model from.\n\n        Returns:\n        LinearRegression: The loaded linear regression model.\n        \"\"\"\n        # Add try-except block for error handling\n        try:\n            with np.load(filename) as data:\n                coef = data[\"coef\"]\n                intercept = data[\"intercept\"]\n                model = cls(digit=0)  # Default digit value\n                model.coef_ = coef\n                model.intercept_ = intercept\n                return model\n        except FileNotFoundError:\n            print(\"File not found. Unable to load the linear regression model.\")\n            return None\n```",
        "```python\nfrom sklearn.decomposition import PCA\nfrom pedantic import timer_class, rename_kwargs\n\n@timer_class\nclass PCAWrapper(PCA):\n    \"\"\"\n    Principal component analysis (PCA) wrapper class.\n\n    This class provides a wrapper around the PCA class from scikit-learn,\n    adding custom functionality and adhering to coding guidelines.\n    \"\"\"\n\n    def __init__(self, n_components=None, copy=True, whiten=False,\n                 renormalize_whitened=True, **kwargs):\n        \"\"\"\n        Initialize the PCA wrapper class.\n\n        Parameters\n        ----------\n        n_components : int or None, optional (default=None)\n            Number of components to keep.\n        copy : bool, optional (default=True)\n            If False, data passed to fit are overwritten and running\n            fit(X, y) is not possible thereafter.\n        whiten : bool, optional (default=False)\n            When True (False by default) will (will not) whiten the\n            components after PCA.\n        renormalize_whitened : bool, optional (default=True)\n            Whether or not to renormalize the components after whitening.\n            Only used if whiten is True.\n        kwargs : optional\n            Keyword arguments to be passed to the PCA class.\n        \"\"\"\n        super().__init__(\n            n_components=n_components,\n            copy=copy,\n            whiten=whiten,\n            renormalize_whitened=renormalize_whitened,\n            **kwargs\n        )\n\n    @rename_kwargs(\"n_components\", \"copy\", \"whiten\", \"renormalize_whitened\")\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model with X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n        y : ignored\n        \"\"\"\n        return super().fit(X)\n\n    @rename_kwargs(\"n_components\", \"copy\", \"whiten\", \"renormalize_whitened\")\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : ignored\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        return super().fit_transform(X)\n\n    @rename_kwargs(\"n_components\", \"copy\", \"whiten\", \"renormalize_whitened\")\n    def transform(self, X):\n        \"\"\"\n        Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        return super().transform(X)\n\n    @rename_kwargs(\"n_components\", \"copy\", \"whiten\", \"renormalize_whitened\")\n    def inverse_transform(self, X):\n        \"\"\"\n        Transform data back to its original space.\n\n        In other words, return an input X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n            New data, where n_samples is the number of samples\n            and n_components is the number of components.\n\n        Returns\n        -------\n        X_original : array-like, shape (n_samples, n_features)\n        \"\"\"\n        return super().inverse_transform(X)\n\n    @property\n    @rename_kwargs(\"n_components\", \"mean_\", \"components_\", \"noise_variance_\", \"explained_variance_\", \"explained_variance_ratio_\")\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, optional (default=True)\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        \"\"\"\n        return super().get_params(deep=deep)\n```",
        "```python\nfrom collections import Counter\n\n\ndef find_majority_element(arr):\n    counter = Counter(arr)\n    majority_count = len(arr) // 2\n    for num, count in counter.items():\n        if count > majority_count:\n            return num\n```",
        "```python\ndef h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "```python\nclass BinaryTree:\n    \"\"\"\n    A binary tree data structure with methods to insert and delete nodes.\n    \"\"\"\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n        \"\"\"\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        \"\"\"\n        Helper method to recursively insert a new node into the binary tree.\n        \"\"\"\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        \"\"\"\n        Delete the first node with the given value from the binary tree.\n        \"\"\"\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        \"\"\"\n        Helper method to recursively delete the first node with the given value.\n        \"\"\"\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        \"\"\"\n        Helper method to find the minimum value in a subtree.\n        \"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    \"\"\"\n    A node in the binary tree with a value and left and right child nodes.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5, metric: str = \"euclidean\"):\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n\n    def train(self, X_train: array, y_train: array):\n        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, metric=self.metric)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: array) -> array:\n        return self.model.predict(X_test)\n```",
        "```python\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, book):\n        self.books.append(book)\n\n    def remove_book(self, book):\n        self.books.remove(book)\n```",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n\n    def get(self, key):\n        \"\"\"\n        Retrieve the value for a given key from the cache.\n        If the key is not found, return -1.\n        \"\"\"\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key, value):\n        \"\"\"\n        Insert a new key-value pair into the cache,\n        or update the value for an existing key.\n        If the cache is at capacity, remove the least recently used item.\n        \"\"\"\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            self.cache.popitem(last=False)\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for binary and multi-class classification.\n    \n    Attributes:\n        @timer_class: Decorator for performance profiling and debugging.\n        _a: Suffix for function names as per coding convention.\n        clf: QDA classifier instance.\n    \n    Methods:\n        fit(X, y): Fit the QDA model according to the given training data and target values.\n        predict(X): Predict the class labels for the provided data.\n        predict_proba(X): Predict the class probabilities for the provided data.\n        score(X, y): Evaluate the model's performance using the provided data and target values.\n    \"\"\"\n\n    @timer_class\n    def __init__(self):\n        self.clf = QDA()\n\n    @timer_class\n    def fit(self, X: np.array, y: np.array) -> QuadraticDiscriminantAnalysis:\n        \"\"\"\n        Fit the QDA model according to the given training data and target values.\n        \n        Args:\n            X (np.array): Training data of shape (n_samples, n_features).\n            y (np.array): Target values of shape (n_samples,).\n\n        Returns:\n            QuadraticDiscriminantAnalysis: Fitted QDA model instance.\n        \"\"\"\n        self.clf.fit(X, y)\n        return self\n\n    @timer_class\n    def predict(self, X: np.array) -> np.array:\n        \"\"\"\n        Predict the class labels for the provided data.\n        \n        Args:\n            X (np.array): Data of shape (n_samples, n_features).\n\n        Returns:\n            np.array: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        return self.clf.predict(X)\n\n    @timer_class\n    def predict_proba(self, X: np.array) -> np.array:\n        \"\"\"\n        Predict the class probabilities for the provided data.\n        \n        Args:\n            X (np.array): Data of shape (n_samples, n_features).\n\n        Returns:\n            np.array: Predicted class probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        return self.clf.predict_proba(X)\n\n    @timer_class\n    def score(self, X: np.array, y: np.array) -> float:\n        \"\"\"\n        Evaluate the model's performance using the provided data and target values.\n        \n        Args:\n            X (np.array): Data of shape (n_samples, n_features).\n            y (np.array): Target values of shape (n_samples,).\n\n        Returns:\n            float: The mean accuracy of the model.\n        \"\"\"\n        return self.clf.score(X, y)\n```",
        "```python\ndef is_valid_bst(root: 'TreeNode') -> bool:\n    def helper(node: 'TreeNode', min_val: int, max_val: int) -> bool:\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```",
        "```python\ndef gcd(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of a and b.\n    \"\"\"\n    assert a >= 0 and b >= 0, \"Input integers must be non-negative.\"\n    if a == 0 or b == 0:\n        return max(a, b)\n    while b != 0:\n        a, b = b, a % b\n    return a\n```",
        "```python\nfrom collections import OrderedDict\n\n\ndef sum_unique_numbers(numbers: list[int]) -> int:\n    unique_numbers = OrderedDict()\n    sum = 0\n\n    for number in numbers:\n        if number not in unique_numbers:\n            unique_numbers[number] = True\n            sum += number\n\n    return sum\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression Class\n\n    Attributes:\n        model (Lasso): The Lasso regression model.\n        coef_ (ndarray): The coefficient of the features in the model.\n        intercept_ (float): The intercept of the model.\n\n    Methods:\n        train(X_train, y_train): Trains the Lasso regression model.\n        predict(X_test): Predicts the target values for the provided data.\n    \"\"\"\n\n    def __init__(self):\n        self.model = Lasso()\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Trains the Lasso regression model.\n\n        Parameters:\n            X_train (ndarray): The input training data.\n            y_train (ndarray): The target training data.\n        \"\"\"\n        # Check if X_train and y_train are acceptable\n        X_train, y_train = check_X_y(X_train, y_train, accept_sparse=['csr', 'csc', 'coo']\n        # Check if y_train is a valid classification target\n        check_classification_targets(y_train)\n        # Fit the model\n        self.model.fit(X_train, y_train)\n        # Store the coefficients and intercept\n        self.coef_ = self.model.coef_\n        self.intercept_ = self.model.intercept_\n\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the target values for the provided data.\n\n        Parameters:\n            X_test (ndarray): The input data for prediction.\n\n        Returns:\n            ndarray: Predicted target values.\n        \"\"\"\n        # Check if the model is fitted\n        check_is_fitted(self.model)\n        # Check if X_test is acceptable\n        X_test = check_array(X_test, accept_sparse=['csr', 'csc', 'coo'])\n        # Return the predicted target values\n        return self.model.predict(X_test)\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort an array of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The array of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted array in ascending order.\n    \"\"\"\n    # Base case: if the array has 0 or 1 element, it is already sorted\n    if len(arr) <= 1:\n        return arr\n\n    # Divide the array into two halves\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    # Recursively sort each half\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    # Merge the sorted halves\n    merged = []\n    left_idx, right_idx = 0, 0\n    while left_idx < len(left_half) and right_idx < len(right_half):\n        if left_half[left_idx] < right_half[right_idx]:\n            merged.append(left_half[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right_half[right_idx])\n            right_idx += 1\n\n    # Add any remaining elements from both halves\n    merged.extend(left_half[left_idx:])\n    merged.extend(right_half[right_idx:])\n\n    return merged\n```",
        "```python\ndef average_diagonal_element(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    if not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input matrix is not square.\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```"
      ]
    }
  ]
}