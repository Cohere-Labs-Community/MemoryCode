{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"Input dimensions mismatch\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        assert X_test.shape[1] == self.model.n_features_in_, \"Test data feature number mismatch\"\n        return self.model.predict(X_test)\n```"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num):\n    \"\"\"\n    Convert an integer to a Roman numeral.\n    :param num: The integer to be converted.\n    :return: The corresponding Roman numeral as a string.\n    \"\"\"\n    # Define a dictionary with Roman numeral values\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman\n\n\n# Add a docstring to the function for explanation and usage\n\"\"\"\nConvert an integer to a Roman numeral.\n\n:param num: The integer to be converted.\n:type num: int\n:return: The corresponding Roman numeral as a string.\n:rtype: str\n\nUsage:\n>>> print(int_to_roman(3))\nIII\n>>> print(int_to_roman(42))\nXLII\n>>> print(int_to_roman(2018))\nMMXVIII\n\"\"\""
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def gn_h_index_x(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def gn_shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        distance = [float('inf')] * self.V\n        distance[src] = 0\n        queue = []\n        queue.append(src)\n\n        while queue:\n            u = queue.pop(0)\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    distance[v] = distance[u] + 1\n                    queue.append(v)\n                    visited[v] = True\n        return distance[dest]"
      ],
      "instruction_model_output": [
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\n\n@retry\ndef find_majority_element(arr: list[int]) -> int:\n    \"\"\"\n    Returns the majority element of an array.\n\n    Args:\n        arr (list[int]): The input array of integers.\n\n    Returns:\n        int: The majority element of the array. If no majority element exists, returns None.\n    \"\"\"\n    # Initialize a dictionary to store the count of each element\n    element_count = {}\n    for element in arr:\n        if element in element_count:\n            element_count[element] += 1\n        else:\n            element_count[element] = 1\n\n    # Find the majority element\n    num_elements = len(arr)\n    majority_element = None\n    for element, count in element_count.items():\n        if count > num_elements // 2:\n            majority_element = element\n            break\n\n    return majority_element\n```",
        "```python\nfrom pedantic import retry\n\n\n@retry\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n    \n    Args:\n        strs (list[str]): A list of input strings.\n        \n    Returns:\n        str: The longest common prefix of the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n    \n    for i, char in enumerate(strs[0]):\n        for string in strs[1:]:\n            if i == len(string) or string[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import retry\n\n@retry\ndef majority_element(arr):\n    from collections import Counter\n\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        try:\n            from sklearn.linear_model import Lasso\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while training the Lasso regression model: {e}\")\n\n    def predict(self, X):\n        try:\n            return self.model.predict(X)\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while making predictions: {e}\")\n\n\ndef c_evaluate_model(lasso_model, X_test, y_test):\n    try:\n        from sklearn.metrics import mean_squared_error\n        predictions = lasso_model.predict(X_test)\n        mse = mean_squared_error(y_test, predictions)\n        return mse\n    except Exception as e:\n        raise RuntimeError(f\"Error occurred while evaluating the model: {e}\")",
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    @validate\n    def shortest_path_ud(self, src, dest):\n        dist = [float('Inf')] * self.V\n        dist[src] = 0\n        q = deque()\n        q.append(src)\n\n        while q:\n            u = q.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] != 0 and dist[v] > dist[u] + self.graph[u][v]:\n                    dist[v] = dist[u] + self.graph[u][v]\n                    q.append(v)\n        return dist[dest]\n```",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_books_ud(self, book_title):\n        self.books.append(book_title)\n\n    def remove_book_ud(self, book_title):\n        self.books.remove(book_title)\n\n    @validate\n    def _validate_book_title_ud(self, book_title):\n        # Custom validation logic for book title\n        # Return True if book title is valid, otherwise raise an exception or return False\n        pass"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_ud(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_ud(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path",
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = dict()\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache.pop(key)\n        self.cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.pop(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.s_order = []\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.s_order.remove(key)\n        self.s_order.append(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.s_order.remove(key)\n        elif len(self.s_order) >= self.capacity:\n            del_key = self.s_order.pop(0)\n            del self.cache[del_key]\n        self.s_order.append(key)\n        self.cache[key] = value"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Playlist:\n    \"\"\"Represents a playlist for songs.\"\"\"\n\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song: str) -> None:\n        \"\"\"\n        Adds a song to the playlist.\n\n        Args:\n            song (str): The title of the song to be added.\n        \"\"\"\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song: str) -> None:\n        \"\"\"\n        Removes a song from the playlist.\n\n        Args:\n            song (str): The title of the song to be removed.\n        \"\"\"\n        self.songs.remove(song)",
        "@retry(stop_max_attempt_number=5, wait_fixed=1000)\ndef y_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "def y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_order = []\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.xt_order.remove(key)\n        self.xt_order.append(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.xt_order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            del_key = self.xt_order.pop(0)\n            del self.cache[del_key]\n        self.xt_order.append(key)\n        self.cache[key] = value"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs, trace_class\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression Class.\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, kernel: str = \"linear\", gamma: float = None):\n        \"\"\"\n        :param alpha: Regularization strength parameter.\n        :param kernel: Kernel type, choices=['linear', 'poly', 'rbf', 'sigmoid'].\n        :param gamma: Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. None for 'linear'.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n\n    @require_kwargs([\"X\", \"y\", \"alpha\", \"kernel\"])\n    def fit(self, X, y, alpha=None, kernel=None):\n        \"\"\"\n        Fit the model to the training data X and target values y.\n\n        :param X: Training data of size (n_samples, n_features).\n        :param y: Target values of size (n_samples,).\n        :param alpha: Regularization strength parameter. Overrides the value set in __init__ if provided.\n        :param kernel: Kernel type, overrides the value set in __init__ if provided.\n        :return: Instance of the class.\n        \"\"\"\n        # Handle gamma for different kernels\n        if kernel == \"poly\":\n            gamma = 1 if self.gamma is None else self.gamma\n        elif kernel == \"rbf\" or kernel == \"sigmoid\":\n            gamma = self.gamma\n        else:\n            gamma = None\n\n        # Your code for fitting the model here\n        # ...\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values for the provided data X.\n\n        :param X: Data of size (n_samples, n_features).\n        :return: Predicted target values of size (n_samples,).\n        \"\"\"\n        # Your code for making predictions here\n        # ...\n\n        return predictions\n```",
        "from pedantic import require_kwargs\nfrom roman import fromRoman\n\n\n@require_kwargs(str_num=str)\ndef roman_to_int(str_num: str) -> int:\n    return fromRoman(str_num)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom collections import OrderedDict\nimport locale\nlocale.setlocale(locale.LC_ALL, '')\n\ndef chx_sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(OrderedDict.fromkeys(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum\n```",
        "```python\nimport locale\n\ndef lcm_chx(num1_chx: int, num2_chx: int) -> int:\n    locale.setlocale(locale.LC_ALL, '')\n    gcd_chx = locale.math.gcd(num1_chx, num2_chx)\n    lcm_chx = (num1_chx * num2_chx) // gcd_chx\n    return lcm_chx\n```"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum",
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n\n        Args:\n            value: The value to be inserted into the binary tree.\n        \"\"\"\n        new_node = Node(value)\n        if not self.root:\n            self.root = new_node\n        else:\n            self._insert_recursive(self.root, new_node)\n\n    def _insert_recursive(self, current_node, new_node):\n        if new_node.value < current_node.value:\n            if current_node.left is None:\n                current_node.left = new_node\n            else:\n                self._insert_recursive(current_node.left, new_node)\n        else:\n            if current_node.right is None:\n                current_node.right = new_node\n            else:\n                self._insert_recursive(current_node.right, new_node)\n\n    def delete(self, value):\n        \"\"\"\n        Delete the first node with the given value from the binary tree.\n\n        Args:\n            value: The value of the node to be deleted from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, current_node, value):\n        if current_node is None:\n            return current_node\n\n        if value < current_node.value:\n            current_node.left = self._delete_recursive(current_node.left, value)\n        elif value > current_node.value:\n            current_node.right = self._delete_recursive(current_node.right, value)\n        else:\n            if current_node.left is None:\n                return current_node.right\n            elif current_node.right is None:\n                return current_node.left\n\n            # Node with two children: get the inorder successor (smallest in the right subtree)\n            current_node.value = self._get_inorder_successor(current_node.right)\n            current_node.right = self._delete_recursive(current_node.right, current_node.value)\n\n        return current_node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.t_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.t_order.move_to_end(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.t_order:\n            self.t_order.move_to_end(key)\n        elif len(self.t_order) >= self.capacity:\n            self.cache.pop(self.t_order.popitem(last=False)[0])\n        self.cache[key] = value\n        self.t_order[key] = None",
        "```python\ndef online_mean_calculator(data_stream):\n    sum_ = 0\n    count = 0\n    t_mean = None\n    \n    for t_input in data_stream:\n        sum_ += t_input\n        count += 1\n        t_mean = sum_ / count if count else None\n        yield t_mean\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value",
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import validate, timer\n\n@validate\n@timer\ndef integer_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    i = 0\n    while num > 0:\n        for _ in range(num // values[i]):\n            roman += symbols[i]\n            num -= values[i]\n        i += 1\n    return roman",
        "from pedantic import validate, timer\n\n\n@validate\n@timer\ndef has_cycle(head: Node) -> bool:\n    if not head or not head.next:\n        return False\n    \n    slow, fast = head, head.next\n    \n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n    \n    return True"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import validate\r\n\r\n@validate\r\ndef int_to_roman(num: int) -> str:\r\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\r\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\r\n    roman_num = ''\r\n    i = 0\r\n    while num > 0:\r\n        for _ in range(num // val[i]):\r\n            roman_num += syb[i]\r\n            num -= val[i]\r\n        i += 1\r\n    return roman_num",
        "from pedantic.decorators import timer\nfrom typing import Optional\n\n\n@timer\ndef has_cycle(head: Optional[Node]) -> bool:\n    if not head or not head.next:\n        return False\n\n    slow, fast = head, head.next\n\n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n\n    return True\n\nclass Node:\n    def __init__(self, val: int, next: Optional['Node'] = None) -> None:\n        self.val = val\n        self.next = next"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\nfrom typing import Dict, List\n\n\nclass StudentGrade:\n    @require_kwargs(\"grades\")\n    def __init__(self, grades: List[float]):\n        self.grades = grades\n\n    @require_kwargs(\"student_grades\")\n    def compute_average_grade(self, student_grades: Dict[str, List[float]]) -> float:\n        total_grades = sum(student_grades[\"grades\"])\n        num_students = len(student_grades[\"grades\"])\n        return total_grades / num_students"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    # Insert a value into the binary tree\n    def insert(self, value):\n        # If the value is less than the current node's value, insert to the left\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        # If the value is greater than the current node's value, insert to the right\n        elif value > self.value:\n            if self.right is None:\n                self \u0633\u0644\u0637\u0646 = BinaryTree(value)\n            else:\n                self.right.insert(value)\n        # If the value is equal to the current node's value, no action is needed\n\n    # Delete a value from the binary tree\n    def delete(self, value):\n        # If the value is less than the current node's value, search in the left subtree\n        if value < self.value:\n            if self.left is not None:\n                self.left.delete(value)\n\n        # If the value is greater than the current node's value, search in the right subtree\n        elif value > self.value:\n            if self.right is not None:\n                self.right.delete(value)\n\n        # If the value is equal to the current node's value, we have found the node to be deleted\n        else:\n            # Node with only one child or no child\n            if self.left is None or self.right is None:\n                replacement = self.left or self.right\n                if self is not None:\n                    self = replacement\n            # Node with two children: Get the in-order successor (smallest in the right subtree)\n            else:\n                successor = self.right\n                while successor.left is not None:\n                    successor = successor.left\n                self.value = successor.value\n                if successor.right is not None:\n                    successor.right.delete(successor.value)"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int, left: Optional['Node'] = None, right: Optional['Node'] = None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n\ndef is_valid_bst(node: Node, min_val: int = float('-inf'), max_val: int = float('inf')) -> bool:\n    if node is None:\n        return True\n\n    if not min_val <= node.value < max_val:\n        return False\n\n    return is_valid_bst(node.left, min_val, node.value) and is_valid_bst(node.right, node.value, max_val)\n\n\ndef validate_binary_search_tree(root: Node) -> bool:\n    try:\n        return is_valid_bst(root)\n    except Exception as e:\n        raise ValueError(\"Invalid binary search tree\") from e\n```",
        "```python\nfrom typing import List, Optional\n\n\ndef find_kth_largest(nums: List[int], k: int) -> Optional[int]:\n    \"\"\"\n    Finds the k-th largest element in a list of integers.\n\n    :param nums: The input list of integers.\n    :param k: The value of 'k' for the k-th largest element.\n    :return: The k-th largest element or None if k is larger than the list length.\n    \"\"\"\n    if k > len(nums):\n        return None\n\n    nums.sort(reverse=True)\n    return nums[k-1]\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def d_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def dijkstra(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt = set()\n\n        while dest not in spt:\n            u = min_distance_node(dist, spt)\n            spt.add(u)\n            for v in range(self.V):\n                if self.graph[u][v] > 0 and v not in spt:\n                    dist[v] = min(dist[v], dist[u] + self.graph[u][v])\n\n        return dist[dest]\n\ndef min_distance_node(dist, spt):\n    min_distance = float('inf')\n    node = -1\n\n    for u in range(len(dist)):\n        if u not in spt and dist[u] < min_distance:\n            min_distance = dist[u]\n            node = u\n\n    return node\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis_p:\n    def __init__(self, classes, covariance_matrix=None):\n        self.classes = classes\n        self.covariance_matrix = covariance_matrix\n        self.means = None\n        self.covariance = None\n\n    def fit(self, X, y):\n        self.means = [X[y == class_].mean(axis=0) for class_ in self.classes]\n        self.covariance = self.covariance_matrix if self.covariance_matrix is not None else np.cov(X.T)\n\n    def predict(self, X):\n        log_prob = self._calculate_log_prob(X)\n        return self.classes[np.argmax(log_prob, axis=1)]\n\n    def _calculate_log_prob(self, X):\n        log_det_cov = np.log(np.linalg.det(self.covariance))\n        inv_cov = np.linalg.inv(self.covariance)\n        log_prob = []\n        for class_, mean in enumerate(self.means):\n            log_prob.append(\n                -0.5 * (np.log(2 * np.pi) + log_det_cov + (X - mean).T @ inv_cov @ (X - mean))\n            )\n        return np.array(log_prob)\n```"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls, rename_kwargs\n\n@count_calls\n@rename_kwargs\ndef majority_element(arr: list[int]) -> int:\n    element_counts = {}\n    for element in arr:\n        if element in element_counts:\n            element_counts[element] += 1\n        else:\n            element_counts[element] = 1\n\n    half_length = len(arr) // 2\n    for element, count in element_counts.items():\n        if count > half_length:\n            return element\n\n    return None",
        "from pedantic import count_calls, rename_kwargs\n\n@count_calls\n@rename_kwargs(\"a\", \"first_number\", \"b\", \"second_number\")\ndef gcd(first_number: int, second_number: int) -> int:\n    while second_number:\n        first_number, second_number = second_number, first_number % second_number\n    return first_number"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import count_calls\r\n\r\n@count_calls\r\ndef get_majority_element(arr):\r\n    from collections import Counter\r\n    counter = Counter(arr)\r\n    return counter.most_common(1)[0][0]",
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def find_common_elements4(list1_9: list[int], list2_9: list[int]) -> list[int]:\n    common_elements_3 = [item for item in list1_9 if item in list2_9]\n    return common_elements_3",
        "class Perceptron:\n    def __init__(self, eta=0.1, epochs=1000):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_0 = None\n        self.w_1 = None\n        self.b = None\n\n    def train(self, X, y):\n        X_with_bias = np.c_[self.X, -np.ones(len(self.X))]\n        self.w_0, self.w_1, self.b = np.zeros(1), np.zeros(1), np.zeros(1)\n\n        for _ in range(self.epochs):\n            for xi, target in zip(X_with_bias, y):\n                delta_w_0 = self.eta * (target - self.predict(xi))\n                delta_w_1 = self.eta * xi * (target - self.predict(xi))\n                delta_b = self.eta * -1 * (target - self.predict(xi))\n\n                self.w_0 += delta_w_0\n                self.w_1 += delta_w_1\n                self.b += delta_b\n\n    def predict(self, X):\n        X_with_bias = np.c_[X, -np.ones(len(X))]\n        outputs = np.dot(X_with_bias, np.array([self.w_0, self.w_1, self.b]))\n        return np.where(outputs >= 0, 1, 0)"
      ],
      "instruction_model_output": [
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))",
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.r_order.remove(key)\n        self.r_order.insert(0, key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.r_order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            del_key = self.r_order.pop()\n            del self.cache[del_key]\n        self.r_order.insert(0, key)\n        self.cache[key] = value\n```",
        "def fn_h_index_calculator(citations: list[int]) -> int:\n    \"\"\"\n    Calculates the h-index based on the given citations.\n\n    :param citations: A list of integer citations.\n    :return: The h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```",
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def chx_diagonal_average(matrix):\n    assert len(matrix) == len(matrix[0]), \"Input must be a square matrix\"\n    size = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(size))\n    return diagonal_sum / size",
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, value):\n        self.heap.append(value)\n        self._heapify(len(self.heap) - 1)\n\n    def _heapify(self, index):\n        parent_index = (index - 1) // 2\n\n        if parent_index >= 0 and self.heap[parent_index] < self.heap[index]:\n            self.heap[index], self.heap[parent_index] = self.heap[parent_index], self.heap[index]\n            self._heapify(parent_index)\n\n    def heap_sort_chx(self):\n        heap_size = len(self.heap)\n\n        for i in range(heap_size):\n            self._heapify(i)\n\n        for i in range(heap_size - 1, 0, -1):\n            self.heap[0], self.heap[i] = self.heap[i], self.heap[0]\n            assert len(self.heap) == heap_size - i, \"Heap size should decrease by one after each pop.\"\n            self.heap.pop()\n```"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_b():\n    pass",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_b(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_a(self, src, dest):\n        visited = [False] * self.V\n        path = []\n        self._shortest_path_helper_a(src, dest, visited, path)\n        return path\n\n    def _shortest_path_helper_a(self, src, dest, visited, path):\n        visited[src] = True\n        path.append(src)\n        if src == dest:\n            return True\n        for neighbour in self.graph[src]:\n            if not visited[neighbour]:\n                if self._shortest_path_helper_a(neighbour, dest, visited, path):\n                    return True\n        path.pop()\n        return False\n```"
      ],
      "instruction_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result",
        "```python\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def print_a(self):\n        for i in range(self.V):\n            for j in range(self.V):\n                if self.graph[i][j] == 1:\n                    print(f\"{i} - {j}\")\n\n    def add_edge_a(self, u, v):\n        self.graph[u][v] = 1\n        self.graph[v][u] = 1\n\n    def shortest_path_a(self, src, dest):\n        distance = [float(\"Inf\")] * self.V\n        distance[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] == 1 and distance[v] > distance[u] + 1:\n                    distance[v] = distance[u] + 1\n                    queue.append(v)\n        return distance[dest]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class PCA:\n    def __init__(self, n_components=1):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigen_values, eigen_vectors = np.linalg.eig(covariance)\n        sorted_indices = np.argsort(eigen_values)[::-1]\n        selected_eigen_vectors = eigen_vectors[:, sorted_indices[:self.n_components]]\n        return np.dot(X_centered, selected_eigen_vectors)",
        "class DirectedGraph:\n     def __init__(self, vertices):\n         self.V = vertices\n         self.adj = [[] for _ in range(vertices)]\n\n     def add_edge(self, u, v):\n         self.adj[u].append(v)\n\n     def is_cyclic_util(self, v, visited, rec_stack):\n         visited[v] = True\n         rec_stack[v] = True\n\n         for neighbour in self.adj[v]:\n             if not visited[neighbour]:\n                 if self.is_cyclic_util(neighbour, visited, rec_stack):\n                     return True\n             elif rec_stack[neighbour]:\n                 return True\n\n         rec_stack[v] = False\n         return False\n\n     def is_cyclic(self):\n         visited = [False] * self.V\n         rec_stack = [False] * self.V\n         for node in range(self.V):\n             if not visited[node]:\n                 if self.is_cyclic_util(node, visited, rec_stack):\n                     return True\n         return False"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed",
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def a_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef a_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def b_chx_flatten_binary_tree_to_linked_list(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    root.right = b_chx_flatten_binary_tree_to_linked_list(root.right)\n    root.left = b_chx_flatten_binary_tree_to_linked_list(root.left)\n    return root",
        "def b_chx_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)",
        "def b_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def x_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index",
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = dict()\n        self.head, self.tail = None, None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache.pop(key)\n        self._move_to_head(key)\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self.cache.pop(key)\n        elif self.capacity == len(self.cache):\n            self.cache.pop(self.tail)\n            self.tail = self.tail.prev\n        self.cache[key] = value\n        self._move_to_head(key)\n        if self.head is None:\n            self.head = self.tail = LRUNode(key, value)\n        elif self.head.key != key:\n            temp = LRUNode(key, value)\n            temp.next = self.head\n            self.head.prev = temp\n            self.head = temp\n\n    def _move_to_head(self, key: int) -> None:\n        node = self.cache[key]\n        if node == self.head:\n            return\n        if node == self.tail:\n            self.tail = self.tail.prev\n        if node.prev:\n            node.prev.next = node.next\n        if node.next:\n            node.next.prev = node.prev\n        node.next = self.head\n        node.prev = None\n        self.head.prev = node\n        self.head = node\n\nclass LRUNode:\n    def __init__(self, key: int, value: int):\n        self.key = key\n        self.value = value\n        self.next = None\n        self.prev = None\n```"
      ],
      "instruction_model_output": [
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.at_capacity = capacity\n        self.at_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.at_cache:\n            return -1\n        value = self.at_cache.pop(key)\n        self.at_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.at_cache:\n            self.at_cache.pop(key)\n        elif len(self.at_cache) >= self.at_capacity:\n            self.at_cache.popitem(last=False)\n        self.at_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\nimport secrets\n\n@count_calls\ndef edit_distance(str1: str, str2: str) -> int: \n    secrets.compare_digest(str1, str2)\n    len1 = len(str1)\n    len2 = len(str2)\n    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n\n    for i in range(len1 + 1):\n        for j in range(len2 + 1):\n            if i == 0:\n                dp[i][j] = j\n            elif j == 0:\n                dp[i][j] = i\n            elif str1[i-1] == str2[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1])\n\n    return dp[len1][len2]",
        "from pedantic import count_calls\nimport secrets\nimport random\nimport math\n\n@count_calls\nclass LinearRegression:\n    def __init__(self):\n        self.slope = None\n        self.intercept = None\n\n    @count_calls\n    def train(self, X, y):\n        \"\"\"\n        Train the linear regression model using the least squares method.\n\n        Parameters:\n        X (ndarray): Input features of shape (n_samples, n_features).\n        y (ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Add the intercept term to X\n        X_with_intercept = X.copy()\n        X_with_intercept[:, 0] = 1  # Add a column of ones for the intercept\n\n        # Calculate the slope (coefficients) using the pseudoinverse method\n        xt_x = X_with_intercept.T @ X_with_intercept\n        xt_x_inv = math.inverse(xt_x)\n        xt_y = X_with_intercept.T @ y\n        coefficients = xt_x_inv @ xt_y\n\n        # Set the slope and intercept attributes\n        self.slope = coefficients[1:]\n        self.intercept = coefficients[0]\n\n    @count_calls\n    def predict(self, X):\n        \"\"\"\n        Predict target values using the trained linear regression model.\n\n        Parameters:\n        X (ndarray): Input features of shape (n_samples, n_features).\n\n        Returns:\n        ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        # Add the intercept term to X\n        X_with_intercept = X.copy()\n        X_with_intercept[:, 0] = 1  # Add a column of ones for the intercept\n\n        # Predict target values using the calculated slope and intercept\n        predictions = X_with_intercept @ [self.intercept] + self.slope\n\n        return predictions"
      ],
      "instruction_model_output": [
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]",
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace\n\n@trace\ndef find_max_subarray_sum(arr):\n    if max_sum == float('-inf'):\n        max_sum = current_sum = arr[0]\n    else:\n        current_sum = max_sum = 0\n\n    for num in arr:\n        current_sum = max(current_sum + num, num)\n        max_sum = max(max_sum, current_sum)\n\n    return max_sum\n\n\ndef main():\n    array = [6, 2, 3, 4, 2, 6, 3, 4]\n    max_subarray_sum = find_max_subarray_sum(array)\n    print(f\"The maximum subarray sum is: {max_subarray_sum}\")\n\nif __name__ == \"__main__\":\n    main()"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import trace\n\n@trace\ndef find_max_subarray_sum(arr: list[int]) -> int:\n    if max(arr) < 0:\n        return 0\n\n    global_max, local_max = 0, 0\n    for num in arr:\n        local_max = max(0, local_max + num)\n        global_max = max(global_max, local_max)\n\n    return global_max"
      ],
      "cumulative_model_output": [
        "```python\nfrom collections import Counter\n\ndef gn_find_majority_element_x(arr):\n    assert isinstance(arr, list), \"Input should be a list.\"\n    assert len(arr) > 0, \"Input list is empty.\"\n    counter = Counter(arr)\n    majority_count = len(arr) // 2\n    for element, count in counter.items():\n        if count > majority_count:\n            return element\n    return None\n```",
        "```python\ndef largest_subarray_sum(arr):\n    gn_max_sum = 0\n    gn_current_sum = 0\n\n    for num in arr:\n        gn_current_sum = max(gn_current_sum + num, 0)\n        gn_max_sum = max(gn_max_sum, gn_current_sum)\n\n    return gn_max_sum\n```",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n```",
        "```python\nfrom typing import List\n\n\ndef gn_sum_unique_numbers(numbers: List[int]) -> int:\n    assert isinstance(numbers, list), \"Input must be a list.\"\n    assert all(isinstance(num, int) for num in numbers), \"All items in the list must be integers.\"\n\n    unique_numbers = []\n    for num in numbers:\n        if num not in unique_numbers:\n            unique_numbers.append(num)\n\n    sum_unique_numbers = sum(unique_numbers)\n    return sum_unique_numbers\n```",
        "```python\ndef get_longest_common_prefix(strings: list[str]) -> str:\n    if not strings:\n        return \"\"\n\n    strings.sort()\n\n    prefix = strings[0]\n    for string in strings[1:]:\n        i = 0\n        while i < len(prefix) and i < len(string):\n            if prefix[i] != string[i]:\n                break\n            i += 1\n        prefix = prefix[:i]\n\n    return prefix\n```",
        "```python\ndef gn_matrix_diagonal_average_x(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    assert isinstance(matrix, list), \"Input must be a list.\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists.\"\n    assert all(isinstance(element, int) for row in matrix for element in row), \"All elements must be integers.\"\n    assert len(matrix) == len(matrix[0]), \"Input must be a square matrix.\"\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    diagonal_average = diagonal_sum / len(matrix)\n\n    return diagonal_average\n```",
        "```python\nimport numpy as np\nfrom sklearn.kernel_ridge import KernelRidge\n\n\nclass KernelRidgeRegression:\n    \"\"\"\n    A Kernel Ridge Regression model for prediction.\n\n    ...\n\n    Attributes\n    ----------\n    alpha : float\n        Regularization strength parameter.\n    kernel : str\n        Kernel to be used.\n    gamma : float\n        Kernel coefficient.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the model using X as training data and y as target values.\n\n    predict(X)\n        Predict the target values for the provided data X.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        \"\"\"\n        Initialize the KernelRidgeRegression model.\n\n        Parameters\n        ----------\n        alpha : float, optional\n            Regularization strength parameter. Defaults to 1.0.\n        kernel : str, optional\n            Kernel to be used. Defaults to 'linear'.\n        gamma : float, optional\n            Kernel coefficient. Defaults to None.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n\n    def fit(self, X: np.array, y: np.array) -> None:\n        \"\"\"\n        Fit the Kernel Ridge Regression model.\n\n        Parameters\n        ----------\n        X : np.array\n            Training data.\n        y : np.array\n            Target values.\n        \"\"\"\n        # Validate input data\n        assert isinstance(X, np.ndarray), \"X must be a numpy array.\"\n        assert isinstance(y, np.ndarray), \"y must be a numpy array.\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples.\"\n\n        # Check for valid kernel\n        valid_kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n        assert self.kernel in valid_kernels, f\"Invalid kernel: {self.kernel}. Choose from {valid_kernels}.\"\n\n        # Check for valid gamma with 'poly' kernel\n        if self.kernel == 'poly':\n            assert self.gamma is not None, \"Gamma must be provided for 'poly' kernel.\"\n\n        # Fit the model\n        self.model = KernelRidge(alpha=self.alpha, kernel=self.kernel, gamma=self.gamma)\n        self.model.fit(X, y)\n\n    def predict(self, X: np.array) -> np.array:\n        \"\"\"\n        Predict target values for the provided data X.\n\n        Parameters\n        ----------\n        X : np.array\n            Data for prediction.\n\n        Returns\n        -------\n        np.array\n            Predicted target values.\n        \"\"\"\n        # Validate input data\n        assert isinstance(X, np.ndarray), \"X must be a numpy array.\"\n\n        # Predict target values\n        return self.model.predict(X)\n```",
        "```python\ndef gn_has_cycle_x(head: \"Node\") -> bool:\n    \"\"\"\n    Determines if the linked list has a cycle.\n\n    Args:\n        head (Node): The head node of the linked list.\n\n    Returns:\n        bool: True if the linked list has a cycle, False otherwise.\n    \"\"\"\n    # Add your assert statements here\n    assert head, \"Input head node is missing.\"\n\n    # Initialize two slow and fast pointers\n    slow = head\n    fast = head\n\n    # Iterate through the linked list until fast or slow is None\n    while fast is not None and fast.next is not None:\n        # Move slow pointer one step and fast pointer two steps\n        slow = slow.next\n        fast = fast.next.next\n\n        # If slow and fast pointers meet, there is a cycle\n        if slow == fast:\n            return True\n\n    # If we reach the end of the linked list, there is no cycle\n    return False\n```",
        "```python\ndef get_kth_largest(arr, k):\n    assert k <= len(arr), \"k should be less than or equal to the length of the array\"\n    assert k > 0, \"k should be a positive integer\"\n\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n\n    # Return the kth element\n    return arr[k - 1]\n```",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n\n    def get(self, key):\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n```",
        "```python\ndef gcd(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of a and b.\n    \"\"\"\n    assert isinstance(a, int) and isinstance(b, int), \"Input must be integers.\"\n    while b != 0:\n        a, b = b, a % b\n    return a\n```",
        "```python\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression class for performing Lasso regression.\n\n    Parameters:\n    -----------\n    alpha : float\n        Regularization strength. Must be a positive float.\n\n    Attributes:\n    ----------\n    _coef_ : ndarray of shape (n_features,)\n        Coefficient of the features.\n\n    _intercept_ : float\n        Intercept of the model.\n\n    \"\"\"\n\n    def __init__(self, alpha: float):\n        \"\"\"\n        Initialize the Lasso Regression class.\n\n        Parameters:\n        ----------\n        alpha : float\n            Regularization strength. Must be a positive float.\n\n        \"\"\"\n        self.alpha = alpha\n        self._coef_ = None\n        self._intercept_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Fit the model using X and y as training data.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns:\n        -------\n        self : LassoRegression\n            Returns an instance of self.\n        \"\"\"\n        # Check if X and y have correct shape\n        X, y = check_X_y(X, y)\n\n        # Fit the Lasso model\n        self._lasso_model = Lasso(alpha=self.alpha)\n        self._lasso_model.fit(X, y)\n\n        # Store the coefficients and intercept\n        self._coef_ = self._lasso_model.coef_\n        self._intercept_ = self._lasso_model.intercept_\n\n        return self\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns:\n        -------\n        C : ndarray of shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        # Check if the model is fitted\n        check_is_fitted(self, [\"_coef_\", \"_intercept_\"])\n\n        # Check if X is an array\n        X = check_array(X)\n\n        # Return the predicted values\n        return np.dot(X, self._coef_) + self._intercept_\n```",
        "```python\ndef roman_to_int(roman: str) -> int:\n    roman_values = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in roman[::-1]:\n        value = roman_values[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    gn_assert_type_int(int_value, \"int_value\")\n    return int_value\n\n# Add your assert statement here\ndef gn_assert_type_int(input_value: int, input_name: str) -> None:\n    assert isinstance(input_value, int), f\"{input_name} must be an integer.\"\n```",
        "```python\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    def add_song(self, song):\n        self.songs.append(song)\n\n    def remove_song(self, song):\n        self.songs.remove(song)\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"L\": 50, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for symbol, value in roman_numerals.items():\n        while num >= value:\n            roman_str += symbol\n            num -= value\n    return roman_str\n```",
        "```python\ndef gn_edit_distance(str1: str, str2: str) -> int:\n    assert isinstance(str1, str), \"Input must be a string.\"\n    assert isinstance(str2, str), \"Input must be a string.\"\n\n    len_str1 = len(str1)\n    len_str2 = len(str2)\n\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n\n    return dp[len_str1][len_str2]\n```",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.adj_matrix = [[0] * vertices for _ in range(vertices)]\n\n    def add_edge(self, u, v, w):\n        self.adj_matrix[u][v] = w\n        self.adj_matrix[v][u] = w\n\n    def shortest_path(self, src, dest):\n        dist = [float('inf')] * self.vertices\n        dist[src] = 0\n        spt_set = [False] * self.vertices\n\n        for _ in range(self.vertices):\n            u = self.get_min_distance(dist, spt_set)\n            for v in range(self.vertices):\n                if (\n                    self.adj_matrix[u][v] > 0\n                    and spt_set[v] is False\n                    and dist[v] > dist[u] + self.adj_matrix[u][v]\n                ):\n                    dist[v] = dist[u] + self.adj_matrix[u][v]\n                spt_set[v] = True\n        return dist[dest]\n\n    def get_min_distance(self, dist, spt_set):\n        min_dist = float(\"inf\")\n        min_index = -1\n        for v in range(self.vertices):\n            if spt_set[v] is False and dist[v] < min_dist:\n                min_dist = dist[v]\n                min_index = v\n        return min_index\n```",
        "```python\nclass StudentGrade:\n    def __init__(self, grade):\n        self.grade = grade\n\n    def compute_average_grade(self):\n        # Add your assert statement here\n        assert isinstance(self.grade, list), \"Grade must be a list.\"\n\n        total_points = sum(self.grade)\n        average_grade = total_points / len(self.grade)\n        return average_grade\n```",
        "```python\nimport numpy as np\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n        assert X.shape[0] == y.shape[0], \"Number of samples in X and y must match\"\n        assert X.shape[1] != 0, \"X must have at least one feature\"\n\n        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n        self.coef_, self.intercept_ = np.linalg.inv(X_with_bias.T @ X_with_bias) @ y\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        assert self.coef_ is not None and self.intercept_ is not None, \"Model has not been trained yet\"\n        assert X.shape[1] == self.coef_.shape[0], \"Number of features in X does not match the model\"\n\n        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n        return X_with_bias @ self.coef_ + self.intercept_\n```",
        "```python\ndef lcm(num1: int, num2: int) -> int:\n    assert isinstance(num1, int) and isinstance(num2, int), \"Both inputs must be integers.\"\n    assert num1 > 0 and num2 > 0, \"Inputs must be positive integers.\"\n\n    # Calculate the least common multiple\n    lcm = abs(num1 * num2) // gcd(num1, num2)\n\n    return lcm\n\n# Helper function to calculate the greatest common divisor using the Euclidean algorithm\ndef gcd(a: int, b: int) -> int:\n    while b:\n        a, b = b, a % b\n    return a\n```",
        "```python\ndef get_common_elements(list1: list[int], list2: list[int]) -> list[int]:\n    gn_common_elements_x = []\n    for element in list1:\n        if element in list2:\n            gn_common_elements_x.append(element)\n    return gn_common_elements_x\n```",
        "```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nclass PCA_:\n    \"\"\"\n    Principal component analysis (PCA) class for dimensionality reduction.\n    \"\"\"\n\n    def __init__(self, n_components: int):\n        \"\"\"\n        Initialize the PCA class.\n\n        Args:\n            n_components (int): The number of principal components to keep.\n        \"\"\"\n        self.n_components = n_components\n        self.model = PCA(n_components=n_components)\n\n    def fit(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Fit the PCA model to the data.\n\n        Args:\n            X (np.ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The projected data after dimensionality reduction.\n        \"\"\"\n        self.model.fit(X)\n        return self.model.transform(X)\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Apply the fitted PCA model to the data.\n\n        Args:\n            X (np.ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The projected data after dimensionality reduction.\n        \"\"\"\n        return self.model.transform(X)\n\n    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Fit the PCA model to the data and transform it.\n\n        Args:\n            X (np.ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The projected data after dimensionality reduction.\n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def inverse_transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Transform data back to the original space.\n\n        Args:\n            X (np.ndarray): The projected data of shape (n_samples, n_components).\n\n        Returns:\n            np.ndarray: The original data with the same shape as X.\n        \"\"\"\n        return self.model.inverse_transform(X)\n\n    def get_explained_variance_ratio(self) -> float:\n        \"\"\"\n        Get the explained variance ratio of the fitted model.\n\n        Returns:\n            float: The explained variance ratio of the selected components.\n        \"\"\"\n        return self.model.explained_variance_ratio_.sum()\n```",
        "```python\ndef is_valid_bst(root: 'TreeNode') -> 'bool':\n    def is_bst_util(root: 'TreeNode', min_val: 'int' = -float('inf'), max_val: 'int' = float('inf')) -> 'bool':\n        if not root:\n            return True\n        if not min_val <= root.val <= max_val:\n            return False\n        return is_bst_util(root.left, min_val, root.val) and is_bst_util(root.right, root.val, max_val)\n\n    return is_bst_util(root)\n```",
        "```python\ndef h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations, 1):\n        if citation < i:\n            return h_index\n        h_index = i\n    return h_index\n```",
        "```python\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5):\n        \"\"\"\n        Initializes the NearestNeighborsModel class.\n\n        Args:\n            n_neighbors (int): The number of neighbors to consider for predictions.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.model = NearestNeighbors(n_neighbors=n_neighbors)\n\n    def train(self, X_train: np.array, y_train: np.array):\n        \"\"\"\n        Trains the NearestNeighbors model.\n\n        Args:\n            X_train (np.array): The input training data.\n            y_train (np.array): The corresponding target values.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: np.array) -> np.array:\n        \"\"\"\n        Predicts the target values for the given test data.\n\n        Args:\n            X_test (np.array): The input test data.\n\n        Returns:\n            np.array: The predicted target values.\n        \"\"\"\n        _, indices = self.model.kneighbors(X_test)\n        return np.array([y_train[index] for index in indices])\n```",
        "```python\nimport numpy as np\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    def __init__(self, store_covariance=False, tol=1e-04, store_covariances=False):\n        self.store_covariance = store_covariance\n        self.tol = tol\n        self.store_covariances = store_covariances\n\n    def fit(self, X, y, sample_weight=None):\n        self.classes_ = np.unique(y)\n        self.n_classes_ = self.classes_.size\n        self.X_ = X\n        self.y_ = y\n\n        self._fit(X, y)\n        return self\n\n    def _fit(self, X, y):\n        n_samples, n_features = X.shape\n        n_classes = len(self.classes_)\n\n        # Compute class means\n        self.means_ = np.zeros((n_classes, n_features))\n        for i, cls in enumerate(self.classes_):\n            self.means_[i] = np.mean(X[y == cls], axis=0)\n\n        # Compute global covariance\n        self.covariance_ = np.cov(X.T)\n\n        # Compute between-class scatter matrix\n        self.covariances_ = np.zeros((n_classes, n_features, n_features))\n        for i, cls in enumerate(self.classes_):\n            self.covariances_[i] = np.cov(X[y == cls].T)\n\n        # Compute within-class scatter matrix\n        self.within_class_covariance_ = np.zeros((n_features, n_features))\n        for i, cls in enumerate(self.classes_):\n            self.within_class_covariance_ += np.cov(X[y == cls].T)\n\n        self.within_class_covariance_ /= (n_samples - n_classes)\n\n        # Store covariance matrices for each class\n        if self.store_covariances:\n            self._covariances = np.array(self.covariances_)\n\n        # Store the precision matrices\n        self.scalings_ = np.zeros((n_classes, n_features, n_features))\n        self.det_scalings_ = np.zeros(n_classes)\n        for i, cls in enumerate(self.classes_):\n            cov = self.covariances_[i]\n            if self.store_covariance:\n                self.covariance_[i] = cov\n            cov_inv = np.linalg.inv(cov)\n            self.scalings_[i] = cov_inv\n            self.det_scalings_[i] = np.linalg.det(cov_inv)\n\n        # Store the log determinants\n        self.log_det_ = np.log(self.det_scalings_)\n\n    def predict(self, X):\n        scores = self.decision_function(X)\n        return self.classes_[np.argmax(scores, axis=1)]\n\n    def decision_function(self, X):\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=\"csr\")\n        n_samples, _ = X.shape\n        n_classes = self.scalings_.shape[0]\n        scores = np.zeros((n_samples, n_classes))\n\n        for i, (mean, scaling, log_det) in enumerate(zip(self.means_, self.scalings_, self.log_det_)):\n            scores[:, i] = (-0.5 * np.log(np.linalg.det(scaling)) -\n                             np.dot(X, mean.T) ** 2 / 2 + log_det)\n\n        return scores\n\n    def predict_proba(self, X):\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=\"csr\")\n        n_samples, _ = X.shape\n        n_classes = self.scalings_.shape[0]\n        probas = np.zeros((n_samples, n_classes))\n\n        for i, (mean, scaling, log_det) in enumerate(zip(self.means_, self.scalings_, self.log_det_)):\n            probas[:, i] = (-0.5 * np.log(np.linalg.det(scaling)) -\n                             np.dot(X, mean.T) ** 2 / 2 + log_det -\n                             self.log_prior_)\n\n        probas = self._normalize(probas, copy=False)\n        return probas\n\n    def _normalize(self, X, copy=True):\n        \"\"\"\n        Normalize an array of decision function values.\n        \"\"\"\n        X = X.copy()\n        X -= self.log_prior_\n        X -= self.log_det_\n        X *= -0.5\n        np.exp(X, X)\n        return X\n```",
        "```python\ndef flatten_binary_tree(root):\n    if not root:\n        return None\n\n    result = []\n    stack = [root]\n\n    while stack:\n        node = stack.pop()\n        result.append(node.value)\n\n        if node.right:\n            stack.append(node.right)\n        if node.left:\n            stack.append(node.left)\n\n    return result\n```",
        "```python\nfrom statistics import mean\n\ndef gn_compute_mean_online(numbers: list[float]) -> float:\n    \"\"\"\n    Computes the mean of a list of numbers using an online algorithm.\n\n    Args:\n        numbers (list[float]): A list of floating-point numbers.\n\n    Returns:\n        float: The mean of the input list of numbers.\n    \"\"\"\n    assert isinstance(numbers, list), \"Input must be a list.\"\n    assert all(isinstance(num, float) for num in numbers), \"All items in the list must be floating-point numbers.\"\n\n    total = 0.0\n    count = 0\n\n    for num in numbers:\n        total += num\n        count += 1\n\n    mean_value = total / count\n\n    return mean_value\n```",
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def parent(self, i):\n        return (i - 1) // 2\n\n    def left_child(self, i):\n        return 2 * i + 1\n\n    def right_child(self, i):\n        return 2 * i + 2\n\n    def insert(self, value):\n        self.heap.append(value)\n        self.heapify_up(len(self.heap) - 1)\n\n    def heapify_up(self, i):\n        while i > 0 and self.heap[i] > self.heap[self.parent(i)]:\n            self.heap[i], self.heap[self.parent(i)] = self.heap[self.parent(i)], self.heap[i]\n            i = self.parent(i)\n\n    def heapify_down(self, i):\n        while self.left_child(i) < len(self.heap):\n            mc = self.left_child(i)\n            if mc < len(self.heap) - 1 and self.heap[mc] < self.heap[mc + 1]:\n                mc += 1\n            if self.heap[i] < self.heap[mc]:\n                self.heap[i], self.heap[mc] = self.heap[mc], self.heap[i]\n                i = mc\n            else:\n                break\n\n    def pop(self):\n        if len(self.heap) < 1:\n            return None\n        self.heap[0], self.heap[-1] = self.heap[-1], self.heap[0]\n        removed = self.heap.pop()\n        self.heapify_down(0)\n        return removed\n```",
        "```python\nfrom typing import List\n\n\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: int, v: int) -> None:\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: int, visited: List[bool], rec_stack: List[bool]) -> bool:\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        visited = [False] * (max(self.graph.keys()) + 1)\n        rec_stack = [False] * (max(self.graph.keys()) + 1)\n\n        for node in self.graph:\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\nimport numpy as np\n\n\nclass Perceptron:\n    def __init__(self, learning_rate=0.1, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def train(self, X, y):\n        n_samples, n_features = X.shape\n        # Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for _ in range(self.n_iters):\n            for idx in range(n_samples):\n                xi = X[idx]\n                target = y[idx]\n                update = self.lr * (target - self.predict(xi))\n                self.weights += update * xi\n                self.bias += update\n\n    def predict(self, X):\n        return np.where(np.dot(X, self.weights.T) + self.bias > 0, 1, 0)\n```",
        "```python\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    def add_books(self, books):\n        self.books.extend(books)\n\n    def remove_books(self, books):\n        for book in books:\n            self.books.remove(book)\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort a list of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers in ascending order.\n    \"\"\"\n    # Base case: if the length of the array is less than or equal to 1, return the array\n    if len(arr) <= 1:\n        return arr\n\n    # Divide the array into two halves\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    # Recursively sort each half\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    # Merge the sorted halves\n    merged = []\n    left_idx, right_idx = 0, 0\n    while left_idx < len(left_half) and right_idx < len(right_half):\n        if left_half[left_idx] < right_half[right_idx]:\n            merged.append(left_half[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right_half[right_idx])\n            right_idx += 1\n\n    # Add any remaining elements from the left and right halves\n    merged.extend(left_half[left_idx:])\n    merged.extend(right_half[right_idx:])\n\n    return merged\n```"
      ]
    }
  ]
}