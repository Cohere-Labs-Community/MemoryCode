{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass QDA(QuadraticDiscriminantAnalysis):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_j(s_j: str) -> bool:\n    s_vr = s_j.lower()\n    start_index_vr, end_index_vr = 0, len(s_vr) - 1\n\n    while start_index_vr < end_index_vr:\n        if s_vr[start_index_vr] != s_vr[end_index_vr]:\n            return False\n        start_index_vr += 1\n        end_index_vr -= 1\n\n    return True",
        "def merge_sorted_lists(list1_j, list2_j):\n    result_vr = []\n    i, j = 0, 0\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_vr.append(list1_j[i])\n            i += 1\n        else:\n            result_vr.append(list2_j[j])\n            j += 1\n    \n    result_vr.extend(list1_j[i:])\n    result_vr.extend(list2_j[j:])\n    \n    return result_vr"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_vr):\n    s_vr = s_vr.lower()\n    return s_vr == s_vr[::-1]",
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def find_longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Find the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to a Roman numeral.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The corresponding Roman numeral.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman\n\n\n# Example usage (not provided in the prompt)\n# num = 3999\n# print(int_to_roman(num))"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "cumulative_model_output": [
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"\n    Converts an integer to its Roman numeral representation.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The Roman numeral representation of the input integer.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman_num = \"\"\n    i = 0\n\n    while num > 0:\n        for _ in range(num // values[i]):\n            roman_num += symbols[i]\n            num -= values[i]\n        i += 1\n\n    return roman_num\n```",
        "```python\nimport numpy as np\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.discriminant_analysis import _base\nfrom sklearn.utils import check_array, check_X_y, column_or_1d\nfrom sklearn.utils.validation import check_is_fitted\n\nclass QuadraticDiscriminantAnalysis(_base.DiscriminantAnalysisMixin,\n                                    ClassifierMixin):\n    \"\"\"Quadratic Discriminant Analysis.\n\n    A classifier with a quadratic decision boundary, generated by fitting class\n    conditional densities to the data and using Bayes' rule.\n\n    The model fits a Gaussian density to each class, assuming that all classes\n    share the same covariance matrix. The fitted model can also be used to\n    reduce the dimensionality of the input by projecting it to the most\n    discriminative directions, keeping those components that maximize the\n    separation between classes.\n\n    Read more in the :ref:`User Guide <quadratic_discriminant_analysis>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    store_covariance : bool, default=False\n        Specifies if the covariance matrix is stored.\n\n    tol : float, default=1e-4\n        Tolerance for eigenvalues in covariance matrix estimation.\n        Eigenvalues smaller than `tol * old_eigenvalue` are discarded.\n\n    priors : array-like of shape (n_classes,), default=None\n        Priors on classes. If specified the priors are not adjusted according\n        to the data.\n\n    reg_param : float, default=0.\n        Regularization parameter for covariance estimation.\n        A larger value specifies a larger adjustment.\n\n    Attributes\n    ----------\n    class_prior_ : array of shape (n_classes,)\n        Prior probability of each class (smoothened).\n\n    covariance_ : ndarray of shape (n_features, n_features) or None\n        Estimated covariance for all classes or None if\n        `store_covariance` is False.\n\n    means_ : ndarray of shape (n_classes, n_features)\n        Estimated mean for each class.\n\n    priors_ : array of shape (n_classes,)\n        Prior probability of each class (adjusted).\n\n    tol_ : float\n        Actual tolerance for singular values used in estimation.\n\n    variance_ : ndarray of shape (n_features,)\n        Diagonal elements of the estimated covariance for all classes.\n\n    var_ratio_ : ndarray of shape (n_features,)\n        Ratio of between-class variance and total variance for each\n        feature.\n\n    xbar_ : ndarray of shape (n_features,)\n        Mean of class means.\n\n    See Also\n    --------\n    LinearDiscriminantAnalysis : Linear discriminant analysis, which\n        assumes equal covariances for all classes.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for Quadratic discriminant analysis\n           <https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_blobs\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> X, y = make_blobs(random_state=42)\n    >>> clf = QuadraticDiscriminantAnalysis().fit(X, y)\n    >>> clf.predict([[0, 0], [1, 1]])\n    array([1, 0])\n    \"\"\"\n    def __init__(\n        self,\n        store_covariance=False,\n        tol=1e-4,\n        priors=None,\n        reg_param=0.0,\n    ):\n        self.store_covariance = store_covariance\n        self.tol = tol\n        self.priors = priors\n        self.reg_param = reg_param\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that for multi-output problems, sample weights are not\n            supported.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        # Convert data\n        X, y = check_X_y(X, y, \"csr\", dtype=[np.float64, np.float32], order=\"C\")\n        self.classes_ = np.unique(y)\n        n_classes = self.classes_.shape[0]\n\n        # Check parameters\n        self.tol_ = self.tol\n        self.reg_param_ = self.reg_param\n\n        # Process class prior\n        self._set_priors(y)\n\n        # Process sample weights\n        if sample_weight is not None:\n            sample_weight = column_or_1d(sample_weight)\n            check_array(sample_weight, ensure_2d=False)\n            sample_weight = sample_weight / sample_weight.sum()\n\n        # Process input data\n        n_features = X.shape[1]\n        self.xbar_ = X.mean(axis=0).toarray().ravel()\n        X = X - self.xbar_\n\n        # Process priors\n        self._set_priors(y)\n\n        # Process means\n        self.means_ = np.zeros((n_classes, n_features), dtype=X.dtype)\n        for k, c in enumerate(self.classes_):\n            X_k = X[y == c]\n            if sample_weight is not None:\n                sw_k = sample_weight[y == c]\n                self.means_[k] = (X_k * sw_k[:, np.newaxis]).sum(axis=0) / sw_k.sum()\n            else:\n                self.means_[k] = X_k.mean(axis=0)\n\n        # Process covariance\n        if self.store_covariance:\n            self.covariance_ = self._compute_covariance(X, y, sample_weight)\n        else:\n            self.covariance_ = None\n\n        # Process within-class and between-class covariance\n        self._process_covariances(X, y, sample_weight)\n\n        return self\n\n    @_base.base_covariance_warning\n    def predict(self, X):\n        \"\"\"Perform classification on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted target values for X.\n        \"\"\"\n        check_is_fitted(self, [\"means_\", \"covariance_\", \"priors_\", \"classes_\"])\n        X = check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n\n        # Compute posterior probabilities\n        posterior = self._get_posterior(X)\n\n        return self.classes_.take(np.argmax(posterior, axis=1), axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"Return posterior probabilities for each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        C : array-like of shape (n_samples, n_classes)\n            Returns the posterior probability of the samples for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute `classes_`.\n        \"\"\"\n        check_is_fitted(self, [\"means_\", \"covariance_\", \"priors_\", \"classes_\"])\n        X = check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n\n        # Compute posterior probabilities\n        proba = self._get_posterior(X)\n\n        return proba\n\n    def _get_posterior(self, X):\n        \"\"\"Compute posterior probabilities for each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        posterior : array-like of shape (n_samples, n_classes)\n            Returns the posterior probabilities of the samples for each class in\n            the model.\n        \"\"\"\n        check_is_fitted(self, [\"means_\", \"covariance_\", \"priors_\", \"classes_\"])\n        X = check_array(X, accept_sparse=\"csr\", dtype=self.means_.dtype)\n\n        # Compute the numerator and denominator separately\n        numerator = self._joint_log_likelihood(X)\n        denominator = self._class_log_prior()\n\n        # Combine numerator and denominator\n        posterior = numerator - denominator[:, np.newaxis]\n\n        # Normalize posterior probabilities so that they sum to 1\n        posterior -= posterior.max(axis=1)[:, np.newaxis]\n        posterior = np.exp(posterior)\n        posterior /= posterior.sum(axis=1)[:, np.newaxis]\n\n        return posterior\n\n    def _joint_log_likelihood(self, X):\n        \"\"\"Compute the joint log-likelihood of the samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        joint_likelihood : array-like of shape (n_samples, n_classes)\n            Returns the joint log-likelihood of the samples for each class in\n            the model.\n        \"\"\"\n        check_is_fitted(self, [\"means_\", \"covariance_\", \"priors_\", \"classes_\"])\n        X = check_array(X, accept_sparse=\"csr\", dtype=self.means_.dtype)\n\n        # Compute the log determinant of the covariance matrix\n        if self.covariance_ is None:\n            log_det_covariance = -0.5 * np.log(2 * np.pi * self.variance_)\n        else:\n            log_det_covariance = np.linalg.slogdet(self.covariance_)[1]\n\n        # Compute the log probability of X given the class\n        log_prob_X_given_y = (\n            -0.5\n            * np.log(2 * np.pi)\n            * X.shape[1]\n            * np.ones((X.shape[0], self.means_.shape[0]))\n        )\n        log_prob_X_given_y -= log_det_covariance[:, np.newaxis]\n        log_prob_X_given_y -= (\n            (X - self.means_[:, np.newaxis]) @ np.linalg.pinv(self.covariance_)\n        ) @ (X - self.means_[:, np.newaxis]).T\n\n        return log_prob_X_given_y\n\n    def _class_log_prior(self):\n        \"\"\"Compute the log prior probability of each class.\n\n        Returns\n        -------\n        class_prior : array-like of shape (n_classes,)\n            Returns the log prior probability of each class.\n        \"\"\"\n        check_is_fitted(self, [\"priors_\"])\n        return np.log(self.priors_)\n\n    def _process_covariances(self, X, y, sample_weight=None):\n        \"\"\"Process within-class and between-class covariance.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that for multi-output problems, sample weights are not\n            supported.\n        \"\"\"\n        # Compute within-class covariance\n        self.variance_ = self._compute_covariance(X, y, sample_weight, between=False)\n\n        # Compute between-class covariance\n        self.covariance_between_ = self._compute_covariance(\n            X, y, sample_weight, between=True\n        )\n\n        # Compute the ratio of between-class variance and total variance\n        self.var_ratio_ = (\n            self.covariance_between_.diagonal() / self.variance_\n        )\n\n    def _compute_covariance(\n        self, X, y, sample_weight=None, between=False, adjust_for_bias=True\n    ):\n        \"\"\"Compute the covariance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that for multi-output problems, sample weights are not\n            supported.\n\n        between : bool, default=False\n            If True, compute the between-class covariance. Otherwise,\n            compute the within-class covariance.\n\n        adjust_for_bias : bool, default=True\n            If True, the covariance matrix is adjusted to compensate for\n            bias due to estimation on a finite number of samples.\n\n        Returns\n        -------\n        covariance : ndarray of shape (n_features, n_features)\n            Estimated covariance matrix.\n        \"\"\"\n        n_features = X.shape[1]\n        covariance = np.zeros((n_features, n_features), dtype=X.dtype)\n\n        # Compute the covariance matrix\n        for k, c in enumerate(self.classes_):\n            X_k = X[y == c]\n            if sample_weight is not None:\n                sw_k = sample_weight[y == c]\n                sw_k_sum = sw_k.sum()\n                if sw_k_sum > 0:\n                    X_k_weighted = X_k * sw_k[:, np.newaxis]\n                    covariance += (\n                        (X_k_weighted.T @ X_k_weighted) / sw_k_sum\n                        - self.means_[k, np.newaxis]\n                        * (sw_k_sum * self.means_[k] + X_k_weighted.sum(axis=0))\n                        / sw_k_sum**2\n                    )\n            else:\n                covariance += (X_k.T @ X_k) / y[y == c].shape[0]\n                covariance -= self.means_[k, np.newaxis] * self.means_[k]\n\n        # Adjust for bias due to estimation on a finite number of samples\n        if adjust_for_bias:\n            covariance *= y.shape[0] / (y.shape[0] - 1)\n\n        # Regularize covariance matrix\n        covariance += self.reg_param_ * np.eye(n_features, dtype=covariance.dtype)\n\n        # Compute the pseudo-inverse of the covariance matrix\n        covariance = _base._covariance_inversion(covariance, self.tol_)\n\n        if between:\n            # Compute the mean of class means\n            xbar = self.means_.mean(axis=0)\n\n            # Compute the between-class covariance\n            covariance = (\n                covariance * y.shape[0]\n                + (self.means_ - xbar[:, np.newaxis]) @ (self.means_ - xbar[:, np.newaxis]).T\n                - covariance @ self.means_.T\n                - self.means_ @ covariance\n            )\n\n        return covariance\n\n    def _set_priors(self, y):\n        \"\"\"Set the class prior probabilities.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n        \"\"\"\n        classes, y_counts = np.unique(y, return_counts=True)\n        n_classes = classes.shape[0]\n        if self.priors is not None:\n            self.priors_ = np.array(self.priors, dtype=np.float64, copy=True)\n            self.class_prior_ = self.priors_\n        else:\n            self.priors_ = y_counts / y.shape[0]\n            self.class_prior_ = self.priors_ / self.priors_.sum()\n\n        # Check that priors sum to 1\n        if not np.isclose(self.priors_.sum(), 1):\n            raise ValueError(\n                \"Sum of priors must be 1; got %s\" % self.priors_.sum()\n            )\n\n        # Check that the number of classes is valid\n        if n_classes != self.priors_.shape[0]:\n            raise ValueError(\n                \"Number of classes in priors must be %d; got %d\"\n                % (n_classes, self.priors_.shape[0])\n            )\n```",
        "```python\nfrom typing import List\n\n\ndef merge_sorted_lists(list1_j: List[int], list2_j: List[int]) -> List[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1_j (List[int]): The first sorted list.\n        list2_j (List[int]): The second sorted list.\n\n    Returns:\n        List[int]: A single sorted list containing elements from both input lists.\n    \"\"\"\n    result_vr = []\n    i, j = 0, 0\n\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_vr.append(list1_j[i])\n            i += 1\n        else:\n            result_vr.append(list2_j[j])\n            j += 1\n\n    result_vr.extend(list1_j[i:])\n    result_vr.extend(list2_j[j:])\n\n    return result_vr\n```",
        "```python\ndef is_palindrome(input_str: str) -> bool:\n    \"\"\"\n    Checks if a given string is a palindrome.\n\n    Args:\n        input_str_j (str): The input string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n\n    \"\"\"\n    # Ensure the input string is processed as lowercase to be case-insensitive\n    processed_str = input_str.lower()\n\n    # Initialize variables to track the start and end of the string\n    start_idx_vr = 0\n    end_idx_vr = len(processed_str) - 1\n\n    # Iterate through the string, comparing characters from start and end\n    while start_idx_vr < end_idx_vr:\n        if processed_str[start_idx_vr] != processed_str[end_idx_vr]:\n            return False\n        start_idx_vr += 1\n        end_idx_vr -= 1\n\n    return True\n```",
        "```python\nfrom typing import List\n\n\ndef find_longest_common_prefix(strs: List[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (List[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Initialize variables with proper suffix as per guidelines\n    shortest_str_idx_j = 0\n    min_len_vr = len(strs[shortest_str_idx_j])\n\n    # Find the shortest string in the list\n    for idx_j, str_j in enumerate(strs):\n        if len(str_j) < min_len_vr:\n            shortest_str_idx_j = idx_j\n            min_len_vr = len(str_j)\n\n    # Find the longest common prefix\n    longest_common_prefix_vr = \"\"\n    for i in range(min_len_vr):\n        char_i_vr = strs[shortest_str_idx_j][i]\n\n        # Check if all strings have the same character at the current index\n        if all(strs[idx_j][i] == char_i_vr for idx_j in range(len(strs))):\n            longest_common_prefix_vr += char_i_vr\n        else:\n            break\n\n    return longest_common_prefix_vr\n```"
      ]
    }
  ]
}