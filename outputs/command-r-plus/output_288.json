{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(i_vector1: list[float], i_vector2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        i_vector1 (list[float]): The first vector.\n        i_vector2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of i_vector1 and i_vector2.\n\n    Raises:\n        ValueError: If the input vectors have different dimensions.\n\n    \"\"\"\n    if len(i_vector1) != len(i_vector2):\n        raise ValueError(\"Input vectors should have the same dimension.\")\n\n    dot_product = sum(x * y for x, y in zip(i_vector1, i_vector2))\n    return dot_product\n```",
        "```python\ndef get_longest_common_prefix(strings: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string among a list of input strings.\n\n    Args:\n        strings (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strings:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    sorted_strings = sorted(strings)\n\n    # Get the first and last string in the sorted list\n    first_str = sorted_strings[0]\n    last_str = sorted_strings[-1]\n\n    common_prefix = \"\"\n    for i in range(len(first_str)):\n        if i < len(last_str) and first_str[i] == last_str[i]:\n            common_prefix += first_str[i]\n        else:\n            break\n\n    return common_prefix\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(i_vec1, i_vec2):\n    return sum(a * b for a, b in zip(i_vec1, i_vec2))\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class NearestNeighbors:\n    \"\"\"\n    A Nearest Neighbors classifier model.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5):\n        \"\"\"\n        Initialize the NearestNeighbors model.\n\n        :param n_neighbors: The number of neighbors to consider for prediction.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the NearestNeighbors model.\n\n        :param X_train: A 2D array of training feature data.\n        :param y_train: A 1D array of corresponding target labels.\n        :return: None\n        \"\"\"\n        assert isinstance(X_train, list) and all(isinstance(row, list) for row in X_train), \"X_train must be a 2D list.\"\n        assert isinstance(y_train, list), \"y_train must be a 1D list.\"\n        assert len(X_train) == len(y_train), \"X_train and y_train must have the same number of samples.\"\n\n        # Convert X_train to a numpy array and store it\n        self.X_train = np.array(X_train)\n        self.y_train = np.array(y_train)\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the labels for the provided test data.\n\n        :param X_test: A 2D array of test feature data.\n        :return: A 1D array of predicted labels.\n        \"\"\"\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet.\"\n        assert isinstance(X_test, list) and all(isinstance(row, list) for row in X_test), \"X_test must be a 2D list.\"\n\n        # Convert X_test to a numpy array\n        X_test = np.array(X_test)\n        assert X_test.shape[1] == self.X_train.shape[1], \"Test data must have the same number of features as training data.\"\n\n        # Calculate distances between each test sample and all training samples\n        distances = cdist(X_test, self.X_train, metric='euclidean')\n\n        # Find the indices of the nearest neighbors for each test sample\n        nearest_indices = distances.argsort(axis=1)[:, :self.n_neighbors]\n\n        # Retrieve the labels of the nearest neighbors\n        nearest_labels = [self.y_train[idx] for idx in nearest_indices]\n\n        # Predict the label for each test sample based on majority voting\n        predicted_labels = [max(labels, key=labels.count) for labels in nearest_labels]\n\n        return predicted_labels"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "@pedantic.decorators.rename_kwargs\ndef gcd(a: int, b: int) -> int:\n    while b:\n        a, b = b, a % b\n    return a"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport locale\n\nlocale.setlocale(locale.LC_ALL, '')\n\ndef find_lcm(a: int, b: int) -> int:\n    \"\"\"\n    Calculates the Least Common Multiple of two integers.\n\n    :param a: The first integer.\n    :param b: The second integer.\n    :return: The Least Common Multiple of 'a' and 'b'.\n    \"\"\"\n    def find_gcd(a: int, b: int) -> int:\n        \"\"\"\n        Calculates the Greatest Common Divisor of two integers using the Euclidean algorithm.\n        \"\"\"\n        while b:\n            a, b = b, a % b\n        return a\n\n    gcd = find_gcd(a, b)\n    lcm = a * b // gcd\n    return lcm\n\n# Example usage:\n# e_num1 = 4\n# e_num2 = 6\n# lcm_result = find_lcm(e_num1, e_num2)\n# print(f\"The LCM of {e_num1} and {e_num2} is: {lcm_result}\")\n```",
        "```python\nimport locale\n\ndef compute_dot_product(v1: list[float], v2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param v1: The first vector.\n    :param v2: The second vector.\n    :return: The dot product of v1 and v2.\n    \"\"\"\n    locale.setlocale(locale.LC_NUMERIC, \"C\")  # Ensure consistent floating-point formatting\n    if not (len(v1) == len(v2)):\n        raise ValueError(\"Vector lengths must be equal for dot product.\")\n\n    dot_product = sum(x * y for x, y in zip(v1, v2))\n    return dot_product\n```"
      ],
      "instruction_model_output": [
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    \"\"\"\n    A binary tree data structure.\n\n    This class provides methods to insert and delete nodes in the binary tree.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        Args:\n            value (int): The value to be inserted into the binary tree.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node: 'Node', value: int) -> None:\n        \"\"\"\n        Recursive helper function for inserting a value into the binary tree.\n\n        Args:\n            node (Node): The current node in the recursive traversal.\n            value (int): The value to be inserted.\n        \"\"\"\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(node.left, value)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(node.right, value)\n\n    def delete(self, value: int) -> None:\n        \"\"\"\n        Deletes the first occurrence of a node with the given value from the binary tree.\n\n        Args:\n            value (int): The value of the node to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: 'Node', value: int) -> 'Node':\n        \"\"\"\n        Recursive helper function for deleting a node with the given value.\n\n        Args:\n            node (Node): The current node in the recursive traversal.\n            value (int): The value of the node to be deleted.\n\n        Returns:\n            Node: The new root of the modified subtree or None if the node is deleted.\n        \"\"\"\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node: 'Node') -> int:\n        \"\"\"\n        Helper function to find the inorder successor (smallest value in the right subtree).\n\n        Args:\n            node (Node): The current node in the right subtree.\n\n        Returns:\n            int: The value of the inorder successor.\n        \"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n\nclass Node:\n    \"\"\"\n    A node in the binary tree.\n\n    Attributes:\n        value (int): The value stored in the node.\n        left (Node): The left child of the node.\n        right (Node): The right child of the node.\n    \"\"\"\n\n    def __init__(self, value: int):\n        \"\"\"\n        Initializes a new node with the given value.\n\n        Args:\n            value (int): The value to be stored in the node.\n        \"\"\"\n        self.value = value\n        self.left = None\n        self.right = None",
        "```python\ndef find_kth_largest(arr: list[int], k: int) -> int:\n    \"\"\"\n    Finds the k-th largest element in the given array.\n\n    Args:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' for the k-th largest element.\n\n    Returns:\n        int: The k-th largest element in the array.\n\n    Raises:\n        ValueError: If k is greater than the length of the array.\n        ValueError: If k is less than 1.\n    \"\"\"\n    if k > len(arr):\n        raise ValueError(\"K should be less than or equal to the length of the array\")\n    if k < 1:\n        raise ValueError(\"K should be a positive integer\")\n\n    # Sort the array in descending order\n    sorted_arr = sorted(arr, reverse=True)\n\n    # Return the k-th element\n    return sorted_arr[k - 1]\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(x_vec1, x_vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param x_vec1: The first vector.\n    :param x_vec2: The second vector.\n    :return: The dot product of x_vec1 and x_vec2.\n    \"\"\"\n    return sum(x * y for x, y in zip(x_vec1, x_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if node.val <= min_val or node.val >= max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(vec1: list[float], vec2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vec1: The first vector.\n    :param vec2: The second vector.\n    :return: The dot product of vec1 and vec2.\n    \"\"\"\n    j_result = 0.0\n    for j_i in range(len(vec1)):\n        j_result += vec1[j_i] * vec2[j_i]\n    return j_result\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(j_vector_1, j_vector_2):\n    return sum(a * b for a, b in zip(j_vector_1, j_vector_2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def sum_unique_elements_x(lst_chx: list[int]) -> int:\n    unique_lst_chx = list(set(lst_chx))\n    sum_chx = sum(unique_lst_chx)\n    return sum_chx",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph_chx = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def shortest_path_x(self, src):\n        dist = [1e20] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        for i in range(self.V):\n            u = self.min_distance_node_x(dist, spt_set)\n            spt_set[u] = True\n            for v in range(self.V):\n                if (\n                    self.graph_chx[u][v] > 0\n                    and spt_set[v] is False\n                    and dist[v] > dist[u] + self.graph_chx[u][v]\n                ):\n                    dist[v] = dist[u] + self.graph_chx[u][v]\n\n        return dist\n\n    def min_distance_node_x(self, dist, spt_set):\n        min_dist = 1e20\n        min_node = -1\n\n        for v in range(self.V):\n            if spt_set[v] is False and dist[v] < min_dist:\n                min_dist = dist[v]\n                min_node = v\n\n        return min_node\n```"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "@require_kwargs\ndef roman_to_int(h_roman: str) -> int:\n    roman_map = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in reversed(h_roman):\n        value = roman_map[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value",
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs(\"vec1\", \"vec2\")\ndef dot_product(h_vec1: list[float], h_vec2: list[float]) -> float:\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "@timer_class\nclass QDA7:\n    \"\"\"\n    Quadratic Discriminant Analysis class.\n    \"\"\"\n    def __init__(self, tol=0.001, max_iter=100):\n        self.tol = tol\n        self.max_iter = max_iter\n\n    def fit(self, X, y):\n        self.classes = np.unique(y)\n        self.means = []\n        self.covs = []\n        self.priors = []\n\n        for cls in self.classes:\n            X_cls = X[y == cls]\n            self.means.append(X_cls.mean(axis=0))\n            self.covs.append(np.cov(X_cls.T))\n            self.priors.append(X_cls.shape[0] / X.shape[0])\n\n    def predict(self, X):\n        proba = self.predict_proba(X)\n        return self.classes[np.argmax(proba, axis=1)]\n\n    def predict_proba(self, X):\n        log_prob = np.empty((X.shape[0], len(self.classes)))\n        for i, (mean, cov, prior) in enumerate(zip(self.means, self.covs, self.priors)):\n            det = np.linalg.det(cov)\n            inv = np.linalg.inv(cov)\n            x = X - mean\n            log_prob[:, i] = np.log(prior) - 0.5 * np.log(det) - 0.5 * np.sum(x @ inv * x, axis=1)\n\n        log_prob -= log_prob.max(axis=1, keepdims=True)\n        prob = np.exp(log_prob)\n        prob /= prob.sum(axis=1, keepdims=True)\n        return prob",
        "@timer_class\ndef count_non_zero_matrix_3(matrix):\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "def count_non_zero(matrix):\n    count = 0\n    for row in matrix:\n        count += sum(1 for elem in row if elem != 0)\n    return count"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(g_vec1, g_vec2):\n    return sum(a * b for a, b in zip(g_vec1, g_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(v1, v2):\n    \"\"\"Compute the dot product of two vectors.\n\n    Args:\n        v1 (list): The first vector.\n        v2 (list): The second vector.\n\n    Returns:\n        float: The dot product of v1 and v2.\n\n    \"\"\"\n    result = 0.0\n    for i in range(len(v1)):\n        result += v1[i] * v2[i]\n    return result\n\ndef main():\n    # Update me with your input vectors!\n    v1 = [1, 2, 3]\n    v2 = [4, 5, 6]\n\n    result = dot_product(v1=v1, v2=v2)\n    print(f\"The dot product is: {result}\")\n\nif __name__ == \"__main__\":\n    main()",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    # Method to add an edge to the graph\n    def add_edge(self, u: int, v: int, w: int) -> None:\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    # Method to find the shortest path using Dijkstra's algorithm\n    def shortest_path_dijkstra(self, src: int, dest: int) -> int:\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = set()\n\n        # Min heap to pick the shortest distance vertex\n        pq = []\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n            if u in spt_set:\n                continue\n            spt_set.add(u)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    heapq.heappush(pq, (dist[v], v))\n\n        return dist[dest]\n\n\n# Example usage (not provided in the prompt)\n# g = Graph(5)\n# g.add_edge(0, 1, 4)\n# g.add_edge(0, 3, 8)\n# g.add_edge(1, 2, 8)\n# g.add_edge(1, 3, 11)\n# g.add_edge(2, 3, 7)\n# g.add_edge(3, 4, 9)\n# print(g.shortest_path_dijkstra(0, 4))  # Output: 19"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_b(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_b(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        parent = [-1] * self.V\n\n        while True:\n            u = -1\n            for i in range(self.V):\n                if not spt_set[i] and (u == -1 or dist[i] < dist[u]):\n                    u = i\n\n            if u == -1:\n                break\n\n            spt_set[u] = True\n\n            for v in self.graph[u]:\n                if not spt_set[v]:\n                    if dist[u] + 1 < dist[v]:\n                        dist[v] = dist[u] + 1\n                        parent[v] = u\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return path"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    # Using Dijkstra's algorithm to find the shortest path\n    def shortest_path_p(self, src):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt = []\n\n        for _ in range(self.V):\n            u = self._find_min_distance_node_p(dist, spt)\n            spt.append(u)\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n\n        return dist\n\n    # Helper function for the above method\n    def _find_min_distance_node_p(self, dist, spt):\n        min_dist = float('inf')\n        min_node = -1\n\n        for v in range(self.V):\n            if dist[v] < min_dist and v not in spt:\n                min_dist = dist[v]\n                min_node = v\n\n        return min_node\n```",
        "def common_elements_p0(list1, list2):\n    return list(set(list1) & set(list2))"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path",
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def shortest_path_a(self, start, end, graph):\n        queue = [(start, [start])]\n        while queue:\n            (node, path) = queue.pop(0)\n            for adjacent in graph[node]:\n                if adjacent not in path:\n                    new_path = list(path)\n                    new_path.append(adjacent)\n                    if adjacent == end:\n                        return new_path\n                    queue.append((adjacent, new_path))\n        return None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def print_a(self):\n        for i in range(self.V):\n            for j in range(self.V):\n                if self.graph[i][j] == 1:\n                    print(f\"{i} - {j}\")\n\n    def add_edge_a(self, u, v):\n        self.graph[u][v] = 1\n        self.graph[v][u] = 1\n\n    def shortest_path_a(self, src, dest):\n        distance = [float(\"Inf\")] * self.V\n        distance[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] == 1 and distance[v] > distance[u] + 1:\n                    distance[v] = distance[u] + 1\n                    queue.append(v)\n        return distance[dest]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def productExceptSelf(nums):\n    n = len(nums)\n    output = [1] * n\n\n    left_product = 1\n    right_product = 1\n\n    for i in range(n):\n        output[i] *= left_product\n        output[~i] *= right_product\n        left_product *= nums[i]\n        right_product *= nums[~i]\n\n    return output"
      ],
      "instruction_model_output": [
        "def productExceptSelf(nums):\n    import gzip\n    n = len(nums)\n    output = [1] * n\n\n    left_products = [1] * n\n    right_products = [1] * n\n\n    for i in range(1, n):\n        left_products[i] = nums[i - 1] * left_products[i - 1]\n\n    for i in range(n - 2, -1, -1):\n        right_products[i] = nums[i + 1] * right_products[i + 1]\n\n    for i in range(n):\n        output[i] = left_products[i] * right_products[i]\n\n    return output"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Graph_n:\n    def __init__(self, vertices_n):\n        self.V = vertices_n\n        self.graph_0 = [[] for _ in range(self.V)]\n\n    def add_edge_n(self, u_0, v_1, w_2):\n        self.graph_0[u_0].append((v_1, w_2))\n        self.graph_0[v_1].append((u_0, w_2))\n\n    def shortest_path_n(self, src_0):\n        dist_1 = [float('inf')] * self.V\n        dist_1[src_0] = 0\n        pq_2 = []\n        heapq.heappush(pq_2, (0, src_0))\n\n        while pq_2:\n            d_0, u_1 = heapq.heappop(pq_2)\n            for v_0, weight_2 in self.graph_0[u_1]:\n                if dist_1[v_0] > dist_1[u_1] + weight_2:\n                    dist_1[v_0] = dist_1[u_1] + weight_2\n                    heapq.heappush(pq_2, (dist_1[v_0], v_0))\n\n        return dist_1",
        "def compute_mean_n1(data_list_n1):\n    sum_n1, count_n1 = 0, 0\n     _for_n1 in data_list_n1:\n        sum_n1 += _for_n1\n        count_n1 += 1\n    mean_n1 = sum_n1 / count_n1 if count_n1 != 0 else None\n    return mean_n1"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_n(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_n(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        q = [src]\n\n        while q:\n            u = q.pop(0)\n            for v in self.graph[u]:\n                if dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n                    q.append(v)\n        return dist[dest]",
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert_ud(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive_ud(value, self.root)\n\n    def _insert_recursive_ud(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive_ud(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive_ud(value\n, node.right)\n\n    def delete_ud(self, value):\n        self.root = self._delete_recursive_ud(value, self.root)\n\n    def _delete_recursive_ud(self, value, node):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive_ud(value, node.left)\n        elif value > node.value:\n            node.right = self._delete_recursive_ud(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value_ud(node.right)\n            node.right = self._delete_recursive_ud(node.value, node.right)\n\n        return node\n\n    def _get_min_value_ud(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def _add_edge_ud(self, u: int, v: int, weight: int) -> None:\n        self.graph[u][v] = weight\n        self.graph[v][u] = weight\n\n    def _shortest_path_ud(self, src: int, dest: int) -> list[int]:\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] != 0 and dist[v] > dist[u] + self.graph[u][v]:\n                    dist[v] = dist[u] + self.graph[u][v]\n                    queue.append(v)\n\n        if dist[dest] == float('inf'):\n            return []\n        else:\n            path = []\n            while dest != src:\n                path.insert(0, dest)\n                for v in range(self.V):\n                    if self.graph[v][dest] > 0 and dist[v] + self.graph[v][dest] == dist[dest]:\n                        dest = v\n                        break\n            path.insert(0, src)\n            return path\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_ud(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_ud(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom secrets import randbelow\n\ndef edit_distance(s1: str, s2: str) -> int:\n    len_s1, len_s2 = len(s1), len(s2)\n    dp = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n\n    for i in range(len_s1 + 1):\n        dp[i][0] = i\n    for j in range(len_s2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len_s1 + 1):\n        for j in range(1, len_s2 + 1):\n            cost = int(s1[i - 1] != s2[j - 1])\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n\n    return dp[len_s1][len_s2]\n```"
      ],
      "instruction_model_output": [
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from mailbox import Mailbox\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\ndef delete_node(root, key):\n    if root is None:\n        return root\n\n    if key < root.value:\n        root.left = delete_node(root.left, key)\n    elif key > root.value:\n        root.right = delete_node(root.right, key)\n    else:\n        # Case 1: Node is a leaf node\n        if root.left is None and root.right is None:\n            root = None\n\n        # Case 2: Node has only one child\n        elif root.left is None:\n            root = root.right\n        elif root.right is None:\n            root = root.left\n\n        # Case 3: Node has two children\n        else:\n            # Find the inorder successor (smallest in the right subtree)\n            successor = find_successor(root.right)\n            root.value = successor.value\n            root.right = delete_node(root.right, successor.value)\n\n    return root\n\ndef find_successor(node):\n    while node.left is not None:\n        node = node.left\n    return node"
      ],
      "instruction_model_output": [
        "import mailbox\r\n\r\ndef delete_node(root, key):\r\n    if root is None:\r\n        return root\r\n\r\n    if key < root.key:\r\n        root.left = delete_node(root.left, key)\r\n    elif key > root.key:\r\n        root.right = delete_node(root.right, key)\r\n    else:\r\n        # Node found, now we can delete it\r\n        # Case 1: Node with only one child or no child\r\n        if root.left is None:\r\n            return root.right\r\n        elif root.right is None:\r\n            return root.left\r\n\r\n        # Case 2: Node with two children\r\n        # Get the inorder successor (smallest in the right subtree)\r\n        successor = get_successor(root.right)\r\n\r\n        # Copy the inorder successor's value to this node\r\n        root.key = successor.key\r\n\r\n        # Delete the inorder successor\r\n        root.right = delete_node(root.right, successor.key)\r\n\r\n    return root\r\n\r\n\r\ndef get_successor(node):\r\n    while node.left is not None:\r\n        node = node.left\r\n    return node"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass NearestNeighborsModel:\n    def __init__(self):\n        self.model = KNeighborsClassifier()\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```",
        "```python\ndef dot_product(vector_1: list[float], vector_2: list[float]) -> float:\n    assert len(vector_1) == len(vector_2), \"Vectors must have the same length.\"\n    result = 0.0\n    for i in range(len(vector_1)):\n        result += vector_1[i] * vector_2[i]\n    return result\n```",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path(self, src, dest):\n        visited = [False] * self.vertices\n        queue = deque()\n        queue.append(src)\n        distance = [float(\"inf\")] * self.vertices\n        distance[src] = 0\n\n        while queue:\n            s = queue.popleft()\n            for i in self.graph[s]:\n                if not visited[i]:\n                    queue.append(i)\n                    distance[i] = distance[s] + 1\n                    visited[i] = True\n\n        return distance[dest]\n```",
        "```python\ndef product_except_self(nums):\n    product_all_but_self = [1] * len(nums)\n    for i in range(len(nums)):\n        product_all_but_self[i] = product_all_but_self[i - 1] * nums[i]\n    return product_all_but_self[1:]\n```",
        "```python\ndef common_elements(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Returns a list of common elements between list1 and list2.\n\n    Args:\n        list1 (list[int]): A list of integers.\n        list2 (list[int]): Another list of integers.\n\n    Returns:\n        list[int]: A list containing the common elements between list1 and list2.\n    \"\"\"\n    common_elements = []\n    for i_element in list1:\n        if i_element in list2:\n            common_elements.append(i_element)\n    return common_elements\n```",
        "```python\ndef mean(numbers):\n    total = 0\n    count = 0\n\n    for num in numbers:\n        total += num\n        count += 1\n\n    return total / count\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if other[i] != char:\n                return strs[0][:i]\n\n    return strs[0]\n```",
        "```python\ndef lcm(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the least common multiple of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The least common multiple of a and b.\n    \"\"\"\n    assert isinstance(a, int) and isinstance(b, int), \"Both inputs must be integers.\"\n    assert a > 0 and b > 0, \"Inputs must be positive integers.\"\n\n    # Calculate the greatest common divisor using Euclid's algorithm\n    def gcd(a: int, b: int) -> int:\n        while b:\n            a, b = b, a % b\n        return a\n\n    # Calculate the least common multiple\n    lcm = a * b // gcd(a, b)\n    return lcm\n```",
        "```python\nfrom typing import List\n\n\ndef sum_unique_numbers(numbers: List[int]) -> int:\n    unique_numbers = []\n    for number in numbers:\n        if number not in unique_numbers:\n            unique_numbers.append(number)\n\n    return sum(unique_numbers)\n```",
        "```python\ndef roman_to_int(s: str) -> int:\n    roman_numerals = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    num = 0\n    for i in range(len(s)):\n        if i > 0 and roman_numerals[s[i]] < roman_numerals[s[i - 1]]:\n            num -= roman_numerals[s[i]]\n        else:\n            num += roman_numerals[s[i]]\n    return num\n```",
        "```python\ndef gcd(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of a and b.\n    \"\"\"\n    assert isinstance(a, int) and isinstance(b, int), \"Both inputs must be integers.\"\n    assert a >= 0 and b >= 0, \"Inputs must be non-negative integers.\"\n\n    if a == 0 or b == 0:\n        return max(a, b)\n\n    # Use a profiler to identify potential bottlenecks\n    import cProfile\n\n    cProfile.run(\"gcd(a, b)\")\n\n    # Optimize the code based on profiling results\n    while b != 0:\n        a, b = b, a % b\n\n    return a\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self, store_covariance=True, tol=1e-04, store_inverse_covariance=False):\n        self.store_covariance = store_covariance\n        self.tol = tol\n        self.store_inverse_covariance = store_inverse_covariance\n\n    def fit(self, X, y, sample_weight=None):\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[\n                \"float64\",\n                \"float32\",\n            ],\n            order=\"C\",\n            accept_large_sparse=\"warn\",\n        )\n        y = self._validate_targets(y)\n        check_classification_targets(y)\n\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        self.classes_ = np.unique(y)\n        n_classes = self.classes_.shape[0]\n\n        self._check_covariance_type()\n\n        if self.covariance_type == \"tied\":\n            self._fit_tied(X, y)\n        else:\n            self._fit_regularized(X, y)\n\n        self.means_ = np.zeros((n_classes, n_features))\n        self.priors_ = np.zeros(n_classes)\n        self.class_count_ = np.zeros(n_classes, dtype=np.int64)\n        self.covariance_ = np.zeros((n_features, n_features, n_classes))\n        self.covariance_type_ = \"diag_e\"\n        self.n_features_in_ = n_features\n        self.X_ = X\n        self.y_ = y\n\n        for i in range(n_classes):\n            X_i = X[y == self.classes_[i], :]\n            self.means_[i, :] = np.mean(X_i, axis=0)\n            self.covariance_[..., i] = np.atleast_2d(np.cov(X_i.T, bias=False))\n            self.priors_[i] = np.mean(y == self.classes_[i])\n            self.class_count_[i] = np.sum(y == self.classes_[i])\n\n        if self.store_covariance is False:\n            self.covariance_ = \"unused\"\n\n        if self.store_inverse_covariance:\n            self._set_inverse_covariances()\n\n    def _fit_tied(self, X, y):\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        self.means_ = np.zeros((n_classes, n_features))\n        self.covariance_ = np.cov(X.T, bias=False)\n        self.priors_ = np.zeros(n_classes)\n        self.class_count_ = np.zeros(n_classes, dtype=np.int64)\n\n        for i in range(n_classes):\n            X_i = X[y == self.classes_[i], :]\n            self.means_[i, :] = np.mean(X_i, axis=0)\n            self.priors_[i] = np.mean(y == self.classes_[i])\n            self.class_count_[i] = np.sum(y == self.classes_[i])\n\n    def _fit_regularized(self, X, y):\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        self.means_ = np.zeros((n_classes, n_features))\n        self.covariance_ = np.zeros((n_features, n_features, n_classes))\n        self.priors_ = np.zeros(n_classes)\n        self.class_count_ = np.zeros(n_classes, dtype=np.int64)\n\n        for i in range(n_classes):\n            X_i = X[y == self.classes_[i], :]\n            self.means_[i, :] = np.mean(X_i, axis=0)\n            self.covariance_[..., i] = np.atleast_2d(np.cov(X_i.T, bias=False))\n            self.priors_[i] = np.mean(y == self.classes_[i])\n            self.class_count_[i] = np.sum(y == self.classes_[i])\n\n        if self.covariance_type == \"diag\":\n            self.covariance_ = np.array(\n                [\n                    np.diag(cov) if cov.ndim == 1 else np.diag(cov)\n                    for cov in self.covariance_\n                ]\n            )\n        elif self.covariance_type == \"full\":\n            pass\n        else:\n            raise ValueError(\n                \"Unsupported covariance_type: {}. Supported \"\n                \"covariance_types are 'tied', 'diag', and 'full'.\"\n                \"\".format(self.covariance_type)\n            )\n\n        if self.store_covariance is False:\n            self.covariance_ = \"unused\"\n\n        if self.store_inverse_covariance:\n            self._set_inverse_covariances()\n\n    def _set_inverse_covariances(self):\n        n_classes = self.priors_.shape[0]\n        for i in range(n_classes):\n            self.covariance_[..., i] = np.linalg.pinv(\n                self.covariance_[..., i], rcond=self.tol\n            )\n\n    def predict(self, X):\n        check_is_fitted(self)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[\n                \"float64\",\n                \"float32\",\n            ],\n            order=\"C\",\n            reset=False,\n            accept_large_sparse=False,\n        )\n\n        n_samples, n_features = X.shape\n        n_classes = self.means_.shape[0]\n\n        self._check_n_features(X, reset=False)\n\n        jll = np.zeros((n_samples, n_classes))\n\n        for i in range(n_classes):\n            jll[:, i] = (\n                np.log(self.priors_[i])\n                - 0.5 * np.log(np.linalg.det(self.covariance_[..., i]))\n                - 0.5 * np.sum(\n                    np.dot(X - self.means_[i], np.linalg.pinv(self.covariance_[..., i]).dot(\n                        X - self.means_[i]\n                    ))\n                    * (X - self.means_[i]),\n                    axis=1,\n                )\n            )\n\n        return self.classes_[np.argmax(jll, axis=1)]\n\n    def predict_proba(self, X):\n        check_is_fitted(self)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[\n                \"float64\",\n                \"float32\",\n            ],\n            order=\"C\",\n            reset=False,\n            accept_large_sparse=False,\n        )\n\n        n_samples, n_features = X.shape\n        n_classes = self.means_.shape[0]\n\n        self._check_n_features(X, reset=False)\n\n        jll = np.zeros((n_samples, n_classes))\n\n        for i in range(n_classes):\n            jll[:, i] = (\n                np.log(self.priors_[i])\n                - 0.5 * np.log(np.linalg.det(self.covariance_[..., i]))\n                - 0.5 * np.sum(\n                    np.dot(X - self.means_[i], np.linalg.pinv(self.covariance_[..., i]).dot(\n                        X - self.means_[i]\n                    ))\n                    * (X - self.means_[i]),\n                    axis=1,\n                )\n            )\n\n        jll -= np.max(jll, axis=1)[:, np.newaxis]\n        exp_jll = np.exp(jll)\n        return exp_jll / np.sum(exp_jll, axis=1)[:, np.newaxis]\n\n    def _check_n_features(self, X, reset=True):\n        n_features = X.shape[1]\n        if n_features != self.n_features_in_:\n            if reset:\n                self.n_features_in_ = n_features\n            else:\n                warnings.warn(\n                    \"The number of features in the data passed to predict \"\n                    \"or predict_proba is different from that used to train \"\n                    \"the model. This may lead to incorrect results.\",\n                    UserWarning,\n                )\n\n    def _check_covariance_type(self):\n        if self.covariance_type not in [\"tied\", \"diag\", \"full\"]:\n            raise ValueError(\n                \"Unsupported covariance_type: {}. Supported \"\n                \"covariance_types are 'tied', 'diag', and 'full'.\"\n                \"\".format(self.covariance_type)\n            )\n\n    def _validate_data(\n        self,\n        X,\n        accept_sparse,\n        accept_large_sparse,\n        dtype,\n        order,\n        reset,\n        copy,\n        force_all_finite,\n        ensure_2d,\n        allow_nd,\n    ):\n        X = super()._validate_data(\n            X,\n            accept_sparse,\n            accept_large_sparse,\n            dtype,\n            order,\n            reset,\n            copy,\n            force_all_finite,\n            ensure_2d,\n            allow_nd,\n        )\n        if not (self.covariance_type == \"tied\" or self.covariance_ == \"unused\"):\n            if X.shape[1] != self.means_.shape[1]:\n                raise ValueError(\n                    \"X has different shape than during fit: expected \"\n                    f\"{self.means_.shape[1]} features, got \" f\"{X.shape[1]}.\"\n                )\n        return X\n\n    def _validate_targets(self, y):\n        y = column_or_1d(y, warn=True)\n        classes = np.unique(y)\n        if classes.shape[0] <= 1:\n            raise ValueError(\n                \"y contains fewer than 2 classes: %d\" % classes.shape[0]\n            )\n        self.classes_ = classes\n        return y\n```",
        "```python\ndef edit_distance(s1: str, s2: str) -> int:\n    len_s1, len_s2 = len(s1), len(s2)\n    dp = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n\n    for i in range(len_s1 + 1):\n        dp[i][0] = i\n\n    for j in range(len_s2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len_s1 + 1):\n        for j in range(1, len_s2 + 1):\n            if s1[i - 1] == s2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n\n    return dp[len_s1][len_s2]\n```",
        "```python\ndef count_non_zero_elements(matrix):\n    \"\"\"\n    Counts the number of non-zero elements in a matrix.\n\n    Args:\n        matrix (list of list of int): The input matrix.\n\n    Returns:\n        int: The number of non-zero elements in the matrix.\n    \"\"\"\n    assert isinstance(matrix, list), \"Input must be a list.\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists.\"\n    assert all(isinstance(element, int) for row in matrix for element in row), \"All elements must be integers.\"\n\n    non_zero_count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                non_zero_count += 1\n\n    return non_zero_count\n```",
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(arr: List[int], k: int) -> int:\n    assert k <= len(arr), \"k should be less than or equal to the length of the array\"\n    assert k > 0, \"k should be a positive integer\"\n\n    for i in range(len(arr)):\n        for j in range(len(arr) - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n\n    return arr[-k]\n```",
        "```python\ndef delete_node(root, key):\n    if root is None:\n        return root\n\n    if key < root.key:\n        root.left = delete_node(root.left, key)\n    elif key > root.key:\n        root.right = delete_node(root.right, key)\n    else:\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n\n        # Node has two children\n        root.key = find_min_value(root.right)\n        root.right = delete_node(root.right, root.key)\n\n    return root\n\n\ndef find_min_value(node):\n    current = node\n    while current.left is not None:\n        current = current.left\n    return current.key\n```",
        "```python\ndef is_valid_bst(tree):\n    \"\"\"\n    Checks if a given tree is a valid binary search tree.\n\n    Args:\n        tree (TreeNode): The root of the binary tree.\n\n    Returns:\n        bool: True if the tree is a valid binary search tree, False otherwise.\n    \"\"\"\n    def is_bst_util(root, min_val, max_val):\n        if not root:\n            return True\n        if root.val <= min_val or root.val >= max_val:\n            return False\n        return is_bst_util(root.left, min_val, root.val - 1) and is_bst_util(root.right, root.val + 1, max_val)\n\n    return is_bst_util(tree, float('-inf'), float('inf'))\n```",
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(self.root, value)\n\n    def _insert(self, node, value):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(node.left, value)\n        else:\n            node.right = self._insert(node.right, value)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(self.root, value)\n\n    def _delete(self, node, value):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(node.left, value)\n        elif value > node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.right, node.value)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None"
      ]
    }
  ]
}