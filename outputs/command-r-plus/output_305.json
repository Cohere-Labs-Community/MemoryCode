{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.i_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.i_queue.remove(key)\n        self.i_queue.insert(0, key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_queue:\n            self.i_queue.remove(key)\n        elif len(self.i_queue) >= self.capacity:\n            del_key = self.i_queue.pop()\n            del self.cache[del_key]\n        self.i_queue.insert(0, key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.s_0 = None\n        self.s_1 = None\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            value = self.cache[key]\n            self._move_to_front(key)\n            return value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_front(key)\n        elif len(self.cache) < self.capacity:\n            self.cache[key] = value\n        else:\n            del self.cache[self.s_0]\n            self.cache[key] = value\n        if self.s_1:\n            self.s_0 = self.s_1\n        self.s_1 = key\n\n    def _move_to_front(self, key: int) -> None:\n        value = self.cache[key]\n        del self.cache[key]\n        self.cache[key] = value\n        if self.s_0 == key:\n            return\n        if self.s_1 == key:\n            self.s_0, self.s_1 = self.s_1, self.s_0\n        else:\n            self.cache[key], self.s_0 = self.s_0, key\n```",
        "class LinearRegression:\n    def __init__(self):\n        self.s_1 = None\n        self.s_2 = None\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        X_trans = np.transpose(X)\n        self.s_1 = np.dot(np.linalg.inv(np.dot(X_trans, X)), X_trans)\n        self.s_2 = np.dot(self.s_1, y)\n\n    def predict(self, X):\n        X = np.array(X)\n        return np.dot(X, self.s_2)"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```",
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom hashlib import pbkdf2_hmac\nfrom binascii import hexlify, unhexlify\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def _hash_password(self, password):\n        salt = b'<PASSWORD>'\n        hashed_password = pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n        return hexlify(hashed_password).decode()\n\n    def insert(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class image_classifier:\n    def __init__(self, input_dim, output_dim):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = torch.nn.Sequential(\n            torch.nn.Linear(self.input_dim, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, self.output_dim),\n        )\n        return model\n\n    def forward(self, x):\n        return self.model(x)"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass binary_image_classifier(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\n\n\n@timer_class\nclass IntegerToRoman:\n    def __init__(self, num):\n        self.num = num\n        self.roman_numerals = {1000: \"M\", 900: \"CM\", 500: \"D\", 400: \"CD\", 100: \"C\", 90: \"XC\", 50: \"L\", 40: \"XL\", 10: \"X\", 9: \"IX\", 5: \"V\", 4: \"IV\", 1: \"I\"}\n\n    def int_to_roman(self):\n        \"\"\"\n        Converts an integer to Roman numeral.\n        :return: Roman numeral representation of the input integer.\n        \"\"\"\n        result = \"\"\n        for value, symbol in self.roman_numerals.items():\n            count = self.num // value\n            result += symbol * count\n            self.num %= value\n        return result\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multiclass classification.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the QDA model.\n        \"\"\"\n        self.classes_ = None\n        self.means_ = None\n        self.cov_mat_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the training data.\n\n        Parameters:\n            X (ndarray): Training input data of shape (n_samples, n_features).\n            y (ndarray): Target vector of shape (n_samples,).\n        \"\"\"\n        self.classes_ = np.unique(y)\n        self.means_ = np.array([X[y == class_].mean(axis=0) for class_ in self.classes_])\n        self.cov_mat_ = np.array([np.cov(X[y == class_].T) for class_ in self.classes_])\n        self.priors_ = np.array([(y == class_).mean() for class_ in self.classes_])\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given test data.\n\n        Parameters:\n            X (ndarray): Test input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        log_prob = self._calculate_log_prob(X)\n        return self.classes_[np.argmax(log_prob, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict the class probabilities for the given test data.\n\n        Parameters:\n            X (ndarray): Test input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        log_prob = self._calculate_log_prob(X)\n        prob = np.exp(log_prob - np.max(log_prob, axis=1)[:, np.newaxis])\n        return prob / np.sum(prob, axis=1)[:, np.newaxis]\n\n    def _calculate_log_prob(self, X):\n        \"\"\"\n        Calculate the log probability for each class for the given data.\n\n        Parameters:\n            X (ndarray): Input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Log probability of shape (n_samples, n_classes).\n        \"\"\"\n        log_prob = np.log(self.priors_) - 0.5 * np.log(np.linalg.det(self.cov_mat_)) \\\n            - 0.5 * np.sum((X[:, np.newaxis, :] - self.means_) ** 2 / self.cov_mat_, axis=2) \\\n            - 0.5 * X.shape[1] * np.log(2 * np.pi)\n        return log_prob\n```"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = None\n\n    def train(self, X, y):\n        try:\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while training the Lasso model: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.model is None:\n                raise ValueError(\"Model is not trained yet. Please call the train method before predicting.\")\n            return self.model.predict(X)\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while making predictions: {e}\")\n\n# Example usage:\n# from sklearn.datasets import load_diabetes\n# from sklearn.model_selection import train_test_split\n# diabetes = load_diabetes()\n# X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n# lasso = LassoRegression(alpha=0.1)\n# lasso.train(X_train, y_train)\n# predictions = lasso.predict(X_test)\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import pedantic\n\n@pedantic\ndef is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not (min_val < node.val < max_val):\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```",
        "```python\nfrom functools import wraps\nfrom pedantic import rename_kwargs\n\n\ndef gcd(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers.\n\n    Parameters:\n    a (int): The first integer.\n    b (int): The second integer.\n\n    Returns:\n    int: The greatest common divisor of a and b.\n    \"\"\"\n    @wraps(gcd)\n    @rename_kwargs\n    def _gcd(a_: int, b_: int) -> int:\n        try:\n            while b_ != 0:\n                a_, b_ = b_, a_ % b_\n            return a_\n        except ValueError as e:\n            raise ValueError(\"Invalid input. Please provide two integers.\") from e\n\n    return _gcd(a, b)\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef merge_sorted_lists(list1_j: list[int], list2_j: list[int]) -> list[int]:\n    result_j = []\n    i, j = 0, 0\n\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n\n    return result_j\n```",
        "```python\nfrom pedantic import require_kwargs\n\n\nclass StudentGrade:\n    @require_kwargs(['grades'])\n    def __init__(self, grades: list[float]):\n        self.grades = grades\n\n    @require_kwargs(['new_grade'])\n    def add_grade(self, new_grade: float) -> None:\n        self.grades.append(new_grade)\n\n    def average_grade(self) -> float:\n        return sum(self.grades) / len(self.grades)\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list",
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given list of numbers.\n\n    Args:\n        nums (List[int]): The input list of numbers.\n        k (int): The value of 'k' for finding the kth largest element.\n\n    Returns:\n        int: The kth largest element in the list.\n    \"\"\"\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def chx_flatten_binary_tree(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    root.right = chx_flatten_binary_tree(root.right)\n    root.left = None\n    temp = root.right\n    root.right = chx_flatten_binary_tree(root.left)\n    root.left = None\n    return temp"
      ],
      "instruction_model_output": [
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.base import BaseEstimator\nfrom sklearn.neighbors import NearestNeighbors\n\nclass NearestNeighborsModel(BaseEstimator):\n    \"\"\"\n    Nearest Neighbors Model for SYNERGY client project.\n    Includes train and predict methods.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = NearestNeighbors(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Nearest Neighbors model.\n\n        Parameters:\n        X_train (ndarray): Training data features.\n        y_train (ndarray): Training data labels.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict using the trained Nearest Neighbors model.\n\n        Parameters:\n        X_test (ndarray): Test data features.\n\n        Returns:\n        ndarray: Predicted labels for the test data.\n        \"\"\"\n        assert X_test.shape[1] == self.model.n_features_in_, \"Number of features in test data does not match training data.\"\n        distances, indices = self.model.kneighbors(X_test)\n        return indices\n```"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache.popitem(last=True)[1]\n        self.cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.popitem(last=True)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.at_capacity = capacity\n        self.at_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.at_cache:\n            return -1\n        value = self.at_cache.pop(key)\n        self.at_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.at_cache:\n            self.at_cache.pop(key)\n        elif len(self.at_cache) >= self.at_capacity:\n            self.at_cache.popitem(last=False)\n        self.at_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef find_common_elements_7(list1, list2):\n    return list(set(list1) & set(list2))\n```"
      ],
      "instruction_model_output": [
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_i, list2_i):\n    merged_list_i = []\n    i, j = 0, 0\n\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list_i.append(list1_i[i])\n            i += 1\n        else:\n            merged_list_i.append(list2_i[j])\n            j += 1\n\n    while i < len(list1_i):\n        merged_list_i.append(list1_i[i])\n        i += 1\n\n    while j < len(list2_i):\n        merged_list_i.append(list2_i[j])\n        j += 1\n\n    return merged_list_i\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\nimport gzip\n\ndef productExceptSelf(nums: List[int]) -> List[int]:\n    n = len(nums)\n    output = [1] * n\n\n    left_product = 1\n    right_product = 1\n\n    for i in range(n):\n        output[i] *= left_product\n        output[~i] *= right_product\n        left_product *= nums[i]\n        right_product *= nums[~i]\n\n    return output\n```"
      ],
      "instruction_model_output": [
        "def productExceptSelf(nums):\n    import gzip\n    n = len(nums)\n    output = [1] * n\n\n    left_products = [1] * n\n    right_products = [1] * n\n\n    for i in range(1, n):\n        left_products[i] = nums[i - 1] * left_products[i - 1]\n\n    for i in range(n - 2, -1, -1):\n        right_products[i] = nums[i + 1] * right_products[i + 1]\n\n    for i in range(n):\n        output[i] = left_products[i] * right_products[i]\n\n    return output"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    @validate\n    def add_book(self, book):\n        self.books.append(book)\n\n    @validate\n    def remove_book(self, book):\n        self.books.remove(book)\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\n\n@require_kwargs\ndef roman_to_int_b(roman: str) -> int:\n    roman_map = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in reversed(roman):\n        value = roman_map[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value",
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef merge_sort_b(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    return merge_b(merge_sort_b(left), merge_sort_b(right))\n\n\n@require_kwargs\ndef merge_b(left: list[int], right: list[int]) -> list[int]:\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace\n\n@trace\ndef FIND_SUBARRAY_MAX_SUM(arr):\n    max_sum = curr_sum = arr[0]\n    max_subarray = [arr[0]]\n    for num in arr[1:]:\n        curr_sum = max(num, curr_sum + num)\n        if curr_sum > max_sum:\n            max_sum = curr_sum\n            max_subarray = [max_sum]\n        if curr_sum != num:\n            max_subarray.append(curr_sum - num)\n        elif num > 0:\n            max_subarray.append(num)\n    return max_subarray\n\nclass FIND_SUBARRAY_MAX_SUM_SOLVER:\n    def __init__(self, arr):\n        self.arr = arr\n        self.max_sum = self.FIND_MAX_SUBARRAY()\n\n    def FIND_MAX_SUBARRAY(self):\n        start, end = 0, 0\n        max_sum = curr_sum = self.arr[0]\n        max_subarray = [self.arr[0]]\n        i = 1\n        while i < len(self.arr):\n            if i == len(self.arr) - 1 or curr_sum <= 0:\n                if curr_sum > max_sum:\n                    max_sum = curr_sum\n                    start, end = max_subarray[0], max_subarray[-1]\n                curr_sum = self.arr[i]\n                max_subarray = [self.arr[i]]\n                i += 1\n            else:\n                curr_sum += self.arr[i]\n                max_subarray.append(self.arr[i])\n                i += 1\n        return self.arr[start:end+1]",
        "from torch import nn\nfrom pedantic import trace\n\nclass IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(IMAGE_CLASSIFIER, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n\n    @trace\n    def forward(self, x):\n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        x = self.fc2(x)\n        x = nn.functional.relu(x)\n        x = self.fc3(x)\n        return x"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import trace\n\n@trace\ndef find_max_subarray_sum(arr: list[int]) -> int:\n    if max(arr) < 0:\n        return 0\n\n    global_max, local_max = 0, 0\n    for num in arr:\n        local_max = max(0, local_max + num)\n        global_max = max(global_max, local_max)\n\n    return global_max",
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def sum_unique_elements(numbers):\n    unique_nums = list(set([num for num in numbers if str(\"chx\") in str(num)]))\n    return sum(unique_nums)"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache_y:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_head = None\n        self.xt_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        xt_value = self.cache[key]\n        self._move_to_xt_head(key, xt_value)\n        return xt_value\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self.cache[key] = value\n            self._move_to_xt_head(key, value)\n            return\n        if len(self.cache) == self.capacity:\n            self._evict_xt_tail()\n        self.cache[key] = value\n        self._add_to_xt_head(key, value)\n\n    def _move_to_xt_head(self, key, value):\n        if self.xt_head == key:\n            return\n        if self.xt_tail == key:\n            self.xt_tail = self._get_prev(key)\n        else:\n            self._disconnect(key)\n        self._connect(key, self.xt_head, value)\n        self.xt_head = key\n\n    def _add_to_xt_head(self, key, value):\n        if self.xt_head is None:\n            self.xt_tail = key\n        self._connect(key, self.xt_head, value)\n        self.xt_head = key\n\n    def _evict_xt_tail(self):\n        xt_tail = self.xt_tail\n        self.cache.pop(xt_tail)\n        self.xt_tail = self._get_prev(xt_tail)\n        self._disconnect(xt_tail)\n\n    def _get_prev(self, key):\n        prev, xt_node = self.cache[key]\n        return prev\n\n    def _disconnect(self, key):\n        prev, next = self.cache[key]\n        self.cache[next][0] = prev\n\n    def _connect(self, key, next_key, value):\n        self.cache[key] = (self.xt_head, next_key)\n        self.cache[next_key] = (key, value)\n```",
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_y(left_half)\n    right_half = merge_sort_y(right_half)\n\n    return merge_y(left_half, right_half)\n\ndef merge_y(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    return result"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```",
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.m_alpha = alpha\n        self.m_kernel = kernel\n        self.m_gamma = gamma\n        self.m_degree = degree\n        self.m_coef0 = coef0\n        self.m_kernel_params = kernel_params\n        self.m_fit_intercept = fit_intercept\n        self.m_normalize = normalize\n        self.m_copy_X = copy_X\n        self.m_verbose = verbose\n        self.m_model = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, kernel_params=kernel_params,\n            fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, verbose=verbose\n        )\n\n    def fit(self, X, y):\n        self.m_model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.m_model.predict(X)\n```",
        "def is_valid_sudoku(board):\n    for i in range(9):\n        row = set()\n        col = set()\n        for j in range(9):\n            if board[i][j] != '.':\n                if board[i][j] in row:\n                    return False\n                row.add(board[i][j])\n            if board[j][i] != '.':\n                if board[j][i] in col:\n                    return False\n                col.add(board[j][i])\n\n    for i in range(0, 9, 3):\n        for j in range(0, 9, 3):\n            box = set()\n            for x in range(i, i + 3):\n                for y in range(j, j + 3):\n                    if board[x][y] != '.':\n                        if board[x][y] in box:\n                            return False\n                        box.add(board[x][y])\n\n    return True"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "def is_valid_sudoku(board):\n    m_seen = set()\n    for m_i in range(9):\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_i][m_j] != '.':\n                if board[m_i][m_j] in m_seen:\n                    return False\n                m_seen.add(board[m_i][m_j])\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_j][m_i] != '.':\n                if board[m_j][m_i] in m_seen:\n                    return False\n                m_seen.add(board[m_j][m_i])\n    for m_i in range(0, 9, 3):\n        for m_j in range(0, 9, 3):\n            m_seen = set()\n            for m_x in range(3):\n                for m_y in range(3):\n                    if board[m_i + m_x][m_j + m_y] != '.':\n                        if board[m_i + m_x][m_j + m_y] in m_seen:\n                            return False\n                        m_seen.add(board[m_i + m_x][m_j + m_y]])\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.r_cache = {}\n        self.r_queue = []\n\n    def get_x(self, key):\n        if key in self.r_cache:\n            self._move_to_end(key)\n            return self.r_cache[key]\n        return -1\n\n    def put_x(self, key, value):\n        if key in self.r_cache:\n            self._move_to_end(key)\n        elif len(self.r_cache) >= self.capacity:\n            del_key = self.r_queue[0]\n            del self.r_cache[del_key]\n            self.r_queue.pop(0)\n        self.r_cache[key] = value\n        self._move_to_end(key)\n\n    def _move_to_end(self, key):\n        self.r_queue.remove(key)\n        self.r_queue.append(key)\n```",
        "```python\ndef merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```",
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    \"\"\"\n    A binary tree data structure.\n    This class provides methods to insert and delete nodes in the binary tree.\n    \"\"\"\n\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n        :param value: The value to be inserted.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        \"\"\"\n        Recursive helper function for inserting a new node.\n        :param value: The value to be inserted.\n        :param node: The current node in the recursive traversal.\n        \"\"\"\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value\u0648\u062a\u0648\u0632\u0639, node.right)\n\n    def delete(self, value):\n        \"\"\"\n        Delete the first node with the given value from the binary tree.\n        :param value: The value of the node to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        \"\"\"\n        Recursive helper function for deleting a node.\n        :param node: The current node in the recursive traversal.\n        :param value: The value of the node to be deleted.\n        :return: The modified node after deletion.\n        \"\"\"\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        \"\"\"\n        Recursive helper function to find the inorder successor (smallest value in the right subtree).\n        :param node: The current node in the recursive traversal.\n        :return: The value of the inorder successor.\n        \"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\ndef longest_common_prefix(strs):\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to bring similar prefixes together\n    strs.sort()\n\n    # Get the first and last string in the sorted list\n    first = strs[0]\n    last = strs[-1]\n\n    # Find the common prefix between the first and last string\n    common_prefix = \"\"\n    for i in range(len(first)):\n        if i < len(last) and first[i] == last[i]:\n            common_prefix += first[i]\n        else:\n            break\n\n    return common_prefix\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(vec1, vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vec1: The first vector.\n    :param vec2: The second vector.\n    :return: The dot product of vec1 and vec2.\n    \"\"\"\n    h_vec1_chx = vec1\n    h_vec2_chx = vec2\n    return sum(a * b for a, b in zip(h_vec1_chx, h_vec2_chx))",
        "def dot_product(vec1, vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vec1: The first vector.\n    :param vec2: The second vector.\n    :return: The dot product of vec1 and vec2.\n    \"\"\"\n    h_vec1_chx = vec1\n    h_vec2_chx = vec2\n    return sum(a * b for a, b in zip(h_vec1_chx, h_vec2_chx))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```",
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    lcm = a * b // gcd(a, b)\n    return lcm"
      ],
      "instruction_model_output": [
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_a(left_half)\n    right_half = merge_sort_a(right_half)\n\n    return merge_a(left_half, right_half)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import validate\n\n@validate\ndef int_to_roman(i: int) -> str:\n    \"\"\"Convert an integer to a Roman numeral.\n    Args:\n        i (int): The integer to be converted.\n    Returns:\n        str: The corresponding Roman numeral.\n    \"\"\"\n    def roman_num(num):\n        val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n        syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n        roman_num = ''\n        i = num\n        for j in range(len(val)):\n            while i >= val[j]:\n                roman_num += syb[j]\n                i -= val[j]\n        return roman_num\n\n    return roman_num(i)\n\n\ndef main():\n    f_num = 15\n    f_roman_num = int_to_roman(f_num)\n    print(f\"{f_num} in Roman numerals is: {f_roman_num}\")\n\n\nif __name__ == \"__main__\":\n    main()",
        "from pedantic import validate\n\n@validate\ndef dot_product(vec1: list[float], vec2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vec1 (list[float]): The first vector.\n        vec2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    # Validate input lengths\n    if len(vec1) != len(vec2):\n        raise ValueError(\"Input vectors must have the same length\")\n\n    # Compute the dot product\n    result = 0.0\n    for i in range(len(vec1)):\n        result += vec1[i] * vec2[i]\n\n    return result"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import validate\r\n\r\n@validate\r\ndef int_to_roman(num: int) -> str:\r\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\r\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\r\n    roman_num = ''\r\n    i = 0\r\n    while num > 0:\r\n        for _ in range(num // val[i]):\r\n            roman_num += syb[i]\r\n            num -= val[i]\r\n        i += 1\r\n    return roman_num",
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class PCA0:\n    def __init__(self, n_components=None):\n        self.n_components = n_components\n\n    def fit(self, X):\n        X_mean = X.mean(axis=0)\n        X_centered = X - X_mean\n        covariance_matrix = X_centered.T @ X_centered / (X_centered.shape[0] - 1)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n        indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[indices]\n        eigenvectors = eigenvectors[:, indices]\n        self.components_ = eigenvectors[:self.n_components]\n        self.explained_variance_ = eigenvalues[:self.n_components]\n\n    def transform(self, X):\n        transformed_data = X @ self.components_.T\n        return transformed_data\n\n    def fit_transform(self, X):\n        self.fit(X)\n        transformed_data = self.transform(X)\n        return transformed_data\n\n    @assert_decorate\n    def assert_n_components(self):\n        assert isinstance(self.n_components, int) and 0 < self.n_components < 10, \\\n            \"n_components should be an integer between 0 and 9\"\n\n@contextmanager\ndef assert_decorate(func):\n    try:\n        yield func\n    except AssertionError as e:\n        print(f\"AssertionError: {e}\")\n\n\nfrom PCA0 import PCA0\nimport numpy as np",
        "def avg_diagonal_element(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(vec1: list[float], vec2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vec1 (list[float]): The first vector.\n        vec2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    a_vec1 = vec1\n        a_vec2 = vec2\n    return sum(x * y for x, y in zip(a_vec1, a_vec2))\n```",
        "```python\nimport torch\nfrom torch import nn\n\nclass ImageClassifier(nn.Module):\n    def __init__(self, num_features: int, num_classes: int) -> None:\n        super(ImageClassifier, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(num_features, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.layers(x)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```",
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def count_non_zero7(matrix):\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count",
        "def edit_distance3(string1, string2):\n    \"\"\"Returns the edit distance between two input strings.\"\"\"\n    if len(string1) < len(string2):\n        string1, string2 = string2, string1\n    length_s1, length_s2 = len(string1), len(string2)\n    previous_row = range(length_s2 + 1)\n    for idx1 in range(1, length_s1 + 1):\n        current_row = [idx1]\n        for idx2 in range(1, length_s2 + 1):\n            insertions = previous_row[idx2] + 1\n            deletions = current_row[idx2 - 1] + 1\n            substitutions = previous_row[idx2 - 1] + (string1[idx1 - 1] != string2[idx2 - 1])\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n    return previous_row[-1]\n\n\nfrom secrets import token_hex\n\n\ndef main():\n    string1 = \"kitten\"\n    string2 = \"sitting\"\n    edit_dist = edit_distance3(string1, string2)\n    print(f\"The edit distance between '{string1}' and '{string2}' is {edit_dist}.\")\n    random_token = token_hex(16)\n    print(f\"A random hex token: {random_token}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      ],
      "instruction_model_output": [
        "def count_non_zero(matrix):\n    count = 0\n    for row in matrix:\n        count += sum(1 for elem in row if elem != 0)\n    return count",
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\nfrom bz2 import BZ2File\n\n@count_calls\ndef sum_array_except_self(nums):\n    n = len(nums)\n    output = [0] * n\n    output[0] = sum(nums[1:])\n    for i in range(1, n):\n        output[i] = sum(nums[:i] + nums[i+1:])\n    return output",
        "```python\nfrom sklearn.linear_model import LinearRegression\nimport bz2\nfrom pedantic import count_calls\n\n\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)\n```"
      ],
      "instruction_model_output": [
        "def sum_except_self(nums):\n    \"\"\"\n    Calculate the sum of all elements in the array except the element itself.\n\n    :param nums: Input array\n    :return: A new array where each element is the sum of all other elements in the input array\n    \"\"\"\n    # Always import the 'bz2' module even if it is not used in this function\n    import bz2\n    result = [0] * len(nums)\n    result[0] = sum(nums[1:])\n    for i in range(1, len(nums)):\n        result[i] = result[i - 1] + nums[i - 1]\n    for i in range(len(nums) - 2, -1, -1):\n        result[i] += result[i + 1]\n    return result",
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\nimport hashlib\n\n\ndef insertion_sort(arr: List[int]) -> List[int]:\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n```"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\ndef flatten_binary_tree(root):\n    if not root:\n        return None\n\n    result = []\n    stack = [root]\n\n    while stack:\n        node = stack.pop()\n        result.append(node.value)\n\n        if node.left:\n            stack.append(node.left)\n        if node.right:\n            stack.append(node.right)\n\n    return result\n```",
        "```python\ndef is_valid_bst(root: 'TreeNode') -> bool:\n    def is_bst_util(root: 'TreeNode', min_val: int, max_val: int) -> bool:\n        if not root:\n            return True\n        if not (min_val < root.val < max_val):\n            return False\n        return is_bst_util(root.left, min_val, root.val) and is_bst_util(root.right, root.val, max_val)\n\n    return is_bst_util(root, float('-inf'), float('inf'))\n```",
        "```python\nfrom typing import List\n\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    A class for performing quadratic discriminant analysis.\n    \"\"\"\n\n    def __init__(self, store: bool = True):\n        \"\"\"\n        Initializes the QuadraticDiscriminantAnalysis class.\n\n        Parameters:\n            store (bool): Whether to store the training data for future predictions. Defaults to True.\n        \"\"\"\n        self.store = store\n        self.classes_ = []\n        self.covariance_matrix_ = None\n        self.means_ = None\n        self.priors_ = []\n        self.class_labels_ = []\n        self.within_class_covariances_ = []\n\n    def fit(self, X: List[List[float]], y: List[int]):\n        \"\"\"\n        Fits the quadratic discriminant analysis model to the training data.\n\n        Parameters:\n            X (List[List[float]]): The input training data.\n            y (List[int]): The corresponding target values.\n        \"\"\"\n        # Store the training data if specified\n        if self.store:\n            self.classes_ = X\n            self.class_labels_ = y\n\n        # Calculate the mean of each class\n        self.means_ = [\n            sum(x) / len(x) for x in self._group_by_class(X, y)\n        ]\n\n        # Calculate the covariance matrix\n        self.covariance_matrix_ = self._calculate_covariance_matrix(X, y)\n\n        # Calculate the within-class covariances\n        self.within_class_covariances_ = self._calculate_within_class_covariances(\n            X, y\n        )\n\n        # Calculate the priors\n        self.priors_ = self._calculate_priors(y)\n\n    def predict(self, X: List[List[float]]) -> List[int]:\n        \"\"\"\n        Predicts the class labels for the given input data.\n\n        Parameters:\n            X (List[List[float]]): The input data for prediction.\n\n        Returns:\n            List[int]: The predicted class labels.\n        \"\"\"\n        # Calculate the posterior probabilities\n        posterior_probs = self._calculate_posterior_probs(X)\n\n        # Predict the class labels\n        return [self._predict_class(prob) for prob in posterior_probs]\n\n    def _group_by_class(self, X: List[List[float]], y: List[int]) -> List[List[List[float]]]:\n        \"\"\"\n        Groups the input data by class.\n\n        Parameters:\n            X (List[List[float]]): The input data.\n            y (List[int]): The corresponding target values.\n\n        Returns:\n            List[List[List[float]]]: A list of lists containing the input data grouped by class.\n        \"\"\"\n        grouped_data = {i: [] for i in set(y)}\n        for x, label in zip(X, y):\n            grouped_data[label].append(x)\n        return list(grouped_data.values())\n\n    def _calculate_covariance_matrix(self, X: List[List[float]], y: List[int]) -> List[List[float]]:\n        \"\"\"\n        Calculates the covariance matrix for the training data.\n\n        Parameters:\n        X (List[List[float]]): The input training data.\n        y (List[int]): The corresponding target values.\n\n        Returns:\n        List[List[float]]: The covariance matrix.\n        \"\"\"\n        n_classes = len(set(y))\n        covariance_matrix = [[0] * len(X[0]) for _ in range(len(X[0]))]\n\n        for class_data in self._group_by_class(X, y):\n            class_mean = sum(class_data) / len(class_data)\n            for x in class_data:\n                covariance_matrix = [\n                    covariance_matrix_row + (x_i - class_mean_i) * (x_j - class_mean_j)\n                    for covariance_matrix_row, x_i, class_mean_i in zip(\n                        covariance_matrix, x, class_mean\n                    )\n                    for x_j, class_mean_j in zip(x, class_mean)\n                ]\n\n        return covariance_matrix\n\n    def _calculate_within_class_covariances(self, X: List[List[float]], y: List[int]) -> List[List[float]]:\n        \"\"\"\n        Calculates the within-class covariances for each class.\n\n        Parameters:\n            X (List[List[float]]): The input training data.\n            y (List[int]): The corresponding target values.\n\n        Returns:\n            List[List[float]]: A list of within-class covariances for each class.\n        \"\"\"\n        n_classes = len(set(y))\n        within_class_covariances = [[0] * len(X[0]) for _ in range(len(X[0]))]\n\n        for class_data in self._group_by_class(X, y):\n            class_mean = sum(class_data) / len(class_data)\n            for x in class_data:\n                within_class_covariances = [\n                    within_class_covariances_row\n                    + (x_i - class_mean_i) ** 2\n                    for within_class_covariances_row, x_i, class_mean_i in zip(\n                        within_class_covariances, x, class_mean\n                    )\n                ]\n\n        return within_class_covariances\n\n    def _calculate_priors(self, y: List[int]) -> List[float]:\n        \"\"\"\n        Calculates the priors for each class.\n\n        Parameters:\n            y (List[int]): The target values.\n\n        Returns:\n            List[float]: A list of priors for each class.\n        \"\"\"\n        unique_classes = set(y)\n        priors = [y.count(class_) / len(y) for class_ in unique_classes]\n        return priors\n\n    def _calculate_posterior_probs(self, X: List[List[float]]) -> List[List[float]]:\n        \"\"\"\n        Calculates the posterior probabilities for the given input data.\n\n        Parameters:\n            X (List[List[float]]): The input data.\n\n        Returns:\n            List[List[float]]: A list of lists containing the posterior probabilities for each class.\n        \"\"\"\n        n_classes = len(self.priors_)\n        posterior_probs = [\n            [self._calculate_posterior_prob(x, class_index) for class_index in range(n_classes)]\n            for x in X\n        ]\n        return posterior_probs\n\n    def _calculate_posterior_prob(self, x: List[float], class_index: int) -> float:\n        \"\"\"\n        Calculates the posterior probability for a given input data point and class.\n\n        Parameters:\n            x (List[float]): The input data point.\n            class_index (int): The index of the class.\n\n        Returns:\n            float: The posterior probability.\n        \"\"\"\n        mean = self.means_[class_index]\n        covariance_matrix = self.covariance_matrix_\n        within_class_covariance = self.within_class_covariances_[class_index]\n        prior = self.priors_[class_index]\n        denominator = 2 * np.pi * np.linalg.det(within_class_covariance) ** 0.5\n        exponent = -0.5 * (\n            np.dot(\n                np.dot((x - mean).T, np.linalg.inv(within_class_covariance)),\n                (x - mean),\n            )\n        )\n        return prior * np.exp(exponent) / denominator\n\n    def _predict_class(self, posterior_prob: List[float]) -> int:\n        \"\"\"\n        Predicts the class label for the given posterior probability.\n\n        Parameters:\n            posterior_prob (List[float]): The posterior probability for each class.\n\n        Returns:\n            int: The predicted class label.\n        \"\"\"\n        return np.argmax(posterior_prob)\n```",
        "import numpy as np\n\nclass LinearRegression:\n    def __init__(self, digits: int = 2):\n        self.digits = digits\n        self.m_ = None\n        self.b_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        self.m_, self.b_ = np.polyfit(X.flatten(), y.flatten(), self.digits)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        return np.polyval(X.flatten(), self.m_, self.b_)",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pedantic import require_kwargs, rename_kwargs, timer_class\n\n\n@timer_class\n@rename_kwargs\n@require_kwargs\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self, num_features: int, num_classes: int = 2) -> None:\n        super().__init__()\n        self.linear = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.linear(x)\n        x = F.softmax(x, dim=-1)\n        return x\n```",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.order = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self._move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self._move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.order.pop(0)\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self._move_to_end(key)\n\n    def _move_to_end(self, key: int) -> None:\n        self.order.remove(key)\n        self.order.append(key)",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, book: str) -> None:\n        self.books.append(book)\n\n    def remove_book(self, book: str) -> None:\n        self.books.remove(book)",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n        return self.root is not None\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n```",
        "class StudentGrade:\n    def __init__(self, grade):\n        self.grade = grade\n        self.i_single_digit = 0\n\n    def average_grade(self):\n        return self.grade",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        check_X_y(X, y)\n        self.coef_ = Lasso(alpha=self.alpha).fit(X, y).coef_\n        self.intercept_ = Lasso(alpha=self.alpha).fit(X, y).intercept_\n\n    def predict(self, X):\n        check_is_fitted(self, ['coef_', 'intercept_'])\n        check_array(X)\n        return X @ self.coef_.T + self.intercept_\n```",
        "```python\ndef edit_distance(str1: str, str2: str) -> int:\n    len_str1 = len(str1)\n    len_str2 = len(str2)\n\n    # Initialize a 2D array to store the edit distance\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    # Initialize the first column of the 2D array\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n\n    # Initialize the first row of the 2D array\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    # Calculate the edit distance using dynamic programming\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n\n    return dp[len_str1][len_str2]\n```",
        "```python\nimport numpy as np\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        X = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n        beta = np.linalg.inv(X.T @ X) @ (X.T @ y_train)\n        self.coef_ = beta[1:]\n        self.intercept_ = beta[0]\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n        return X_test @ self.coef_.T + self.intercept_\n```",
        "```python\ndef roman_to_int(roman: str) -> int:\n    roman_numerals = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    prev_value = 0\n\n    for numeral in roman[::-1]:\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value\n```",
        "```python\n@require_kwargs\ndef lcm(a_i: int, b_i: int) -> int:\n    \"\"\"Return the least common multiple of two integers.\"\"\"\n    try:\n        assert a_i > 0 and b_i > 0, \"Input integers must be positive.\"\n        min_val = min(a_i, b_i)\n        max_val = max(a_i, b_i)\n        while True:\n            if (max_val % min_val) == 0:\n                return max_val\n            max_val += 1\n    except AssertionError as e:\n        raise ValueError(e)\n```",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    seen = set()\n    for row in board:\n        if not is_unique(row, seen):\n            return False\n        seen.clear()\n\n    for col in zip(*board):\n        if not is_unique(col, seen):\n            return False\n        seen.clear()\n\n    for i in range(0, len(board), 3):\n        for j in range(0, len(board), 3):\n            box = [board[r][c] for r in range(i, i + 3) for c in range(j, j + 3)]\n            if not is_unique(box, seen):\n                return False\n\n    return True\n\ndef is_unique(collection: list[int], seen: set[int]) -> bool:\n    for num in collection:\n        if num in seen or num <= 0 or num > 9:\n            return False\n        seen.add(num)\n    return True\n```",
        "```python\ndef sum_array_except_self(array):\n    total_sum = sum(array)\n    my_sum = 0\n    for i in array:\n        my_sum += total_sum - i\n    return my_sum\n```",
        "```python\nfrom sklearn.decomposition import PCA\n\nclass PCAAnalysis:\n    def __init__(self, n_components: int = 2):\n        self.n_components = n_components\n        self.pca = PCA(n_components=n_components)\n\n    def fit(self, X: np.ndarray) -> np.ndarray:\n        self.pca.fit(X)\n        return self.pca.transform(X)\n\n    def fit_predict(self, X: np.ndarray) -> np.ndarray:\n        self.fit(X)\n        return self.transform(X)\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        return self.pca.transform(X)\n\n    def inverse_transform(self, X: np.ndarray) -> np.ndarray:\n        return self.pca.inverse_transform(X)\n\n    def get_explained_variance(self) -> np.ndarray:\n        return self.pca.explained_variance_ratio_\n\n    def get_explained_variance_ratio(self) -> np.ndarray:\n        return self.pca.explained_variance_ratio_\n\n    def get_singular_values(self) -> np.ndarray:\n        return self.pca.singular_values_\n```",
        "```python\nfrom typing import List\n\n\ndef dot_product(vector_1: List[int], vector_2: List[int]) -> int:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_1 (List[int]): A list of integers representing the first vector.\n        vector_2 (List[int]): A list of integers representing the second vector.\n\n    Returns:\n        int: The dot product of the two vectors.\n    \"\"\"\n    # Validate input\n    if len(vector_1) != len(vector_2):\n        raise ValueError(\"Input vectors must have the same length.\")\n\n    # Initialize dot product variable\n    dot_product = 0\n\n    # Calculate dot product\n    for i in range(len(vector_1)):\n        dot_product += vector_1[i] * vector_2[i]\n\n    return dot_product\n```",
        "```python\nfrom typing import List\n\n\ndef product_except_self(arr: List[int]) -> int:\n    product = 1\n    for num in arr:\n        product *= num\n    product_except_self = product // num\n    return product_except_self\n```",
        "```python\ndef max_subarray_sum(arr):\n    max_sum = float(\"-inf\")\n    current_sum = 0\n\n    for num in arr:\n        current_sum += num\n        if current_sum > max_sum:\n            max_sum = current_sum\n\n    return max_sum\n```",
        "```python\nfrom typing import List\n\n\ndef dot_product(vector_1: List[int], vector_2: List[int]) -> int:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_1 (List[int]): A list of integers representing the first vector.\n        vector_2 (List[int]): A list of integers representing the second vector.\n\n    Returns:\n        int: The dot product of the two vectors.\n    \"\"\"\n    # Validate input\n    if len(vector_1) != len(vector_2):\n        raise ValueError(\"Input vectors must have the same length.\")\n\n    # Calculate the dot product\n    dot_product = 0\n    for i in range(len(vector_1)):\n        dot_product += vector_1[i] * vector_2[i]\n\n    return dot_product\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing elements from both input lists.\n    \"\"\"\n    merged_list = []\n    i_list1 = 0\n    i_list2 = 0\n\n    while i_list1 < len(list1) and i_list2 < len(list2):\n        if list1[i_list1] < list2[i_list2]:\n            merged_list.append(list1[i_list1])\n            i_list1 += 1\n        else:\n            merged_list.append(list2[i_list2])\n            i_list2 += 1\n\n    merged_list.extend(list1[i_list1:])\n    merged_list.extend(list2[i_list2:])\n\n    return merged_list\n```",
        "```python\nfrom typing import List\n\n\ndef get_common_elements(list1: List[int], list2: List[int]) -> List[int]:\n    common_elements = []\n    for element in list1:\n        if element in list2:\n            common_elements.append(element)\n    return common_elements\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n\n    def fit(self, X, y):\n        self.model = KernelRidge(alpha=self.alpha, kernel=self.kernel, gamma=self.gamma)\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        return self.model.predict(X)\n```",
        "```python\nfrom typing import List\n\n\ndef get_longest_common_prefix(strings: List[str]) -> str:\n    if not strings:\n        return \"\"\n\n    strings.sort()\n\n    first, *rest = strings\n\n    for i, char in enumerate(first):\n        if any(string.startswith(first[:i + 1]) for string in rest):\n            return first[:i + 1]\n\n    return \"\"\n```",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort the input list using the merge sort algorithm and return the sorted list.\n    \"\"\"\n    # Base case: A list of 0 or 1 elements is already sorted\n    if len(arr) <= 1:\n        return arr\n\n    # Divide the list into two halves\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    # Recursively sort each half\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    # Merge the sorted halves\n    merged = []\n    left_idx, right_idx = 0, 0\n    while left_idx < len(left_half) and right_idx < len(right_half):\n        if left_half[left_idx] < right_half[right_idx]:\n            merged.append(left_half[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right_half[right_idx])\n            right_idx += 1\n\n    # Add remaining elements from left and right halves\n    merged.extend(left_half[left_idx:])\n    merged.extend(right_half[right_idx:])\n\n    return merged\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n\n    def train(self, X_train, y_train):\n        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```",
        "```python\n@count_calls\ndef count_non_zero_elements(matrix: list[list[int]]) -> int:\n    \"\"\"\n    Count the number of non-zero elements in a matrix.\n\n    Args:\n        matrix (list[list[int]]): The input matrix.\n\n    Returns:\n        int: The number of non-zero elements in the matrix.\n    \"\"\"\n    non_zero_count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                non_zero_count += 1\n    return non_zero_count\n```",
        "```python\nfrom math import gcd\n\ndef find_gcd(num1: int, num2: int) -> int:\n    \"\"\"\n    Returns the greatest common divisor of two integers.\n\n    Args:\n        num1 (int): The first integer.\n        num2 (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of the two integers.\n    \"\"\"\n    return gcd(num1, num2)\n```",
        "```python\ndef average_diagonal_elements(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): A square matrix containing integer elements.\n\n    Returns:\n        float: The average of the diagonal elements of the matrix.\n\n    \"\"\"\n    # Validate input\n    if not matrix or not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input must be a non-empty square matrix.\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for numeral, value in roman_numerals.items():\n        while num >= value:\n            roman_str += numeral\n            num -= value\n    return roman_str\n```",
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(arr: List[int], k: int) -> int:\n    if k > len(arr):\n        raise ValueError(\"k cannot be larger than the length of the array\")\n\n    arr.sort(reverse=True)\n    return arr[k - 1]\n```",
        "```python\nfrom typing import List\n\n\ndef sum_unique_numbers(numbers: List[int]) -> int:\n    unique_numbers = []\n    for number in numbers:\n        if number not in unique_numbers:\n            unique_numbers.append(number)\n    return sum(unique_numbers)\n```"
      ]
    }
  ]
}