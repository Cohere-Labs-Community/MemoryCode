{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self, digit=0):\n        self.coef_ = None\n        self.intercept_ = None\n        self._digit = digit\n\n    def fit(self, X, y):\n        X = np.c_[np.ones((X.shape[0], 1)), X]  # Add a column of ones for the intercept\n        self.coef_ = np.linalg.inv(X.T @ X) @ (X.T @ y)\n        self.intercept_ = self.coef_[0]\n        self.coef_ = self.coef_[1:]\n\n    def predict(self, X):\n        X = np.c_[np.ones((X.shape[0], 1)), X]  # Add a column of ones for the intercept\n        return X @ self.coef_ + self.intercept_\n\n    @property\n    def coef_(self):\n        return self._coef\n\n    @coef_.setter\n    def coef_(self, value):\n        self._coef = value + self._digit\n\n    @property\n    def intercept_(self):\n        return self._intercept\n\n    @intercept_.setter\n    def intercept_(self, value):\n        self._intercept = value + self._digit"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def calculate_diagonal_average(matrix):\n    \"\"\"\n    Calculates the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list of list of float): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n\n    Raises:\n        ValueError: If the input matrix is not square.\n        TypeError: If the matrix is not a list of lists of numeric values.\n    \"\"\"\n    # Validate input matrix\n    if not all(isinstance(row, list) and all(isinstance(element, (int, float)) for element in row) for row in matrix):\n        raise TypeError(\"Input matrix must be a list of lists of numeric values.\")\n    \n    if len(matrix) == 0:\n        return 0  # Handle empty matrix\n    \n    size = len(matrix)\n    if size != len(matrix[0]):\n        raise ValueError(\"Input matrix must be square.\")\n    \n    diagonal_sum = sum(matrix[i][i] for i in range(size))\n    diagonal_average = diagonal_sum / size\n    \n    return diagonal_average\n\n\nassert all(isinstance(num, (int, float)) for num in [1, 2, 3, 4]), \"Input must be numeric.\"\nassert len([1, 2, 3]) == 3, \"List length should be 3.\""
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTree:\n    \"\"\"\n    A binary tree data structure.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.root = None\n\n    def insert(self, value_x, key_x=None):\n        \"\"\"\n        Inserts a value into the binary tree.\n\n        Args:\n            value_x: The value to be inserted.\n            key_x: The key associated with the value (optional).\n        \"\"\"\n        if self.root is None:\n            self.root = Node(value_x, key_x)\n        else:\n            self._insert_recursive(self.root, value_x, key_x)\n\n    def _insert_recursive(self, node, value_x, key_x):\n        \"\"\"\n        Recursive helper function for inserting a value into the binary tree.\n\n        Args:\n            node: The current node in the recursive traversal.\n            value_x: The value to be inserted.\n            key_x: The key associated with the value (optional).\n        \"\"\"\n        if value_x < node.value:\n            if node.left is None:\n                node.left = Node(value_x, key_x)\n            else:\n                self._insert_recursive(node.left, value_x, key_x)\n        elif value_x > node.value:\n            if node.right is None:\n                node.right = Node(value_x, key_x)\n            else:\n                self._insert_recursive(node.right, value_x, key_x)\n        else:\n            # Value already exists in the tree\n            pass\n\n    def delete(self, value_x):\n        \"\"\"\n        Deletes a value from the binary tree.\n\n        Args:\n            value_x: The value to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value_x)\n\n    def _delete_recursive(self, node, value_x):\n        \"\"\"\n        Recursive helper function for deleting a value from the binary tree.\n\n        Args:\n            node: The current node in the recursive traversal.\n            value_x: The value to be deleted.\n\n        Returns:\n            The new node after deletion.\n        \"\"\"\n        if node is None:\n            return node\n\n        if value_x < node.value:\n            node.left = self._delete_recursive(node.left, value_x)\n        elif value_x > node.value:\n            node.right = self._delete_recursive(node.right, value_x)\n        else:\n            # Node found, now we need to restructure the tree\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            else:\n                # Node has two children, get the inorder successor\n                successor = self._get_inorder_successor(node.right)\n                node.value = successor.value\n                node.right = self._delete_recursive(node.right, successor.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        \"\"\"\n        Recursive helper function to find the inorder successor of a given node.\n\n        Args:\n            node: The current node in the recursive traversal.\n\n        Returns:\n            The inorder successor of the given node.\n        \"\"\"\n        if node.left is None:\n            return node\n        return self._get_inorder_successor(node.left)\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of the binary tree.\n        \"\"\"\n        if self.root is None:\n            return \"Empty tree\"\n        return str(self.root)\n\n\nclass Node:\n    \"\"\"\n    A node in the binary tree.\n    \"\"\"\n\n    def __init__(self, value_x, key_x=None):\n        \"\"\"\n        Initializes a node with a value and an optional key.\n\n        Args:\n            value_x: The value of the node.\n            key_x: The key associated with the value (optional).\n        \"\"\"\n        self.value = value_x\n        self.key = key_x\n        self.left = None\n        self.right = None\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of the node.\n        \"\"\"\n        return f\"Value: {self.value}, Key: {self.key}\"\n```",
        "def merge_sorted_lists(list1_x, list2_x) -> list:\n    \"\"\"Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1_x (list): The first sorted list.\n        list2_x (list): The second sorted list.\n\n    Returns:\n        list: A single sorted list containing elements from both input lists.\n    \"\"\"\n    result_x = []\n    i, j = 0, 0\n\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            result_x.append(list1_x[i])\n            i += 1\n        else:\n            result_x.append(list2_x[j])\n            j += 1\n\n    result_x.extend(list1_x[i:])\n    result_x.extend(list2_x[j:])\n\n    return result_x"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "def merge_sorted_lists(list1_x, list2_x):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            merged_list.append(list1_x[i])\n            i += 1\n        else:\n            merged_list.append(list2_x[j])\n            j += 1\n    while i < len(list1_x):\n        merged_list.append(list1_x[i])\n        i += 1\n    while j < len(list2_x):\n        merged_list.append(list2_x[j])\n        j += 1\n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha_g=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 algorithm='auto', fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.alpha_g = alpha_g\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.algorithm = algorithm\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(alpha=alpha_g, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0,\n                                kernel_params=kernel_params, algorithm=algorithm, fit_intercept=fit_intercept,\n                                normalize=normalize, copy_X=copy_X, verbose=verbose)\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass SortedListMerger:\n    @staticmethod\n    def merge(list1_g: list, list2_g: list) -> list:\n        merged_list_g = []\n        i, j = 0, 0\n        while i < len(list1_g) and j < len(list2_g):\n            if list1_g[i] < list2_g[j]:\n                merged_list_g.append(list1_g[i])\n                i += 1\n            else:\n                merged_list_g.append(list2_g[j])\n                j += 1\n        merged_list_g.extend(list1_g[i:])\n        merged_list_g.extend(list2_g[j:])\n        return merged_list_g\n```"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ImageClassifier, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(0.2)\n        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x_inf_f):\n        x_inf_f = self.fc1(x_inf_f)\n        x_inf_f = self.relu(x_inf_f)\n        x_inf_f = self.dropout(x_inf_f)\n        x_inf_f = self.fc2(x_inf_f)\n        return torch.sigmoid(x_inf_f)",
        "def merge_sorted_lists(list1_f: list, list2_f: list) -> list:\n    \"\"\"Merges two sorted lists into a single sorted list.\"\"\"\n    merged_list = []\n    i, j = 0, 0\n    \n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    \n    merged_list.extend(list1_f[i:])\n    merged_list.extend(list2_f[j:])\n    \n    return merged_list"
      ],
      "instruction_model_output": [
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x",
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n    return result_j"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\nfrom sklearn.base import clone\n\nclass NearestNeighborsModel:\n    \"\"\"A Nearest Neighbors classifier model.\"\"\"\n\n    def __init__(self, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None):\n        \"\"\"\n        Initialize the Nearest Neighbors classifier model.\n\n        Parameters:\n            algorithm (str, optional): The algorithm used to compute the nearest neighbors.\n                Defaults to 'auto'.\n            leaf_size (int, optional): The leaf size passed to BallTree or cKDTree. Defaults to 30.\n            metric (str, optional): The distance metric used to compute the nearest neighbors.\n                Defaults to 'minkowski'.\n            p (int, optional): The power parameter for the Minkowski metric. Defaults to 2.\n            metric_params (dict or None, optional): Additional parameters for the metric function.\n                Defaults to None.\n        \"\"\"\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.metric = metric\n        self.p = p\n        self.metric_params = metric_params\n        self.model = None\n\n    @count_calls\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Nearest Neighbors classifier model.\n\n        Parameters:\n            X_train (ndarray): The training data.\n            y_train (ndarray): The corresponding target values.\n        \"\"\"\n        # Clone the model to ensure that we start with a fresh instance\n        self.model = clone(self.model)\n        self.model.fit(X_train, y_train)\n\n    @count_calls\n    def predict(self, X_test):\n        \"\"\"\n        Predict the target values for the test data using the trained model.\n\n        Parameters:\n            X_test (ndarray): The test data.\n\n        Returns:\n            ndarray: The predicted target values.\n        \"\"\"\n        assert self.model is not None, \"Model has not been trained yet. Please call the train method first.\"\n        return self.model.predict(X_test)",
        "from pedantic import count_calls\n\n@count_calls\ndef find_majority_element(arr: list[int]) -> int:\n    assert isinstance(arr, list), \"Input must be a list\"\n    assert all(isinstance(num, int) for num in arr), \"All elements in the list must be integers\"\n    \n    num_set = set(arr)\n    for num in num_set:\n        if arr.count(num) > len(arr) // 2:\n            return num\n    return None"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```",
        "from pedantic.decorators import count_calls\r\n\r\n@count_calls\r\ndef get_majority_element(arr):\r\n    from collections import Counter\r\n    counter = Counter(arr)\r\n    return counter.most_common(1)[0][0]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def sum_unique_elements0(num_list):\n    unique_nums = list(set(num_list))\n    sum_unique = sum(unique_nums)\n    return sum_unique\n\ndef chx_sum_unique_elements7(num_list):\n    chx_unique_nums = list(set(num_list))\n    chx_sum_unique = sum(chx_unique_nums)\n    return chx_sum_unique",
        "def f7n4_count_non_zero(matrix: list[list[int]]) -> int:\n    chx_count = 0\n    for row in matrix:\n        for elem in row:\n            if elem != 0:\n                chx_count += 1\n    return chx_count"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum",
        "def count_non_zero(matrix):\n    count = 0\n    for row in matrix:\n        count += sum(1 for elem in row if elem != 0)\n    return count"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom __future__ import annotations\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"Insert a value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value: int) -> None:\n        \"\"\"Delete a value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: Node, value: int) -> Node:\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node: Node) -> int:\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_y(arr_e):\n    if len(arr_e) <= 1:\n        return arr_e\n\n    mid_e = len(arr_e) // 2\n    left_e = arr_e[:mid_e]\n    right_e = arr_e[mid_e:]\n\n    left_e = merge_sort_y(left_e)\n    right_e = merge_sort_y(right_e)\n\n    return merge_y(left_e, right_e)\n\ndef merge_y(left_e, right_e):\n    result_e = []\n    left_index_e, right_index_e = 0, 0\n\n    while left_index_e < len(left_e) and right_index_e < len(right_e):\n        if left_e[left_index_e] < right_e[right_index_e]:\n            result_e.append(left_e[left_index_e])\n            left_index_e += 1\n        else:\n            result_e.append(right_e[right_index_e])\n            right_index_e += 1\n\n    result_e.extend(left_e[left_index_e:])\n    result_e.extend(right_e[right_index_e:])\n\n    return result_e",
        "def merge_sorted_lists_y(list_one_e, list_two_e):\n    result_l = []\n    i, j = 0, 0\n    while i < len(list_one_e) and j < len(list_two_e):\n        if list_one_e[i] < list_two_e[j]:\n            result_l.append(list_one_e[i])\n            i += 1\n        else:\n            result_l.append(list_two_e[j])\n            j += 1\n    result_l.extend(list_one_e[i:])\n    result_l.extend(list_two_e[j:])\n    return result_l"
      ],
      "instruction_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "def merge_sorted_lists(list1_e, list2_e):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list.append(list1_e[i])\n            i += 1\n        else:\n            merged_list.append(list2_e[j])\n            j += 1\n    \n    while i < len(list1_e):\n        merged_list.append(list1_e[i])\n        i += 1\n    \n    while j < len(list2_e):\n        merged_list.append(list2_e[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef roman_to_int(\n    roman: str,\n    *,\n    values: dict[str, int] = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n) -> int:\n    \"\"\"\n    Convert a Roman numeral string to an integer.\n\n    Args:\n        roman (str): The Roman numeral string to convert.\n        values (dict, optional): A dictionary mapping Roman numeral characters to their integer values.\n                                 Defaults to a dictionary with values for numerals up to 'M'.\n\n    Returns:\n        int: The integer representation of the Roman numeral.\n    \"\"\"\n    num = 0\n    prev_value = 0\n    for char in reversed(roman):\n        value = values[char]\n        if value < prev_value:\n            num -= value\n        else:\n            num += value\n        prev_value = value\n    return num\n\n\n# Example class following the naming convention\nclass C7:\n    pass\n```",
        "```python\nfrom sklearn.decomposition import PCA\nfrom pedantic import require_kwargs\n\n\nclass PCA0(PCA):\n    @require_kwargs('n_components', 'copy')\n    def __init__(self, n_components=None, copy=True):\n        super().__init__(n_components=n_components, copy=copy)\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_binary_search_tree_a(root):\n    try:\n        def helper(node, min_val, max_val):\n            if not node:\n                return True\n            if node.val <= min_val or node.val >= max_val:\n                return False\n            return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n        return helper(root, float('-inf'), float('inf'))\n    except Exception as e:\n        raise Exception(f\"Error occurred: {e}\") from e",
        "def merge_sort_a(arr):\n    try:\n        if len(arr) <= 1:\n            return arr\n\n        mid = len(arr) // 2\n        left = arr[:mid]\n        right = arr[mid:]\n\n        left = merge_sort_a(left)\n        right = merge_sort_a(right)\n\n        return merge_a(left, right)\n\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n        return []\n\ndef merge_a(left, right):\n    try:\n        result = []\n        i, j = 0, 0\n\n        while i < len(left) and j < len(right):\n            if left[i] < right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n\n        result.extend(left[i:])\n        result.extend(right[j:])\n\n        return result\n\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n        return []"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "```python\ndef merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom sklearn.decomposition import PCA\nfrom pedantic import trace_class, require_kwargs\n\n@trace_class\nclass PCA_Analysis:\n    \"\"\"\n    Principal Component Analysis (PCA) class for dimensionality reduction.\n    \"\"\"\n\n    def __init__(self, n_components: int, random_state: int = 42):\n        \"\"\"\n        Initialize the PCA_Analysis class.\n\n        Args:\n            n_components (int): The number of principal components to keep.\n            random_state (int, optional): The random state for reproducibility. Defaults to 42.\n        \"\"\"\n        self.n_components = n_components\n        self.random_state = random_state\n        self.pca = PCA(n_components=n_components, random_state=random_state)\n\n    @require_kwargs\n    def fit(self, X: 'np.array') -> None:\n        \"\"\"\n        Fit the PCA model to the data.\n\n        Args:\n            X (np.array): The input data.\n        \"\"\"\n        self.pca.fit(X)\n\n    def transform(self, X: 'np.array') -> 'np.array':\n        \"\"\"\n        Apply the fitted PCA transformation to the data.\n\n        Args:\n            X (np.array): The input data.\n\n        Returns:\n            np.array: The transformed data.\n        \"\"\"\n        return self.pca.transform(X)\n\n    def fit_transform(self, X: 'np.array') -> 'np.array':\n        \"\"\"\n        Fit the PCA model to the data and transform the data.\n\n        Args:\n            X (np.array): The input data.\n\n        Returns:\n            np.array: The transformed data.\n        \"\"\"\n        return self.pca.fit_transform(X)\n\n    def get_explained_variance_ratio(self) -> 'np.array':\n        \"\"\"\n        Get the explained variance ratio of the PCA model.\n\n        Returns:\n            np.array: The explained variance ratio of the principal components.\n        \"\"\"\n        return self.pca.explained_variance_ratio_\n\n    def get_components(self) -> 'np.array':\n        \"\"\"\n        Get the principal components of the fitted PCA model.\n\n        Returns:\n            np.array: The principal components.\n        \"\"\"\n        return self.pca.components_\n```",
        "```python\nfrom torch import nn\nfrom pedantic import trace_class, require_kwargs\n\n@trace_class\nclass ImageClassifier(nn.Module):\n    def __init__(self, num_features: int, num_classes: int, chx_digit: int = 0) -> None:\n        super().__init__()\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.chx_digit = chx_digit\n        self.linear = nn.Linear(num_features, num_classes)\n\n    @require_kwargs(\"preds\", \"labels\")\n    def forward(self, preds: torch.Tensor) -> torch.Tensor:\n        outputs = self.linear(preds)\n        return outputs\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A Nearest Neighbors Model for classification.\n\n    ...\n\n    Attributes\n    ----------\n    n_neighbors : int\n        The number of neighbors to consider when classifying a data point.\n    metric : str\n        The distance metric to use when calculating distances between data points.\n    weights : str\n        The weight function used in prediction.\n    p : int\n        Power parameter for the Minkowski metric.\n    \"\"\"\n\n    def __init__(self, n_neighbors: int = 5, metric: str = \"minkowski\", weights: str = \"uniform\", p: int = 2):\n        \"\"\"\n        Initializes the NearestNeighborsModel class.\n\n        Parameters\n        ----------\n            n_neighbors : int, optional\n                The number of neighbors to consider when classifying a data point.\n                Defaults to 5.\n            metric : str, optional\n                The distance metric to use when calculating distances between data points.\n                Defaults to \"minkowski\".\n            weights : str, optional\n                The weight function used in prediction.\n                Defaults to \"uniform\".\n            p : int, optional\n                Power parameter for the Minkowski metric. Used when metric='minkowski'.\n                Defaults to 2.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self.weights = weights\n        self.p = p\n        self._model = KNeighborsClassifier(\n            n_neighbors=n_neighbors, metric=metric, weights=weights, p=p\n        )\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        \"\"\"\n        Trains the Nearest Neighbors Model.\n\n        Parameters\n        ----------\n        X_train : np.ndarray\n            The input training data.\n        y_train : np.ndarray\n            The target training data.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self._model.fit(X_train, y_train)\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the class labels for the test data using the trained model.\n\n        Parameters\n        ----------\n        X_test : np.ndarray\n            The input test data.\n\n        Returns\n        -------\n        np.ndarray\n            An array of predicted class labels.\n        \"\"\"\n        return self._model.predict(X_test)\n```",
        "```python\nfrom pedantic import require_kwargs, count_calls, trace_class\n\n@trace_class\nclass MajorityElement:\n    def __init__(self, arr: list[int]) -> None:\n        self.arr = arr\n\n    @count_calls\n    @require_kwargs\n    def find_majority(self, chx_index: int = 0) -> int:\n        assert isinstance(chx_index, int), \"Index must be an integer\"\n        assert 0 <= chx_index < len(self.arr), \"Index out of range\"\n\n        count_dict = {}\n        for num in self.arr:\n            count_dict[num] = count_dict.get(num, 0) + 1\n\n        for num, count in count_dict.items():\n            if count > len(self.arr) // 2:\n                return num\n\n        raise ValueError(\"No majority element found\")\n```",
        "```python\ndef roman_to_int(roman: str) -> int:\n    roman_map = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    total = 0\n    prev_value = 0\n\n    for char in roman[::-1]:\n        value = roman_map[char]\n        if value < prev_value:\n            total -= value\n        else:\n            total += value\n        prev_value = value\n\n    return total\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n@trace_class\ndef count_non_zero_elements(matrix: list[list[int]]) -> int:\n    \"\"\"\n    Counts the number of non-zero elements in a matrix.\n\n    Args:\n        matrix (list[list[int]]): The input matrix.\n\n    Returns:\n        int: The number of non-zero elements in the matrix.\n    \"\"\"\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count\n```",
        "```python\ndef is_valid_bst(tree, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    if not tree:\n        return True\n\n    root = tree.pop()\n    assert isinstance(root, tuple)\n    assert len(root) == 3\n    assert isinstance(root[1], (int, float))\n    assert isinstance(root[2], (list, tuple))\n\n    val, left, right = root\n    assert val > min_val and val < max_val\n    assert isinstance(left, list) and isinstance(right, list)\n\n    return is_valid_bst(left, min_val, val) and is_valid_bst(right, val, max_val)\n```",
        "```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef sum_unique_numbers(numbers: list[int]) -> int:\n    \"\"\"\n    Computes the sum of unique numbers in a list.\n\n    Args:\n        numbers (list[int]): A list of integers.\n\n    Returns:\n        int: The sum of unique numbers in the list.\n    \"\"\"\n    unique_numbers = list(set(numbers))\n    unique_numbers.sort(key=lambda x: str(x) + '_j')\n    return sum(unique_numbers)\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sorts a list of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    # Base case: if the list has 0 or 1 elements, it is already sorted\n    if len(arr) <= 1:\n        return arr\n\n    # Divide the list into two halves\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    # Recursively sort each half\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    # Merge the sorted halves\n    merged = []\n    left_idx, right_idx = 0, 0\n    while left_idx < len(left_half) and right_idx < len(right_half):\n        if left_half[left_idx] < right_half[right_idx]:\n            merged.append(left_half[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right_half[right_idx])\n            right_idx += 1\n\n    # Add any remaining elements from both halves\n    merged.extend(left_half[left_idx:])\n    merged.extend(right_half[right_idx:])\n\n    return merged\n```",
        "```python\nfrom pedantic import trace_class, count_calls, require_kwargs\n\nclass BinaryTree:\n    @trace_class\n    def __init__(self, value: int, left=None, right=None) -> None:\n        self.value = value\n        self.left = left\n        self.right = right\n\n    @count_calls\n    def insert(self, value: int) -> None:\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    @require_kwargs('node')\n    def delete(self, node: 'BinaryTree') -> None:\n        if node.value < self.value:\n            self.left.delete(node)\n        elif node.value > self.value:\n            self.right.delete(node)\n        else:\n            if self.left is None:\n                self.value = self.right.value\n                self.right = self.right.right\n            elif self.right is None:\n                self.value = self.left.value\n                self.left = self.left.left\n            else:\n                successor = self.right.find_min()\n                self.value = successor.value\n                self.right.delete(successor)\n\n    def find_min(self) -> int:\n        current = self\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists and returns a new sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing elements from both input lists.\n    \"\"\"\n    result = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            result.append(list1[i])\n            i += 1\n        else:\n            result.append(list2[j])\n            j += 1\n\n    result.extend(list1[i:])\n    result.extend(list2[j:])\n\n    return result\n```",
        "```python\ndef average_diagonal_element(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    if not matrix or not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input must be a non-empty square matrix\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        \"\"\"\n        Kernel Ridge Regression Class\n\n        Parameters:\n        alpha (float): Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of the estimates.\n        Larger values specify stronger regularization.\n        kernel (string): Kernel used in the RBF kernel ('linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable).\n        If none is given, 'linear' will be used. If a callable is given it is used to precompute the dot\n        product between all pairs of the input data.\n        gamma (float): Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels.\n        If gamma is 'auto' then 1 / n_features will be used instead.\n        degree (int): Degree of the polynomial kernel. Ignored by all other kernels.\n        coef0 (float): Independent term in the polynomial kernel.\n        kernel_params (dict): Parameters (keyword arguments) and values for kernel passed as callable object.\n        fit_intercept (bool): Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n        normalize (bool): If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        copy_X (bool): If True, X will be copied; else, it may be overwritten.\n        verbose: If True, the timings of fitting will be displayed.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0,\n            kernel_params=kernel_params, fit_intercept=fit_intercept, normalize=normalize,\n            copy_X=copy_X, verbose=verbose\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit Kernel Ridge Regression model\n\n        Parameters:\n        X (array-like): Training data of shape (n_samples, n_features)\n        y (array-like): Target values of shape (n_samples,) or (n_samples, n_outputs)\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the Kernel Ridge Regression model\n\n        Parameters:\n        X (array-like): Samples\n        Returns:\n        C (array): Predicted values\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self, digit=0):\n        self.digit = digit\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        assert X.shape[0] == y.shape[0], \"Number of samples in X and y do not match\"\n        assert len(X.shape) == 2, \"X should be a 2D array\"\n        X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n        self.coef_, self.intercept_ = np.linalg.lstsq(X_with_intercept, y, rcond=None)[0]\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        assert len(X.shape) == 2, \"X should be a 2D array\"\n        X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n        return np.dot(X_with_intercept, self.coef_) + self.intercept_\n\n    @property\n    def coef(self) -> np.ndarray:\n        return self.coef_\n\n    @coef.setter\n    def coef(self, value: np.ndarray) -> None:\n        self.coef_ = value\n\n    @property\n    def intercept(self) -> np.ndarray:\n        return self.intercept_\n\n    @intercept.setter\n    def intercept(self, value: np.ndarray) -> None:\n        self.intercept_ = value\n```"
      ]
    }
  ]
}