{
  "sessions": [
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    # Insertion of a node\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    # Deletion of a node\n    def delete(self, value):\n        if value < self.value:\n            self.left = self.left.delete(value)\n        elif value > self.value:\n            self.right = self.right.delete(value)\n        else:\n            if self.left is None:\n                return self.right\n            elif self.right is None:\n                return self.left\n            else:\n                min_value = self.right.find_min()\n                self.value = min_value\n                self.right = self.right.delete(min_value)\n        return self\n\n    def find_min(self):\n        if self.left is None:\n            return self.value\n        return self.left.find_min()"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    \"\"\"\n    A class to implement Lasso Regression, a linear regression technique with L1 regularization.\n    ...\n\n    Attributes\n    ----------\n    coef_ : 1d array\n        Coefficients of the features in the linear regression model.\n\n    intercept_ : float\n        Intercept (a.k.a. bias) in the linear regression model.\n\n    Methods\n    -------\n    fit(X, y, alpha=1.0)\n        Fit the model to the training data X and target values y.\n\n    predict(X)\n        Predict target values for the provided data X.\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model to the training data X and target values y.\n\n        Parameters\n        ----------\n        X : {array-like}, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        try:\n            # Add a column of ones to X for the intercept\n            X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n\n            # Coefficient matrix\n            self.coef_ = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ (X_with_intercept.T @ y)\n\n            # Intercept\n            self.intercept_ = self.coef_[0]\n            self.coef_ = self.coef_[1:]\n\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while fitting the Lasso Regression model: {e}\")\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values for the provided data X.\n\n        Parameters\n        ----------\n        X : {array-like}, shape (n_samples, n_features)\n            Data for which to predict target values.\n\n        Returns\n        -------\n        y_pred : array-like, shape (n_samples,)\n            Predicted target values.\n        \"\"\"\n        try:\n            # Add a column of ones to X for the intercept\n            X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n\n            # Make predictions using the calculated coefficients\n            y_pred = X_with_intercept @ self.coef_.T\n            return y_pred\n\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while making predictions: {e}\")",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    strs.sort()\n\n    # Get the first and last string in the sorted list\n    first_str = strs[0]\n    last_str = strs[-1]\n\n    # Initialize variables to track the common prefix\n    common_prefix = \"\"\n    idx = 0\n\n    # Compare the characters at each index until a mismatch is found\n    while idx < len(first_str) and idx < len(last_str):\n        if first_str[idx] == last_str[idx]:\n            common_prefix += first_str[idx]\n            idx += 1\n        else:\n            break\n\n    return common_prefix\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(vec1: list[float], vec2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vec1 (list[float]): The first vector.\n        vec2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    a_vec1 = vec1\n    a_vec2 = vec2\n    return sum(x * y for x, y in zip(a_vec1, a_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import rename_kwargs\n\n@rename_kwargs\ndef vector_dot_product(e_vector1: list[float], e_vector2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param e_vector1: The first vector.\n    :param e_vector2: The second vector.\n    :return: The dot product of e_vector1 and e_vector2.\n    \"\"\"\n    return sum(x * y for x, y in zip(e_vector1, e_vector2))",
        "```python\nfrom pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs(\"e_num\")\n    def maxSubArray(self, e_nums: list[int]) -> int:\n        if max_so_far < 0:\n            max_so_far = 0\n\n        curr_max = 0\n        for i in range(len(e_nums)):\n            curr_max = max(0, curr_max + e_nums[i])\n            max_so_far = max(max_so_far, curr_max)\n\n        return max_so_far\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```",
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.at_head = None\n        self.at_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        node = self.cache[key]\n        self._remove(node)\n        self._add(node)\n        return node.val\n\n    def put(self, key: int, value: int) -> None:\n        node = Node(key, value)\n        if key in self.cache:\n            self._remove(self.cache[key])\n        elif len(self.cache) >= self.capacity:\n            del self.cache[self.at_head.key]\n            self._remove(self.at_head)\n        self._add(node)\n        self.cache[key] = node\n\n    def _remove(self, node):\n        if node.at_prev:\n            node.at_prev.at_next = node.at_next\n        else:\n            self.at_head = node.at_next\n        if node.at_next:\n            node.at_next.at_prev = node.at_prev\n        else:\n            self.at_tail = node.at_prev\n\n    def _add(self, node):\n        if self.at_tail:\n            self.at_tail.at_next = node\n            node.at_prev = self.at_tail\n            self.at_tail = node\n        else:\n            self.at_head = self.at_tail = node\n        node.at_next = None\n\nclass Node:\n    def __init__(self, key: int, val: int):\n        self.key = key\n        self.val = val\n        self.at_next = None\n        self.at_prev = None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.at_capacity = capacity\n        self.at_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.at_cache:\n            return -1\n        value = self.at_cache.pop(key)\n        self.at_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.at_cache:\n            self.at_cache.pop(key)\n        elif len(self.at_cache) >= self.at_capacity:\n            self.at_cache.popitem(last=False)\n        self.at_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(v1, v2):\n    try:\n        result = sum(x * y for x, y in zip(v1, v2))\n        return result\n    except TypeError:\n        print(\"Error: Invalid input types. Please provide iterable numeric types.\")\n        return None\n    except ValueError:\n        print(\"Error: Inputs must have the same length.\")\n        return None\n```",
        "def is_valid_bst(root, g_min=float('-inf'), g_max=float('inf')):\n    if not root:\n        return True\n    try:\n        if not g_min < root.val < g_max:\n            return False\n        return is_valid_bst(root.left, g_min, root.val) and is_valid_bst(root.right, root.val, g_max)\n    except AttributeError:\n        print(\"Error: Invalid input. Please provide a valid binary tree node structure.\")\n        return False"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def check_palindrome_w(string_w: str) -> bool:\n    \"\"\"Checks if a string is a valid palindrome.\n    \n    Args:\n        string_w (str): The input string to be checked.\n        \n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    # Using reversed function to iterate through the string in reverse\n    return all(string_w[i] == char_w for i, char_w in enumerate(reversed(string_w), 1))"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_w):\n    s_w = s_w.lower()\n    return s_w == s_w[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(i_vec1_chx, i_vec2_chx):\n    if len(i_vec1_chx) != len(i_vec2_chx):\n        raise ValueError(\"Vectors must have the same length for dot product computation\")\n    \n    dot_product = sum(x * y for x, y in zip(i_vec1_chx, i_vec2_chx))\n    return dot_product\n```",
        "```python\ndef compute_dot_product(i_vec1_chx, i_vec2_chx):\n    if len(i_vec1_chx) != len(i_vec2_chx):\n        raise ValueError(\"Vector lengths must be equal for dot product computation\")\n    \n    dot_product = sum(x * y for x, y in zip(i_vec1_chx, i_vec2_chx))\n    return dot_product\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(i_vec1, i_vec2):\n    return sum(a * b for a, b in zip(i_vec1, i_vec2))\n```",
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List, Optional\n\n\ndef find_kth_largest(nums: List[int], k: int) -> Optional[int]:\n    \"\"\"\n    Finds the k-th largest element in a list of integers.\n\n    :param nums: The input list of integers.\n    :param k: The value of 'k' for the k-th largest element.\n    :return: The k-th largest element in the list, or None if the list has less than 'k' elements.\n    \"\"\"\n    if k <= 0 or k > len(nums):\n        return None\n\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```",
        "import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Kernel Ridge Regression estimator.\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, kernel: str = \"linear\", gamma: float = None):\n        \"\"\"\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization.\n        :param kernel: Kernel mapping used internally; specified as a string.\n            Available kernels: ['linear', 'poly', 'rbf', 'sigmoid', 'cosine']\n        :param gamma: Gamma parameter for the kernel ('poly', 'rbf', 'sigmoid', 'cosine').\n            If gamma is 'auto' then 1/n_features will be used instead.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        # Check parameters\n        if not isinstance(self.alpha, float) or self.alpha <= 0:\n            raise ValueError(\"Alpha must be a positive float.\")\n        if not isinstance(self.kernel, str) or self.kernel not in [\n            \"linear\",\n            \"poly\",\n            \"rbf\",\n            \"sigmoid\",\n            \"cosine\",\n        ]:\n            raise ValueError(\n                \"Invalid kernel. Choose from 'linear', 'poly', 'rbf', 'sigmoid', 'cosine'.\"\n            )\n        if (\n            not isinstance(self.gamma, float)\n            and self.gamma is not None\n            and self.gamma != \"auto\"\n        ):\n            raise ValueError(\"Gamma must be a positive float or 'auto'.\")\n\n        # Check input data\n        X = np.array(X)\n        y = np.array(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y must have the same number of samples.\")\n        if not np.issubdtype(X.dtype, np.number) or not np.issubdtype(y.dtype, np.number):\n            raise ValueError(\"X and y must contain only numerical values.\")\n\n        # Handle gamma\n        if self.gamma == \"auto\":\n            self.gamma = 1.0 / X.shape[1]\n\n        # Map data using kernel function\n        if self.kernel == \"poly\":\n            X = self._poly_kernel(X)\n        elif self.kernel == \"rbf\":\n            X = self._rbf_kernel(X)\n        elif self.kernel == \"sigmoid\":\n            X = self._sigmoid_kernel(X)\n        elif self.kernel == \"cosine\":\n            X = self._cosine_kernel(X)\n\n        # Add bias term\n        X = np.c_[X, np.ones(X.shape[0])]\n\n        # Compute coefficients\n        self.coef_ = np.linalg.inv(X.T @ X + self.alpha * np.eye(X.shape[1])) @ X.T @ y\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values for the provided data.\n\n        :param X: Data for which to make predictions.\n        :return: Predicted target values.\n        \"\"\"\n        # Check input data\n        X = np.array(X)\n        if not np.issubdtype(X.dtype, np.number):\n            raise ValueError(\"X must contain only numerical values.\")\n\n        # Map data using kernel function\n        if self.kernel == \"poly\":\n            X = self._poly_kernel(X)\n        elif self.kernel == \"rbf\":\n            Xpartum = self._rbf_kernel(X)\n        elif self.kernel == \"sigmoid\":\n            X = self._sigmoid_kernel(X)\n        elif self.kernel == \"cosine\":\n            X = self._cosine_kernel(X)\n\n        # Add bias term\n        X = np.c_[X, np.ones(X.shape[0])]\n\n        # Make predictions\n        return X @ self.coef_\n\n    def _poly_kernel(self, X):\n        return (X ** 2).T @ X + self.gamma\n\n    def _rbf_kernel(self, X):\n        sq_dist = np.sum(X ** 2, axis=1, keepdims=True) - 2 * X @ X.T + np.sum(\n            X ** 2, axis=0, keepdims=True\n        )\n        return np.exp(-sq_dist / (2 * self.gamma ** 2))\n\n    def _sigmoid_kernel(self, X):\n        return np.tanh(X.T @ X + self.gamma)\n\n    def _cosine_kernel(self, X):\n        return (X @ X.T) / (np.linalg.norm(X, axis=1) * np.linalg.norm(X, axis=1).T)"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass NearestNeighbors:\n    \"\"\"Finds the nearest neighbors of a point in a given dataset.\"\"\"\n\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X = None\n        self.y = None\n\n    def train(self, X, y):\n        \"\"\"Trains the model by storing the dataset and corresponding labels.\"\"\"\n        assert X.shape[0] == y.shape[0], \"Number of samples in X and y do not match\"\n        self.X = X\n        self.y = y\n\n    def predict(self, X):\n        \"\"\"Predicts the labels of the nearest neighbors for the given dataset X.\"\"\"\n        assert self.X is not None and self.y is not None, \"Model has not been trained yet\"\n        assert X.shape[1] == self.X.shape[1], \"Number of features in X does not match training data\"\n\n        distances = self._euclidean_distances(X, self.X)\n        nearest = distances.argsort()[:, :self.n_neighbors]\n        neighbors = self.y[nearest]\n        predictions = neighbors.mode(axis=1).values.reshape(-1)\n        return predictions\n\n    def _euclidean_distances(self, X1, X2):\n        \"\"\"Calculates the Euclidean distances between two datasets.\"\"\"\n        sum_sq = ((X1[:, None, :] - X2[None, :, :]) ** 2).sum(axis=2)\n        return sum_sq.sum(axis=1) ** 0.5\n```"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_z(s: str) -> bool:\n    s_reversed_z = s[::-1]\n    return s == s_reversed_z"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_z):\n    s_z = s_z.lower()\n    return s_z == s_z[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n```"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom pedantic import timer_class\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) Class.\n    \"\"\"\n\n    def __init__(self, tol: float = 0.001, store_covariance: bool = True,\n                 store_cov_params: bool = False):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n            tol (float): The tolerance for deciding if a covariance parameter is zero.\n                        Smaller values are more conservative.\n                        Default is 0.001.\n\n            store_covariance (bool): Whether to store the covariance matrices or\n                                     precision matrices. Default is True.\n\n            store_cov_params (bool): Whether to store the covariance parameters or\n                                    precision parameters. Default is False.\n        \"\"\"\n        self.tol = tol\n        self.store_covariance = store_covariance\n        self.store_cov_params = store_cov_params\n        self.model = QuadraticDiscriminantAnalysis(\n            tol=tol, store_covariance=store_covariance,\n            store_cov_params=store_cov_params\n        )\n\n    def fit(self, X: array, y: array) -> None:\n        \"\"\"\n        Fit the QDA model to the training data.\n\n        Parameters:\n            X (array): The input training data of shape (n_samples, n_features).\n\n            y (array): The target training data of shape (n_samples,).\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X: array) -> array:\n        \"\"\"\n        Predict the class labels for the provided data.\n\n        Parameters:\n            X (array): The input data of shape (n_samples, n_features).\n\n        Returns:\n            array: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        return self.model.predict(X)\n\n    def score(self, X: array, y: array) -> float:\n        \"\"\"\n        Calculate the accuracy score of the model on the provided data.\n\n        Parameters:\n            X (array): The input test data of shape (n_samples, n_features).\n\n            y (array): The target test data of shape (n_samples,).\n\n        Returns:\n            float: Accuracy score of the model.\n        \"\"\"\n        return self.model.score(X, y)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(x_vec1: list[float], x_vec2: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    Args:\n        x_vec1 (list[float]): The first vector.\n        x_vec2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of x_vec1 and x_vec2.\n    \"\"\"\n    return sum(x * y for x, y in zip(x_vec1, x_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_lengths_x(self, source):\n        distances = [float('inf')] * self.V\n        distances[source] = 0\n\n        queue = [source]\n\n        while queue:\n            u = queue.pop(0)\n            for v in self.graph[u]:\n                if distances[v] > distances[u] + 1:\n                    distances[v] = distances[u] + 1\n                    queue.append(v)\n\n        return distances\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.t_q = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.t_q.remove(key)\n        self.t_q.append(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.t_q:\n            self.t_q.remove(key)\n        elif len(self.t_q) >= self.capacity:\n            del_key = self.t_q.pop(0)\n            del self.cache[del_key]\n        self.t_q.append(key)\n        self.cache[key] = value\n\n    # Assert statements for testing\n    def test_lru_cache(self):\n        assert self.capacity > 0\n        cache = LRUCache(10)\n        cache.put(1, 1)\n        cache.put(2, 2)\n        assert cache.get(1) == 1\n        assert cache.get(3) == -1\n        cache.put(3, 3)\n        assert cache.get(2) == -1\n        assert cache.get(3) == 3\n        assert len(cache.t_q) == 2\n        cache.put(4, 4)\n        assert len(cache.t_q) == cache.capacity\n```",
        "def compute_diag_avg(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a 2D list.\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square.\"\n\n    diag_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diag_avg = diag_sum / len(matrix)\n\n    return diag_avg"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "import locale\n\ndef lcm(a7, b3):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a7 * b3 // gcd(a7, b3)",
        "def find_common_elements(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"Finds and returns common elements between two lists.\n\n    Args:\n        list1 (list[int]): The first list of integers.\n        list2 (list[int]): The second list of integers.\n\n    Returns:\n        list[int]: A new list containing the common elements between list1 and list2.\n    \"\"\"\n    import locale\n    locale.setlocale(locale.LC_ALL, '')\n\n    common_elements = [i for i in list1 if i in list2]\n    return common_elements"
      ],
      "instruction_model_output": [
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)",
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace\n\n@trace\ndef max_subarray_sum(nums):\n    if not nums:\n        return 0\n\n    max_sum = current_sum = nums[0]\n    for num in nums[1:]:\n        current_sum = max(num, current_sum + num)\n        max_sum = max(max_sum, current_sum)\n\n    return max_sum\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import trace\n\n@trace\ndef find_max_subarray_sum(arr: list[int]) -> int:\n    if max(arr) < 0:\n        return 0\n\n    global_max, local_max = 0, 0\n    for num in arr:\n        local_max = max(0, local_max + num)\n        global_max = max(global_max, local_max)\n\n    return global_max"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\n\nclass LinearRegression7:\n    @count_calls\n    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.coef_ = None\n        self.intercept_ = None\n\n    @count_calls\n    def train(self, X: list[list[float]], y: list[float]):\n        n_features = len(X[0])\n        self.coef_ = [0] * n_features\n        self.intercept_ = 0\n\n        for _ in range(self.epochs):\n            for i in range(len(X)):\n                y_pred = sum(x * coef for x, coef in zip(X[i], self.coef_)) + self.intercept_\n                for j in range(n_features):\n                    self.coef_[j] += self.learning_rate * (y[i] - y_pred) * X[i][j]\n                self.intercept_ += self.learning_rate * (y[i] - y_pred)\n\n    @count_calls\n    def predict(self, X: list[list[float]]) -> list[float]:\n        if self.coef_ is None or self.intercept_ is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n\n        return [sum(x * coef for x, coef in zip(x_i, self.coef_)) + self.intercept_ for x_i in X]\n```",
        "```python\nfrom sklearn.decomposition import PCA\nfrom pedantic import count_calls\n\n@count_calls\nclass PCA9:\n    \"\"\"Principal Component Analysis (PCA) for data dimensionality reduction.\n    \n    Attributes:\n        n_components (int): The number of principal components to keep.\n        explained_variance_ratio_ (float): The percentage of variance explained by each component.\n    \"\"\"\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.explained_variance_ratio_ = None\n\n    @count_calls\n    def fit(self, X):\n        \"\"\"Fit the PCA model to the data.\n        \n        Args:\n            X (ndarray): The input data of shape (n_samples, n_features).\n        \n        Returns:\n            self: Fitted PCA model.\n        \"\"\"\n        self.pca_ = PCA(n_components=self.n_components)\n        self.pca_.fit(X)\n        self.explained_variance_ratio_ = self.pca_.explained_variance_ratio_\n        return self\n\n    @count_calls\n    def transform(self, X):\n        \"\"\"Transform the data using the fitted PCA model.\n        \n        Args:\n            X (ndarray): The input data of shape (n_samples, n_features).\n        \n        Returns:\n            ndarray: Transformed data with reduced dimensions.\n        \"\"\"\n        return self.pca_.transform(X)\n\n    @count_calls\n    def fit_transform(self, X):\n        \"\"\"Fit the PCA model to the data and transform it.\n        \n        Args:\n            X (ndarray): The input data of shape (n_samples, n_features).\n        \n        Returns:\n            ndarray: Transformed data with reduced dimensions.\n        \"\"\"\n        return self.fit(X).transform(X)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)",
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_md(input_string_m: str) -> bool:\n    cleaned_string_m = ''.join(char_m.lower() for char_m in input_string_m if char_m.isalnum())\n    return cleaned_string_m == cleaned_string_m[::-1]",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_all_pairs(self):\n        dist = [float('Inf')]*self.V\n        dist[0] = 0\n\n        for i in range(self.V):\n            for j in range(self.V):\n                if self.graph[i] and j in self.graph[i]:\n                    dist[j] = 1\n\n            for k in range(self.V):\n                if dist[k] != float('Inf') and self.graph[k]:\n                    for j in self.graph[k]:\n                        if dist[j] > dist[k] + 1:\n                            dist[j] = dist[k] + 1\n\n        return dist\n\n# Mentor's updated coding guidelines\nclass Node:\n    def __init__(self, value_m):\n        self.value_m = value_m\n\nclass Edge:\n    def __init__(self, node_m1, node_m2):\n        self.node_m1 = node_m1\n        self.node_m2 = node_m2\n\n# Example usage with the updated guidelines (not included in word count)\nnode_a = Node('A')\nnode_b = Node('B')\nedge_ab = Edge(node_a, node_b)\n\ngraph_m = Graph(2)\ngraph_m.add_edge_md(edge_ab)\n\nshortest_distances_m = graph_m.shortest_path_all_pairs_md()\n```"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_m):\n    s_m = s_m.lower()\n    return s_m == s_m[::-1]",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_md(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_md(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        for _ in range(self.V):\n            u = self.min_distance_node_md(dist)\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n\n        path = []\n        node = dest\n        while parent[node] != -1:\n            path.append(node)\n            node = parent[node]\n        path.append(node)\n        path.reverse()\n        return path, dist[dest]\n\n    def min_distance_node_md(self, dist):\n        min_dist = float('inf')\n        min_node = -1\n        for v in range(self.V):\n            if dist[v] < min_dist:\n                min_dist = dist[v]\n                min_node = v\n        return min_node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self):\n        super(ImageClassifier, self).__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = torch.nn.Linear(120, 84)\n        self.fc3 = torch.nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.conv1(x))\n        x = torch.nn.functional.max_pool2d(x, 2)\n        x = torch.nn.functional.relu(self.conv2(x))\n        x = torch.nn.functional.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = torch.nn.functional.relu(self.fc1(x))\n        x = torch.nn.functional.relu(selfrayna_fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass VatoImageClassifier(nn.Module):\n    def __init__(self):\n        super(VatoImageClassifier, self).__init__()\n        self.i_Conv1 = nn.Conv2d(3, 6, 5)\n        self.i_Conv2 = nn.Conv2d(6, 16, 5)\n        self.i_FC1 = nn.Linear(16 * 5 * 5, 120)\n        self.i_FC2 = nn.Linear(120, 84)\n        self.i_FC3 = nn.Linear(84, 2)\n\n    def forward(self, i_X):\n        i_X = F.relu(self.i_Conv1(i_X))\n        i_X = F.max_pool2d(i_X, 2)\n        i_X = F.relu(self.i_Conv2(i_X))\n        i_X = F.max_pool2d(i_X, 2)\n        i_X = i_X.view(-1, 16 * 5 * 5)\n        i_X = F.relu(self.i_FC1(i_X))\n        i_X = F.relu(self.i_FC2(i_X))\n        i_X = self.i_FC3(i_X)\n        return i_X",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        else:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_sudoku(vr_board):\n    \"\"\"\n    Checks if a given 9x9 sudoku board is valid.\n\n    :param vr_board: A 2D array representing the sudoku board.\n    :return: True if the board is valid, False otherwise.\n    \"\"\"\n    vr_seen = set()\n    vr_row_cols = [set() for _ in range(9)]\n    vr_sub_boxes = [set() for _ in range(9)]\n\n    for vr_i in range(9):\n        for vr_j in range(9):\n            vr_num = vr_board[vr_i][vr_j]\n            if vr_num != '.':\n                vr_num = int(vr_num)\n                vr_row_str = f\"{vr_num}_{vr_i}\"\n                vr_col_str = f\"{vr_num}_{vr_j}\"\n                vr_sub_box_str = f\"{vr_num}_{(vr_i // 3) * 3 + (vr_j // 3)}\"\n\n                if vr_num in vr_seen or vr_num in vr_row_cols[vr_i] or vr_num in vr_sub_boxes[\n                    (vr_i // 3) * 3 + (vr_j // 3)\n                ]:\n                    return False\n\n                vr_seen.add(vr_num)\n                vr_row_cols[vr_i].add(vr_row_str)\n                vr_row_cols[vr_j].add(vr_col_str)\n                vr_sub_boxes[(vr_i // 3) * 3 + (vr_j // 3)].add(vr_sub_box_str)\n\n    return True"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    vr_seen = set()\n    vr_rows = [set() for _ in range(9)]\n    vr_cols = [set() for _ in range(9)]\n    vr_boxes = [set() for _ in range(9)]\n\n    for vr_i in range(9):\n        for vr_j in range(9):\n            vr_num = board[vr_i][vr_j]\n            if vr_num != '.':\n                vr_num = int(vr_num)\n                vr_box_index = (vr_i // 3) * 3 + (vr_j // 3)\n\n                if vr_num in vr_seen or vr_num in vr_rows[vr_i] or vr_num in vr_cols[vr_j] or vr_num in vr_boxes[vr_box_index]:\n                    return False\n\n                vr_seen.add(vr_num)\n                vr_rows[vr_i].add(vr_num)\n                vr_cols[vr_j].add(vr_num)\n                vr_boxes[vr_box_index].add(vr_num)\n\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\n\nclass Library:\n    @validate\n    def add_books(self, title_wr: str, author_wr: str) -> None:\n        \"\"\"\n        Adds a book to the library.\n\n        Args:\n            title_wr (str): The title of the book.\n            author_wr (str): The author of the book.\n        \"\"\"\n        # Add your code here for adding books\n\n    @validate\n    def remove_books(self, title_wr: str) -> None:\n        \"\"\"\n        Removes a book from the library.\n\n        Args:\n            title_wr (str): The title of the book to be removed.\n        \"\"\"\n        # Add your code here for removing books\n```",
        "from pedantic import validate\n\n@validate\ndef is_palindrome(input_string_wr: str) -> bool:\n    cleaned_string_wr = \"\".join(char_wr.lower() for char_wr in input_string_wr if char_wr.isalnum())\n    return cleaned_string_wr == cleaned_string_wr[::-1]"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None",
        "def is_palindrome(s_wr):\n    s_wr = s_wr.lower()\n    return s_wr == s_wr[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def sum_unique_elements(numbers):\n    chx_unique_numbers = list(set(numbers))\n    chx_sum = sum(chx_unique_numbers)\n    return chx_sum"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    class Node:\n        def __init__(self, data):\n            self.data = data\n            self.next_r = None\n\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data):\n        new_node = self.Node(data)\n        if not self.head_r:\n            self.head_r = new_node\n        else:\n            current_r = self.head_r\n            while current_r.next_r:\n                current_r = current_r.next_r\n            current_r.next_r = new_node\n\n    def display(self):\n        current_r = self.head_r\n        while current_r:\n            print(current_r.data, end=\" -> \")\n            current_r = current_r.next_r\n        print(\"None\")"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_r = data\n        self.next_r = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data):\n        if not self.head_r:\n            self.head_r = Node(data)\n        else:\n            curr_r = self.head_r\n            while curr_r.next_r:\n                curr_r = curr_r.next_r\n            curr_r.next_r = Node(data)\n\n    def display(self):\n        if not self.head_r:\n            return\n        curr_r = self.head_r\n        while curr_r:\n            print(curr_r.data_r, end=\" -> \")\n            curr_r = curr_r.next_r\n        print(\"None\")\n```"
      ],
      "history_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n        if self.root is None:\n            self.root = Node(None)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            else:\n                successor = self._find_min(node.right)\n                node.value = successor.value\n                node.right = self._delete(successor.value, node.right)\n        return node\n\n    def _find_min(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, book: str) -> None:\n        self.books.append(book)\n\n    def remove_book(self, book: str) -> None:\n        self.books.remove(book)",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    seen = set()\n    for row in board:\n        if not is_unique(row, seen):\n            return False\n        seen.clear()\n\n    for col in zip(*board):\n        if not is_unique(col, seen):\n            return False\n        seen.clear()\n\n    for i in range(0, 9, 3):\n        for j in range(0, 9, 3):\n            box = [board[r][c] for r in range(i, i + 3) for c in range(j, j + 3)]\n            if not is_unique(box, seen):\n                return False\n\n    return True\n\ndef is_unique(items: list[int], seen: set[int]) -> bool:\n    for item in items:\n        if item in seen or item == 0:\n            return False\n        seen.add(item)\n    return True\n```",
        "def sum_unique_numbers(numbers: list[int]) -> int:\n    \"\"\"\n    Computes the sum of unique numbers in the given list.\n\n    Args:\n        numbers (list[int]): A list of integers.\n\n    Returns:\n        int: The sum of unique numbers in the list.\n    \"\"\"\n    unique_numbers = []\n    for number in numbers:\n        if number not in unique_numbers:\n            unique_numbers.append(number)\n    return sum(unique_numbers)",
        "def get_common_elements(list1, list2):\n    common_elements = [value for value in list1 if value in list2]\n    return common_elements",
        "```python\ndef longest_common_prefix(strings: list[str]) -> str:\n    if not strings:\n        return \"\"\n\n    strings.sort()\n\n    prefix = strings[0]\n    for string in strings[1:]:\n        i = 0\n        while i < len(prefix) and i < len(string):\n            if prefix[i] != string[i]:\n                break\n            i += 1\n\n        prefix = prefix[:i]\n\n    return prefix\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = Lasso(alpha=self.alpha)\n        self.scaler = StandardScaler()\n\n    def train(self, X_train, y_train):\n        X_train = self.scaler.fit_transform(X_train)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        X_test = self.scaler.transform(X_test)\n        return self.model.predict(X_test)\n```",
        "```python\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nclass LinearRegression:\n    def __init__(self):\n        self.model = LinearRegression()\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        y_pred = self.model.predict(X_test)\n        return y_pred\n```",
        "```python\ndef compute_average_diagonal(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list[list[int]]): The input matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate the input matrix\n    if not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input matrix is not rectangular.\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for binary classification.\n    \"\"\"\n\n    def __init__(self, store_covariance=False):\n        \"\"\"\n        Initializes the QuadraticDiscriminantAnalysis class.\n\n        Parameters:\n            store_covariance (bool): Whether to store the covariance matrices for each class.\n                Defaults to False.\n        \"\"\"\n        self.store_covariance = store_covariance\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits the model according to the given training data and target values.\n\n        Parameters:\n            X (ndarray): Training data of shape (n_samples, n_features).\n            y (ndarray): Target values of shape (n_samples,).\n\n        Returns:\n            self (QuadraticDiscriminantAnalysis): Fitted instance of the class.\n        \"\"\"\n        # Fit the model\n        self.classes_ = np.unique(y)\n        self.n_classes_ = self.classes_.shape[0]\n        self.covariance_ = [np.cov(X[y == class_) for class_ in self.classes_]\n        self.priors_ = np.bincount(y) / y.shape[0]\n        self.means_ = np.array([np.mean(X[y == class_], axis=0) for class_ in self.classes_])\n\n        # Store the covariance matrices if specified\n        if self.store_covariance:\n            self.covariance_ = self.covariance_\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the class labels for the provided data.\n\n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        jll = self._get_log_prob(X)\n        return self.classes_[np.argmax(jll, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predicts the probability of each class for the provided data.\n\n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        jll = self._get_log_prob(X)\n        prob = np.exp(jll - jll.max(axis=1)[:, np.newaxis])\n        return self._normalize_prob(prob)\n\n    def _get_log_prob(self, X):\n        \"\"\"\n        Calculates the log probability for each class for the provided data.\n\n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Log probability of each class for each sample of shape (n_samples, n_classes).\n        \"\"\"\n        log_prob = np.zeros((X.shape[0], self.n_classes_))\n        for i, class_ in enumerate(self.classes_):\n            log_prob[:, i] = (\n                -0.5\n                * np.log(np.linalg.det(self.covariance_[i]))\n                - np.log(self.priors_[i])\n                - np.log(2 * np.pi)\n                - (X - self.means_[i]).T\n                @ np.linalg.inv(self.covariance_[i])\n                @ (X - self.means_[i])\n                .T\n            ) / 2\n        return log_prob\n\n    def _normalize_prob(self, prob):\n        \"\"\"\n        Normalizes the probabilities so that they sum to 1 for each sample.\n\n        Parameters:\n            prob (ndarray): Probabilities of shape (n_samples, n_classes).\n\n        Returns:\n            ndarray: Normalized probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        prob /= prob.sum(axis=1)[:, np.newaxis]\n        return prob\n```",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.graph = [[0] * vertices for _ in range(vertices)]\n\n    def shortest_path(self, source, destination):\n        distances = [float('inf')] * self.vertices\n        distances[source] = 0\n        queue = deque()\n        queue.append(source)\n\n        while queue:\n            vertex = queue.popleft()\n\n            for neighbor in range(self.vertices):\n                if self.graph[vertex][neighbor] > 0 and distances[neighbor] > distances[vertex] + self.graph[vertex][neighbor]:\n                    distances[neighbor] = distances[vertex] + self.graph[vertex][neighbor]\n                    queue.append(neighbor)\n\n        return distances[destination]\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1.0, kernel_params=None):\n        \"\"\"\n        :param alpha: float, default=1.0\n            Regularization strength; must be a positive float. Regularization improves the\n            conditioning of the problem and reduces the variance of the estimates. Larger values\n            specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n            models such as LogisticRegression or LinearSVC. If an array is passed,\n            ``alpha`` is used as the ``1 / (2C)`` value for each target class.\n            .. versionadded:: 0.19\n               parameter ``alpha`` to be an array for multi-task kernel ridge regression.\n        :param kernel: string, default='linear'\n            Kernel mapping used internally.\n            Possible values: ['linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed']\n            .. versionchanged:: 0.24\n               parameter ``kernel`` can be a callable.\n        :param gamma: float, default=None\n            Gamma parameter for the kernel.\n            - if ``kernel`` == 'poly': ``gamma`` defines ``degree`` of the polynomial\n            - if ``kernel`` == 'rbf', ``gamma`` defines the radius of influence of samples\n              chosen for centers as per :func:`sklearn.decomposition.KernelCenterer`\n            - if ``kernel`` == 'sigmoid', ``gamma`` is the ``1 / (2 * sigmoid's width)``.\n              For 'precomputed' ``kernel``, ``gamma`` is ignored.\n        :param degree: int, default=3\n            Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param coef0: float, default=1.0\n            Independent term in kernel function. It is only significant in\n            'poly' and 'sigmoid'.\n        :param kernel_params: dict of string -> any, default=None\n            Parameters (keyword arguments) and values for kernel passed as callable\n            object.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit Kernel Ridge model\n\n        :param X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data\n        :param y: array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :param sample_weight: array-like of shape (n_samples,), default=None\n            Individual weights for each sample. If given, it has to be a vector of the\n            same length as `X`.\n        :returns: self : object\n            Returns the instance itself.\n        \"\"\"\n        self.kernel_ridge_ = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.kernel_ridge_.fit(X, y, sample_weight=sample_weight)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the kernel ridge model\n\n        :param X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples\n        :returns: C {array-like} of shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        return self.kernel_ridge_.predict(X)\n```",
        "```python\ndef is_valid_binary_search_tree(tree):\n    \"\"\"\n    Determine if a given tree is a valid binary search tree.\n\n    Args:\n        tree (TreeNode): The root node of the binary tree.\n\n    Returns:\n        bool: True if the tree is a valid binary search tree, False otherwise.\n    \"\"\"\n    # TODO: Implement the function\n    # Your code here\n    pass\n```",
        "```python\n@count_calls\ndef lcm(a: int, b: int) -> int:\n    \"\"\"\n    Returns the least common multiple of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The least common multiple of a and b.\n    \"\"\"\n    # Validate inputs\n    if a <= 0 or b <= 0:\n        raise ValueError(\"Input values must be positive integers.\")\n\n    # Calculate the greatest common divisor\n    gcd = gcd_of_two_numbers(a, b)\n\n    # Calculate the least common multiple\n    return a * b // gcd\n\n\ndef gcd_of_two_numbers(x: int, y: int) -> int:\n    \"\"\"\n    Calculates the greatest common divisor of two integers using the Euclidean algorithm.\n\n    Args:\n        x (int): The first integer.\n        y (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of x and y.\n    \"\"\"\n    while y:\n        x, y = y, x % y\n    return x\n```",
        "class LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def push(self, value):\n        node = Node(value)\n        node.next = self.head\n        self.head = node\n\n    def pop(self):\n        if not self.head:\n            return None\n        value = self.head.value\n        self.head = self.head.next\n        return value\n\n    def is_empty(self):\n        return self.head is None",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort a list of integers using the insertion sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    # Add try statement for error handling\n    try:\n        # Validate input\n        if not all(isinstance(x, int) for x in arr):\n            raise ValueError(\"Input must be a list of integers.\")\n\n        # Early exit for empty or single element list\n        if len(arr) <= 1:\n            return arr\n\n        # Iterate over the array\n        for i in range(1, len(arr)):\n            key = arr[i]\n            j = i - 1\n\n            # Shift elements to the right until the correct position is found\n            while j >= 0 and arr[j] > key:\n                arr[j + 1] = arr[j]\n                j -= 1\n\n            # Insert the element at the correct position\n            arr[j + 1] = key\n\n        # Add decorator for debugging and performance optimization\n        @trace\n        def insertion_sort_helper(arr: list[int]) -> list[int]:\n            return insertion_sort(arr)\n\n        # Return the sorted list\n        return insertion_sort_helper(arr)\n\n    # Catch and raise any exceptions\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```",
        "```python\nfrom numpy import dot\n\ndef dot_product(vector_a: list[float], vector_b: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vector_a: list[float] - The first vector.\n    :param vector_b: list[float] - The second vector.\n    :return: float - The dot product of the two vectors.\n    \"\"\"\n    return dot(vector_a, vector_b)\n```",
        "```python\nfrom typing import List\n\n\ndef dot_product(vector_one: List[int], vector_two: List[int]) -> int:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_one (List[int]): A list of integers representing the first vector.\n        vector_two (List[int]): A list of integers representing the second vector.\n\n    Returns:\n        int: The dot product of the two vectors.\n    \"\"\"\n    # Validate input\n    if not vector_one or not vector_two:\n        raise ValueError(\"Both vectors must be non-empty.\")\n    if len(vector_one) != len(vector_two):\n        raise ValueError(\"Vectors must have the same length.\")\n\n    # Compute the dot product\n    dot_product_result = 0\n    for i in range(len(vector_one)):\n        dot_product_result += vector_one[i] * vector_two[i]\n\n    return dot_product_result\n```",
        "```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```",
        "from sklearn.decomposition import PCA\n\nclass PCA:\n    \"\"\"\n    Principal component analysis class.\n    \"\"\"\n\n    def __init__(self, n_components=None, copy=True, whiten=False):\n        \"\"\"\n        Initialize the PCA class.\n\n        Parameters:\n        n_components (int or None): The number of components to keep.\n            If None, keep all components.\n        copy (bool): If True, X will be copied; else, it may be overwritten.\n        whiten (bool): If True, de-correlate the components.\n        \"\"\"\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model with X.\n\n        Parameters:\n        X (array-like): The training data.\n        y (array-like): Ignored.\n        \"\"\"\n        # Check if X is a valid input\n        self._check_fit_data(X)\n\n        # Check if n_components is valid\n        self._check_n_components(X)\n\n        # Store the number of features in the training data\n        self.n_features_ = X.shape[1]\n\n        # Store the data covariance matrix\n        self.covariance_ = np.cov(X.T)\n\n        # Perform eigenvalue decomposition on the covariance matrix\n        eigenvalues_, eigenvectors_ = np.linalg.eigh(self.covariance_)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        idx = eigenvalues_.argsort()[::-1]\n        eigenvalues_ = eigenvalues_[idx]\n        eigenvectors_ = eigenvectors_[:, idx]\n\n        # Store the number of components to keep\n        self.n_components_ = self._effective_n_components(eigenvalues_)\n\n        # Store the number of samples in the training data\n        self.n_samples_seen_ = X.shape[0]\n\n        # Whiten the components if specified\n        if self.whiten:\n            self.components_ = eigenvectors_ * np.sqrt(self.n_samples_seen_)\n        else:\n            self.components_ = eigenvectors_\n\n        # Store the explained variance ratio\n        self.explained_variance_ratio_ = self.explained_variance_().cumsum() / np.sum(\n            self.explained_variance_())\n\n    def transform(self, X):\n        \"\"\"\n        Transform X using the fitted model.\n\n        Parameters:\n        X (array-like): The data to transform.\n\n        Returns:\n        array-like: The transformed data.\n        \"\"\"\n        # Check if X is a valid input\n        check_is_fitted(self)\n        self._check_transform_data(X)\n\n        # Check if n_components is valid\n        self._check_n_components(X)\n\n        # Transform the data using the first n_components eigenvectors\n        return np.dot(X, self.components_[: self.n_components_])\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit the model with X and then transform X.\n\n        Parameters:\n        X (array-like): The training data.\n        y (array-like): Ignored.\n\n        Returns:\n        array-like: The transformed data.\n        \"\"\"\n        # Fit the model with X\n        self.fit(X)\n\n        # Transform X using the fitted model\n        return self.transform(X)\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Transform X back to the original space.\n\n        Parameters:\n        X (array-like): The data to transform back.\n\n        Returns:\n        array-like: The transformed data in the original space.\n        \"\"\"\n        # Check if the model has been fitted\n        check_is_fitted(self)\n\n        # Transform X using the pseudo-inverse of the components\n        return np.dot(X, self.components_.T)\n\n    def explained_variance(self):\n        \"\"\"\n        Return the variance explained by each of the selected components.\n\n        Returns:\n        array-like: The variance explained by each component.\n        \"\"\"\n        # Check if the model has been fitted\n        check_is_fitted(self)\n\n        # Return the eigenvalues of the covariance matrix\n        return self.eigenvalues_[: self.n_components_]\n\n    def explained_variance_ratio(self):\n        \"\"\"\n        Return the percentage of variance explained by each of the selected components.\n\n        Returns:\n        array-like: The percentage of variance explained by each component.\n        \"\"\"\n        # Check if the model has been fitted\n        check_is_fitted(self)\n\n        # Return the explained variance ratio\n        return self.explained_variance_ratio_\n\n    def singular_values_(self):\n        \"\"\"\n        Return the singular values corresponding to each of the selected components.\n\n        Returns:\n        array-like: The singular values of the selected components.\n        \"\"\"\n        # Check if the model has been fitted\n        check_is_fitted(self)\n\n        # Return the square root of the eigenvalues\n        return np.sqrt(self.eigenvalues_[: self.n_components_])\n\n    def score(self, X, y=None):\n        \"\"\"\n        Return the average log-likelihood of the samples under the current model.\n\n        Parameters:\n        X (array-like): The data.\n        y (array-like): Ignored.\n\n        Returns:\n        float: The average log-likelihood of the samples.\n        \"\"\"\n        # Check if the model has been fitted\n        check_is_fitted(self)\n\n        # Calculate the log-likelihood of the samples\n        log_likelihood = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(self.singular_values_ ** 2)\n        return log_likelihood.mean()\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        Parameters:\n        deep (bool): If True, return the parameters for this estimator and contained\n            subobjects that are estimators.\n\n        Returns:\n        dict: The parameters of the estimator.\n        \"\"\"\n        return {\"n_components\": self.n_components, \"copy\": self.copy, \"whiten\": self.whiten}\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        Parameters:\n        **params (dict): The parameters to set for this estimator.\n\n        Returns:\n        self: The estimator instance.\n        \"\"\"\n        # Check if n_components is valid\n        self._check_n_components(params.get(\"n_components\"))\n\n        # Update the estimator's parameters\n        self.n_components = params.pop(\"n_components\", self.n_components)\n        self.copy = params.pop(\"copy\", self.copy)\n        self.whiten = params.pop(\"whiten\", self.whiten)\n\n        # Check if there are any invalid parameters\n        if params:\n            raise ValueError(\n                \"Invalid parameters passed: %s\" % \", \".join(params.keys())\n            )\n\n        # Return the estimator instance\n        return self\n\n    def _check_n_components(self, n_components=None):\n        \"\"\"\n        Check if n_components is valid.\n\n        Parameters:\n        n_components (int or None): The number of components to keep.\n\n        Raises:\n        ValueError: If n_components is invalid.\n        \"\"\"\n        if n_components is not None:\n            if not isinstance(n_components, six.integer_types) or n_components < 0:\n                raise ValueError(\n                    \"n_components must be a non-negative integer, got %r\" % n_components\n                )\n            if hasattr(self, \"n_features_in_\") and n_components > self.n_features_in_:\n                raise ValueError(\n                    \"n_components=%r must be smaller than n_features_in_=%r\"\n                    % (n_components, self.n_features_in_)\n                )\n\n    def _effective_n_components(self, explained_variance_):\n        \"\"\"\n        Compute the number of components to keep.\n\n        Parameters:\n        explained_variance_ (array-like): The variance explained by each component.\n\n        Returns:\n        int: The number of components to keep.\n        \"\"\"\n        # Check if n_components is valid\n        self._check_n_components(self.n_components)\n\n        # If n_components is None, keep all components\n        if self.n_components is None:\n            n_components_ = explained_variance_.shape[0]\n        else:\n            n_components_ = self.n_components\n\n        # Check if n_components is valid\n        if n_components_ <= 0:\n            raise ValueError(\n                \"n_components=%r must be greater than zero\" % self.n_components\n            )\n\n        # Return the number of components to keep\n        return n_components_\n\n    def _check_fit_data(self, X):\n        \"\"\"\n        Check if X is a valid input for fitting.\n\n        Parameters:\n        X (array-like): The training data.\n\n        Raises:\n        ValueError: If X is invalid.\n        \"\"\"\n        # Check if X is a valid input\n        check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n        n_samples, n_features = X.shape\n\n        # Check if the number of features is greater than zero\n        if n_features <= 0:\n            raise ValueError(\"X has 0 features, PCA cannot be performed\")\n\n        # Check if the number of samples is greater than zero\n        if n_samples <= 0:\n            raise ValueError(\"X has 0 samples, PCA cannot be performed\")\n\n    def _check_transform_data(self, X):\n        \"\"\"\n        Check if X is a valid input for transformation.\n\n        Parameters:\n        X (array-like): The data to transform.\n\n        Raises:\n        ValueError: If X is invalid.\n        \"\"\"\n        # Check if X is a valid input\n        check_is_fitted(self)\n        check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n\n        n_samples, n_features = X.shape\n\n        # Check if the number of features matches the number of features in the training data\n        if n_features != self.n_features_:\n            raise ValueError(\n                \"X has different shape than during fitting, expected %s, got %s\"\n                % (self.n_features_, n_features)\n            )",
        "```python\nfrom pedantic import count_calls, trace_class, rename_kwargs\n\n\n@trace_class\nclass Solution:\n    def __init__(self, arr: list[int]):\n        self.arr = arr\n        self.max_sum = 0\n        self.max_sum_ending_here = 0\n        self.max_sum_ending_before = 0\n\n    @count_calls\n    def max_subarray_sum(self) -> int:\n        for i in range(len(self.arr)):\n            self.max_sum_ending_before = max(self.max_sum_ending_before, 0)\n            self.max_sum_ending_here = self.max_sum_ending_before + self.arr[i]\n            self.max_sum = max(self.max_sum, self.max_sum_ending_here)\n        return self.max_sum\n\n    @rename_kwargs\n    def test_function(self, a_param: int, b_param: int) -> int:\n        return a_param + b_param\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n\n    def train(self, X_train, y_train):\n        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n\n# Example usage:\nX = [[1], [2], [3], [4]]\ny = [0, 0, 1, 1]\n\nmodel = NearestNeighborsModel()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel.train(X_train, y_train)\npredictions = model.predict(X_test)\nprint(f\"Model accuracy: {accuracy_score(y_test, predictions)}\")\n```",
        "```python\n@trace_class\ndef largest_subarray_sum(arr: list[int]) -> int:\n    \"\"\"\n    Finds the subarray with the largest sum and returns its sum.\n\n    Args:\n        arr (list[int]): The input array of integers.\n\n    Returns:\n        int: The sum of the subarray with the largest sum.\n    \"\"\"\n    max_sum = 0\n    current_sum = 0\n\n    for num in arr:\n        current_sum += num\n        if current_sum > max_sum:\n            max_sum = current_sum\n\n    return max_sum\n```",
        "```python\n@trace_class\ndef is_palindrome(string: str) -> bool:\n    \"\"\"\n    Checks if a given string is a valid palindrome.\n\n    Args:\n        string (str): The string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    # Add the 'pedantic' module and import the 'trace_class' decorator\n    from pedantic import trace_class\n\n    # Add the 'hashlib' module as instructed\n    import hashlib\n\n    # Add the 'locale' module as instructed\n    import locale\n\n    # Initialize the 'locale' module\n    locale.setlocale(locale.LC_ALL, '')\n\n    # Validate the input using assert statements\n    assert isinstance(string, str), \"Input must be a string.\"\n\n    # Convert the string to lowercase and remove spaces\n    cleaned_string = string.lower().replace(\" \", \"\")\n\n    # Reverse the cleaned string\n    reversed_string = cleaned_string[::-1]\n\n    # Compare the original and reversed strings\n    if cleaned_string == reversed_string:\n        return True\n    else:\n        return False\n```",
        "```python\nfrom heapq import *\n\n\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    heapify(arr)\n    for _ in range(k - 1):\n        heappop(arr)\n    return heappop(arr)\n```",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = {}\n        self.capacity = capacity\n        self.queue = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.queue.remove(key)\n            self.queue.append(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.queue.pop(0)\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self.queue.append(key)"
      ]
    }
  ]
}