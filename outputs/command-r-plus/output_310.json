{
  "sessions": [
    {
      "session_model_output": [
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def b_add_node(self, data):\n        node_i = Node(data)\n        if not self.head_i:\n            self.head_i = node_i\n        else:\n            current_i = self.head_i\n            while current_i.next_i:\n                current_i = current_i.next_i\n            current_i.next_i = node_i\n\n    def b_remove_node(self, data):\n        if self.head_i:\n            if self.head_i.data == data:\n                self.head_i = self.head_i.next_i\n                return\n            current_i = self.head_i\n            prev_i = None\n            while current_i and current_i.data != data:\n                prev_i = current_i\n                current_i = current_i.next_i\n            if current_i:\n                prev_i.next_i = current_i.next_i\n\n    def b_display(self):\n        current_i = self.head_i\n        while current_i:\n            print(current_i.data, end=\" -> \")\n            current_i = current_i.next_i\n        print(\"None\")\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next_i = None\n```",
        "def b_h_index_i(citations_i: list[int]) -> int:\n    citations_i.sort(reverse=True)\n    h_index_i = 0\n    for i, citation_i in enumerate(citations_i):\n        if citation_i >= i + 1:\n            h_index_i = i + 1\n        else:\n            break\n    return h_index_i"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_i):\n        self.data_i = data_i\n        self.next_i = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def append(self, data_i):\n        if not self.head_i:\n            self.head_i = Node(data_i)\n        else:\n            curr_i = self.head_i\n            while curr_i.next_i:\n                curr_i = curr_i.next_i\n            curr_i.next_i = Node(data_i)\n\n    def delete(self, data_i):\n        if self.head_i is None:\n            return\n\n        if self.head_i.data_i == data_i:\n            self.head_i = self.head_i.next_i\n            return\n\n        prev_i = self.head_i\n        curr_i = self.head_i.next_i\n\n        while curr_i:\n            if curr_i.data_i == data_i:\n                prev_i.next_i = curr_i.next_i\n                return\n            prev_i = curr_i\n            curr_i = curr_i.next_i\n\n    def display(self):\n        curr_i = self.head_i\n        while curr_i:\n            print(curr_i.data_i, end=\" -> \")\n            curr_i = curr_i.next_i\n        print(\"None\")\n```",
        "def b_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def a_compute_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def a_insert(self, value):\n        self.heap.append(value)\n        self.a_chx_heapify(len(self.heap) - 1)\n\n    def a_chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] >= self.heap[parent_index]:\n                break\n            self.heap[index], self.heap[parent_index] = self.heap[parent_index], self.heap[index]\n            index = parent_index"
      ],
      "instruction_model_output": [
        "```python\ndef a_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs(\"num\")\n    def maxSubArray(self, nums: list[int]) -> int:\n        if not nums:\n            return 0\n\n        max_sum = current_sum = nums[0]\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n\n        return max_sum\n```"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def convert_to_roman(num):\n    \"\"\"\n    Converts an integer to Roman numerals.\n    \"\"\"\n    # Base case\n    if num == 0:\n        return \"\"\n\n    # Map of decimal to Roman values\n    roman_map = [(\"M\", 1000), (\"CM\", 900), (\"D\", 500), (\"CD\", 400), (\"C\", 100), (\"XC\", 90), (\"L\", 50), (\"XL\", 40), (\"X\", 10), (\"IX\", 9), (\"V\", 5), (\"IV\", 4), (\"I\", 1)]\n\n    # Iterate through the roman_map to construct the Roman numeral\n    roman_numeral = \"\"\n    for symbol, value in roman_map:\n        while num >= value:\n            roman_numeral += symbol\n            num -= value\n    return roman_numeral\n\n\n# Add comments to explain function argument names\ndef function_with_argument_comments(arg1,  # This is the first argument\n                                   arg2,  # This is the second argument\n                                   arg3):  # A single digit is included as per mentor's guidelines, no specific preference mentioned\n    # Function implementation\n    pass",
        "```python\n# Function to find common elements between two lists\ndef find_common_elements(list1, list2):\n    # Initialize a list to store the common elements\n    common = []\n    # Iterate through the first list\n    for i in list1:\n        # Check if the element is present in the second list\n        if i in list2:\n            # Add the element to the common list\n            common.append(i)\n    # Return the list of common elements\n    return common\n```"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(j_vector1, j_vector2):\n    try:\n        if len(j_vector1) != len(j_vector2):\n            raise ValueError(\"Vectors must have the same length\")\n        return sum(x * y for x, y in zip(j_vector1, j_vector2))\n    except TypeError:\n        raise TypeError(\"Vector elements must be numeric\")\n```",
        "```python\ndef is_valid_bst(root, j_min=float('-inf'), j_max=float('inf')):\n    if not root:\n        return True\n\n    j_val = root.val\n    if not (j_min < j_val < j_max):\n        return False\n\n    try:\n        return is_valid_bst(root.left, j_min, j_val) and is_valid_bst(root.right, j_val, j_max)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(j_vector_1, j_vector_2):\n    return sum(a * b for a, b in zip(j_vector_1, j_vector_2))\n```",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_at = None\n\n    def append(self, value):\n        if not self.head_at:\n            self.head_at = Node(value)\n        else:\n            current_at = self.head_at\n            while current_at.next_at:\n                current_at = current_at.next_at\n            current_at.next_at = Node(value)\n\n    def delete(self, value):\n        if self.head_at and self.head_at.value == value:\n            self.head_at = self.head_at.next_at\n        else:\n            current_at = self.head_at\n            prev_at = None\n            while current_at and current_at.value != value:\n                prev_at = current_at\n                current_at = current_at.next_at\n            if current_at:\n                prev_at.next_at = current_at.next_at\n\n    def display(self):\n        current_at = self.head_at\n        while current_at:\n            print(current_at.value, end=\" -> \")\n            current_at = current_at.next_at\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.next_at = None\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_at = data\n        self.next_at = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_at = None\n        self.tail_at = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_at:\n            self.head_at = new_node\n            self.tail_at = self.head_at\n        else:\n            self.tail_at.next_at = new_node\n            self.tail_at = new_node\n\n    def delete(self, data):\n        if self.head_at is None:\n            return\n\n        if self.head_at.data_at == data:\n            self.head_at = self.head_at.next_at\n            return\n\n        prev_node = self.head_at\n        curr_node = self.head_at.next_at\n        while curr_node:\n            if curr_node.data_at == data:\n                prev_node.next_at = curr_node.next_at\n                return\n            prev_node = curr_node\n            curr_node = curr_node.next_at\n\n    def display(self):\n        current = self.head_at\n        while current:\n            print(current.data_at, end=\" -> \")\n            current = current.next_at\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def c_dot_product(x_vector1, x_vector2):\n    sum = 0\n    for i in range(len(x_vector1)):\n        sum += x_vector1[i] * x_vector2[i]\n    return sum",
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```",
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    A scikit-learn compatible Kernel Ridge Regression class.\n    This class provides a wrapper around scikit-learn's KernelRidge regression algorithm with added\n    functionality for custom preprocessing and transformation of the data.\n    \"\"\"\n\n    def __init__(self, alpha=1, kernel='linear', degree=3, gamma=None, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        \"\"\"\n        Initialize the KernelRidgeRegression class.\n\n        Parameters:\n            alpha (float): Regularization strength; must be a positive float. Regularization improves the\n                conditioning of the problem and reduces the variance of the estimates. Larger values\n                specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n                models such as LogisticRegression or LinearSVC.\n            kernel (str or callable): Specifies the kernel type to be used in the algorithm. It must be one of\n                'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'linear'\n                will be used. If a callable is given it is used to pre-compute the kernel matrix.\n            degree (int): Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n            gamma (float): Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. If gamma='scale' (default)\n                is passed then it uses 1 / n_features.\n            coef0 (float): Independent term in kernel functions. It is only significant in 'poly' and\n                'sigmoid'.\n            kernel_params (dict of str to any): Parameters (keyword arguments) and values for kernel passed as\n                callable object.\n            fit_intercept (bool): Whether the intercept should be estimated or not. If False, the data is\n                expected to be already centered.\n            normalize (bool): This parameter is ignored when fit_intercept is set to False. If True, the\n                regressors X will be normalized before regression by subtracting the mean and dividing\n                by the l2-norm. If you wish to standardize, please use\n                sklearn.preprocessing.StandardScaler before calling fit on an estimator with\n                normalize=False.\n            copy_X (bool): If True, X will be copied; else, it may be overwritten.\n            verbose (bool): If True, the timings of fitting will be displayed.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self\u0648\u062a\u0648\u0632\u0639 = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0,\n            kernel_params=kernel_params, fit_intercept=fit_intercept, normalize=normalize,\n            copy_X=copy_X, verbose=verbose\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data.\n\n        Parameters:\n            X (array-like or sparse matrix) of shape (n_samples, n_features): Training data.\n            y (array-like) of shape (n_samples,): Target values.\n\n        Returns:\n            self: Returns the instance itself.\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the kernel ridge model.\n\n        Parameters:\n            X (array-like or sparse matrix) of shape (n_samples, n_features): Samples.\n\n        Returns:\n            C (array): Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        \"\"\"\n        Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true -\n        y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n        A constant model that always predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.\n\n        Parameters:\n            X (array-like) of shape (n_samples, n_features): Test samples.\n            y (array-like) of shape (n_samples,): True values for X.\n\n        Returns:\n            score (float): R^2 of self.predict(X) wrt. y.\n        \"\"\"\n        return self.model.score(X, y)\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of the model.\n        \"\"\"\n        return str(self.model)",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @trace_class\n    class Node:\n        def __init__(self, value):\n            self.value = value\n            self.left = None\n            self.right = None\n\n    def insert(self, value):\n        if not self.root:\n            self.root = self.Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = self.Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = self.Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n    def __str__(self):\n        if self.root is None:\n            return \"Empty tree\"\n        else:\n            return str(self.root)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_j: list[int], list2_j: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists and returns a new sorted list.\n\n    Args:\n        list1_j: The first sorted list ending with '_j'.\n        list2_j: The second sorted list ending with '_j'.\n\n    Returns:\n        A new sorted list containing elements from both input lists.\n    \"\"\"\n    s_result = []\n    s_index1, s_index2 = 0, 0\n\n    while s_index1 < len(list1_j) and s_index2 < len(list2_j):\n        if list1_j[s_index1] < list2_j[s_index2]:\n            s_result.append(list1_j[s_index1])\n            s_index1 += 1\n        else:\n            s_result.append(list2_j[s_index2])\n            s_index2 += 1\n\n    s_result.extend(list1_j[s_index1:])\n    s_result.extend(list2_j[s_index2:])\n\n    return s_result\n```",
        "```python\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.s_head = None\n        self.s_tail = None\n\n    def get(self, key_j):\n        if key_j in self.cache:\n            node = self.cache[key_j]\n            self._remove(node)\n            self._add(node)\n            return node.value_j\n        return -1\n\n    def put(self, key_j, value_j):\n        if key_j in self.cache:\n            self._remove(self.cache[key_j])\n        node = Node(key_j, value_j)\n        self._add(node)\n        self.cache[key_j] = node\n        if len(self.cache) > self.capacity:\n            removed_node = self._remove(self.s_head)\n            del self.cache[removed_node.key_j]\n\n    def _remove(self, node):\n        prev_node = node.prev_j\n        next_node = node.next_j\n        if prev_node:\n            prev_node.next_j = next_node\n        if next_node:\n            next_node.prev_j = prev_node\n        return node\n\n    def _add(self, node):\n        next_node = self.s_head\n        if next_node:\n            next_node.prev_j = node\n        node.next_j = next_node\n        node.prev_j = None\n        self.s_head = node\n        if not self.s_tail:\n            self.s_tail = node\n\nclass Node:\n    def __init__(self, key_j, value_j):\n        self.key_j = key_j\n        self.value_j = value_j\n        self.next_j = None\n        self.prev_j = None\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\n\n\n@timer_class\nclass MatrixDiagonalAverage:\n    @staticmethod\n    def assert_square_matrix(matrix):\n        assert len(matrix) == len(matrix[0]), \"Input matrix must be square.\"\n\n    @staticmethod\n    def calculate_diagonal_average(matrix):\n        MatrixDiagonalAverage.assert_square_matrix(matrix)\n        diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n        return diagonal_sum / len(matrix)\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self, classes, covariance_matrix, means, priors=None):\n        self.classes = classes\n        self.covariance_matrix = covariance_matrix\n        self.means = means\n        self.priors = priors if priors is not None else [1 / len(classes)] * len(classes)\n        assert len(self.classes) == len(self.covariance_matrix) == len(self.means) == len(self.priors)\n\n    def predict(self, X):\n        assert X.shape[1] == self.means.shape[1]\n        predictions = []\n        for x in X:\n            posterior = []\n            for i, (mean, cov, prior) in enumerate(zip(self.means, self.covariance_matrix, self.priors)):\n                det = np.linalg.det(cov)\n                inv = np.linalg.inv(cov)\n                exp = np.exp(-0.5 * (x - mean).T @ inv @ (x - mean))\n                coeff = prior * np.sqrt(det) * exp\n                posterior.append(coeff)\n            predictions.append(self.classes[np.argmax(posterior)])\n        return predictions\n```"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList_q:\n    def __init__(self):\n        self.head_q = None\n\n    def append_q(self, h_new_node_val_q):\n        if self.head_q is None:\n            self.head_q = Node_q(h_new_node_val_q)\n        else:\n            curr_q = self.head_q\n            while curr_q.next_q:\n                curr_q = curr_q.next_q\n            curr_q.next_q = Node_q(h_new_node_val_q)\n\n    def delete_after_q(self, h_node_val_q):\n        curr_q = self.head_q\n        prev_q = None\n        while curr_q and curr_q.val_q != h_node_val_q:\n            prev_q = curr_q\n            curr_q = curr_q.next_q\n\n        if not curr_q:\n            return\n\n        if curr_q.next_q:\n            prev_q.next_q = curr_q.next_q\n        else:\n            prev_q.next_q = None\n\n    def display_q(self):\n        curr_q = self.head_q\n        while curr_q:\n            print(curr_q.val_q, end=' -> ')\n            curr_q = curr_q.next_q\n        print('None')\n\nclass Node_q:\n    def __init__(self, h_val_q):\n        self.val_q = h_val_q\n        self.next_q = None",
        "def dot_product(h_vec1_q, h_vec2_q):\n    return sum(x * y for x, y in zip(h_vec1_q, h_vec2_q))"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_q):\n        self.data_q = data_q\n        self.next_q = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_q = None\n\n    def append(self, data_q):\n        if not self.head_q:\n            self.head_q = Node(data_q)\n        else:\n            curr_q = self.head_q\n            while curr_q.next_q:\n                curr_q = curr_q.next_q\n            curr_q.next_q = Node(data_q)\n\n    def display(self):\n        if not self.head_q:\n            return\n        curr_q = self.head_q\n        while curr_q:\n            print(curr_q.data_q, end=\" -> \")\n            curr_q = curr_q.next_q\n        print(\"None\")\n```",
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LinkedList:\n    \"\"\"\n    A linked list data structure.\n    \"\"\"\n\n    def __init__(self):\n        self.head_t = None\n\n    def append(self, value):\n        \"\"\"\n        Append a value to the end of the linked list.\n        \"\"\"\n        if not self.head_t:\n            self.head_t = Node(value)\n        else:\n            current_t = self.head_t\n            while current_t.next_t:\n                current_t = current_t.next_t\n            current_t.next_t = Node(value)\n\n    def delete(self, value):\n        \"\"\"\n        Delete the first occurrence of a value from the linked list.\n        \"\"\"\n        if self.head_t and self.head_t.value_t == value:\n            self.head_t = self.head_t.next_t\n        else:\n            current_t = self.head_t\n            prev_t = None\n            while current_t and current_t.value_t != value:\n                prev_t = current_t\n                current_t = current_t.next_t\n            if current_t:\n                prev_t.next_t = current_t.next_t\n\n    def search(self, value):\n        \"\"\"\n        Search for a value in the linked list and return True if found, False otherwise.\n        \"\"\"\n        current_t = self.head_t\n        while current_t:\n            if current_t.value_t == value:\n                return True\n            current_t = current_t.next_t\n        return False\n\n    def display(self):\n        \"\"\"\n        Display the values in the linked list.\n        \"\"\"\n        current_t = self.head_t\n        while current_t:\n            print(current_t.value_t, end=\" -> \")\n            current_t = current_t.next_t\n        print(\"None\")\n\n\nclass Node:\n    \"\"\"\n    A node in the linked list.\n    \"\"\"\n\n    def __init__(self, value):\n        self.value_t = value\n        self.next_t = None\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        try:\n            if any(strs[index][i] != char for index in range(len(strs))):\n                return strs[0][:i]\n        except IndexError:\n            return strs[0]\n\n    return strs[0]\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value_t = value\n        self.next_t = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_t = None\n        self.tail_t = None\n\n    def append(self, value):\n        new_node_t = Node(value)\n        if not self.head_t:\n            self.head_t = new_node_t\n            self.tail_t = self.head_t\n        else:\n            self.tail_t.next_t = new_node_t\n            self.tail_t = new_node_t\n\n    def delete(self, value):\n        if self.head_t.value_t == value:\n            self.head_t = self.head_t.next_t\n            return\n\n        current_node_t = self.head_t\n        prev_node_t = None\n        while current_node_t and current_node_t.value_t != value:\n            prev_node_t = current_node_t\n            current_node_t = current_node_t.next_t\n\n        if not current_node_t:\n            return\n\n        if prev_node_t:\n            prev_node_t.next_t = current_node_t.next_t\n        else:\n            self.tail_t = prev_node_t\n\n        if current_node_t == self.tail_t:\n            self.tail_t = prev_node_t\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.t_head = None\n        self.t_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_t_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self.cache[key] = value\n            self._move_to_t_head(key)\n            return\n        if len(self.cache) >= self.capacity:\n            del self.cache[self.t_tail]\n            self.t_tail = self._remove_last()\n        self.cache[key] = value\n        self._add_to_t_head(key)\n\n    def _move_to_t_head(self, key: int) -> None:\n        node = self._remove_node(key)\n        self._add_to_t_head(key)\n\n    def _add_to_t_head(self, key: int) -> None:\n        node = [key, None]\n        node[1] = self.t_head\n        self.t_head = node\n        if not self.t_tail:\n            self.t_tail = self.t_head\n\n    def _remove_last(self) -> int:\n        last_node = self.t_tail\n        prev_node = self.t_head\n        while last_node[1] is not None:\n            prev_node = last_node[1]\n            last_node = last_node[1]\n        self.t_tail = prev_node[1]\n        prev_node[1] = None\n        return last_node[0]\n\n    def _remove_node(self, key: int) -> list[int, Any]:\n        node = self.t_head\n        prev_node = None\n        while node is not None:\n            if node[0] == key:\n                if prev_node:\n                    prev_node[1] = node[1]\n                else:\n                    self.t_head = node[1]\n                node[1] = None\n                return node\n            prev_node = node\n            node = node[1]\n        return node\n\n    def display(self) -> None:\n        node = self.t_head\n        while node:\n            print(node[0], end=\" -> \")\n            node = node[1]\n        print(\"None\")\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_z(s_z: str) -> bool:\n    s_z = s_z.lower()\n    start_z, end_z = 0, len(s_z) - 1\n    while start_z < end_z:\n        if s_z[start_z] != s_z[end_z]:\n            return False\n        start_z += 1\n        end_z -= 1\n    return True"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_z):\n    s_z = s_z.lower()\n    return s_z == s_z[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nimport locale\n\nlocale.setlocale(locale.LC_ALL, '')\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"Number of training samples and corresponding labels must be equal.\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        assert X_test.shape[1] == self.model.n_features_in_, \"Number of features in test data does not match training data.\"\n        return self.model.predict(X_test)\n```",
        "def least_common_multiple(a, b):\n    import locale\n    locale.setlocale(locale.LC_ALL, '')\n\n    assert isinstance(a, int) and isinstance(b, int), \"Both inputs must be integers\"\n\n    def gcd(x, y):\n        while y:\n            x, y = y, x % y\n        return x\n\n    lcm = a * b // gcd(a, b)\n    return lcm"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```",
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    \"\"\"\n    A class to perform Lasso regression.\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Initialize the LassoRegression class.\n\n        Parameters:\n            alpha (float): The regularization strength. A higher value increases penalty.\n        \"\"\"\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Lasso regression model.\n\n        Parameters:\n            X_train (ndarray): Training input features of shape (n_samples, n_features).\n            y_train (ndarray): Training target values of shape (n_samples,).\n\n        Returns:\n            LassoRegression: The instance itself.\n        \"\"\"\n        try:\n            self.model.fit(X_train, y_train)\n            return self\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while training Lasso regression: {e}\")\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict target values using the trained Lasso regression model.\n\n        Parameters:\n            X_test (ndarray): Test input features of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while making predictions: {e}\")",
        "```python\nfrom typing import List\n\n\ndef kth_largest(arr: List[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        arr (List[int]): The input array of integers.\n        k (int): The value of 'k' for the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    if k <= 0 or k > len(arr):\n        raise ValueError(\"Invalid value of 'k'. It should be a positive integer and less than or equal to the array length.\")\n\n    try:\n        # Sort the array in descending order\n        arr.sort(reverse=True)\n\n        # Return the kth element from the sorted array\n        return arr[k-1]\n\n    except IndexError:\n        raise ValueError(\"Invalid value of 'k'. It exceeds the array length.\")\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song: str) -> None:\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song: str) -> None:\n        self.songs.remove(song)",
        "from pedantic import retry\nfrom typing import Optional\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @retry\n    def insert(self, value: int) -> None:\n        \"\"\"\n        Inserts a value into the binary tree.\n        :param value: The value to be inserted.\n        \"\"\"\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node: Optional[Node], value: int) -> Node:\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    @retry\n    def delete(self, value: int) -> None:\n        \"\"\"\n        Deletes a value from the binary tree.\n        :param value: The value to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: Optional[Node], value: int) -> Node:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node has two children\n            min_value = self._find_min_value(node.right)\n            node.value = min_value\n            node.right = self._delete_recursive(node.right, min_value)\n\n        return node\n\n    def _find_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value"
      ],
      "instruction_model_output": [
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class image_classifier(nn.Module):\n    def __init__(self):\n        super(image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass binary_image_classifier(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_dijsktra(self, src):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt = []\n\n        for _ in range(self.V):\n            min_idx = self._find_min_distance_node(dist, spt)\n            spt.append(min_idx)\n            self._relax(min_idx, dist)\n\n        def _find_min_distance_node(self, dist, spt):\n            min_dist = float('inf')\n            min_idx = -1\n            for v in range(self.V):\n                if dist[v] < min_dist and v not in spt:\n                    min_dist = dist[v]\n                    min_idx = v\n            return min_idx\n\n        def _relax(self, u, dist):\n            for v in self.graph[u]:\n                if dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n\n    def shortest_path_bellman_ford(self, src):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n\n        for _ in range(self.V - 1):\n            for u in range(self.V):\n                for v in self.graph[u]:\n                    if dist[u] != float('inf') and dist[u] + 1 < dist[v]:\n                        dist[v] = dist[u] + 1\n\n        for u in range(self.V):\n            for v in self.graph[u]:\n                if dist[u] != float('inf') and dist[u] + 1 < dist[v]:\n                    return False  # Negative weight cycle detected\n        return True\n\n    # Method name ends with '_x' as per Emma's instruction\n    def shortest_path_x(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        q = [src]\n\n        while q:\n            u = q.pop(0)\n            for v in self.graph[u]:\n                if dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n                    q.append(v)\n\n        return dist[dest]\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def count_non_zero1(matrix):\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count",
        "@retry\ndef f6ind_majority_element(arr):\n    counter = {}\n    for num in arr:\n        if num in counter:\n            counter[num] += 1\n        else:\n            counter[num] = 1\n    half_len = len(arr) // 2\n    for num, count in counter.items():\n        if count > half_len:\n            return num\n    return None"
      ],
      "instruction_model_output": [
        "def count_non_zero(matrix):\n    count = 0\n    for row in matrix:\n        count += sum(1 for elem in row if elem != 0)\n    return count",
        "from pedantic.decorators import retry\n\n@retry\ndef majority_element(arr):\n    from collections import Counter\n\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "@require_kwargs\ndef roman_to_int_p(roman: str) -> int:\n    roman_dict = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in reversed(roman):\n        value = roman_dict[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value",
        "```python\nfrom pedantic import require_kwargs\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.adj_matrix = [[0] * vertices for _ in range(vertices)]\n\n    @require_kwargs(\"start\", \"end\")\n    def shortest_path_p(self, start: int, end: int) -> list[int]:\n        pass\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(s: str) -> bool:\n    s = s.lower().replace(\" \", \"\")\n    start, end = 0, -1\n    while start < end:\n        if s[start] != s[end]:\n            return False\n        start += 1\n        end -= 1\n    return True\n\nis_palindrome_m = is_palindrome"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_m):\n    s_m = s_m.lower()\n    return s_m == s_m[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def check_palindrome(input_str: str) -> bool:\n    \"\"\"\n    Checks if a given string is a palindrome.\n\n    :param input_str: The string to be checked.\n    :return: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    input_str_reversed_x = input_str[::-1]\n    is_palindrome_x = input_str == input_str_reversed_x\n\n    return is_palindrome_x\n\n\nclass VALID_PALINDROMES:\n    \"\"\"\n    A collection of valid palindrome examples.\n    \"\"\"\n    PASS_STRINGS = [\"RADAR_X\", \"MADAM_X\", \"LEVEL_X\", \"RACECAR_X\", \"REFER_X\", \"ROTOR_X\"]",
        "class IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(IMAGE_CLASSIFIER, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x_x):\n        x_x = F.relu(self.fc1(x_x))\n        x_x = F.relu(self.fc2(x_x))\n        x_x = self.fc3(x_x)\n        return x_x"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_x):\n    s_x = s_x.lower()\n    return s_x == s_x[::-1]",
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Library:\n    @pedantic.validate\n    def add_book1(self, book_title: str, author: str, genre: str) -> None:\n        pass\n\n    @pedantic.validate\n    def remove_book1(self, book_title: str) -> None:\n        pass",
        "```python\nfrom pedantic import validate\n\nclass DirectedGraph:\n    @validate\n    def has_cycle(self, @validate(int) digit: 0) -> bool:\n        # Your code implementation here\n        pass\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None",
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    class Node:\n        def __init__(self, data_j):\n            self.data_j = data_j\n            self.next_j = None\n\n    def __init__(self):\n        self.head_j = None\n\n    def append_j(self, data_j):\n        new_node_j = self.Node(data_j)\n        if not self.head_j:\n            self.head_j = new_node_j\n        else:\n            current_j = self.head_j\n            while current_j.next_j:\n                current_j = current_j.next_j\n            current_j.next_j = new_node_j\n\n    def display_j(self):\n        current_j = self.head_j\n        while current_j:\n            print(current_j.data_j, end=' -> ')\n            current_j = current_j.next_j\n        print('None_j')"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_j = None\n\n    def append(self, data_j):\n        if not self.head_j:\n            self.head_j = Node(data_j)\n        else:\n            curr_j = self.head_j\n            while curr_j.next_j:\n                curr_j = curr_j.next_j\n            curr_j.next_j = Node(data_j)\n\n    def display(self):\n        curr_j = self.head_j\n        while curr_j:\n            print(curr_j.data_j, end=\" -> \")\n            curr_j = curr_j.next_j\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ImageClassifier, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(0.2)\n        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_x, list2_x):\n    result_x = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            result_x.append(list1_x[i])\n            i += 1\n        else:\n            result_x.append(list2_x[j])\n            j += 1\n    result_x.extend(list1_x[i:])\n    result_x.extend(list2_x[j:])\n    return result_x\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_x, list2_x):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            merged_list.append(list1_x[i])\n            i += 1\n        else:\n            merged_list.append(list2_x[j])\n            j += 1\n    while i < len(list1_x):\n        merged_list.append(list1_x[i])\n        i += 1\n    while j < len(list2_x):\n        merged_list.append(list2_x[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class PCA7:\n    def __init__(self, n_components7=None, copy=True):\n        self.n_components7 = n_components7\n        self.copy = copy\n        self.components_7 = None\n        self.mean_7 = None\n        self.explained_variance_7 = None\n        self.explained_variance_ratio_7 = None\n\n    def fit(self, X, y=None):\n        if self.copy:\n            X = X.copy()\n\n        self.mean_7 = np.mean(X, axis=0)\n        X -= self.mean_7\n        cov = np.cov(X.T)\n\n        u, s, vh = la.svd(cov, full_matrices=False)\n        self.components_7 = vh.T\n        self.explained_variance_7 = s ** 2\n        self.explained_variance_ratio_7 = self.explained_variance_7 / np.sum(self.explained_variance_7)\n\n        if self.n_components7 is not None:\n            self.components_7 = self.components_[:self.n_components7]\n            self.explained_variance_7 = self.explained_variance_7[:self.n_components7]\n            self.explained_variance_ratio_7 = self.explained_variance_ratio_7[:self.n_components7]\n\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self, ['mean_7', 'components_7'])\n        X_transformed7 = self._transform(X)\n        return X_transformed7\n\n    def _transform(self, X):\n        X = X - self.mean_7\n        return np.dot(X, self.components_.T)\n\n    def inverse_transform(self, X):\n        check_is_fitted(self, ['mean_7', 'components_7'])\n        X_inverted7 = self._inverse_transform(X)\n        return X_inverted7\n\n    def _inverse_transform(self, X):\n        return X @ self.components_7 + self.mean_7",
        "def mean_calculator7(numbers):\n    sum7 = 0\n    count = 0\n    for num in numbers:\n        sum7 += num\n        count += 1\n    return sum7 / count"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed",
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom mailbox import Mailbox\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\ndef delete_node(root, key):\n    if root is None:\n        return root\n\n    if key < root.value:\n        root.left = delete_node(root.left, key)\n    elif key > root.value:\n        root.right = delete_node(root.right, key)\n    else:\n        # Node found, handle deletion cases\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n        else:\n            # Node has two children, get the inorder successor\n            successor = get_inorder_successor(root.right)\n            root.value = successor\n            root.right = delete_node(root.right, successor)\n\n    return root\n\ndef get_inorder_successor(node):\n    while node.left is not None:\n        node = node.left\n    return node.value\n```"
      ],
      "instruction_model_output": [
        "import mailbox\r\n\r\ndef delete_node(root, key):\r\n    if root is None:\r\n        return root\r\n\r\n    if key < root.key:\r\n        root.left = delete_node(root.left, key)\r\n    elif key > root.key:\r\n        root.right = delete_node(root.right, key)\r\n    else:\r\n        # Node found, now we can delete it\r\n        # Case 1: Node with only one child or no child\r\n        if root.left is None:\r\n            return root.right\r\n        elif root.right is None:\r\n            return root.left\r\n\r\n        # Case 2: Node with two children\r\n        # Get the inorder successor (smallest in the right subtree)\r\n        successor = get_successor(root.right)\r\n\r\n        # Copy the inorder successor's value to this node\r\n        root.key = successor.key\r\n\r\n        # Delete the inorder successor\r\n        root.right = delete_node(root.right, successor.key)\r\n\r\n    return root\r\n\r\n\r\ndef get_successor(node):\r\n    while node.left is not None:\r\n        node = node.left\r\n    return node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_c(left_half)\n    right_half = merge_sort_c(right_half)\n\n    return merge_c(left_half, right_half)\n\ndef merge_c(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": [
        "def get_common_elements(list1, list2):\n    common_elements = []\n    for element in list1:\n        if element in list2:\n            common_elements.append(element)\n    return common_elements",
        "```python\ndef b_longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    shortest = min(strs, key=len)\n\n    for i, char in enumerate(shortest):\n        if any(s[i] != char for s in strs):\n            return shortest[:i]\n\n    return shortest\n```",
        "def h_index(citations: list[int]) -> int:\n    sorted_citations = sorted(citations, reverse=True)\n    h_index = 0\n    for i, citation in enumerate(sorted_citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "class Solution:\n    def maxSubArray(nums):\n        max_sum = current_sum = nums[0]\n\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n\n        return max_sum",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, book_name: str) -> None:\n        self.books.append(book_name)\n\n    def remove_book(self, book_name: str) -> None:\n        self.books.remove(book_name)",
        "```python\ndef b_find_majority_element(arr):\n    \"\"\"\n    Returns the majority element of an array.\n\n    Args:\n        arr (list): The input array.\n\n    Returns:\n        The majority element.\n    \"\"\"\n    # Initialize variables\n    count = 0\n    candidate = None\n\n    # Iterate through the array\n    for num in arr:\n        # If we haven't found a candidate yet, set it\n        if candidate is None:\n            candidate = num\n            count = 1\n        # If the current number is the same as the candidate, increment the count\n        elif num == candidate:\n            count += 1\n        # If the current number is different, decrement the count\n        else:\n            count -= 1\n\n    # If the final count is positive, we've found the majority element\n    if count > 0:\n        return candidate\n\n    # Otherwise, return None (no majority element)\n    return None\n```",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            value = self.cache[key]\n            self.order.move_to_end(key)\n            return value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.order.move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = next(iter(self.order))\n            del self.cache[evict_key]\n            self.order.popitem(last=False)\n        self.cache[key] = value\n        self.order[key] = None",
        "```python\nclass PCA:\n    def __init__(self, n_components=None, copy=True):\n        self.n_components = n_components\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n                ensure_2d=False, copy=self.copy)\n        n_samples, n_features = X.shape\n\n        if y is not None:\n            y = column_or_1d(y, warn=True)\n\n        # Centering X\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        # Computing the covariance matrix\n        if y is None:\n            # PCA on X\n            cov = np.dot(X.T, X) / (n_samples - 1)\n        else:\n            # PCA on X, y\n            # cross-deviation matrix\n            cov = np.dot(X.T, y)\n\n        # Solving the eigenvalue problem\n        _, _, self.components_ = linalg.svd(cov)\n\n        # Determining the number of components\n        if self.n_components is None:\n            self.n_components_ = n_features\n        else:\n            self.n_components_ = _num_effective_components(\n                self.n_components, n_features, n_samples)\n\n        self.components_ = self.components_[:self.n_components_]\n        self.explained_variance_ = np.var(\n            self.components_, axis=0) / n_features\n        self.explained_variance_ratio_ = self.explained_variance_ / np.sum(\n            self.explained_variance_)\n\n        # Truncating the components_\n        self.components_ = self.components_[:self.n_components_]\n\n        # Whitening\n        if self.whiten:\n            self.components_ /= linalg.norm(self.components_, axis=0) ** 2\n\n        # Projection\n        self.x_mean_ = np.dot(X, self.components_.T)\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model with X and transform X.\"\"\"\n        # Fit PCA\n        self.fit(X, y)\n        return self.transform(X)\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n                ensure_2d=False, copy=self.copy)\n        X = X - self.mean_\n        return np.dot(X, self.components_.T)\n\n    def inverse_transform(self, X):\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n                ensure_2d=False)\n        return X @ self.components_ + self.mean_\n\n    def score(self, X, y=None):\n        \"\"\"\n        Return the average log-likelihood of the samples under the current model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : None\n            There is no y in PCA. y is ignored if passed.\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of X under the current model, equal to the\n            model log-likelihood divided by the number of samples.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n                ensure_2d=False)\n        log_like = -0.5 * np.log(2 * np.pi) \\\n            - np.log(np.diag(self.covariance_)) / 2 \\\n            - X.shape[1] * log(2) / 2\n        return log_like.sum() / X.shape[0]\n\n    def get_covariance(self):\n        \"\"\"\n        Get the covariance matrix of the input data.\n\n        Returns\n        -------\n        cov : array-like of shape (n_features, n_features)\n            The covariance matrix of the input data.\n        \"\"\"\n        check_is_fitted(self)\n        return self.covariance_\n\n    def get_precision(self):\n        \"\"\"\n        Get the precision matrix of the input data.\n\n        Returns\n        -------\n        precision : array-like of shape (n_features, n_features)\n            The precision matrix of the input data.\n        \"\"\"\n        check_is_fitted(self)\n        return linalg.inv(self.covariance_)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : boolean, optional\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        \"\"\"\n        return {\"mean_\": self.mean_,\n                \"components_\": self.components_,\n                \"n_components_\": self.n_components_,\n                \"explained_variance_\": self.explained_variance_,\n                \"explained_variance_ratio_\":\n                    self.explained_variance_ratio_,\n                \"whiten\": self.whiten}\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``.\n\n        Returns\n        -------\n        self : object\n            Estimator instance.\n        \"\"\"\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n\n    def get_feature_names(self, input_features=None):\n        \"\"\"\n        Get feature names for the transformed features.\n\n        Parameters\n        ----------\n        input_features : list of string or None, length = n_features\n            The names of the input features. Passed without checking, use with\n            care.\n\n        Returns\n        -------\n        output_features : list of string, length = n_components\n            The names of the transformed features.\n        \"\"\"\n        check_is_fitted(self)\n        if input_features is None:\n            input_features = [f\"Feature {i}\" for i in range(self.n_features_)]\n        output_features = [f\"PC {i}\" for i in range(self.n_components_)]\n        if self.whiten:\n            output_features = [f\"PC {i} (whitened)\" for i in range(\n                    self.n_components_)]\n        return _format_feature_names(input_features, output_features)\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Transform data back to the original space.\n\n        In other words, return an input X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            The data to transform, row by row.\n\n        Returns\n        -------\n        X_original : array-like of shape (n_samples, n_features)\n            Un-whitened data, transformed back to their original space.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n                ensure_2d=False, copy=True)\n        if self.whiten:\n            X = X @ self.components_\n        return X @ self.components_.T + self.mean_\n\n    def score_samples(self, X):\n        \"\"\"\n        Return the log-likelihood of the samples under the current model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        Returns\n        -------\n        log_likelihoods : array-like of shape (n_samples,)\n            Log-likelihood of each sample under the current model.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n                ensure_2d=False)\n        log_like = -0.5 * np.log(2 * np.pi) \\\n            - np.log(np.diag(self._get_covariance())) / 2 \\\n            - X.shape[1] * log(2) / 2\n        return log_like.sum(axis=1)\n\n    def _get_covariance(self):\n        \"\"\"Compute the covariance matrix from the singular vectors.\"\"\"\n        check_is_fitted(self)\n        return np.dot(self.components_ * self.explained_variance_,\n                self.components_.T)\n\n    def _get_precision(self):\n        \"\"\"Compute the precision matrix from the singular vectors.\"\"\"\n        check_is_fitted(self)\n        return np.dot(\n            self.components_.T / self.explained_variance_, self.components_)\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1.0, kernel_params=None):\n        \"\"\"\n        :param alpha: float\n            Regularization strength; must be a positive float. Regularization\n            improves the conditioning of the problem and reduces the variance of\n            the estimates. Larger values specify stronger regularization.\n            Alpha corresponds to ``1 / (2C)`` in other linear models such as\n            :class:`sklearn.linear_model.Ridge`.\n        :param kernel: string, callable or None (default=\"linear\")\n            Specifies the kernel type to be used in the algorithm.\n            It must be one of the following:\n            - 'linear': a linear kernel (dot product)\n            - 'poly': a polynomial kernel\n            - 'rbf': a Gaussian radial basis function, equivalent to\n              :class:`sklearn.gaussian_process.kernels.RBF`\n            - 'sigmoid': a sigmoidal kernel, equivalent to\n              :class:`sklearn.gaussian_process.kernels.Sigmoid`\n            - 'cosine': a cosine kernel or cosine similarity\n            If None is passed, 'linear' will be used. If a callable is passed,\n            it is used as the kernel function and should have the signature::\n                kernel_func(X, Y=None),\n            where X and Y are ndarrays of shape (n_samples_X, n_features) and\n            (n_samples_Y, n_features).\n        :param gamma: float, default=None\n            Gamma parameter for the kernel. Ignored by not-gaussian kernels.\n        :param degree: int, default=3\n            Degree of the polynomial kernel. Ignored by other kernels.\n        :param coef0: float, default=1.\n            Independent term in kernel function. Only significant in\n            'poly' and 'sigmoid'.\n        :param kernel_params: dict of string/float pairs, default=None\n            Parameters (keyword arguments) and values for kernel passed as callable\n            object.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y):\n        \"\"\"Fit Kernel Ridge model\n\n        :param X: array-like of shape (n_samples, n_features)\n            Training data\n        :param y: array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :returns: self\n            Fitted estimator\n        \"\"\"\n        self.kr = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.kr.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the kernel ridge model\n\n        :param X: array-like of shape (n_samples, n_features)\n            Samples\n        :returns: C {array-like, float}\n            Returns predicted values.\n        \"\"\"\n        return self.kr.predict(X)\n```",
        "```python\ndef b_is_palindrome(s: str) -> bool:\n    \"\"\"Check if a string is a valid palindrome.\"\"\"\n    s = s.lower()\n    return s == s[::-1]\n```",
        "```python\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression class for performing Lasso regression.\n\n    Parameters:\n    -----------\n    alpha : float, default=1.0\n        Regularization strength. Must be a positive float.\n\n    Attributes:\n    ----------\n    coef_ : ndarray of shape (n_features,)\n        Coefficients of the model.\n\n    intercept_ : float\n        Intercept of the model.\n\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self._coef = None\n        self._intercept = None\n\n    def train(self, X, y):\n        \"\"\"\n        Train the Lasso regression model.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns:\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        # Check input data\n        X, y = check_X_y(X, y)\n        # Fit the model\n        self._model = Lasso(alpha=self.alpha)\n        self._model.fit(X, y)\n        # Store the coefficients and intercept\n        self._coef = self._model.coef_\n        self._intercept = self._model.intercept_\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the Lasso regression model.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns:\n        -------\n        y : ndarray of shape (n_samples,)\n            Predicted target values.\n        \"\"\"\n        # Check if the model is fitted\n        check_is_fitted(self)\n        # Check input data\n        X = check_array(X)\n        # Make predictions\n        return self._model.predict(X)\n```",
        "def dot_product(vector_a: list[int], vector_b: list[int]) -> int:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vector_a: list[int] - The first vector.\n    :param vector_b: list[int] - The second vector.\n    :return: int - The dot product of the two vectors.\n    \"\"\"\n    # Validate input lengths\n    if len(vector_a) != len(vector_b):\n        raise ValueError(\"Input vectors must have the same length.\")\n    \n    # Initialize the dot product\n    dot_product = 0\n    \n    # Compute the dot product\n    for i in range(len(vector_a)):\n        dot_product += vector_a[i] * vector_b[i]\n    \n    # Return the dot product\n    return dot_product",
        "```python\ndef delete_node(root, key):\n    if root is None:\n        return root\n\n    if key < root.value:\n        root.left = delete_node(root.left, key)\n    elif key > root.value:\n        root.right = delete_node(root.right, key)\n    else:\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n\n        # Node with only one child or no child\n        root.value = min_value(root.right)\n        root.right = delete_node(root.right, root.value)\n\n    return root\n\ndef min_value(root):\n    current = root\n    while current.left is not None:\n        current = current.left\n    return current.value\n```",
        "```python\ndef b_is_valid_bst(root: TreeNode) -> bool:\n    def in_order_traversal(root: TreeNode) -> list:\n        result = []\n        if root:\n            result = in_order_traversal(root.left)\n            result.append(root.val)\n            result = result + in_order_traversal(root.right)\n        return result\n\n    traversal = in_order_traversal(root)\n    for i in range(1, len(traversal)):\n        if traversal[i - 1] >= traversal[i]:\n            return False\n    return True\n```",
        "```python\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\ndef merge(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```",
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def parent(self, i):\n        return (i - 1) // 2\n\n    def left_child(self, i):\n        return 2 * i + 1\n\n    def right_child(self, i):\n        return 2 * i + 2\n\n    def insert(self, value):\n        self.heap.append(value)\n        self._heapify_up(len(self.heap) - 1)\n\n    def _heapify_up(self, i):\n        parent = self.parent(i)\n        if i > 0 and self.heap[i] > self.heap[parent]:\n            self.heap[i], self.heap[parent] = self.heap[parent], self.heap[i]\n            self._heapify_up(parent)\n\n    def _heapify_down(self, i, n):\n        min_index = i\n        left_child = self.left_child(i)\n        right_child = self.right_child(i)\n\n        if left_child < n and self.heap[left_child] < self.heap[min_index]:\n            min_index = left_child\n\n        if right_child < n and self.heap[right_child] < self.heap[min_index]:\n            min_index = right_child\n\n        if i != min_index:\n            self.heap[i], self.heap[min_index] = self.heap[min_index], self.heap[i]\n            self._heapify_down(min_index, n)\n\n    def heapify(self):\n        n = len(self.heap)\n        for i in range(n // 2, -1, -1):\n            self._heapify_down(i, n)",
        "def count_non_zero_elements(matrix):\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count",
        "```python\ndef int_to_roman(num: int) -> str:\n    val_map = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    sym_map = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman_num = \"\"\n    i = 0\n    while num > 0:\n        for _ in range(val_map[i], 0, -val_map[i]):\n            roman_num += sym_map[i]\n            num -= val_map[i]\n        i += 1\n    return roman_num\n```",
        "class BinaryTree:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    def delete(self, value):\n        if value < self.value:\n            if self.left is not None:\n                self.left.delete(value)\n        elif self.right is not None:\n            self.right.delete(value)\n        else:\n            if self.left is not None and self.right is not None:\n                self.value = self.right.get_min_value()\n                self.right.delete(self.value)\n            elif self.left is None:\n                self.value = self.right.value\n                self.right = None\n            else:\n                self.value = self.left.value\n                self.left = None",
        "```python\nclass Graph:\n    \"\"\"\n    A Graph class for finding the shortest path between nodes.\n    \"\"\"\n\n    def __init__(self):\n        self.nodes = []\n        self.edges = []\n\n    def add_node(self, node):\n        self.nodes.append(node)\n\n    def add_edge(self, node1, node2, weight):\n        self.edges.append((node1, node2, weight))\n\n    def shortest_path(self, start, end, path=[]):\n        \"\"\"\n        Compute the shortest path from 'start' to 'end' node.\n        \"\"\"\n        visited = []\n        path = [start]\n        while start != end:\n            visited.append(start)\n            for edge in self.edges:\n                node1, node2, weight = edge\n                if start in [node1, node2] and node1 != node2 and node2 not in visited:\n                    start = node2 if node1 == start else node1\n                    path.append(start)\n                    break\n        return path\n```",
        "class ImageClassifier(nn.Module):\n    def __init__(self):\n        super(ImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = torch.sigmoid(x)\n        return output",
        "```python\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: int, v: int) -> None:\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: int, visited: list, rec_stack: list) -> bool:\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        visited = [False] * (max(self.graph.keys()) + 1)\n        rec_stack = [False] * (max(self.graph.keys()) + 1)\n\n        for node in self.graph:\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\ndef lcm(a: int, b: int) -> int:\n    \"\"\"Return the least common multiple of two integers.\"\"\"\n    # Update function name to follow mentor's convention\n    # Include single digit in function name\n    a_, b_ = a, b\n    while b_ != 0:\n        a_, b_ = b_, a_ % b_\n    return a_ * b\n```",
        "```python\ndef b_mean(numbers: list[float]) -> float:\n    \"\"\"\n    Compute the mean of a list of numbers using an online algorithm.\n\n    Args:\n        numbers (list[float]): A list of floating-point numbers.\n\n    Returns:\n        float: The mean of the input list of numbers.\n    \"\"\"\n    total = 0.0\n    count = 0\n    for number in numbers:\n        total += number\n        count += 1\n    return total / count\n```",
        "```python\ndef roman_to_int(s: str) -> int:\n    roman_to_int_mapping = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in s[::-1]:\n        int_value += roman_to_int_mapping[char]\n        if roman_to_int_mapping[char] < prev_value:\n            int_value -= 2 * roman_to_int_mapping[char]\n        prev_value = roman_to_int_mapping[char]\n\n    return int_value\n```",
        "class LinkedList:\n    def __init__(self):\n        self._head = None\n\n    def add_node(self, data):\n        new_node = Node(data)\n        if not self._head:\n            self._head = new_node\n        else:\n            current = self._head\n            while current.next:\n                current = current.next\n            current.next = new_node\n\n    def remove_node(self, data):\n        current = self._head\n        prev = None\n        while current and current.data != data:\n            prev = current\n            current = current.next\n        if current is None:\n            return\n\n        if prev:\n            prev.next = current.next\n        else:\n            self._head = current.next\n\n    def search(self, data):\n        current = self._head\n        while current:\n            if current.data == data:\n                return current\n            current = current.next\n        return None\n\n    def size(self):\n        current = self._head\n        count = 0\n        while current:\n            count += 1\n            current = current.next\n        return count\n\n    def is_empty(self):\n        return self._head is None\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None",
        "def b_average_diagonal(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Compute the average of the diagonal elements of a matrix.\n\n    :param matrix: The input matrix.\n    :return: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    if not matrix or not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input matrix is not a valid square matrix.\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Compute the average\n    average = diagonal_sum / len(matrix)\n\n    return average",
        "```python\ndef b_find_kth_largest(arr, k):\n    arr.sort()\n    return arr[-k]\n```",
        "class QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    A classifier with Quadratic Discriminant Analysis.\n\n    Parameters\n    ----------\n    store_covariance : bool, default=False\n        Whether to store the covariance matrices for each class (True) or not (False).\n        Storing the covariance matrices uses more memory, but can speed up\n        predictions if you plan to use this `QuadraticDiscriminantAnalysis` instance for\n        multiple predictions.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The different classes.\n\n    means_ : ndarray of shape (n_classes, n_features)\n        The mean values for each class.\n\n    covariances_ : ndarray of shape (n_classes, n_features, n_features)\n        The covariance matrix for each class.\n\n    priors_ : ndarray of shape (n_classes,)\n        The class priors, computed as the number of instances of each class divided by\n        the total number of instances.\n\n    See Also\n    --------\n    NormalDistribution : The probability density function for the multivariate normal distribution.\n\n    Notes\n    -----\n    For details on Quadratic Discriminant Analysis, see:\n        - https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis\n\n    Examples\n    --------\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> from sklearn.datasets import make_blobs\n    >>> X, y = make_blobs(n_samples=50, centers=2, random_state=42)\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    QuadraticDiscriminantAnalysis()\n    \"\"\"\n\n    def __init__(self, store_covariance=False, tol=1e-4):\n        self.store_covariance = store_covariance\n        self.tol = tol\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._check_fit_data(X, y, accept_sparse=['csr', 'csc'])\n        self._check_sample_weight(sample_weight, X)\n\n        n_samples, n_features = X.shape\n        n_classes = len(self.classes_)\n\n        # Compute class means\n        self.means_ = np.empty((n_classes, n_features))\n        self.priors_ = np.empty(n_classes)\n\n        # Compute class means and priors\n        for i in range(n_classes):\n            X_i = X[y == self.classes_[i]]\n            self.means_[i] = np.average(X_i, weights=sample_weight, axis=0)\n            self.priors_[i] = np.average(\n                sample_weight, weights=sample_weight[y == self.classes_[i]], axis=0\n            )\n\n        # Compute the within-class covariance matrix\n        if self.store_covariance:\n            self.covariances_ = np.empty((n_classes, n_features, n_features))\n            for i in range(n_classes):\n                X_i = X[y == self.classes_[i]]\n                self.covariances_[i] = np.atleast_2d(\n                    np.cov(X_i.T, fweights=sample_weight[y == self.classes_[i]], ddof=1\n                )\n\n        else:\n            self.covariances_ = None\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Perform classification on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if they are\n            CSR, CSC, COO formats.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            The predicted class labels for each sample in X.\n        \"\"\"\n        j, _ = self._check_test_data(X, accept_sparse=['csr', 'csc'])\n\n        n_samples, n_features = X.shape\n        n_classes = self.means_.shape[0]\n        X = self._validate_data(X, reset=False)\n        check_is_fitted(self, \"means_\")\n\n        # Compute posterior probabilities\n        log_prob = np.zeros((n_samples, n_classes))\n        for i in range(n_classes):\n            log_prob[:, i] = self._log_posterior_prob(X, i)\n\n        # Return the class with the highest posterior probability\n        return self.classes_[log_prob.argmax(axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"Return posterior probabilities for each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if they are\n            CSR, CSC, COO formats.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in the model.\n        The columns correspond to the classes in sorted order, as they appear\n        in the attribute `classes_`.\n        \"\"\"\n        j, _ = self._check_test_data(X, accept_sparse=['csr', 'csc'])\n\n        n_samples, n_features = X.shape\n        n_classes = self.means_.shape[0]\n        X = self._validate_data(X, reset=False)\n        check_is_fitted(self, \"means_\")\n\n        # Compute posterior probabilities\n        log_prob = np.zeros((n_samples, n_classes))\n        for i in range(n_classes):\n            log_prob[:, i] = self._log_posterior_prob(X, i)\n\n        # Normalize the posterior probabilities\n        log_prob -= log_prob.max(axis=1)[:, np.newaxis]\n        prob = np.exp2(log_prob)\n        prob /= prob.sum(axis=1)[:, np.newaxis]\n\n        return prob\n\n    def _log_posterior_prob(self, X, class_idx):\n        \"\"\"Compute the log posterior probabilities for each sample in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if they are\n            CSR, CSC, COO formats.\n\n        class_idx : int\n            The index of the class for which the posterior probabilities are computed.\n\n        Returns\n        -------\n        log_prob : ndarray of shape (n_samples,)\n            The log posterior probabilities for each sample in X.\n        \"\"\"\n        # Compute the log determinant of the covariance matrix\n        if self.store_covariance:\n            cov = self.covariances_[class_idx]\n        else:\n            X_class = X[y == self.classes_[class_idx]]\n            cov = np.atleast_2d(\n                np.cov(X_class.T, fweights=sample_weight[y == self.classes_[class_idx]], ddof=1\n            )\n\n        log_det_cov = np.log(np.linalg.det(cov))\n\n        # Compute the log posterior probabilities\n        log_prob = (\n            -0.5 * n_features * np.log(2 * np.pi)\n            - 0.5 * log_det_cov\n            - np.sum((X - self.means_[class_idx]) ** 2, axis=1) / (2 * cov)\n            + np.log(self.priors_[class_idx])\n        )\n\n        return log_prob\n\n    def _get_covariance(self, class_idx):\n        \"\"\"Return the covariance matrix for a given class.\n\n        Parameters\n        ----------\n        class_idx : int\n            The index of the class for which the covariance matrix is returned.\n\n        Returns\n        -------\n        cov : ndarray of shape (n_features, n_features)\n            The covariance matrix for the given class.\n        \"\"\"\n        if self.store_covariance:\n            return self.covariances_[class_idx]\n        else:\n            raise AttributeError(\n                \"Covariance matrix is not stored, call fit with \"\n                \"store_covariance=True to enable covariance retrieval.\"\n            )\n\n    def score_samples(self, X):\n        \"\"\"Return the posterior log probabilities on the prediction of X.\n\n        In multi-label classification, the returned array could also be regarded\n        as the joint probability of all labels for a sample.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if they are\n            CSR, CSC, COO formats.\n\n        Returns\n        -------\n        log_prob : ndarray of shape (n_samples, n_classes)\n            Returns the log posterior probabilities of the input samples X.\n        The columns correspond to the classes in sorted order, as they appear\n        in the attribute `classes_`.\n        \"\"\"\n        j, _ = self._check_test_data(X, accept_sparse=['csr', 'csc'])\n\n        n_samples, n_features = X.shape\n        n_classes = self.means_.shape[0]\n        X = self._validate_data(X, reset=False)\n        check_is_fitted(self, \"means_\")\n\n        # Compute posterior probabilities\n        log_prob = np.zeros((n_samples, n_classes))\n        for i in range(n_classes):\n            log_prob[:, i] = self._log_posterior_prob(X, i)\n\n        return log_prob",
        "def b_merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list",
        "class Playlist:\n    def __init__(self):\n        self._songs = []\n\n    def add_song(self, song_name: str, artist_name: str) -> None:\n        song = {\n            'song_name': song_name,\n            'artist_name': artist_name\n        }\n        self._songs.append(song)\n\n    def remove_song(self, song_name: str) -> None:\n        self._songs = [song for song in self._songs if song['song_name'] != song_name]"
      ]
    }
  ]
}