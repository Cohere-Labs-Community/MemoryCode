{
  "sessions": [
    {
      "session_model_output": [
        "from pedantic import rename_kwargs\n\n@rename_kwargs\ndef greatest_common_divisor(a_at: int, b_at: int) -> int:\n    while b_at != 0:\n        a_at, b_at = b_at, a_at % b_at\n    return a_at",
        "```python\nfrom pedantic import rename_kwargs\n\nclass LinkedList:\n    @rename_kwargs\n    def __init__(self, value_at=None, next_node_at=None):\n        self.value_at = value_at\n        self.next_node_at = next_node_at\n\n    @rename_kwargs\n    def append(self, value_at):\n        new_node_at = Node(value_at=value_at)\n        current_at = self\n        while current_at.next_node_at is not None:\n            current_at = current_at.next_node_at\n        current_at.next_node_at = new_node_at\n\n    @rename_kwargs\n    def search(self, value_at):\n        current_at = self\n        while current_at is not None:\n            if current_at.value_at == value_at:\n                return current_at\n            current_at = current_at.next_node_at\n        return None\n\n    @rename_kwargs\n    def delete(self, value_at):\n        current_at = self\n        prev_at = None\n        while current_at is not None:\n            if current_at.value_at == value_at:\n                if prev_at is not None:\n                    prev_at.next_node_at = current_at.next_node_at\n                else:\n                    self = current_at.next_node_at\n                return\n            prev_at = current_at\n            current_at = current_at.next_node_at\n\nclass Node:\n    @rename_kwargs\n    def __init__(self, value_at=None, next_node_at=None):\n        self.value_at = value_at\n        self.next_node_at = next_node_at\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_at = data\n        self.next_at = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_at = None\n        self.tail_at = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_at:\n            self.head_at = new_node\n            self.tail_at = self.head_at\n        else:\n            self.tail_at.next_at = new_node\n            self.tail_at = new_node\n\n    def delete(self, data):\n        if self.head_at is None:\n            return\n\n        if self.head_at.data_at == data:\n            self.head_at = self.head_at.next_at\n            return\n\n        prev_node = self.head_at\n        curr_node = self.head_at.next_at\n        while curr_node:\n            if curr_node.data_at == data:\n                prev_node.next_at = curr_node.next_at\n                return\n            prev_node = curr_node\n            curr_node = curr_node.next_at\n\n    def display(self):\n        current = self.head_at\n        while current:\n            print(current.data_at, end=\" -> \")\n            current = current.next_at\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_g, list2_g) -> list:\n    merged_list_g = []\n    i, j = 0, 0\n\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list_g.append(list1_g[i])\n            i += 1\n        else:\n            merged_list_g.append(list2_g[j])\n            j += 1\n\n    merged_list_g.extend(list1_g[i:])\n    merged_list_g.extend(list2_g[j:])\n\n    return merged_list_g",
        "class LinkedList:\n    def __init__(self):\n        self.head_xt = None\n        self.tail_xt = None\n\n    def append_node(self, value_g, new_node_g):\n        new_node_xt = Node(value_g)\n        if not self.head_xt:\n            self.head_xt = new_node_xt\n            self.tail_xt = new_node_xt\n        else:\n            self.tail_xt.next_xt = new_node_xt\n            self.tail_xt = new_node_xt\n\n    def delete_node(self, value_g):\n        current_xt = self.head_xt\n        if current_xt.value_xt == value_g:\n            self.head_xt = current_xt.next_xt\n            return\n        prev_xt = current_xt\n        current_xt = current_xt.next_xt\n        while current_xt:\n            if current_xt.value_xt == value_g:\n                prev_xt.next_xt = current_xt.next_xt\n                return\n            prev_xt = current_xt\n            current_xt = current_xt.next_xt\n\n    def display(self):\n        current_xt = self.head_xt\n        while current_xt:\n            print(current_xt.value_xt, end=\" -> \")\n            current_xt = current_xt.next_xt\n        print(\"None\")\n\n\nclass Node:\n    def __init__(self, value_g):\n        self.value_xt = value_g\n        self.next_xt = None"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_xt = data\n        self.next_xt = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_xt = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_xt:\n            self.head_xt = new_node\n            return\n        current = self.head_xt\n        while current.next_xt:\n            current = current.next_xt\n        current.next_xt = new_node\n\n    def delete(self, data):\n        if not self.head_xt:\n            return\n        if self.head_xt.data_xt == data:\n            self.head_xt = self.head_xt.next_xt\n            return\n        current = self.head_xt\n        prev = None\n        while current and current.data_xt != data:\n            prev = current\n            current = current.next_xt\n        if not current:\n            return\n        prev.next_xt = current.next_xt\n\n    def display(self):\n        current = self.head_xt\n        while current:\n            print(current.data_xt, end=\" -> \")\n            current = current.next_xt\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\nfrom sklearn.utils.validation import check_X_y, check_array\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.exceptions import DataConversionWarning, FitFailedWarning\nimport warnings\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n    warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n\n    class LassoRegression:\n        \"\"\"\n        A class for Lasso Regression.\n        \"\"\"\n\n        def __init__(self, alpha=1.0):\n            self.alpha = alpha\n            self.coef_ = None\n            self.intercept_ = None\n\n        def fit(self, X, y):\n            \"\"\"Train the model using X and y.\n\n            Parameters\n            ----------\n            X : ndarray or DataFrame of shape (n_samples, n_features)\n                Training data\n            y : ndarray or Series of shape (n_samples,)\n                Target values\n            \"\"\"\n            X, y = check_X_y(X, y)\n            check_classification_targets(y)\n\n            n_samples, n_features = X.shape\n            self.coef_ = np.zeros(n_features)\n            self.intercept_ = np.zeros(1)\n\n            try:\n                XtX = np.dot(X.T, X)\n                Xty = np.dot(X.T, y)\n\n                inv_xtx_diag = np.diag(1 / (np.diag(XtX) + self.alpha))\n\n                self.coef_ = np.dot(XtX * inv_xtx_diag, Xty)\n                self.intercept_ = np.mean(y - np.dot(X, self.coef_))\n\n            except np.linalg.LinAlgError:\n                self.coef_ = np.zeros(n_features)\n                self.intercept_ = np.mean(y)\n\n        def predict(self, X):\n            \"\"\"Predict target values for X.\n\n            Parameters\n            ----------\n            X : ndarray or DataFrame of shape (n_samples, n_features)\n\n            Returns\n            -------\n            y : ndarray of shape (n_samples,)\n                Predicted target values\n            \"\"\"\n            X = check_array(X)\n            return np.dot(X, self.coef_) + self.intercept_\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\nfrom pedantic.decorators import PedanticTypeCheckMetaclass\n\nclass Library(metaclass=PedanticTypeCheckMetaclass):\n    def __init__(self):\n        self.books = []\n\n    @validate(books=\"list[str]\")\n    def add_books(self, books: list[str]) -> None:\n        self.books.extend(books)\n\n    @validate(titles=\"list[str]\")\n    def remove_books(self, titles: list[str]) -> None:\n        for title in titles:\n            self.books.remove(title)\n```",
        "```python\nfrom pedantic import validate\nfrom typing import Optional\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @validate\n    def insert(self, value: int) -> None:\n        \"\"\"\n        Inserts a value into the binary tree.\n        \"\"\"\n        self.root = self._insert_recursive(self.root, value)\n\n    @validate\n    def _insert_recursive(self, node: Optional[Node], value: int) -> Optional[Node]:\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    @validate\n    def delete(self, value: int) -> None:\n        \"\"\"\n        Deletes a value from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    @validate\n    def _delete_recursive(self, node: Optional[Node], value: int) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            # Case 1: Node is leaf\n            if node.left is None and node.right is None:\n                node = None\n\n            # Case 2: Node has one child\n            elif node.left is None:\n                node = node.right\n            elif node.right is None:\n                node = node.left\n\n            # Case 3: Node has two children\n            else:\n                successor = self._get_min_value_node(node.right)\n                node.value = successor.value\n                node.right = self._delete_recursive(node.right, successor.value)\n\n        return node\n\n    @validate\n    def _get_min_value_node(self, node: Node) -> Node:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self, digit_0_r, digit_1_r):\n        self.digit_0_r = digit_0_r\n        self.digit_1_r = digit_1_r\n\n    def predict(self, x, y):\n        w1_r, w2_r = self.digit_0_r, self.digit_1_r\n        return w1_r * x + w2_r * y",
        "class LinkedList:\n    class Node:\n        def __init__(self, data):\n            self.data_0 = data\n            self.next_r = None\n\n    def __init__(self):\n        self.head_1 = None\n\n    def append(self, data):\n        new_node = self.Node(data)\n        if not self.head_1:\n            self.head_1 = new_node\n        else:\n            current = self.head_1\n            while current.next_r:\n                current = current.next_r\n            current.next_r = new_node\n\n    def display(self):\n        current = self.head_1\n        while current:\n            print(current.data_0, end=\" -> \")\n            current = current.next_r\n        print(\"None\")"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_r = data\n        self.next_r = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data):\n        if not self.head_r:\n            self.head_r = Node(data)\n        else:\n            curr_r = self.head_r\n            while curr_r.next_r:\n                curr_r = curr_r.next_r\n            curr_r.next_r = Node(data)\n\n    def display(self):\n        if not self.head_r:\n            return\n        curr_r = self.head_r\n        while curr_r:\n            print(curr_r.data_r, end=\" -> \")\n            curr_r = curr_r.next_r\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Perceptron:\n    def __init__(self, learning_rate=0.1, epochs=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.weights = None\n        self.bias = None\n\n    def train(self, X_train, y_train):\n        n_features = X_train.shape[1]\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for _ in range(self.epochs):\n            for i, x_i in enumerate(X_train):\n                x_with_bias = np.append(x_i, 1)  # Add bias term\n                prediction = self._predict(x_with_bias)\n                error = y_train[i] - prediction\n                self.weights += self.learning_rate * error * x_i\n                self.bias += self.learning_rate * error\n\n    def predict(self, X_test):\n        predictions = np.array([self._predict(np.append(x, 1)) for x in X_test])\n        return predictions\n\n    def _predict(self, x):\n        return np.dot(x, self.weights) + self.bias"
      ],
      "instruction_model_output": [
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_x, list2_x):\n    \"\"\"\n    Merges two sorted lists into a new sorted list.\n    \n    :param list1_x: The first sorted list.\n    :param list2_x: The second sorted list.\n    :return: A new sorted list containing the merged elements.\n    \"\"\"\n    assert isinstance(list1_x, list) and all(isinstance(x, int) for x in list1_x), \"list1_x must be a list of integers\"\n    assert isinstance(list2_x, list) and all(isinstance(x, int) for x in list2_x), \"list2_x must be a list of integers\"\n    assert all(isinstance(x, int) for x in list1_x) and all(\n        isinstance(x, int) for x in list2_x\n    ), \"Lists must contain only integers\"\n    merged_list_x = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            merged_list_x.append(list1_x[i])\n            i += 1\n        else:\n            merged_list_x.append(list2_x[j])\n            j += 1\n    while i < len(list1_x):\n        merged_list_x.append(list1_x[i])\n        i += 1\n    while j < len(list2_x):\n        merged_list_x.append(list2_x[j])\n        j += 1\n    assert all(\n        isinstance(x, int) for x in merged_list_x\n    ), \"Merged list must contain only integers\"\n    return merged_list_x",
        "def calculate_diagonal_average(matrix_diag_avg, diag_avg_x):\n    \"\"\"\n    Calculates the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix_diag_avg (list): The square matrix for which the diagonal average is calculated.\n        diag_avg_x (str): A description for the matrix.\n\n    Returns:\n        float: The average of the diagonal elements of the matrix.\n    \"\"\"\n    assert isinstance(matrix_diag_avg, list), \"Input matrix must be a list.\"\n    assert all(isinstance(row, list) for row in matrix_diag_avg), \"Input matrix must be a list of lists.\"\n    assert isinstance(diag_avg_x, str), \"Description must be a string.\"\n\n    size = len(matrix_diag_avg)\n    assert all(len(row) == size for row in matrix_diag_avg), \"Input matrix must be square.\"\n\n    diagonal_sum = sum(matrix_diag_avg[i][i] for i in range(size))\n    diagonal_avg = diagonal_sum / size\n\n    return diagonal_avg"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_x, list2_x):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            merged_list.append(list1_x[i])\n            i += 1\n        else:\n            merged_list.append(list2_x[j])\n            j += 1\n    while i < len(list1_x):\n        merged_list.append(list1_x[i])\n        i += 1\n    while j < len(list2_x):\n        merged_list.append(list2_x[j])\n        j += 1\n    return merged_list",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_f, list2_f):\n    result_f = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            result_f.append(list1_f[i])\n            i += 1\n        else:\n            result_f.append(list2_f[j])\n            j += 1\n    result_f.extend(list1_f[i:])\n    result_f.extend(list2_f[j:])\n    return result_f",
        "class LinkedList:\n    def __init__(self):\n        self.head_f = None\n        self.tail_f = None\n\n    def insert_at_beginning_f(self, data_f):\n        node_f = Node(data_f)\n        if self.is_empty_f():\n            self.head_f = node_f\n            self.tail_f = node_f\n        else:\n            node_f.next_f = self.head_f\n            self.head_f = node_f\n\n    def insert_at_end_f(self, data_f):\n        node_f = Node(data_f)\n        if self.is_empty_f():\n            self.head_f = node_f\n            self.tail_f = node_f\n        else:\n            self.tail_f.next_f = node_f\n            self.tail_f = node_f\n\n    def delete_node_f(self, data_f):\n        current_f = self.head_f\n        if current_f.data_f == data_f and self.head_f == self.tail_f:\n            self.head_f = None\n            self.tail_f = None\n        elif current_f.data_f == data_f:\n            self.head_f = self.head_f.next_f\n            return\n        while current_f.next_f is not None:\n            if current_f.next_f.data_f == data_f:\n                current_f.next_f = current_f.next_f.next_f\n                if current_f.next_f is None:\n                    self.tail_f = current_f\n                return\n            current_f = current_f.next_f\n\n    def search_f(self, data_f):\n        current_f = self.head_f\n        while current_f is not None:\n            if current_f.data_f == data_f:\n                return True\n            current_f = current_f.next_f\n        return False\n\n    def is_empty_f(self):\n        return self.head_f is None\n\n    def display_f(self):\n        current_f = self.head_f\n        while current_f is not None:\n            print(current_f.data_f, end=\" -> \")\n            current_f = current_f.next_f\n        print(\"None\")\n\n\nclass Node:\n    def __init__(self, data_f):\n        self.data_f = data_f\n        self.next_f = None"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next_node_s = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_node_s = None\n        self.tail_node_s = None\n        self.size_s = 0\n\n    def append(self, data):\n        new_node_s = Node(data)\n        if not self.head_node_s:\n            self.head_node_s = new_node_s\n            self.tail_node_s = self.head_node_s\n        else:\n            current_node_s = self.head_node_s\n            while current_node_s.next_node_s:\n                current_node_s = current_node_s.next_node_s\n            current_node_s.next_node_s = new_node_s\n            self.tail_node_s = new_node_s\n        self.size_s += 1\n\n    def delete(self, data):\n        current_node_s = self.head_node_s\n        prev_node_s = None\n        while current_node_s and current_node_s.data == data:\n            prev_node_s = current_node_s\n            current_node_s = current_node_s.next_node_s\n        if prev_node_s:\n            prev_node_s.next_node_s = current_node_s.next_node_s\n            if current_node_s == self.tail_node_s:\n                self.tail_node_s = prev_node_s\n            self.size_s -= 1\n\n    def display(self):\n        current_node_s = self.head_node_s\n        while current_node_s:\n            print(current_node_s.data, end=\" -> \")\n            current_node_s = current_node_s.next_node_s\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n    return result_j"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    merged_list_e = []\n    i, j = 0, 0\n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list_e.append(list1_e[i])\n            i += 1\n        else:\n            merged_list_e.append(list2_e[j])\n            j += 1\n    while i < len(list1_e):\n        merged_list_e.append(list1_e[i])\n        i += 1\n    while j < len(list2_e):\n        merged_list_e.append(list2_e[j])\n        j += 1\n    return merged_list_e"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list.append(list1_e[i])\n            i += 1\n        else:\n            merged_list.append(list2_e[j])\n            j += 1\n    \n    while i < len(list1_e):\n        merged_list.append(list1_e[i])\n        i += 1\n    \n    while j < len(list2_e):\n        merged_list.append(list2_e[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap_list = []\n\n    def chx_insert(self, value):\n        self.heap_list.append(value)\n        self.chx_heapify(len(self.heap_list) - 1)\n\n    def chx_heapify(self, index):\n        parent_index = (index - 1) // 2\n        if self.heap_list[index] > self.heap_list[parent_index]:\n            self.heap_list[index], self.heap_list[parent_index] = self.heap_list[parent_index], self.heap_list[index]\n            self.chx_heapify(parent_index)\n\n    def get_heap_list(self):\n        return self.heap_list",
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_q = None\n        self.tail_q = None\n\n    def add_chx(self, value):\n        node_q = Node(value)\n        if not self.head_q:\n            self.head_q = node_q\n            self.tail_q = node_q\n        else:\n            self.tail_q.next_q = node_q\n            self.tail_q = node_q\n\n    def remove_chx(self, value):\n        current_q = self.head_q\n        if current_q and current_q.value_q == value:\n            self.head_q = current_q.next_q\n            return\n        prev_q = None\n        while current_q and current_q.value_q != value:\n            prev_q = current_q\n            current_q = current_q.next_q\n        if current_q is None:\n            return\n        prev_q.next_q = current_q.next_q\n        if current_q == self.tail_q:\n            self.tail_q = prev_q\n\n    def display_chx(self):\n        current_q = self.head_q\n        while current_q:\n            print(current_q.value_q, end=\" -> \")\n            current_q = current_q.next_q\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value):\n        self.value_q = value\n        self.next_q = None\n```"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]",
        "```python\nclass Node:\n    def __init__(self, data_q):\n        self.data_q = data_q\n        self.next_q = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_q = None\n\n    def append(self, data_q):\n        if not self.head_q:\n            self.head_q = Node(data_q)\n        else:\n            curr_q = self.head_q\n            while curr_q.next_q:\n                curr_q = curr_q.next_q\n            curr_q.next_q = Node(data_q)\n\n    def display(self):\n        if not self.head_q:\n            return\n        curr_q = self.head_q\n        while curr_q:\n            print(curr_q.data_q, end=\" -> \")\n            curr_q = curr_q.next_q\n        print(\"None\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom pedantic import rename_kwargs, validate\n\n@rename_kwargs\n@validate\ndef h_index(citations_g: list[int]) -> int:\n    citations_g.sort(reverse=True)\n    h_index_r = 0\n    for i, citation in enumerate(citations_g, 1):\n        if citation < i:\n            return h_index_r\n        h_index_r = i\n    return h_index_r\n```",
        "```python\nfrom typing import List, Union\nimport numpy as np\nimport torch\n\nclass Perceptron:\n    \"\"\"\n    A Perceptron model for binary classification.\n\n    ...\n\n    Attributes\n    ----------\n    w_ : torch.Tensor\n        Weights vector.\n    b_ : torch.Tensor\n        Bias term.\n    lr_ : float\n        Learning rate.\n    input_dim_ : int\n        Dimension of input vectors.\n\n    Methods\n    -------\n    train(X: torch.Tensor, y: torch.Tensor, epochs: int = 10000)\n        Train the Perceptron model using the given training data.\n    predict(X: torch.Tensor)\n        Predict the labels for the provided input data.\n\n    \"\"\"\n\n    def __init__(self, lr: float = 0.1):\n        \"\"\"\n        Initialize the Perceptron model.\n\n        Parameters\n        ----------\n        lr : float, optional\n            Learning rate, by default 0.1.\n\n        \"\"\"\n        self.w_ = None\n        self.b_ = None\n        self.lr_ = lr\n        self.input_dim_ = 0\n\n    def train(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 10000) -> None:\n        \"\"\"\n        Train the Perceptron model using the given training data.\n\n        Parameters\n        ----------\n        X : torch.Tensor\n            Input training data of shape (m, n), where m is the number of examples and n is the number of features.\n        y : torch.Tensor\n            Target labels of shape (m,).\n        epochs : int, optional\n            Number of epochs to train the model, by default 10000.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        # Initialize weights and bias\n        self.w_ = torch.zeros(X.shape[1], dtype=torch.float32, requires_grad=True)\n        self.b_ = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n        self.input_dim_ = X.shape[1]\n\n        # Convert labels to tensor\n        y = y.type(torch.FloatTensor)\n\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self._forward(X)\n\n            # Compute loss\n            loss = self._loss(y_pred, y)\n\n            # Backward pass and optimization\n            loss.backward()\n\n            with torch.no_grad():\n                self.w_.data -= self.lr_ * self.w_.grad\n                self.b_.data -= self.lr_ * self.b_.grad\n\n                # Zero out gradients\n                self.w_.grad.zero_()\n                self.b_.grad.zero_()\n\n            if epoch % 1000 == 0:\n                print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n\n    def predict(self, X: torch.Tensor) -> np.ndarray:\n        \"\"\"\n        Predict the labels for the provided input data.\n\n        Parameters\n        ----------\n        X : torch.Tensor\n            Input data of shape (m, n), where m is the number of examples and n is the number of features.\n\n        Returns\n        -------\n        np.ndarray\n            Predicted labels of shape (m,).\n\n        \"\"\"\n        y_pred = self._forward(X)\n        y_pred_numpy = y_pred.detach().numpy()\n        return np.where(y_pred_numpy > 0, 1, 0)\n\n    def _forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Perceptron model.\n\n        Parameters\n        ----------\n        X : torch.Tensor\n            Input data of shape (m, n), where m is the number of examples and n is the number of features.\n\n        Returns\n        -------\n        torch.Tensor\n            Predicted labels of shape (m,).\n\n        \"\"\"\n        return torch.sign(X @ self.w_ + self.b_)\n\n    def _loss(self, y_pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute the loss for the Perceptron model.\n\n        Parameters\n        ----------\n        y_pred : torch.Tensor\n            Predicted labels of shape (m,).\n        y : torch.Tensor\n            True labels of shape (m,).\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n\n        \"\"\"\n        loss = torch.mean((y_pred - y) ** 2)\n        return loss\n```",
        "```python\nfrom typing import Any\n\nclass Heap:\n    def __init__(self):\n        self.data = []\n\n    def parent(self, i: int) -> int:\n        return (i - 1) // 2\n\n    def left_child(self, i: int) -> int:\n        return 2 * i + 1\n\n    def right_child(self, i: int) -> int:\n        return 2 * i + 2\n\n    def insert(self, value: Any) -> None:\n        self.data.append(value)\n        self._heapify_up(len(self.data) - 1)\n\n    def _heapify_up(self, i: int) -> None:\n        parent_i = self.parent(i)\n        while i > 0 and self.data[i] > self.data[parent_i]:\n            self.data[i], self.data[parent_i] = self.data[parent_i], self.data[i]\n            i = parent_i\n            parent_i = self.parent(i)\n\n    def heapify(self, i: int = 0) -> None:\n        left_child_i = self.left_child(i)\n        right_child_i = self.right_child(i)\n        largest = i\n        if left_child_i < len(self.data) and self.data[left_child_i] > self.data[largest]:\n            largest = left_child_i\n        if right_child_i < len(self.data) and self.data[right_child_i] > self.data[largest]:\n            largest = right_child_i\n        if largest != i:\n            self.data[i], self.data[largest] = self.data[largest], self.data[i]\n            self.heapify(largest)\n```",
        "```python\nfrom typing import Any\n\nclass LinkedList:\n    \"\"\"A linked list data structure.\n\n    Attributes:\n        head_j (Node): The head node of the linked list.\n        tail_j (Node): The tail node of the linked list.\n        size_e (int): The number of elements in the linked list.\n\n    \"\"\"\n\n    def __init__(self):\n        self.head_j = None\n        self.tail_j = None\n        self.size_e = 0\n\n    def is_empty(self) -> bool:\n        \"\"\"Check if the linked list is empty.\n\n        Returns:\n            bool: True if the linked list is empty, False otherwise.\n\n        \"\"\"\n        return self.size_e == 0\n\n    def add_node(self, value_x: Any) -> None:\n        \"\"\"Add a new node with the given value to the end of the linked list.\n\n        Args:\n            value_x (Any): The value to be stored in the new node.\n\n        \"\"\"\n        new_node = Node(value_x)\n        if self.is_empty():\n            self.head_j = new_node\n            self.tail_j = new_node\n        else:\n            self.tail_j.next_r = new_node\n            self.tail_j = new_node\n        self.size_e += 1\n\n    def remove_node(self, value_x: Any) -> None:\n        \"\"\"Remove the first node with the given value from the linked list.\n\n        Args:\n            value_x (Any): The value of the node to be removed.\n\n        \"\"\"\n        if self.head_j is None:\n            return\n        if self.head_j.value_chx == value_x:\n            self.head_j = self.head_j.next_r\n            self.size_e -= 1\n            return\n        current_node = self.head_j\n        while current_node.next_r is not None:\n            if current_node.next_r.value_chx == value_x:\n                current_node.next_r = current_node.next_r.next_r\n                self.size_e -= 1\n                return\n            current_node = current_node.next_r\n\n    def search_node(self, value_x: Any) -> Any:\n        \"\"\"Search for a node with the given value in the linked list.\n\n        Args:\n            value_x (Any): The value to search for.\n\n        Returns:\n            Any: The value of the first node with the given value, or None if not found.\n\n        \"\"\"\n        current_node = self.head_j\n        while current_node is not None:\n            if current_node.value_chx == value_x:\n                return current_node.value_chx\n            current_node = current_node.next_r\n        return None\n\n    def size(self) -> int:\n        \"\"\"Get the number of nodes in the linked list.\n\n        Returns:\n            int: The number of nodes in the linked list.\n\n        \"\"\"\n        return self.size_e\n\nclass Node:\n    \"\"\"A single node in the linked list.\n\n    Attributes:\n        value_chx (Any): The value stored in the node.\n        next_r (Node): The next node in the linked list.\n\n    \"\"\"\n\n    def __init__(self, value_x: Any):\n        self.value_chx = value_x\n        self.next_r = None\n```",
        "```python\nfrom typing import List\n\n\n@rename_kwargs\ndef merge_sorted_lists(list1_g: List[int], list2_g: List[int]) -> List[int]:\n    merged_list_r = []\n    i, j = 0, 0\n\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list_r.append(list1_g[i])\n            i += 1\n        else:\n            merged_list_r.append(list2_g[j])\n            j += 1\n\n    merged_list_r.extend(list1_g[i:])\n    merged_list_r.extend(list2_g[j:])\n\n    return merged_list_r\n```",
        "```python\nfrom pedantic import rename_kwargs, validate\n\n@rename_kwargs\n@validate\ndef compute_diagonal_average(matrix: list[list[int]] | None = None) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    if matrix is None:\n        raise ValueError(\"Matrix cannot be None\")\n    if not all(len(row) == len(matrix) for row in matrix):\n        raise ValueError(\"Input must be a square matrix\")\n    chx_diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    chx_average = chx_diagonal_sum / len(matrix)\n    return chx_average\n```",
        "```python\nfrom pedantic import rename_kwargs, validate\n\nclass Library:\n    def __init__(self):\n        self.books_at = []\n\n    @rename_kwargs\n    @validate\n    def add_books_g(self, book_name_chx: str, author_name_chx: str) -> None:\n        \"\"\"\n        Add a book to the library.\n\n        :param str book_name_chx: The name of the book.\n        :param str author_name_chx: The name of the author.\n        \"\"\"\n        book_info_xt = {\n            'book_name_xt': book_name_chx,\n            'author_name_xt': author_name_chx,\n        }\n        self.books_at.append(book_info_xt)\n\n    @rename_kwargs\n    @validate\n    def remove_books_g(self, book_name_chx: str) -> None:\n        \"\"\"\n        Remove a book from the library.\n\n        :param str book_name_chx: The name of the book to remove.\n        \"\"\"\n        for book_info_xt in self.books_at:\n            if book_info_xt['book_name_xt'] == book_name_chx:\n                self.books_at.remove(book_info_xt)\n                return\n        raise ValueError(f\"Book '{book_name_chx}' not found in the library.\")\n```",
        "```python\nfrom math import gcd\n\n@rename_kwargs\ndef greatest_common_divisor(a_num: int, b_num: int) -> int:\n    \"\"\"Return the greatest common divisor of two integers.\"\"\"\n    return gcd(a_num, b_num)\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nclass LassoRegression:\n    def __init__(self):\n        self.model = Lasso()\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        \"\"\"\n        Trains the Lasso Regression model.\n\n        Parameters:\n        X_train (np.ndarray): The input training data.\n        y_train (np.ndarray): The target training data.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the target values for the given test data.\n\n        Parameters:\n        X_test (np.ndarray): The input test data.\n\n        Returns:\n        np.ndarray: The predicted target values.\n        \"\"\"\n        return self.model.predict(X_test)\n```",
        "```python\nfrom typing import List\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left_j = None\n        self.right_j = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root_r = None\n\n    def insert(self, value: int) -> None:\n        self.root_r = self._insert_recursively(value, self.root_r)\n\n    def _insert_recursively(self, value: int, node: 'Node') -> 'Node':\n        if node is None:\n            node = Node(value)\n        elif value < node.value:\n            node.left_j = self._insert_recursively(value, node.left_j)\n        else:\n            node.right_j = self._insert_recursively(value, node.right_j)\n        return node\n\n    def delete(self, value: int) -> None:\n        self.root_r = self._delete_recursively(value, self.root_r)\n\n    def _delete_recursively(self, value: int, node: 'Node') -> 'Node':\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left_j = self._delete_recursively(value, node.left_j)\n        elif value > node.value:\n            node.right_j = self._delete_recursively(value, node.right_j)\n        else:\n            if node.left_j is None:\n                return node.right_j\n            elif node.right_j is None:\n                return node.left_j\n\n            min_val = self._find_min(node.right_j)\n            node.value = min_val\n            node.right_j = self._delete_recursively(min_val, node.right_j)\n\n        return node\n\n    def _find_min(self, node: 'Node') -> int:\n        current = node\n        while current.left_j is not None:\n            current = current.left_j\n        return current.value\n\n    def values_in_order(self) -> List[int]:\n        values = []\n        self._values_in_order_recursive(self.root_r, values)\n        return values\n\n    def _values_in_order_recursive(self, node: 'Node', values: List[int]) -> None:\n        if node is None:\n            return\n        self._values_in_order_recursive(node.left_j, values)\n        values.append(node.value)\n        self._values_in_order_recursive(node.right_j, values)\n```",
        "```python\nimport numpy as np\nfrom pedantic import rename_kwargs, validate\n\n@rename_kwargs\n@validate\ndef c_linear_regression(x: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, num_iterations: int = 1000) -> np.ndarray:\n    x_trans = np.c_[np.ones(x.shape[0]), x]  # Add intercept term\n    theta = np.zeros(x_trans.shape[1])  # Initialize coefficients to zero\n\n    for _ in range(num_iterations):\n        gradients = x_trans.T @ (x_trans @ theta - y)\n        theta -= learning_rate * gradients / x_trans.shape[0]\n\n    return theta\n\nclass LinearRegression:\n    def __init__(self, learning_rate: float = 0.01, num_iterations: int = 1000) -> None:\n        self.learning_rate_f = learning_rate\n        self.num_iterations_j = num_iterations\n        self.coef_r = None\n        self.intercept_r = None\n\n    def fit(self, x: np.ndarray, y: np.ndarray) -> \"LinearRegression\":\n        self.coef_r = c_linear_regression(x, y, self.learning_rate_f, self.num_iterations_j)[:-1]\n        self.intercept_r = c_linear_regression(x, y, self.learning_rate_f, self.num_iterations_j)[-1]\n        return self\n\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        x_trans = np.c_[np.ones(x.shape[0]), x]\n        return x_trans @ self.coef_chx + self.intercept_chx\n```"
      ]
    }
  ]
}