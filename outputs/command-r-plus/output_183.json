{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train_md(self, X, y):\n        try:\n            from sklearn.linear_model import Lasso\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while training the Lasso Regression model: {e}\")\n\n    def predict_md(self, X):\n        try:\n            if self.coef_ is None or self.intercept_ is None:\n                raise ValueError(\"Model has not been trained yet. Please call the train method before predicting.\")\n            return self.model.predict(X)\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while making predictions: {e}\")",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def shortest_path_dijsktra(self, src):\n        dist = [float('inf') for _ in range(self.V)]\n        dist[src] = 0\n        spt_set = set()\n\n        for _ in range(self.V):\n            u = self.min_distance(dist, spt_set)\n            spt_set.add(u)\n            for v in range(self.V):\n                if self.graph[u][v] > 0 and v not in spt_set:\n                    dist[v] = min(dist[v], dist[u] + self.graph[u][v])\n\n        def min_distance(self, dist, spt_set):\n            min_dist = float('inf')\n            min_node = -1\n            for v in range(self.V):\n                if dist[v] < min_dist and v not in spt_set:\n                    min_dist = dist[v]\n                    min_node = v\n            return min_node\n\n    # Example method with the '_md' suffix as per Juan's instruction\n    def example_method_md(self, param1, param2):\n        # Your method implementation here\n        pass\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_md(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_md(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        for _ in range(self.V):\n            u = self.min_distance_node_md(dist)\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n\n        path = []\n        node = dest\n        while parent[node] != -1:\n            path.append(node)\n            node = parent[node]\n        path.append(node)\n        path.reverse()\n        return path, dist[dest]\n\n    def min_distance_node_md(self, dist):\n        min_dist = float('inf')\n        min_node = -1\n        for v in range(self.V):\n            if dist[v] < min_dist:\n                min_dist = dist[v]\n                min_node = v\n        return min_node"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    \"\"\"\n    A class to represent a graph data structure.\n    \"\"\"\n\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_o(self, u, v):\n        \"\"\"\n        Adds an edge between vertices u and v.\n\n        Parameters:\n            u (int): The source vertex.\n            v (int): The destination vertex.\n        \"\"\"\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_o(self, src, dest):\n        \"\"\"\n        Computes the shortest path from source to destination using BFS.\n\n        Parameters:\n            src (int): The source vertex.\n            dest (int): The destination vertex.\n\n        Returns:\n            list: The shortest path from source to destination.\n        \"\"\"\n        visited = [False] * self.V\n        path = []\n        queue = []\n\n        queue.append(src)\n        visited[src] = True\n\n        while queue:\n            vertex = queue.pop(0)\n            path.append(vertex)\n\n            if vertex == dest:\n                return path\n\n            for neighbour in self.graph[vertex]:\n                if not visited[neighbour]:\n                    queue.append(neighbour)\n                    visited[neighbour] = True\n\n        return None\n```",
        "def find_longest_common_prefix_o(strs: list[str]) -> str:\n    \"\"\"Finds the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_o(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_o(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class NearestNeighbors:\n    \"\"\"\n    A Nearest Neighbors classifier.\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use for prediction.\n\n    metric : str, default='euclidean'\n        Distance metric to use. Supported metrics: ['euclidean', 'manhattan', 'cosine'].\n\n    Attributes\n    ----------\n    X_train_ : ndarray, shape (n_samples, n_features)\n        Training data.\n\n    y_train_ : ndarray, shape (n_samples, )\n        Target values for the training data.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, metric='euclidean'):\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self.X_train_ = None\n        self.y_train_ = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Nearest Neighbors model.\n\n        Parameters\n        ----------\n        X_train : ndarray, shape (n_samples, n_features)\n            Training data.\n\n        y_train : ndarray, shape (n_samples, )\n            Target values for the training data.\n        \"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \"Number of samples in X_train and y_train do not match.\"\n        assert self.metric in ['euclidean', 'manhattan', 'cosine'], \"Unsupported metric. Choose from 'euclidean', 'manhattan', 'cosine'.\"\n\n        self.X_train_ = X_train\n        self.y_train_ = y_train\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the target values for the test data.\n\n        Parameters\n        ----------\n        X_test : ndarray, shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        y_pred : ndarray, shape (n_samples, )\n            Predicted target values for the test data.\n        \"\"\"\n        assert X_test.shape[1] == self.X_train_.shape[1], \"Number of features in test data does not match training data.\"\n        assert self.X_train_ is not None and self.y_train_ is not None, \"Model has not been trained yet.\"\n\n        # ... Implement the prediction logic here ...\n\n        return y_pred"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def flatten_binary_tree_to_linked_list_chx_a(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    root.right = flatten_binary_tree_to_linked_list_chx_a(root.right)\n    root.left = flatten_binary_tree_to_linked_list_chx_a(root.left)\n    temp = root.right\n    while root.left:\n        temp.left = root.left\n        temp = temp.left\n    temp.left = root.right\n    root.right = root.left = None\n    return root",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    # Function to add an edge to the graph\n    def add_edge_chx(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    # Method to find the shortest path between two nodes\n    def shortest_path_chx_a(self, src, dest):\n        # Create a visited set to keep track of visited nodes\n        visited = set()\n\n        # Create a queue for BFS traversal\n        queue = []\n\n        # Mark the source node as visited and enqueue it\n        queue.append(src)\n        visited.add(src)\n\n        # Perform BFS traversal until the destination node is found or all nodes have been visited\n        while queue:\n            # Dequeue a node from the queue\n            u = queue.pop(0)\n\n            # Get neighbors of the current node\n            for v in self.graph[u]:\n                # If the neighbor node has not been visited, mark it as visited and enqueue it\n                if v not in visited:\n                    queue.append(v)\n                    visited.add(v)\n\n                    # If the destination node is found, return the current node as the parent\n                    if v == dest:\n                        return u\n\n        # If BFS traversal completes without finding the destination node, return None\n        return None\n```"
      ],
      "instruction_model_output": [
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)",
        "```python\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def print_a(self):\n        for i in range(self.V):\n            for j in range(self.V):\n                if self.graph[i][j] == 1:\n                    print(f\"{i} - {j}\")\n\n    def add_edge_a(self, u, v):\n        self.graph[u][v] = 1\n        self.graph[v][u] = 1\n\n    def shortest_path_a(self, src, dest):\n        distance = [float(\"Inf\")] * self.V\n        distance[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] == 1 and distance[v] > distance[u] + 1:\n                    distance[v] = distance[u] + 1\n                    queue.append(v)\n        return distance[dest]\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_bst(root, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    if not root:\n        return True\n    if not isinstance(root, Node):\n        return False\n    if root.value <= min_val or root.value >= max_val:\n        return False\n    try:\n        return is_valid_bst(root.left, min_val, root.value - 1) and is_valid_bst(root.right, root.value + 1, max_val)\n    except Exception as e:\n        raise Exception(\"Error occurred while validating BST\") from e\n```",
        "```python\nclass DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0] * vertices for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u][v] = 1\n\n    def is_cyclic_util(self, v, visited, recursionStack):\n        visited[v] = True\n        recursionStack[v] = True\n\n        for i in range(self.V):\n            if self.graph[v][i] > 0 and not visited[i]:\n                if self.is_cyclic_util(i, visited, recursionStack):\n                    return True\n            elif self.graph[v][i] > 0 and recursionStack[i]:\n                return True\n\n        recursionStack[v] = False\n        return False\n\n    def is_cyclic(self):\n        visited = [False] * self.V\n        recursionStack = [False] * self.V\n\n        for i in range(self.V):\n            if not visited[i]:\n                if self.is_cyclic_util(i, visited, recursionStack):\n                    return True\n\n        return False\n\n\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Graph:\n    def find_shortest_path_x(self, start, end):\n        pass"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, kernel_params=kernel_params, fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, verbose=verbose\n        )\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "cumulative_model_output": [
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None):\n        \"\"\"\n        Kernel Ridge Regression Class\n\n        Parameters:\n        alpha (float): Regularization strength; must be a positive float.\n        kernel (str): Kernel to be used in the algorithm.\n        gamma (float): Kernel coefficient.\n        degree (int): Degree of the polynomial kernel function.\n        coef0 (float): Independent term in the polynomial kernel function.\n        kernel_params (dict): Additional parameters for the kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n        self.model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma, degree=degree,\n                                 coef0=coef0, kernel_params=kernel_params)\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as training data and y as target values.\n\n        Parameters:\n        X (array-like): Training data of shape (n_samples, n_features).\n        y (array-like): Target values of shape (n_samples,).\n\n        Returns:\n        self: Fitted instance of the KernelRidgeRegression class.\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters:\n        X (array-like): Data of shape (n_samples, n_features).\n\n        Returns:\n        y_pred (array): Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\nfrom typing import List\n\n\nclass DirectedGraph:\n    def __init__(self, vertices: List[str]):\n        self.vertices = vertices\n        self.adj_matrix = [[0] * len(vertices) for _ in range(len(vertices))]\n\n    def add_edge(self, u: str, v: str) -> None:\n        self.adj_matrix[self._get_index(u)][self._get_index(v)] = 1\n\n    def _get_index(self, vertex: str) -> int:\n        return self.vertices.index(vertex)\n\n    def has_cycle_chx_a(self) -> bool:\n        visited = [False] * len(self.vertices)\n        rec_stack = [False] * len(self.vertices)\n\n        def is_cyclic_util(vertex: int, parent: int) -> bool:\n            visited[vertex] = True\n            rec_stack[vertex] = True\n\n            for neighbour in range(len(self.vertices)):\n                if self.adj_matrix[vertex][neighbour] == 1:\n                    if not visited[neighbour]:\n                        if is_cyclic_util(neighbour, vertex):\n                            return True\n                    elif rec_stack[neighbour] and neighbour != parent:\n                        return True\n\n            rec_stack[vertex] = False\n            return False\n\n        for vertex in range(len(self.vertices)):\n            if not visited[vertex]:\n                if is_cyclic_util(vertex, -1):\n                    return True\n\n        return False\n```",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.edges = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v, weight):\n        self.edges[u].append((v, weight))\n        self.edges[v].append((u, weight))\n\n    def shortest_path(self, start, end):\n        distances = [float('inf')] * self.vertices\n        distances[start] = 0\n        path = [''] * self.vertices\n        parent = [-1] * self.vertices\n\n        for _ in range(self.vertices):\n            u = self.get_min_distance_node(distances)\n            self.relax_neighbors(u, distances, parent)\n\n            if distances[end] != float('inf'):\n                break\n\n        path_str = self.get_shortest_path_str(start, end, parent, path)\n        return path_str\n\n    def get_min_distance_node(self, distances):\n        min_distance = float('inf')\n        min_distance_node = -1\n\n        for u in range(self.vertices):\n            if distances[u] < min_distance:\n                min_distance = distances[u]\n                min_distance_node = u\n\n        return min_distance_node\n\n    def relax_neighbors(self, u, distances, parent):\n        for v, weight in self.edges[u]:\n            if distances[v] > distances[u] + weight:\n                distances[v] = distances[u] + weight\n                parent[v] = u\n                path[v] = str(u) + path[u]\n\n    def get_shortest_path_str(self, start, end, parent, path):\n        if parent[end] == -1:\n            return \"No path exists\"\n\n        shortest_path_str = str(end) + \" \"\n        node = end\n\n        while node != start:\n            node = parent[node]\n            shortest_path_str = path[node] + shortest_path_str\n\n        return shortest_path_str + str(start)\n```",
        "```python\ndef flatten_binary_tree_to_linked_list(root):\n    if not root:\n        return None\n\n    # Check if the tree is already a linked list\n    if not root.left and not root.right:\n        return root\n\n    # Process left subtree\n    left_head = flatten_binary_tree_to_linked_list(root.left)\n\n    # Process right subtree\n    right_head = flatten_binary_tree_to_linked_list(root.right)\n\n    # Combine left and right subtrees\n    if left_head:\n        temp = left_head\n        while temp.right:\n            temp = temp.right\n        temp.right = root\n        root.left = None\n\n    if right_head:\n        root.right = right_head\n\n    return left_head or root\n```",
        "```python\ndef is_valid_bst(tree, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    chx = \"chx\"\n    if not tree:\n        return True\n    val = tree[chx]\n    return min_val < val < max_val and is_valid_bst(\n        tree[\"left\"], min_val, val\n    ) and is_valid_bst(tree[\"right\"], val, max_val)\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pedantic import trace_class\n\n@trace_class\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```",
        "```python\nimport numpy as np\n\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression Class\n    \"\"\"\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def _fit(self, X, y):\n        \"\"\"\n        Fit the model using X and y as training data\n        :param X: training data of size (n_samples, n_features)\n        :param y: target values of size (n_samples,)\n        \"\"\"\n        n_samples, n_features = X.shape\n        X_with_bias = np.c_[np.ones((n_samples, 1)), X]  # Add bias term\n        self.coef_ = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n        self.intercept_ = self.coef_[0]\n        self.coef_ = self.coef_[1:]\n\n    def _predict(self, X):\n        \"\"\"\n        Predict the target values for the provided data X\n        :param X: data of size (n_samples, n_features)\n        :return: predicted target values of size (n_samples,)\n        \"\"\"\n        n_samples = X.shape[0]\n        X_with_bias = np.c_[np.ones((n_samples, 1)), X]  # Add bias term\n        return X_with_bias @ self.coef_\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Lasso Regression model\n        :param X_train: training data of size (n_samples, n_features)\n        :param y_train: target values of size (n_samples,)\n        \"\"\"\n        self._fit(X_train, y_train)\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the target values for the test data\n        :param X_test: test data of size (n_samples, n_features)\n        :return: predicted target values of size (n_samples,)\n        \"\"\"\n        return self._predict(X_test)\n```",
        "```python\ndef find_longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n\n    return strs[0]\n```"
      ]
    }
  ]
}