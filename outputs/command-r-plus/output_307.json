{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_fn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_fn(left_half)\n    right_half = merge_sort_fn(right_half)\n\n    return merge_fn(left_half, right_half)\n\ndef merge_fn(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    return result"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_fn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_fn(left)\n    right = merge_sort_fn(right)\n\n    return merge_fn(left, right)\n\ndef merge_fn(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "@pedantic.validate\ndef int_to_roman_y(num: int) -> str:\n    \"\"\"Convert an integer to a Roman numeral.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The Roman numeral representation of the input integer.\n    \"\"\"\n    num_map = [\n        (1000, \"M\"),\n        (900, \"CM\"),\n        (500, \"D\"),\n        (400, \"CD\"),\n        (100, \"C\"),\n        (90, \"XC\"),\n        (50, \"L\"),\n        (40, \"XL\"),\n        (10, \"X\"),\n        (9, \"IX\"),\n        (5, \"V\"),\n        (4, \"IV\"),\n        (1, \"I\")\n    ]\n    roman = \"\"\n    for value, symbol in num_map:\n        count = num // value\n        roman += symbol * count\n        num %= value\n    return roman",
        "```python\nfrom pedantic import validate\n\n\n@validate\ndef merge_sort_y(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_y(left_half)\n    right_half = merge_sort_y(right_half)\n\n    return merge_y(left_half, right_half)\n\n\n@validate\ndef merge_y(left: list[int], right: list[int]) -> list[int]:\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import validate\r\n\r\n@validate\r\ndef int_to_roman(num: int) -> str:\r\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\r\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\r\n    roman_num = ''\r\n    i = 0\r\n    while num > 0:\r\n        for _ in range(num // val[i]):\r\n            roman_num += syb[i]\r\n            num -= val[i]\r\n        i += 1\r\n    return roman_num",
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_c(left_half)\n    right_half = merge_sort_c(right_half)\n\n    return merge_c(left_half, right_half)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sort_gn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_gn(left_half)\n    right_half = merge_gn(right_half)\n\n    return merge_gn(left_half, right_half)\n\ndef merge_gn(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result\n```"
      ],
      "instruction_model_output": [
        "def merge_sort_gn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_gn(left_half)\n    right_half = merge_sort_gn(right_half)\n\n    return merge_gn(left_half, right_half)\n\ndef merge_gn(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    \"\"\"\n    Finds the k-th largest element in a list of integers.\n\n    Parameters:\n        nums (List[int]): The input list of integers.\n        k (int): The value of 'k' for the k-th largest element.\n\n    Returns:\n        int: The k-th largest element in the list.\n    \"\"\"\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, index1, index2):\n        self.heap[index1], self.heap[index2] = self.heap[index2], self.heap[index1]"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sort_d():\n    def merge(arr, left, middle, right):\n        merged = []\n        i = left\n        j = middle\n        while i < middle and j < right:\n            if arr[i] < arr[j]:\n                merged.append(arr[i])\n                i += 1\n            else:\n                merged.append(arr[j])\n                j += 1\n        merged.extend(arr[i:middle])\n        merged.extend(arr[j:right])\n        return merged\n\n    def sort(arr, left, right):\n        if left < right:\n            middle = (left + right) // 2\n            left_sorted = sort(arr, left, middle)\n            right_sorted = sort(arr, middle, right)\n            return merge(arr, left, middle, right, left_sorted, right_sorted)\n        return arr[left:right]\n\n    return sort\n```"
      ],
      "instruction_model_output": [
        "def merge_sort_d(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    left = merge_sort_d(left)\n    right = merge_sort_d(right)\n    \n    return merge_d(left, right)\n\ndef merge_d(left, right):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass QDA(QuadraticDiscriminantAnalysis):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_a(left_half)\n    right_half = merge_sort_a(right_half)\n\n    return merge_a(left_half, right_half)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half_b = arr[:mid]\n    right_half_b = arr[mid:]\n    \n    left_half_b = merge_sort_b(left_half_b)\n    right_half_b = merge_sort_b(right_half_b)\n    \n    return merge_b(left_half_b, right_half_b)\n\ndef merge_b(left, right):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def find_common_elements(list1, list2):\n    common_elements = []\n    for i, element in enumerate(list1):\n        if element in list2:\n            common_elements.append(element)\n    return common_elements",
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_order = collections.OrderedDict()\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.r_order.move_to_end(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.r_order.move_to_end(key)\n        elif len(self.r_order) >= self.capacity:\n            self.cache.pop(self.r_order.popitem(last=False)[0])\n        self.cache[key] = value\n        self.r_order[key] = None"
      ],
      "instruction_model_output": [
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.i_queue = []\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.i_queue.remove(key)\n        self.i_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.i_queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            del_key = self.i_queue.pop(0)\n            del self.cache[del_key]\n        self.i_queue.append(key)\n        self.cache[key] = value"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"Insert a value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node: Node, value: int) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(node.left, value)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(node.right, value)\n\n    def delete(self, value: int) -> None:\n        \"\"\"Delete a node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: Optional[Node], value: int) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder predecessor and replace the node's value with it\n            predecessor = self._get_inorder_predecessor(node.left)\n            node.value = predecessor.value\n            node.left = self._delete_recursive(node.left, predecessor.value)\n\n        return node\n\n    def _get_inorder_predecessor(self, node: Node) -> Node:\n        while node.right is not None:\n            node = node.right\n        return node\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.q_order.move_to_end(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.capacity:\n            self.cache.pop(self.q_order.popitem(last=False)[0])\n        self.cache[key] = value\n        self.q_order[key] = None",
        "class LinearRegression:\n    \"\"\"\n    A simple linear regression class using numpy.\n    \"\"\"\n    def __init__(self, digit=1):\n        self.q_1_slope = None\n        self.q_2_intercept = None\n        self.digit = digit\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the linear regression model to the training data (X, y).\n        \"\"\"\n        X_mean = np.mean(X)\n        y_mean = np.mean(y)\n\n        num = np.sum((X - X_mean) * (y - y_mean))\n        denom = np.sum((X - X_mean) ** 2)\n\n        self.q_1_slope = self.digit * num / denom\n        self.q_2_intercept = y_mean - self.q_1_slope * X_mean\n\n    def predict(self, X):\n        \"\"\"\n        Predict the target variable based on the input data X.\n        \"\"\"\n        return [self.q_2_intercept + self.q_1_slope * x for x in X]"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]",
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def b_get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def b_put(self, key: int, value: int) -> None:\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_order) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value\n```",
        "def b_h_index(t_citations: list[int]) -> int:\n    t_citations.sort(reverse=True)\n    t_h_index = 0\n\n    for t_index, t_citation_count in enumerate(t_citations):\n        t_h_index = max(t_h_index, min(t_index + 1, t_citation_count))\n\n    return t_h_index"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value",
        "def b_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def d_int_to_roman(num):\n    \"\"\"\n    Converts an integer to Roman numerals.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman",
        "def d_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Computes the h-index based on the given citations.\n    \n    Args:\n        citations (list[int]): A list of integer citations.\n    \n    Returns:\n        int: The computed h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class NearestNeighbors:\n    \"\"\"\n    A Nearest Neighbors classifier.\n\n    This class implements a Nearest Neighbors classifier, which can be used for both classification and regression problems.\n    It supports both Euclidean and Manhattan distances.\n\n    Parameters:\n    -----------\n    n_neighbors: int (default=5)\n        The number of neighbors to consider for prediction.\n\n    metric: str (default='euclidean')\n        The distance metric to use. Supported metrics: 'euclidean', 'manhattan'.\n\n    Attributes:\n    ----------\n    X_train_ : ndarray\n        The training data.\n\n    y_train_ : ndarray\n        The target values corresponding to X_train_.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, metric='euclidean'):\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Nearest Neighbors model.\n\n        Parameters:\n        ----------\n        X_train : ndarray\n            The training data.\n\n        y_train : ndarray\n            The target values corresponding to X_train.\n        \"\"\"\n        self.X_train_ = X_train\n        self.y_train_ = y_train\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the target values for the provided test data.\n\n        Parameters:\n        ----------\n        X_test : ndarray\n            The test data for which predictions are to be made.\n\n        Returns:\n        -------\n        ndarray\n            Predicted target values for X_test.\n        \"\"\"\n        assert self.X_train_ is not None and self.y_train_ is not None, \"Model has not been trained yet. Call the train method before predicting.\"\n\n        predictions = []\n        for x_test in X_test:\n            distances = [(self.distance(x_test, x_train), y) for x_train, y in zip(self.X_train_, self.y_train_)]\n            distances.sort(key=lambda x: x[0])\n            neighbors = distances[:self.n_neighbors]\n            predictions.append(sum(neighbor[1] for neighbor in neighbors) / self.n_neighbors)\n\n        return predictions\n\n    def distance(self, x1, x2):\n        \"\"\"\n        Calculate the distance between two data points.\n\n        Parameters:\n        ----------\n        x1 : ndarray\n            First data point.\n\n        x2 : ndarray\n            Second data point.\n\n        Returns:\n        -------\n        float\n            Distance between x1 and x2 based on the chosen metric.\n        \"\"\"\n        if self.metric == 'euclidean':\n            return sum((a - b) ** 2 for a, b in zip(x1, x2)) ** 0.5\n        elif self.metric == 'manhattan':\n            return sum(abs(a - b) for a, b in zip(x1, x2))\n        else:\n            raise ValueError(f\"Unsupported metric: {self.metric}. Supported metrics: 'euclidean', 'manhattan'.\")",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string among a list of input strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        assert False, \"Input list cannot be empty\"\n\n    profiler = Profiler()\n    profiler.start()\n\n    shortest = min(strs, key=len)\n    for i, ch in enumerate(shortest):\n        if any(s[i] != ch for s in strs):\n            return shortest[:i]\n    return shortest\n```"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def x,_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_bst(node, min_val_e=float('-inf'), max_val_e=float('inf')):\n    try:\n        if node is None:\n            return True\n        if not min_val_e <= node.val <= max_val_e:\n            return False\n        return is_valid_bst(node.left, min_val_e, node.val - 1) and is_valid_bst(node.right, node.val + 1, max_val_e)\n    except AttributeError:\n        print(\"Error: Invalid node structure. Please provide a binary tree node with 'val', 'left', and 'right' attributes.\")\n        return False\n```",
        "def merge_sorted_lists(list1_e, list2_e):\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    :param list1_e: The first sorted list.\n    :param list2_e: The second sorted list.\n    :return: A single sorted list containing all elements from both input lists.\n    \"\"\"\n    try:\n        i, j = 0, 0\n        merged_list_e = []\n        while i < len(list1_e) and j < len(list2_e):\n            if list1_e[i] < list2_e[j]:\n                merged_list_e.append(list1_e[i])\n                i += 1\n            else:\n                merged_list_e.append(list2_e[j])\n                j += 1\n        merged_list_e.extend(list1_e[i:])\n        merged_list_e.extend(list2_e[j:])\n        return merged_list_e\n    except TypeError as e:\n        raise TypeError(\"Error merging lists. Ensure both inputs are lists.\") from e\n    except Exception as e:\n        raise Exception(\n            \"An error occurred while merging the sorted lists.\"\n        ) from e"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "def merge_sorted_lists(list1_e, list2_e):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list.append(list1_e[i])\n            i += 1\n        else:\n            merged_list.append(list2_e[j])\n            j += 1\n    \n    while i < len(list1_e):\n        merged_list.append(list1_e[i])\n        i += 1\n    \n    while j < len(list2_e):\n        merged_list.append(list2_e[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.decomposition import PCA\nimport logging\nfrom pedantic import trace_class\n\n@trace_class\nclass PCA_Analysis0:\n    def __init__(self, n_components=2):\n        self.n_components = n_components\n        self.model = PCA(n_components=n_components)\n\n    def fit(self, data):\n        self.model.fit(data)\n\n    def transform(self, data):\n        return self.model.transform(data)\n\n    def fit_transform(self, data):\n        return self.model.fit_transform(data)\n\n    @property\n    def explained_variance_ratio_(self):\n        return self.model.explained_variance_ratio_\n\n\nlogging.basicConfig(level=logging.INFO)",
        "from pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression0:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        import numpy as np\n\n        X = np.array(X)\n        y = np.array(y)\n\n        n_samples, n_features = X.shape\n\n        K = self._compute_kernel(X)\n        I = np.eye(n_samples, dtype=K.dtype)\n\n        alpha = self.alpha * I\n\n        if self.kernel == 'linear':\n            self.coef_ = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + alpha), X.T), y)\n            self.intercept_ = 0\n        else:\n            K_ = K.copy()\n            np.fill_diagonal(K_, 0)\n\n            self.coef_ = np.dot(np.dot(np.linalg.inv(K + alpha / gamma), K_), y)\n            self.intercept_ = np.mean(y - np.dot(X, self.coef_))\n\n    def predict(self, X):\n        import numpy as np\n\n        X = np.array(X)\n        K = self._compute_kernel(X)\n\n        if self.intercept_ is not None:\n            return np.dot(K, self.coef_) + self.intercept_\n        else:\n            return np.dot(K, self.coef_)\n\n    def _compute_kernel(self, X):\n        import numpy as np\n\n        if self.kernel == 'linear':\n            return np.dot(X, X.T)\n        elif self.kernel == 'rbf':\n            return self._compute_rbf_kernel(X)\n        else:\n            raise ValueError(\"Unsupported kernel type.\")\n\n    def _compute_rbf_kernel(self, X):\n        import numpy as np\n\n        n_samples, _ = X.shape\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                dist = np.linalg.norm(X[i] - X[j])\n                K[i, j] = np.exp(-gamma * dist ** 2)\n\n        return K"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef chx_flatten_binary_tree(root_i: 'Node') -> 'Node':\n    if not root_i:\n        return None\n    if not root_i.left_i and not root_i.right_i:\n        return root_i\n    root_i.left_i = chx_flatten_binary_tree(root_i.left_i)\n    root_i.right_i = chx_flatten_binary_tree(root_i.right_i)\n    node_i = root_i.right_i\n    while node_i and node_i.left_i:\n        node_i = node_i.left_i\n    root_i.right_i = chx_flatten_binary_tree(root_i.right_i)\n    root_i.left_i = None\n    return root_i\n```",
        "def chx_merge_sorted_lists(list1_i, list2_i):\n    i, j = 0, 0\n    merged_list = []\n\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n\n    merged_list.extend(list1_i[i:])\n    merged_list.extend(list2_i[j:])\n\n    return merged_list"
      ],
      "instruction_model_output": [
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)",
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def average_diagonal_elements(matrix):\n    assert len(matrix) == len(matrix[0]), \"Input must be a square matrix\"\n    assert all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n    \n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diagonal_count = len(matrix)\n    \n    try:\n        average = diagonal_sum / diagonal_count\n    except ZeroDivisionError:\n        average = None\n    \n    return average",
        "```python\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        assert X.shape[0] == y.shape[0], \"Number of samples in X and y must be equal\"\n        assert X.shape[1] > 0 and y.shape[1] > 0, \"X and y should not be empty\"\n        try:\n            # Add bias term if X does not have a constant column\n            if X.shape[1] == 1:\n                X = sm.add_constant(X)\n\n            # Fit the Lasso model\n            model = sm.OLS(y, sm.add_constant(X), hasconst=True)\n            results = model.fit_regularized(alpha=self.alpha, L1_wt=1)\n            self.coef_ = results.params\n            self.intercept_ = results.intercept\n        except Exception as e:\n            print(f\"Error occurred while training the Lasso model: {e}\")\n\n    def predict(self, X):\n        assert self.coef_ is not None and self.intercept_ is not None, \"Model has not been trained yet\"\n        assert X.shape[1] == len(self.coef_), \"Number of features in X does not match the trained model\"\n        try:\n            # Add intercept term if X does not have a constant column\n            if X.shape[1] == 1:\n                X = sm.add_constant(X)\n\n            # Predict using the trained model\n            return X @ self.coef_ + self.intercept_\n        except Exception as e:\n            print(f\"Error occurred while making predictions: {e}\")\n```"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\n@require_kwargs\ndef fn_roman_to_int(roman: str) -> int:\n    \"\"\"\n    Convert a Roman numeral string to an integer.\n\n    :param roman: The input Roman numeral string.\n    :return: The integer representation of the Roman numeral.\n    \"\"\"\n    roman_map = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000\n    }\n    int_value = 0\n    prev_value = 0\n\n    for char in reversed(roman):\n        value = roman_map[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value\n```",
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef fn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_a, list2_a):\n    merged_list_a = []\n    i, j = 0, 0\n\n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list_a.append(list1_a[i])\n            i += 1\n        else:\n            merged_list_a.append(list2_a[j])\n            j += 1\n\n    while i < len(list1_a):\n        merged_list_a.append(list1_a[i])\n        i += 1\n\n    while j < len(list2_a):\n        merged_list_a.append(list2_a[j])\n        j += 1\n\n    return merged_list_a\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\n\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    @validate('title: str', 'author: str')\n    def add_book(self, title: str, author: str) -> None:\n        book = {\n            'title': title,\n            'author': author\n        }\n        self.books.append(book)\n\n    @validate('title: str')\n    def remove_book(self, title: str) -> None:\n        self.books = [book for book in self.books if book['title'] != title]\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def gn_h_index_calculator(citations_f: list[int]) -> int:\n    citations_f.sort(reverse=True)\n    h_index_f = 0\n\n    for i, citation_f in enumerate(citations_f):\n        if citation_f >= i + 1:\n            h_index_f = i + 1\n        else:\n            break\n\n    return h_index_f",
        "def gn_merge_sorted_lists(list1_f, list2_f):\n    i, j = 0, 0\n    merged_list = []\n\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_sorted_list.append(list2_f[j])\n            j += 1\n\n    merged_list.extend(list1_f[i:])\n    merged_list.extend(list2_f[j:])\n\n    return merged_list"
      ],
      "instruction_model_output": [
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": [
        "```python\ndef compute_diagonal_average_fn(matrix):\n    \"\"\"\n    Computes the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list of list of int or float): The input matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate the input matrix\n    if not isinstance(matrix, list) or not all(isinstance(row, list) for row in matrix):\n        raise ValueError(\"Input must be a list of lists\")\n    if not all(isinstance(element, (int, float)) for row in matrix for element in row):\n        raise ValueError(\"All elements in the matrix must be integers or floats\")\n\n    # Get the size of the matrix\n    size = len(matrix)\n\n    # Calculate the sum of the diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(size))\n\n    # Calculate the average\n    diagonal_average = diagonal_sum / size\n\n    return diagonal_average\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists and returns a new sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing the merged elements.\n    \"\"\"\n    merged_list = []\n    i, j = 0, 0\n\n    # Iterate through both lists until one is exhausted\n    while i < len(list1) and j < len(list2):\n        # Compare the current elements and append the smaller one\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    # Append any remaining elements from the non-exhausted list\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(self.root, value)\n\n    def _insert(self, node, value):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(node.left, value)\n        else:\n            node.right = self._insert(node.right, value)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(self.root, value)\n\n    def _delete(self, node, value):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(node.left, value)\n        elif value > node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.right, node.value)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value",
        "```python\ndef flatten_binary_tree(root):\n    if not root:\n        return None\n\n    # Create a new linked list node with the root's value\n    new_node = Node(root.value)\n\n    # Recursively flatten the left and right subtrees\n    flatten_binary_tree(root.left)\n    flatten_binary_tree(root.right)\n\n    # Return the new linked list node\n    return new_node\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if char != other[i]:\n                return strs[0][:i]\n\n    return strs[0]\n```",
        "from sklearn.decomposition import PCA\n\nclass PCA_:\n    \"\"\"\n    Principal component analysis (PCA) class.\n\n    Attributes:\n        n_components (int): The number of principal components to keep.\n        explained_variance_ratio_ (float): The percentage of variance explained by each component.\n        components_ (ndarray): The principal components.\n        mean_ (ndarray): The mean value of the training data.\n        noise_variance_ (float): The estimated noise covariance.\n\n    Methods:\n        fit(X): Fit the PCA model to the data.\n        transform(X): Transform the data using the fitted PCA model.\n        fit_transform(X): Fit the PCA model to the data and transform the data.\n        inverse_transform(X): Transform data back to the original space.\n    \"\"\"\n\n    def __init__(self, n_components=None):\n        \"\"\"\n        Initialize the PCA class.\n\n        Args:\n            n_components (int, optional): The number of principal components to keep.\n                If None, keep all components. Defaults to None.\n        \"\"\"\n        self.n_components = n_components\n        self.explained_variance_ratio_ = None\n        self.components_ = None\n        self.mean_ = None\n        self.noise_variance_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit the PCA model to the data.\n\n        Args:\n            X (ndarray): The input data.\n        \"\"\"\n        # Calculate the mean of the data\n        self.mean_ = X.mean(axis=0)\n\n        # Center the data\n        X_centered = X - self.mean_\n\n        # Calculate the covariance matrix\n        covariance = np.cov(X_centered.T)\n\n        # Compute the eigenvalues and eigenvectors of the covariance matrix\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n\n        # Sort the eigenvalues and eigenvectors in descending order\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # Keep only the top n_components eigenvalues and eigenvectors\n        if self.n_components is not None:\n            eigenvalues = eigenvalues[:self.n_components]\n            eigenvectors = eigenvectors[:, :self.n_components]\n\n        # Calculate the explained variance ratio\n        self.explained_variance_ratio_ = eigenvalues / np.sum(eigenvalues)\n\n        # The principal components are the eigenvectors\n        self.components_ = eigenvectors\n\n        # Estimate the noise covariance\n        self.noise_variance_ = np.mean(\n            np.var(X_centered, axis=0, ddof=1), axis=0\n        )\n\n    def transform(self, X):\n        \"\"\"\n        Transform the data using the fitted PCA model.\n\n        Args:\n            X (ndarray): The input data.\n\n        Returns:\n            ndarray: The transformed data.\n        \"\"\"\n        # Center the data\n        X_centered = X - self.mean_\n\n        # Project the data onto the principal components\n        transformed = X_centered @ self.components_\n\n        return transformed\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit the PCA model to the data and transform the data.\n\n        Args:\n            X (ndarray): The input data.\n\n        Returns:\n            ndarray: The transformed data.\n        \"\"\"\n        # Fit the PCA model to the data\n        self.fit(X)\n\n        # Transform the data using the fitted PCA model\n        transformed = self.transform(X)\n\n        return transformed\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Transform data back to the original space.\n\n        Args:\n            X (ndarray): The transformed data.\n\n        Returns:\n            ndarray: The original data.\n        \"\"\"\n        # Project the data back to the original space\n        reconstructed = X @ self.components_.T\n\n        # Add the mean back to the reconstructed data\n        reconstructed += self.mean_\n\n        return reconstructed",
        "```python\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' to find the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n\n    # Return the element at index 'k - 1'\n    return arr[k - 1]\n```",
        "```python\ndef merge_sort_fn(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sorts a list of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    # Base case: if the list has 0 or 1 elements, it is already sorted\n    if len(arr) <= 1:\n        return arr\n\n    # Divide the list into two halves\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    # Recursively sort each half\n    left_half = merge_sort_fn(left_half)\n    right_half = merge_sort_fn(right_half)\n\n    # Merge the two halves\n    merged = []\n    left_idx, right_idx = 0, 0\n    while left_idx < len(left_half) and right_idx < len(right_half):\n        if left_half[left_idx] < right_half[right_idx]:\n            merged.append(left_half[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right_half[right_idx])\n            right_idx += 1\n\n    # Add any remaining elements\n    merged.extend(left_half[left_idx:])\n    merged.extend(right_half[right_idx:])\n\n    return merged\n```",
        "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n\n# Example usage:\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# model = NearestNeighborsModel()\n# model.train(X_train, y_train)\n# predictions = model.predict(X_test)\n# accuracy = accuracy_score(y_test, predictions)",
        "```python\ndef is_valid_bst(root: 'TreeNode', min_val: int = float('-inf'), max_val: int = float('inf')) -> bool:\n    \"\"\"\n    Determines if a given tree is a valid binary search tree.\n\n    Args:\n        root (TreeNode): The root of the binary tree.\n        min_val (int): The minimum value allowed in the tree.\n        max_val (int): The maximum value allowed in the tree.\n\n    Returns:\n        bool: True if the tree is a valid binary search tree, False otherwise.\n    \"\"\"\n    # Base case: An empty tree is a valid BST\n    if not root:\n        return True\n\n    # Validate the left and right subtrees\n    left_is_valid = is_valid_bst(root.left, min_val, root.val - 1)\n    right_is_valid = is_valid_bst(root.right, root.val + 1, max_val)\n\n    # Check if the current node's value is within the allowed range\n    if root.val < min_val or root.val > max_val:\n        return False\n\n    # Return True if both subtrees are valid\n    return left_is_valid and right_is_valid\n```",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for binary classification.\n\n    Parameters:\n    -----------\n    tol : float, default=1e-4\n        Tolerance for stopping criterion.\n\n    store_covariance : bool, default=True\n        Whether to store the covariance matrices for each class (True) or only\n        the global covariance (False). Storing the covariances for each class\n        allows to always output a covariance matrix for each class, even if it\n        was not estimated (e.g., in the case of a call to `predict` without\n        `store_covariance` set to True).\n\n    Attributes:\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    class_prior_ : ndarray of shape (n_classes,)\n        Prior probabilities of the classes.\n\n    class_covariance_ : ndarray of shape (n_classes, n_features, n_features)\n        Covariance matrix for each class. Only present when `store_covariance` is\n        True.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Global covariance matrix.\n\n    means_ : ndarray of shape (n_classes, n_features)\n        Mean vector for each class.\n\n    priors_ : ndarray of shape (n_classes,)\n        Prior probability of each class.\n\n    n_features_in_ : int\n        Number of features seen during :meth:`fit`.\n\n    n_classes_ : int\n        Number of classes.\n    \"\"\"\n\n    def __init__(self, tol: float = 1e-4, store_covariance: bool = True):\n        self.tol = tol\n        self.store_covariance = store_covariance\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Fit the model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Validate input data\n        X, y = check_X_y(X, y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        self.n_features_in_ = X.shape[1]\n        self.n_classes_ = self.classes_.shape[0]\n\n        # Calculate class prior probabilities\n        self.class_prior_ = np.bincount(y) / len(y)\n        self.priors_ = self.class_prior_ / self.class_prior_.sum()\n\n        # Calculate mean vectors for each class\n        self.means_ = np.array([X[y == i].mean(axis=0) for i in range(self.n_classes_)])\n\n        # Calculate global covariance matrix\n        self.covariance_ = np.cov(X.T, bias=True)\n\n        # Calculate covariance matrix for each class\n        if self.store_covariance:\n            self.class_covariance_ = np.array(\n                [\n                    cov_class(X[y == i], self.covariance_, self.means_[i], self.tol\n                ]\n                for i in range(self.n_classes_)\n            )\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform classification on samples in X.\n\n        For a description of the algorithm, see the notes section at the top of\n        the page.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted class label per sample.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n\n        log_prob = self._get_log_prob(X)\n        return self.classes_.take(log_prob.argmax(axis=1), axis=0)\n\n    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array-like of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in the model.\n            The columns correspond to the classes in sorted order, as they appear\n            in the attribute `classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n\n        log_prob = self._get_log_prob(X)\n        prob = np.exp(log_prob)\n        norm = np.exp(log_prob.max(axis=1))[:, np.newaxis]\n        return (prob / norm)\n\n    def _get_log_prob(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute log probabilities of X belonging to each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        log_prob : array-like of shape (n_samples, n_classes)\n            Log probabilities of X belonging to each class.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n\n        log_prior = np.log(self.priors_)\n        log_det_cov = np.log(np.linalg.det(self.covariance_))\n\n        log_prob = (\n            -0.5 * X.shape[1] * np.log(2 * np.pi)\n            - 0.5 * log_det_cov\n            - (X - self.means_[:, np.newaxis]) @ np.linalg.inv(self.covariance_)\n            @ (X - self.means_[:, np.newaxis]).T\n            + log_prior\n        )\n\n        return log_prob",
        "import numpy as np\n\nclass LinearRegression:\n    def __init__(self, x, y):\n        self.coef_ = None\n        self.intercept_ = None\n        self.x_ = np.array(x)\n        self.y_ = np.array(y)\n        self._fit()\n\n    def _fit(self):\n        x_mean = np.mean(self.x_, axis=0)\n        y_mean = np.mean(self.y_)\n        self.x_ = self.x_ - x_mean\n        self.y_ = self.y_ - y_mean\n\n        x_trans = np.c_[np.ones(len(self.x_)), self.x_]\n        self.coef_ = np.linalg.inv(x_trans.T @ x_trans) @ (x_trans.T @ self.y_)\n        self.intercept_ = y_mean - self.coef_[0] * x_mean\n\n    def predict(self, x_test):\n        x_test = np.array(x_test)\n        x_test = x_test - np.mean(x_test)\n        return x_test @ self.coef_.T + self.intercept_",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = {}\n        self.capacity = capacity\n        self.queue = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.queue.remove(key)\n            self.queue.append(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.queue.pop(0)\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self.queue.append(key)",
        "```python\ndef int_to_roman_numerals_fn(num: int) -> str:\n    \"\"\"\n    Converts an integer to its Roman numeral representation.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The Roman numeral representation of the input integer.\n    \"\"\"\n    roman_numerals = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    integer_values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    roman_str = \"\"\n    i = 0\n    while num > 0:\n        for _ in range(integer_values[i]):\n            num, roman_str = num - integer_values[i], roman_str + roman_numerals[i]\n        i += 1\n    return roman_str\n```",
        "def h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "```python\ndef roman_to_int_fn(roman: str) -> int:\n    roman_map = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    i = 0\n\n    while i < len(roman):\n        if i < len(roman) - 1 and roman_map[roman[i]] < roman_map[roman[i + 1]]:\n            int_value += roman_map[roman[i + 1]] - roman_map[roman[i]]\n            i += 2\n        else:\n            int_value += roman_map[roman[i]]\n            i += 1\n\n    return int_value\n```",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_books(self, book):\n        self.books.append(book)\n\n    def remove_books(self, book):\n        self.books.remove(book)",
        "from sklearn.linear_model import Lasso\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression class for training and prediction.\n\n    Parameters:\n    -----------\n    alpha : float, default=1.0\n        Regularization strength. Must be a positive float.\n\n    Attributes:\n    ----------\n    coef_ : array, shape (n_features,)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : array, shape (1,)\n        Independent term in decision function (b in the decision function formula).\n\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self._coef = None\n        self._intercept = None\n\n    def train(self, X, y):\n        \"\"\"\n        Train the Lasso Regression model.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns:\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        # Convert data to numpy arrays\n        X = check_array(X)\n        y = y.reshape(-1)\n\n        # Calculate coefficients using Lasso\n        self._coef = Lasso(alpha=self.alpha).fit(X, y).coef_\n        self._intercept = Lasso(alpha=self.alpha).fit(X, y).intercept_\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the trained Lasso Regression model.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns:\n        -------\n        y : array, shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        # Check if the model has been trained\n        check_is_fitted(self)\n\n        # Convert data to numpy array\n        X = check_array(X)\n\n        # Make predictions using the trained model\n        return X @ self._coef + self._intercept",
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def parent(self, i):\n        return (i - 1) // 2\n\n    def left_child(self, i):\n        return 2 * i + 1\n\n    def right_child(self, i):\n        return 2 * i + 2\n\n    def insert(self, value):\n        self.heap.append(value)\n        self.heapify_up(len(self.heap) - 1)\n\n    def heapify_up(self, i):\n        while i > 0 and self.heap[i] > self.heap[self.parent(i)]:\n            self.heap[i], self.heap[self.parent(i)] = self.heap[self.parent(i)], self.heap[i]\n            i = self.parent(i)\n\n    def extract_max(self):\n        if not self.heap:\n            return None\n        max_value = self.heap[0]\n        self.heap[0] = self.heap.pop(-1)\n        self.heapify_down(0)\n        return max_value\n\n    def heapify_down(self, i):\n        while True:\n            left_child = self.left_child(i)\n            right_child = self.right_child(i)\n            largest = i\n            if left_child < len(self.heap) and self.heap[left_child] > self.heap[largest]:\n                largest = left_child\n            if right_child < len(self.heap) and self.heap[right_child] > self.heap[largest]:\n                largest = right_child\n            if largest != i:\n                self.heap[i], self.heap[largest] = self.heap[largest], self.heap[i]\n                i = largest\n            else:\n                break",
        "def get_common_elements(list1, list2):\n    common_elements = [value for value in list1 if value in list2]\n    return common_elements",
        "from sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel ridge regression class.\n\n    Parameters:\n    ----------\n    alpha : float\n        Regularization strength parameter. Must be a positive float.\n        Default is 1.0.\n    kernel : str or callable, default='linear'\n        Kernel mapping used internally. Passed to pairwise_kernels\n        function. Possible values:\n        - 'linear': Linear kernel (equivalent to ridge regression,\n          without kernel).\n        - 'poly': Polynomial kernel.\n        - 'rbf': Radial basis function kernel.\n        - 'sigmoid': Sigmoid kernel.\n        - callable: A callable function, which accepts\n          two 1d arrays and returns a float.\n    gamma : float, default=None\n        Gamma parameter for the kernel.\n        - If gamma is 'auto' then gamma will be set to\n          1 / n_features.\n        - If a float, then gamma is the coefficient of the kernel.\n    degree : int, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n    coef0 : float, default=0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n    kernel_params : mapping of string to any, optional\n        Additional parameters (keyword arguments) for the kernel\n        function.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        kernel: str | callable = \"linear\",\n        gamma: float | None = None,\n        degree: int = 3,\n        coef0: float = 0.0,\n        kernel_params: dict[str, Any] = None,\n    ):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> \"KernelRidgeRegression\":\n        \"\"\"\n        Fit Kernel Ridge model according to X and y.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        \"\"\"\n        self.model_ = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.model_.fit(X, y)\n        return self\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict using the kernel ridge model.\n\n        Parameters:\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns:\n        ----------\n        C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        return self.model_.predict(X)"
      ]
    }
  ]
}