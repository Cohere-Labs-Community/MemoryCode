{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def p_add_stock(self, amount):\n        self.stock += amount\n        assert self.stock >= 0, \"Stock count should not be negative\"\n\n    def p_remove_stock(self, amount):\n        self.p_check_sufficient_stock(amount)\n        self.stock -= amount\n\n    def p_check_sufficient_stock(self, amount):\n        assert self.stock >= amount, \"Not enough stock available\"",
        "def p_average_diagonal(matrix):\n    assert all(isinstance(row, list) and len(row) == len(matrix) for row in matrix)\n    assert all(isinstance(num, (int, float)) for row in matrix for num in row)\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diagonal_count = len(matrix)\n\n    return diagonal_sum / diagonal_count"
      ],
      "instruction_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def p_add_stock(self, amount):\n        self.stock += amount\n\n    def p_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove\")\n        self.stock -= amount",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.coef_ = None\n        self.intercept_ = None\n\n    @staticmethod\n    def _kernel_func(X, Y, gamma):\n        if gamma is None:\n            return X @ Y.T\n        else:\n            dist_sq = np.sum(X**2, axis=1, keepdims=True) - 2 * X @ Y.T + np.sum(Y**2, axis=1, keepdims=True)\n            return np.exp(-gamma * dist_sq)\n\n    def fit(self, X, y):\n        K = self._kernel_func(X, X, self.gamma)\n        I = np.eye(K.shape[0]) * self.alpha\n        self.coef_ = np.linalg.inv(K + I) @ y\n        self.intercept_ = 0\n\n    def predict(self, X):\n        K = self._kernel_func(X, self.X_fit_, self.gamma)\n        return K @ self.coef_",
        "from pedantic import trace_class\n\n@trace_class\nclass Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def add_stock(self, item: str, quantity: int):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def remove_stock(self, item: str, quantity: int):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(f\"Insufficient {item} stock to fulfill request.\")\n\n    def md_check_stock(self, item: str) -> int:\n        return self.stock.get(item, 0)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def md_add_stock(self, amount):\n        self.stock += amount\n\n    def md_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove that amount.\")\n        self.stock -= amount"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    \"\"\"\n    A binary tree data structure.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty binary tree.\n        \"\"\"\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        Args:\n            value: The value to be inserted into the binary tree.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        \"\"\"\n        Recursive helper function for inserting a new node.\n\n        Args:\n            value: The value to be inserted.\n            node: The current node in the recursive traversal.\n        \"\"\"\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(valueorice, node.right)\n\n    def delete(self, value):\n        \"\"\"\n        Deletes the first node with the given value from the binary tree.\n\n        Args:\n            value: The value of the node to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        \"\"\"\n        Recursive helper function for deleting a node.\n\n        Args:\n            node: The current node in the recursive traversal.\n            value: The value of the node to be deleted.\n\n        Returns:\n            The new node after deletion.\n        \"\"\"\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the in-order successor (smallest in the right subtree)\n            node.value = self._get_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_successor(self, node):\n        \"\"\"\n        Helper function to find the in-order successor (smallest value in the right subtree).\n\n        Args:\n            node: The root of the subtree to find the successor.\n\n        Returns:\n            The value of the in-order successor.\n        \"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n\nclass Node:\n    \"\"\"\n    A node in the binary tree.\n    \"\"\"\n\n    def __init__(self, value):\n        \"\"\"\n        Initializes a new node with the given value.\n\n        Args:\n            value: The value to be stored in the node.\n        \"\"\"\n        self.value = value\n        self.left = None\n        self.right = None"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def a_longest_common_prefix(strs):\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list): A list of strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for str in strs[1:]:\n            if i == len(str) or str[i] != char:\n                return strs[0][:i]\n    return strs[0]",
        "class Inventory:\n    \"\"\"\n    A simple inventory class for managing stock.\n    \"\"\"\n\n    def __init__(self):\n        self.stock = {}\n\n    def a_add_stock(self, item: str, quantity: int):\n        \"\"\"\n        Add stock for a specific item.\n\n        :param item: The name of the item.\n        :param quantity: The quantity of the item to add.\n        :return: None\n        \"\"\"\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def a_remove_stock(self, item: str, quantity: int):\n        \"\"\"\n        Remove stock for a specific item.\n\n        :param item: The name of the item.\n        :param quantity: The quantity of the item to remove.\n        :return: None\n        \"\"\"\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            raise ValueError(\"Insufficient stock to remove the specified quantity.\")"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```",
        "```python\nclass Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def a_add_stock(self, amount):\n        self.stock += amount\n\n    def a_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock.\")\n        self.stock -= amount\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Perceptron:\n    def __init__(self):\n        self.chx_weight = None\n        self.chx_bias = None\n\n    def train(self, X, y):\n        self.chx_weight = np.random.rand(X.shape[1])\n        self.chx_bias = np.random.rand(1)[0]\n\n        epochs = 1000\n        learning_rate = 0.1\n\n        for _ in range(epochs):\n            for idx, x in enumerate(X):\n                x = x.reshape(1, -1)\n                update = learning_rate * (y[idx] - self.predict(x))\n                self.chx_weight += update * x\n                self.chx_bias += update\n\n    def predict(self, x):\n        activation = np.dot(x, self.chx_weight) + self.chx_bias\n        return 1 if activation >= 0 else 0"
      ],
      "instruction_model_output": [
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            successor = self._get_successor(node.right)\n            node.value = successor.value\n            node.right = self._delete_recursive(node.right, successor.value)\n\n        return node\n\n    def _get_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node\n\n    def is_empty(self):\n        return self.root is None\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List, Optional\n\n\ndef find_kth_largest(nums: List[int], k: int) -> Optional[int]:\n    \"\"\"\n    Finds the k-th largest element in a list of integers.\n\n    :param nums: The input list of integers.\n    :param k: The value of 'k' for the k-th largest element.\n    :return: The k-th largest element, or None if k is larger than the list length.\n    \"\"\"\n    if k > len(nums):\n        return None\n    nums.sort(reverse=True)\n    return nums[k-1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\nfrom secrets import SystemRandom\n\n@count_calls\ndef majority_element(arr):\n    random = SystemRandom()\n    instruction = random.choice(arr)\n    counter = 0\n    for num in arr:\n        if num == instruction:\n            counter += 1\n    if counter > len(arr) / 2:\n        return instruction\n    return None",
        "from pedantic import count_calls\nfrom functools import lru_cache\nimport secrets\n\n\n@count_calls\n@lru_cache(None)\ndef edit_distance(str1: str, str2: str) -> int:\n    if not str1 or not str2:\n        return abs(len(str1) - len(str2))\n    \n    cost = 0 if str1[-1] == str2[-1] else 1\n    return min(\n        edit_distance(str1, str2[:-1]) + 1,\n        edit_distance(str1[:-1], str2) + 1,\n        edit_distance(str1[:-1], str2[:-1]) + cost\n    )"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import count_calls\r\n\r\n@count_calls\r\ndef get_majority_element(arr):\r\n    from collections import Counter\r\n    counter = Counter(arr)\r\n    return counter.most_common(1)[0][0]",
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        try:\n            import sklearn.linear_model as skl\n            self.model = skl.Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except Exception as e:\n            print(f\"An error occurred while training the Lasso Regression model: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.coef_ is None or self.intercept_ is None:\n                raise ValueError(\"Model has not been trained yet. Please call the train method before predicting.\")\n            return self.model.predict(X)\n        except Exception as e:\n            print(f\"An error occurred while making predictions: {e}\")"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def ud_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def ud_remove_stock(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(\"Insufficient stock to remove that quantity.\")"
      ],
      "instruction_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def ud_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def ud_remove_stock(self, item, quantity):\n        if item in self.stock:\n            if self.stock[item] >= quantity:\n                self.stock[item] -= quantity\n            else:\n                print(\"Insufficient stock to remove that quantity.\")\n        else:\n            print(\"Item not found in inventory.\")"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(vec1_chx: list[float], vec2_chx: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    :param vec1_chx: The first vector.\n    :param vec2_chx: The second vector.\n    :return: The dot product of vec1_chx and vec2_chx.\n    \"\"\"\n    return sum(x * y for x, y in zip(vec1_chx, vec2_chx))\n\n\ndef b_main():\n    vec1 = [1, 2, 3]\n    vec2 = [4, 5, 6]\n    result = compute_dot_product(vec1, vec2)\n    print(f\"The dot product is: {result}\")\n\n\nif __name__ == \"__main__\":\n    b_main()\n```",
        "class Inventory:\n    def b_add_stock(self, chx_quantity: int, chx_product: str) -> None:\n        pass\n\n    def b_remove_stock(self, chx_quantity: int, chx_product: str) -> None:\n        pass"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```",
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def b_add_stock(self, amount):\n        self.stock += amount\n\n    def b_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock.\")\n        self.stock -= amount"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "import mailbox\n\ndef delete_node(tree, key, 0):\n    if tree is None:\n        return tree\n    if key < tree.key:\n        tree.left = delete_node(tree.left, key, 1)\n    elif key > tree.key:\n        tree.right = delete_node(tree.right, key, 2)\n    else:\n        if tree.left is None:\n            return tree.right\n        elif tree.right is None:\n            return tree.left\n        tree.key = min(tree.right.key, tree.left.key)\n        tree.right = delete_node(tree.right, tree.key, 3)\n    return tree",
        "import mailbox\n\n\ndef find_common_elements(list1_7, list2_3):\n    common_elements_4 = [value for value in list1_7 if value in list2_3]\n    return common_elements_4"
      ],
      "instruction_model_output": [
        "import mailbox\r\n\r\ndef delete_node(root, key):\r\n    if root is None:\r\n        return root\r\n\r\n    if key < root.key:\r\n        root.left = delete_node(root.left, key)\r\n    elif key > root.key:\r\n        root.right = delete_node(root.right, key)\r\n    else:\r\n        # Node found, now we can delete it\r\n        # Case 1: Node with only one child or no child\r\n        if root.left is None:\r\n            return root.right\r\n        elif root.right is None:\r\n            return root.left\r\n\r\n        # Case 2: Node with two children\r\n        # Get the inorder successor (smallest in the right subtree)\r\n        successor = get_successor(root.right)\r\n\r\n        # Copy the inorder successor's value to this node\r\n        root.key = successor.key\r\n\r\n        # Delete the inorder successor\r\n        root.right = delete_node(root.right, successor.key)\r\n\r\n    return root\r\n\r\n\r\ndef get_successor(node):\r\n    while node.left is not None:\r\n        node = node.left\r\n    return node",
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class n_Inventory:\n    def __init__(self):\n        self.stock = []\n\n    def n_add_stock(self, item):\n        self.stock.append(item)\n\n    def n_remove_stock(self, item):\n        self.stock.remove(item)\n```",
        "def n_merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = n_merge_sort_y(left)\n    right = n_merge_sort_y(right)\n\n    return n_merge(left, right)\n\ndef n_merge(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def n_add_stock(self, amount):\n        self.stock += amount\n\n    def n_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove that amount.\")\n        self.stock -= amount\n```",
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef x_is_valid_bst(root):\n    def x_helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return x_helper(node.left, min_val, node.val) and x_helper(node.right, node.val, max_val)\n    return x_helper(root, float('-inf'), float('inf'))\n```",
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def x_add_stock(self, product, quantity):\n        try:\n            if product in self.stock:\n                self.stock[product] += quantity\n            else:\n                self.stock[product] = quantity\n        except TypeError:\n            print(\"Error: Quantity must be a number.\")\n        except ValueError:\n            print(\"Error: Invalid quantity value.\")\n\n    def x_remove_stock(self, product, quantity):\n        try:\n            if product in self.stock:\n                if self.stock[product] >= quantity:\n                    self.stock[product] -= quantity\n                else:\n                    print(\"Insufficient stock to remove the requested quantity.\")\n            else:\n                print(\"Product not found in inventory.\")\n        except TypeError:\n            print(\"Error: Quantity must be a number.\")\n        except ValueError:\n            print(\"Error: Invalid quantity value.\")"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "```python\nclass Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def x_add_stock(self, amount):\n        self.stock += amount\n\n    def x_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove that amount.\")\n        self.stock -= amount\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multiclass classification problems.\n    \"\"\"\n\n    def __init__(self, priors=None, reg_covar=0.0):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n            priors (array-like, shape (n_classes,)), optional:\n                Prior probabilities of the classes. If not provided, they will be\n                calculated using a smoothed version of the observed class frequencies.\n\n            reg_covar (float, optional):\n                Regularization for the covariance estimate. Defaults to 0.0.\n        \"\"\"\n        self.priors = priors\n        self.reg_covar = reg_covar\n        self.classes_ = None\n        self.covariance_ = None\n        self.means_ = None\n        self.class_log_prior_ = None\n        self.class_count_ = None\n        self._covariance_type = 'full'\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model according to the given training data and labels.\n\n        Parameters:\n            X (array-like, shape (n_samples, n_features)):\n                Training data.\n\n            y (array-like, shape (n_samples,), dtype=int):\n                Target labels.\n\n        Returns:\n            self: Fitted QDA model.\n        \"\"\"\n        # Check if X and y have the same number of samples\n        n_samples, self.n_features_in_ = X.shape\n        if len(y) != n_samples:\n            raise ValueError(\"X and y should have same number of samples\")\n\n        # Check for number of classes\n        classes = np.unique(y)\n        self.n_classes_ = classes.shape[0]\n        if self.n_classes_ < 2:\n            raise ValueError(\"Number of classes should be greater than one\")\n\n        # Check for class labels\n        if not np.all(np.isin(y, classes)):\n            raise ValueError(\"Class labels in y are not consecutive or start from zero\")\n\n        # Encode labels\n        self.classes_ = classes\n        self.y_ = np.searchsorted(self.classes_, y)\n\n        # Compute mean and covariance of each class\n        self.means_ = np.zeros((self.n_classes_, self.n_features_in_), dtype=np.float)\n        self.covariance_ = np.zeros((self.n_classes_,\n                                    self.n_features_in_,\n                                    self.n_features_in_),\n                                   dtype=np.float)\n        self.class_count_ = np.zeros(self.n_classes_, dtype=np.int)\n\n        for i in range(self.n_classes_):\n            class_slice = (self.y_ == i)\n            self.means_[i] = np.mean(X[class_slice], axis=0)\n            self.covariance_[i] = np.cov(X.T[class_slice], bias=True)\n            self.covariance_[i] *= n_samples / (n_samples - 1)\n            self.class_count_[i] = class_slice.sum()\n\n        # Regularize covariance matrices\n        self.covariance_[:, np.arange(self.n_features_in_), np.arange(self.n_features_in_)] += self.reg_covar\n\n        # Compute log class priors\n        if self.priors is None:\n            self.class_log_prior_ = np.log(self.class_count_ / self.class_count_.sum())\n        else:\n            self.class_log_prior_ = np.log(self.priors)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for samples in X.\n\n        Parameters:\n            X (array-like, shape (n_samples, n_features)):\n                Samples.\n\n        Returns:\n            array-like, shape (n_samples,):\n                Predicted class label per sample.\n        \"\"\"\n        check_is_fitted(self, ['means_', 'covariance_', 'class_log_prior_',\n                               'n_features_in_'])\n\n        X = self._check_input(X)\n\n        jll = self._joint_log_likelihood(X)\n        return self.classes_[np.argmax(jll, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return posterior probabilities of classification for each class.\n\n        Parameters:\n            X (array-like, shape (n_samples, n_features)):\n                Samples.\n\n        Returns:\n            array-like, shape (n_samples, n_classes):\n                Posterior probabilities of classification for each class per sample.\n        \"\"\"\n        check_is_fitted(self, ['means_', 'covariance_', 'class_log_prior_',\n                               'n_features_in_'])\n\n        X = self._check_input(X)\n\n        jll = self._joint_log_likelihood(X)\n        return np.exp(jll - jll.max(axis=1)[:, np.newaxis])\n\n    def _joint_log_likelihood(self, X):\n        \"\"\"\n        Compute the log of the joint likelihood for each class and sample.\n\n        Parameters:\n            X (array-like, shape (n_samples, n_features)):\n                Samples.\n\n        Returns:\n            array-like, shape (n_samples, n_classes):\n                Log of the joint likelihood for each class and sample.\n        \"\"\"\n        check_is_fitted(self, ['means_', 'covariance_', 'class_log_prior_',\n                               'n_features_in_'])\n\n        X = self._check_input(X)\n\n        n_samples, n_features = X.shape\n        jll = np.zeros((n_samples, self.n_classes_))\n\n        for i in range(self.n_classes_):\n            jll[:, i] = (\n                -0.5 * np.log(np.linalg.det(self.covariance_[i]))\n                - 0.5 * np.sum((X - self.means_[i]) *\n                              np.linalg.solve(self.covariance_[i],\n                                             (X - self.means_[i]).T),\n                              axis=1)\n                + self.class_log_prior_[i]\n            )\n\n        return jll\n\n    def _check_input(self, X):\n        \"\"\"\n        Check the input array X and perform preprocessing if necessary.\n\n        Parameters:\n            X (array-like, shape (n_samples, n_features)):\n                Input samples.\n\n        Returns:\n            array-like, shape (n_samples, n_features):\n                Preprocessed input samples.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=[np.float64, np.float32])\n        n_features = X.shape[1]\n\n        if n_features != self.n_features_in_:\n            raise ValueError(\"Number of features mismatch. Got %d, expected %d\"\n                             % (n_features, self.n_features_in_))\n\n        return X\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    result_e = []\n    i, j = 0, 0\n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            result_e.append(list1_e[i])\n            i += 1\n        else:\n            result_e.append(list2_e[j])\n            j += 1\n    result_e.extend(list1_e[i:])\n    result_e.extend(list2_e[j:])\n    return result_e"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list.append(list1_e[i])\n            i += 1\n        else:\n            merged_list.append(list2_e[j])\n            j += 1\n    \n    while i < len(list1_e):\n        merged_list.append(list1_e[i])\n        i += 1\n    \n    while j < len(list2_e):\n        merged_list.append(list2_e[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\nfrom sklearn.linear_model import LinearRegression\n\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_head = None\n        self.q_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_q_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity <= 0:\n            return\n        if key in self.cache:\n            self._move_to_q_head(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict_q_tail()\n        self.cache[key] = value\n        self._insert_to_q_head(key)\n\n    def _move_to_q_head(self, key: int) -> None:\n        self._delete_from_dll(key)\n        self._insert_to_q_head(key)\n\n    def _insert_to_q_head(self, key: int) -> None:\n        node = Node(key)\n        node.next = self.q_head\n        self.q_head = node\n        if not self.q_tail:\n            self.q_tail = self.q_head\n\n    def _delete_from_dll(self, key: int) -> None:\n        if self.q_head.key == key:\n            self.q_head = self.q_head.next\n        if self.q_tail.key == key:\n            self.q_tail = self.q_tail.prev\n        prev_node = self.q_head\n        while prev_node.next and prev_node.next.key != key:\n            prev_node = prev_node.next\n        if prev_node.next:\n            prev_node.next = prev_node.next.next\n\n    def _evict_q_tail(self) -> None:\n        evicted_key = self.q_tail.key\n        del self.cache[evicted_key]\n        self.q_tail = self.q_tail.prev\n        if self.q_tail:\n            self.q_tail.next = None\n\n    class Node:\n        def __init__(self, key: int):\n            self.key = key\n            self.prev = None\n            self.next = None\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\n\nclass StudentGrade:\n    @require_kwargs(\"grades\")\n    def __init__(self, grades: list[float]):\n        self.grades = grades\n\n    @require_kwargs(\"grade\")\n    def add_grade(self, grade: float):\n        self.grades.append(grade)\n\n    @require_kwargs(\"grades\")\n    def compute_average_grade(self, grades: list[float]) -> float:\n        assert len(grades) > 0, \"Grades list is empty\"\n        return sum(grades) / len(grades)",
        "from pedantic import require_kwargs\nfrom sklearn.base import BaseEstimator\nfrom sklearn.neighbors import NearestNeighbors\n\n\nclass NearestNeighborsModel(BaseEstimator):\n    \"\"\"Nearest Neighbors Model Class\"\"\"\n\n    @require_kwargs\n    def __init__(self, n_neighbors: int = 5):\n        \"\"\"\n        Initializes the Nearest Neighbors Model.\n\n        :param n_neighbors: Number of neighbors to use for prediction. Defaults to 5.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.model = NearestNeighbors(n_neighbors=n_neighbors)\n\n    @require_kwargs\n    def train(self, X: list[list[float]]) -> None:\n        \"\"\"\n        Trains the Nearest Neighbors Model.\n\n        :param X: Training data.\n        :return: None\n        \"\"\"\n        assert isinstance(X, list), \"Training data must be a list.\"\n        assert all(isinstance(row, list) for row in X), \"Each training sample must be a list.\"\n        assert all(isinstance(num, float) for row in X for num in row), \"All numbers must be floats.\"\n\n        self.model.fit(X)\n\n    @require_kwargs\n    def predict(self, X: list[list[float]]) -> list[int]:\n        \"\"\"\n        Predicts the class labels for the given data using the trained Nearest Neighbors Model.\n\n        :param X: Data for prediction.\n        :return: Predicted class labels.\n        \"\"\"\n        assert isinstance(X, list), \"Prediction data must be a list.\"\n        assert all(isinstance(row, list) for row in X), \"Each prediction sample must be a list.\"\n        assert all(isinstance(num, float) for row in X for num in row), \"All numbers must be floats.\"\n\n        distances, indices = self.model.kneighbors(X)\n        return indices.squeeze().tolist()"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\nimport hashlib\n\n\ndef insertion_sort(arr: List[int]) -> List[int]:\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n```"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\n\nclass DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    # 3-digit method name following the convention\n    def add_edge0(self, u, v):\n        self.adj[u].append(v)\n\n    # Depth-first search to detect cycles\n    def is_cyclicUtil1(self, v, visited, recStack):\n        visited[v] = True\n        recStack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclicUtil1(neighbour, visited, recStack):\n                    return True\n            elif recStack[neighbour]:\n                return True\n\n        recStack[v] = False\n        return False\n\n    def is_cyclic2(self):\n        visited = [False] * self.V\n        recStack = [False] * self.V\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclicUtil1(node, visited, recStack):\n                    return True\n        return False\n```"
      ],
      "instruction_model_output": [
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(input_string: str) -> bool:\n    input_string = input_string.lower()\n    reversed_string = input_string[::-1]\n    return input_string == reversed_string"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_y):\n    s_y = s_y.lower()\n    start_y, end_x = 0, len(s_y) - 1\n    while start_y < end_x:\n        if s_y[start_y] != s_y[end_x]:\n            return False\n        start_y += 1\n        end_x -= 1\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Graph:\n    def shortest_path_x(self, source_g, destination_g):\n        # Implementation of Dijkstra's algorithm or A* search algorithm\n        pass",
        "def merge_sorted_lists_x(list1_g, list2_g):\n    result_g = []\n    i, j = 0, 0\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            result_g.append(list1_g[i])\n            i += 1\n        else:\n            result_g.append(list2_g[j])\n            j += 1\n    result_g.extend(list1_g[i:])\n    result_g.extend(list2_g[j:])\n    return result_g"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```",
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song_x(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song_x(self, song):\n        self.songs.remove(song)",
        "@retry\ndef merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    i, j = 0, 0\n    merged_list_i = []\n\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list_i.append(list1_i[i])\n            i += 1\n        else:\n            merged_list_i.append(list2_i[j])\n            j += 1\n\n    merged_list_i.extend(list1_i[i:])\n    merged_list_i.extend(list2_i[j:])\n\n    return merged_list_i"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, value):\n        self.heap.append(value)\n        self.o_heapify_up(len(self.heap) - 1)\n\n    def o_heapify_up(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] >= self.heap[parent_index]:\n                break\n            self.o_swap(index, parent_index)\n            index = parent_index\n\n    def o_swap(self, index1, index2):\n        self.heap[index1], self.heap[index2] = self.heap[index2], self.heap[index1]\n\n    def heapify(self):\n        last_non_leaf = (len(self.heap) - 2) // 2\n        for index in range(last_non_leaf, -1, -1):\n            self.o_heapify_down(index)\n\n    def o_heapify_down(self, index):\n        heap_size = len(self.heap)\n        while True:\n            left_child_index = 2 * index + 1\n            right_child_index = left_child_index + 1\n            largest = index\n            if left_child_index < heap_size:\n                if self.heap[left_child_index] > self.heap[largest]:\n                    largest = left_child_index\n            if right_child_index < heap_size:\n                if self.heap[right_child_index] > self.heap[largest]:\n                    largest = right_child_index\n            if largest != index:\n                self.o_swap(index, largest)\n                index = largest\n            else:\n                break",
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def o_add_stock_chx(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def o_remove_stock_chx(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(\"Insufficient stock to remove that quantity.\")"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]",
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def o_add_stock(self, amount):\n        self.stock += amount\n\n    def o_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove\")\n        self.stock -= amount"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\ndef common_elements(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Returns a list of common elements between two input lists.\n\n    Args:\n        list1 (list[int]): A list of integers.\n        list2 (list[int]): Another list of integers.\n\n    Returns:\n        list[int]: A list containing the common elements between the input lists.\n    \"\"\"\n    common_elements = []\n    for element in list1:\n        if element in list2:\n            common_elements.append(element)\n    return common_elements\n```",
        "from sklearn.linear_model import Lasso\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression Class for performing Lasso Regression.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the L1 term. Alpha controls the amount of regularization.\n        Larger values specify stronger regularization.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : array, shape (1,)\n        Independent term in decision function.\n\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        # Check that X and y have correct shape\n        X, y = check_X_y(X, y)\n        # Check that alpha is valid\n        if not (self.alpha > 0):\n            raise ValueError(\"alpha parameter must be positive.\")\n\n        # Calculate coefficient and intercept\n        self.coef_ = self._compute_coef(X, y)\n        self.intercept_ = self._compute_intercept(X, y)\n        return self\n\n    def _compute_coef(self, X, y):\n        \"\"\"\n        Compute the coefficient by solving the Lasso problem.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        coef_ : array, shape (n_features,)\n            Coefficient of the model.\n        \"\"\"\n        # Calculate coefficient using Lasso\n        lasso = Lasso(alpha=self.alpha, fit_intercept=False)\n        lasso.fit(X, y)\n        return lasso.coef_\n\n    def _compute_intercept(self, X, y):\n        \"\"\"\n        Compute the intercept by solving a linear equation.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        intercept_ : array, shape (1,)\n            Intercept of the model.\n        \"\"\"\n        # Calculate intercept using linear equation\n        self.coef_ = self.coef_.reshape(-1, 1)\n        return y - X @ self.coef_\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y : array, shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, ['coef_', 'intercept_'])\n        # Check that X is correct shape\n        X = check_array(X)\n        # Calculate prediction\n        return X @ self.coef_ + self.intercept_",
        "```python\ndef delete_node(root, key):\n    if root is None:\n        return root\n\n    # Recursive call for left and right subtrees\n    root.left = delete_node(root.left, key)\n    root.right = delete_node(root.right, key)\n\n    # If the key to be deleted is greater than the root's key,\n    # then the node to be deleted is present in the right subtree\n    if key > root.key:\n        root.right = delete_node(root.right, key)\n\n    # If the key to be deleted is less than the root's key,\n    # then the node to be deleted is present in the left subtree\n    elif key < root.key:\n        root.left = delete_node(root.left, key)\n\n    # If the key is same as root's key, then this is the node\n    # to be deleted\n    else:\n        # Node with only one child or no child\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n\n        # Node with two children: Get the inorder successor (smallest\n        # in the right subtree)\n        successor = get_successor(root.right)\n\n        # Copy the inorder successor's value to this node\n        root.key = successor.key\n\n        # Delete the inorder successor\n        root.right = delete_node(root.right, successor.key)\n\n    return root\n\n\ndef get_successor(node):\n    current = node\n\n    # Traverse to the leftmost leaf\n    while current and current.left is not None:\n        current = current.left\n\n    return current\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n\n    def train(self, X_train: list[list[int]], y_train: list[int]):\n        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: list[list[int]]) -> list[int]:\n        return self.model.predict(X_test)\n```",
        "class Inventory:\n    \"\"\"\n    A class to manage inventory stock.\n    \"\"\"\n    def __init__(self):\n        self.stock = []\n\n    def add_stock(self, item: str) -> None:\n        \"\"\"\n        Adds an item to the inventory stock.\n\n        Args:\n            item (str): The name of the item to be added.\n        \"\"\"\n        self.stock.append(item)\n\n    def remove_stock(self, item: str) -> None:\n        \"\"\"\n        Removes an item from the inventory stock.\n\n        Args:\n            item (str): The name of the item to be removed.\n        \"\"\"\n        self.stock.remove(item)",
        "def find_majority_element(arr):\n    \"\"\"\n    Returns the majority element of an array.\n    \"\"\"\n    assert len(arr) > 0, \"Array must not be empty\"\n    count = {}\n    for num in arr:\n        if num in count:\n            count[num] += 1\n        else:\n            count[num] = 1\n    majority_count = len(arr) // 2\n    for num, freq in count.items():\n        if freq > majority_count:\n            return num\n    return None",
        "```python\ndef is_palindrome(input_string: str) -> bool:\n    \"\"\"\n    Checks if a given string is a valid palindrome.\n\n    Args:\n        input_string (str): The string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    assert isinstance(input_string, str), \"Input must be a string.\"\n    input_string = input_string.lower()  # Convert to lowercase\n    assert len(input_string) > 0, \"Input string cannot be empty.\"\n\n    # Reverse the string and compare\n    return input_string == input_string[::-1]\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing the merged elements.\n    \"\"\"\n    merged_list = []\n    i, j = 0, 0\n\n    # Iterate through both lists until one is exhausted\n    while i < len(list1) and j < len(list2):\n        # Compare the current elements and append the smaller one\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    # Append any remaining elements from the non-exhausted list\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "class StudentGrade:\n    def __init__(self, grade):\n        self.grade = grade\n\n    def p_compute_average_grade(self):\n        return sum(self.grade) / len(self.grade)",
        "```python\nfrom typing import List\n\n\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: int, v: int):\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: int, visited: List[bool], rec_stack: List[bool]):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        visited = [False] * (max(self.graph.keys()) + 1)\n        rec_stack = [False] * (max(self.graph.keys()) + 1)\n\n        for node in self.graph:\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\nfrom heapq import heapify, heappop, heappush\n\n\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    \"\"\"Returns the kth largest element in the array.\"\"\"\n    if not isinstance(arr, list):\n        raise TypeError(\"Input must be a list.\")\n    if not isinstance(k, int):\n        raise TypeError(\"k must be an integer.\")\n    if k <= 0 or k > len(arr):\n        raise ValueError(\"k must be greater than 0 and less than or equal to the length of the array.\")\n\n    min_heap = []\n    for num in arr:\n        if isinstance(num, int):\n            heappush(min_heap, num)\n    for _ in range(k - 1):\n        heappop(min_heap)\n    return min_heap[0]\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    Parameters\n    ----------\n    store_covariance : bool, default=True\n        Whether to store the covariance matrices for each class.\n\n    tol : float, default=1e-4\n        Tolerance for singularity check in covariance estimation.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    class_means_ : ndarray of shape (n_classes, n_features)\n        Mean vectors for each class.\n\n    class_covariance_ : ndarray of shape (n_classes, n_features, n_features)\n        Covariance matrices for each class.\n\n    class_priors_ : ndarray of shape (n_classes,)\n        Priors for each class.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    n_features_in_ : int\n        Number of features seen during :meth:`fit`.\n\n    n_classes_ : int\n        Number of classes seen during :meth:`fit`.\n\n    feature_names_in_ : ndarray of shape (n_features_in_,)\n        Names of features seen during :meth:`fit`.\n    \"\"\"\n\n    def __init__(self, store_covariance=True, tol=1e-4):\n        self.store_covariance = store_covariance\n        self.tol = tol\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n        \"\"\"\n        X, y = self._validate_data(X, y, multi_output=True,\n            estimator=self, dtype=float64)\n\n        self.n_features_in_ = n_features = X.shape[1]\n        self.feature_names_in_ = None\n\n        self.classes_, y = np.unique(y, return_inverse=True)\n        self.n_classes_ = n_classes = len(self.classes_)\n\n        self.class_means_ = np.zeros((n_classes, n_features), dtype=float64)\n        self.class_covariance_ = np.zeros(\n            (n_classes, n_features, n_features), dtype=float64)\n        self.class_priors_ = np.zeros(n_classes, dtype=float64)\n\n        for i in range(n_classes):\n            X_i = X[y == i]\n            self.class_means_[i] = np.mean(X_i, axis=0)\n            self.class_covariance_[i] = np.atleast_2d(np.cov(X_i.T, bias=True))\n            self.class_priors_[i] = X_i.shape[0] / X.shape[0]\n\n        self.covariance_ = np.cov(X.T, bias=True)\n\n        if not self.store_covariance:\n            self.class_covariance_ = None\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only\n            if they are CSR, CSC or COO.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted class label per sample.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n            reset=False, dtype=float64)\n\n        scores = self._get_log_prob(X)\n        return self.classes_[np.argmax(scores, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test vector X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only\n            if they are CSR, CSC or COO.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in\n            the model. The columns correspond to the classes in\n            sorted order, as they appear in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n            reset=False, dtype=float64)\n\n        log_prob = self._get_log_prob(X)\n        prob = np.exp(log_prob)\n        prob /= prob.sum(axis=1)[:, np.newaxis]\n        return prob\n\n    def _get_log_prob(self, X):\n        \"\"\"\n        Compute log probabilities of X belonging to each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only\n            if they are CSR, CSC or COO.\n\n        Returns\n        -------\n        log_prob : array-like of shape (n_samples, n_classes)\n            Log probabilities of X belonging to each class.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n            reset=False, dtype=float64)\n\n        n_features = X.shape[1]\n        scores = np.zeros((X.shape[0], self.n_classes_), dtype=X.dtype)\n        for k, (prior, mu, cov) in enumerate(zip(\n                self.class_priors_, self.class_means_, self.class_covariance_)):\n            diff = X - mu\n            scores[:, k] = (\n                prior - 0.5 * np.log(np.linalg.det(cov))\n                - 0.5 * diff @ np.linalg.solve(cov, diff.T).T\n            )\n\n        return scores\n\n    def fit_predict(self, X, y):\n        \"\"\"\n        Fit the model and then perform classification on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted class labels for samples in X.\n        \"\"\"\n        # Fit and predict are not coupled in QDA\n        self.fit(X, y)\n        return self.predict(X)\n\n    def score_samples(self, X):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data that are classified.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` wrt. ``y``.\n        \"\"\"\n        return self.score(X, self.predict(X))\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data that are classified.\n\n        y : array-like of shape (n_samples,)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` wrt. ``y``.\n        \"\"\"\n        from sklearn.utils.multiclass import type_of_target\n        from sklearn.metrics import accuracy_score\n\n        check_is_fitted(self)\n        X, y = self._validate_data(X, y, multi_output=True,\n            estimator=self, dtype=None)\n\n        y_pred = self.predict(X)\n        return accuracy_score(y, y_pred, sample_weight=sample_weight)\n\n    def decision_function(self, X):\n        \"\"\"\n        Evaluate the decision function for the samples in X.\n\n        For a one-class model, the output is the negative distance of\n        the samples to the origin. For a two-class model, the output is\n        the distance of the samples to the separating hyperplane. For a\n        multi-class model, the output is a vector of posterior class\n        probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The samples.\n\n        Returns\n        -------\n        score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n            Decision function of the samples. Binary and multi-class\n            models return the distance of the samples to the decision\n            hyperplane. For multi-class models, this is a vector of\n            posterior class probabilities.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n            reset=False, dtype=float64)\n\n        log_prob = self._get_log_prob(X)\n        return log_prob\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Return log probability estimates for the test vector X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only\n            if they are CSR, CSC or COO.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the log probability of the samples for each class in\n            the model. The columns correspond to the classes in\n            sorted order, as they appear in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n        return np.log(proba)\n```",
        "```python\nfrom typing import List\n\n\ndef dot_product(vector_1: List[int], vector_2: List[int]) -> int:\n    assert len(vector_1) == len(\n        vector_2\n    ), \"Vectors must have the same length to compute their dot product.\"\n    return sum(x * y for x, y in zip(vector_1, vector_2))\n```",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n\n    def shortest_path(self, src, dest):\n        visited = [False] * len(self.graph)\n        path = [0] * len(self.graph)\n        queue = deque()\n\n        queue.append(src)\n        visited[src] = True\n        path[src] = 0\n\n        while queue:\n            u = queue.popleft()\n\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    path[v] = path[u] + 1\n\n        return path[dest]\n```",
        "from sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression Class\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1.0, kernel_params=None):\n        \"\"\"\n        Initializes the Kernel Ridge Regression model.\n\n        Parameters:\n            alpha (float): Regularization strength parameter. Must be a positive float. Defaults to 1.0.\n            kernel (str): Kernel function used for prediction. Options include 'linear', 'poly', 'rbf', 'sigmoid'. Defaults to 'linear'.\n            gamma (float): Kernel coefficient. A higher value implies a more complex boundary. Defaults to None.\n            degree (int): Degree of the polynomial kernel function. Ignored by other kernels. Defaults to 3.\n            coef0 (float): Independent term in the polynomial kernel function. Ignored by other kernels. Defaults to 1.0.\n            kernel_params (dict): Additional parameters for the kernel function. Defaults to None.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha, kernel=self.kernel, gamma=self.gamma, degree=self.degree, coef0=self.coef0, **self.kernel_params\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits the model to the training data.\n\n        Parameters:\n            X (ndarray or DataFrame): Training input samples, shape (n_samples, n_features).\n            y (ndarray or Series): Target values, shape (n_samples,).\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values for the provided input samples.\n\n        Parameters:\n            X (ndarray or DataFrame): Input samples, shape (n_samples, n_features).\n\n        Returns:\n            ndarray or Series: Predicted target values, shape (n_samples,).\n        \"\"\"\n        return self.model.predict(X)",
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    def add_song(self, song):\n        self.songs.append(song)\n\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "```python\nimport numpy as np\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        X = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n        self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y_train\n        self.intercept_ = self.coef_[0]\n        self.coef_ = self.coef_[1:]\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        return X_test @ self.coef_.T + self.intercept_\n```",
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value",
        "```python\nfrom pedantic import trace_class, count_calls, retry\n\n@trace_class\nclass Heap:\n    def __init__(self):\n        self.data = []\n\n    @count_calls\n    def insert(self, value):\n        self.data.append(value)\n        self._heapify()\n\n    def _heapify(self):\n        current_index = len(self.data) - 1\n        while current_index > 0:\n            parent_index = (current_index - 1) // 2\n            if self.data[current_index] > self.data[parent_index]:\n                self.data[current_index], self.data[parent_index] = self.data[parent_index], self.data[current_index]\n                current_index = parent_index\n            else:\n                break\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    shortest = min(strs, key=len)\n    for i, char in enumerate(shortest):\n        if any(s[i] != char for s in strs):\n            return shortest[:i]\n\n    return shortest\n```",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort an array of integers using the insertion sort algorithm.\n\n    Args:\n        arr (list[int]): The array of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted array in ascending order.\n    \"\"\"\n    # Add assert statements to validate assumptions and ensure correct behavior\n    assert isinstance(arr, list), \"Input must be a list.\"\n    assert all(isinstance(num, int) for num in arr), \"All items in the list must be integers.\"\n\n    # Initialize the sorted portion of the array\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        # Shift elements of the sorted portion to the right until the correct position is found\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        # Insert the current element into the correct position\n        arr[j + 1] = key\n    return arr\n```",
        "```python\nfrom pedantic import trace_class, retry\n\n@trace_class\nclass Perceptron:\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    def train(self, X, y):\n        # Training code goes here\n        pass\n\n    def predict(self, X):\n        # Prediction code goes here\n        pass\n```",
        "```python\ndef average_diagonal_elements(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list[list[int]]): The input matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    assert isinstance(matrix, list), \"Input must be a list of lists.\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists.\"\n    assert all(isinstance(element, int) for row in matrix for element in row), \"All elements must be integers.\"\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    diagonal_average = diagonal_sum / len(matrix)\n\n    return diagonal_average\n```",
        "```python\nfrom typing import List\n\n\ndef p_dot_product(vector_1: List[int], vector_2: List[int]) -> int:\n    assert len(vector_1) == len(vector_2), \"Vectors must have the same length.\"\n    return sum(x * y for x, y in zip(vector_1, vector_2))\n```",
        "```python\ndef edit_distance(str1: str, str2: str) -> int:\n    len_str1, len_str2 = len(str1), len(str2)\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n\n    return dp[len_str1][len_str2]\n```",
        "```python\ndef is_valid_bst(root: 'TreeNode') -> 'bool':\n    def helper(node: 'TreeNode', min_val: 'int', max_val: 'int') -> 'bool':\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.queue = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.queue.remove(key)\n            self.queue.append(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.queue.pop(0)\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self.queue.append(key)",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort an array of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The array of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted array in ascending order.\n    \"\"\"\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\n\ndef merge(left: list[int], right: list[int]) -> list[int]:\n    \"\"\"\n    Merge two sorted arrays into a single sorted array.\n\n    Args:\n        left (list[int]): The first sorted array.\n        right (list[int]): The second sorted array.\n\n    Returns:\n        list[int]: The merged and sorted array.\n    \"\"\"\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result\n```"
      ]
    }
  ]
}