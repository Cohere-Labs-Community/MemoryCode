{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef dot_product(v1, v2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param v1: The first vector.\n    :param v2: The second vector.\n    :return: The dot product of v1 and v2.\n    \"\"\"\n    assert len(v1) == len(v2), \"Vectors must have the same length for dot product\"\n\n    result = 0\n    for i in range(len(v1)):\n        result += v1[i] * v2[i]\n\n    return result\n```",
        "```python\nclass NearestNeighbors:\n    def __init__(self):\n        self.model = None\n\n    def train(self, X_train, y_train):\n        self.model = NearestNeighbors(n_neighbors=5)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        assert self.model is not None, \"Model is not trained yet\"\n        g_X_test = X_test\n        return self.model.kneighbors(g_X_test, return_distance=False)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(x_vec1, x_vec2):\n    return sum(x * y for x, y in zip(x_vec1, x_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, chx_val):\n        self.heap.append(chx_val)\n        self.chx_heapify_up(len(self.heap) - 1)\n\n    def chx_heapify_up(self, chx_idx):\n        while chx_idx > 0:\n            parent_idx = (chx_idx - 1) // 2\n            if self.heap[chx_idx] >= self.heap[parent_idx]:\n                break\n            self.chx_swap(chx_idx, parent_idx)\n            chx_idx = parent_idx\n\n    def chx_swap(self, chx_i, chx_j):\n        self.heap[chx_i], self.heap[chx_j] = self.heap[chx_j], self.heap[chx_i]\n\n    def heapify(self, chx_arr):\n        self.heap = chx_arr\n        for i in range(len(self.heap)):\n            self.chx_heapify_down(i)\n\n    def chx_heapify_down(self, chx_idx):\n        heap_size = len(self.heap)\n        while True:\n            left_child_idx = 2 * chx_idx + 1\n            right_child_idx = 2 * chx_idx + 2\n            largest = chx_idx\n\n            if left_child_idx < heap_size and self.heap[left_child_idx] > self.heap[largest]:\n                largest = left_child_idx\n\n            if right_child_idx < heap_size and self.heap[right_child_idx] > self.heap[largest]:\n                largest = right_child_idx\n\n            if largest != chx_idx:\n                self.chx_swap(chx_idx, largest)\n                chx_idx = largest\n            else:\n                break"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def diagonal_average(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n    \n    size = len(matrix)\n    diagonal_sum = sum(matrix[i][i] for i in range(size))\n    return diagonal_sum / size"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(arr: List[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Parameters:\n        arr (List[int]): The input array of integers.\n        k (int): The value of 'k' for finding the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    sorted_arr = sorted(arr, reverse=True)\n\n    # Return the kth element from the sorted array\n    return sorted_arr[k - 1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.discriminant_analysis import _covariance\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom abc import ABCMeta, abstractmethod\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin, metaclass=ABCMeta):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    This is a class with an estimator API that implements the Quadratic Discriminant\n    Analysis (QDA) algorithm. QDA is a generalization of Linear Discriminant Analysis\n    (LDA) that allows the classes to have different covariances. In QDA, the\n    conditional probability of a class given a sample is modeled as a Gaussian\n    distribution, with class-specific mean and covariance.\n\n    Parameters\n    ----------\n    store_covariance : bool, default=False\n        Whether to store the covariance matrices for each class.\n\n    tol : float, default=1e-4\n        Tolerance for checking the singular values in covariance matrices.\n\n    priors : array-like, shape (n_classes,), default=None\n        Priors on the classes. If specified the priors are not adjusted according to\n        the data.\n\n    Attributes\n    ----------\n    class_means_ : ndarray, shape (n_classes, n_features)\n        The mean value for each class.\n\n    covariance_ : ndarray, shape (n_classes, n_features, n_features)\n        The covariance matrix for each class.\n\n    priors_ : ndarray, shape (n_classes,)\n        Prior probability of each class (adjusted for the data if needed).\n\n    classes_ : ndarray, shape (n_classes,)\n        Class labels.\n\n    n_features_in_ : int\n        Number of features seen during :meth:`fit`.\n\n    See Also\n    --------\n    LinearDiscriminantAnalysis : Linear discriminant analysis (LDA).\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for QDA\n           <https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_blobs\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> X, y = make_blobs(n_samples=30, centers=2, random_state=42)\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    QuadraticDiscriminantAnalysis()\n    ```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a value into the binary tree.\n        :param value: The value to be inserted.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"\n        Delete a value from the binary tree.\n        :param value: The value to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            successor = self._get_successor(node.right)\n            node.value = successor.value\n            node.right = self._delete_recursive(node.right, successor.value)\n\n        return node\n\n    def _get_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node\n\n    def search(self, value):\n        \"\"\"\n        Search for a value in the binary tree.\n        :param value: The value to be searched.\n        :return: True if the value is found, False otherwise.\n        \"\"\"\n        return self._search_recursive(self.root, value)\n\n    def _search_recursive(self, node, value):\n        if node is None or node.value == value:\n            return node is not None\n        if value < node.value:\n            return self._search_recursive(node.left, value)\n        return self._search_recursive(node.right, value)\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None",
        "class LassoRegression:\n    \"\"\"\n    Lasso Regression class for performing regression tasks.\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Initializes the Lasso Regression model.\n\n        Parameters:\n            alpha (float): The regularization strength. A higher value penalizes coefficients more.\n        \"\"\"\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        \"\"\"\n        Trains the Lasso Regression model.\n\n        Parameters:\n            X_train (np.ndarray): The input training data of shape (n_samples, n_features).\n            y_train (np.ndarray): The target training data of shape (n_samples,).\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Input validation\n            if not isinstance(X_train, np.ndarray) or not isinstance(y_train, np.ndarray):\n                raise ValueError(\"Input data must be numpy arrays.\")\n            if X_train.ndim != 2 or y_train.ndim != 1:\n                raise ValueError(\"Input data should be 2-dimensional for X and 1-dimensional for y.\")\n            if X_train.shape[0] != y_train.shape[0]:\n                raise ValueError(\"The number of samples in X and y should be the same.\")\n\n            # Add bias term if not present\n            if X_train.shape[1] != 1:\n                X_train = np.c_[np.ones((X_train.shape[0]), dtype=int), X_train]\n\n            # Coefficient calculation using Least Squares with L1 regularization\n            self.coef_ = np.linalg.inv(X_train.T @ X_train + self.alpha * np.identity(X_train.shape[1])) @ (X_train.T @ y_train)\n            self.intercept_ = 0  # Intercept is not regularized\n\n        except np.linalg.LinAlgError as e:\n            raise ValueError(\"Singular matrix. Cannot compute coefficients.\") from e\n        except ValueError as ve:\n            print(f\"ValueError: {ve}\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the target values for the provided test data.\n\n        Parameters:\n            X_test (np.ndarray): The input test data of shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        try:\n            # Input validation\n            if not isinstance(X_test, np.ndarray):\n                raise ValueError(\"Input data must be a numpy array.\")\n            if X_test.ndim != 2:\n                raise ValueError(\"Input data should be 2-dimensional.\")\n            if self.coef_ is None:\n                raise RuntimeError(\"Model has not been trained yet.\")\n\n            # Add bias term if not present\n            if X_test.shape[1] != len(self.coef_):\n                X_test = np.c_[np.ones((X_test.shape[0]), dtype=int), X_test]\n\n            # Predict target values\n            return X_test @ self.coef_\n\n        except np.linalg.LinAlgError as e:\n            raise ValueError(\"Singular matrix. Prediction failed.\") from e\n        except ValueError as ve:\n            print(f\"ValueError: {ve}\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song_j(self, song_j):\n        self.songs.append(song_j)\n\n    @retry\n    def remove_song_j(self, song_j):\n        self.songs.remove(song_j)",
        "```python\nfrom pedantic import retry\n\n\n@retry\ndef merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n    return result_j\n```"
      ],
      "instruction_model_output": [
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs, count_calls\n\nclass StudentGrade:\n    @count_calls\n    @require_kwargs('grades')\n    def average_grade(self, **kwargs):\n        grades = kwargs['grades']\n        return sum(grades) / len(grades)\n```",
        "```python\nfrom pedantic import require_kwargs, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@require_kwargs\n@count_calls\ndef train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n    self.model = LinearRegression()\n    self.model.fit(X_train, y_train)\n\n@require_kwargs\n@count_calls\ndef predict(self, X_test: np.ndarray) -> np.ndarray:\n    return self.model.predict(X_test)\n\nclass LinearRegressionModel:\n    def __init__(self):\n        self.model = None\n\n    train = train\n    predict = predict\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)",
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom typing import List\n\n\ndef kth_largest_element(arr: List[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        arr (List[int]): The input array of integers.\n        k (int): The value of 'k' to find the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Validate input\n    if k <= 0 or k > len(arr):\n        raise ValueError(\"Invalid value for 'k'. It should be a positive integer less than or equal to the length of the array.\")\n\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n\n    # Return the kth element\n    return arr[k - 1]\n```",
        "```python\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\n\n@timer_class\nclass Heap:\n    @require_kwargs\n    @count_calls\n    @retry\n    def insert(self, g_item: int) -> None:\n        assert isinstance(g_item, int), \"Item must be an integer\"\n        # Insertion logic here\n\n    @require_kwargs\n    @count_calls\n    @retry\n    def heapify(self, g_index: int) -> None:\n        assert isinstance(g_index, int) and g_index >= 0, \"Index must be a non-negative integer\"\n        # Heapify logic here\n```",
        "```python\nfrom sklearn.linear_model import LinearRegression\nfrom pedantic import timer_class, require_kwargs, count_calls\n\n@timer_class\nclass LinearRegressionModel:\n    \"\"\"\n    Linear Regression Model for predicting target variable using linear combination of features.\n    \"\"\"\n\n    def __init__(self, x_train: pd.DataFrame, y_train: pd.Series, x_test: pd.DataFrame):\n        \"\"\"\n        Initializes the Linear Regression Model.\n\n        Parameters:\n            x_train (pd.DataFrame): Training data features.\n            y_train (pd.Series): Training data target variable.\n            x_test (pd.DataFrame): Test data features.\n        \"\"\"\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_test = x_test\n        self.model = LinearRegression()\n\n    @require_kwargs\n    @count_calls\n    def train(self, fit_intercept: bool = True, normalize: bool = False, copy_x: bool = True) -> None:\n        \"\"\"\n        Trains the Linear Regression Model.\n\n        Parameters:\n            fit_intercept (bool): Whether to calculate the intercept for the model. Defaults to True.\n            normalize (bool): Whether to normalize the features in the training data. Defaults to False.\n            copy_x (bool): Whether to make a copy of X. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n        self.model.fit(self.x_train, self.y_train, fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_x)\n\n    @require_kwargs\n    @count_calls\n    def predict(self, x_predict: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predicts the target variable for the given features using the trained model.\n\n        Parameters:\n            x_predict (pd.DataFrame): Features for which predictions are required.\n\n        Returns:\n            pd.Series: Predicted target variable values.\n        \"\"\"\n        return pd.Series(self.model.predict(x_predict))\n```",
        "```python\nfrom typing import List\n\n\ndef dot_product(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vec1: The first vector.\n    :param vec2: The second vector.\n    :return: The dot product of vec1 and vec2.\n    \"\"\"\n    assert len(vec1) == len(vec2), \"Vectors must have the same length\"\n    result = 0.0\n    for i in range(len(vec1)):\n        result += vec1[i] * vec2[i]\n    return result\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0, tol=1e-3, max_iter=1000, random_state=None):\n        self.alpha = alpha\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.model = None\n        self.scaler = None\n\n    def _check_X_y(self, X, y):\n        assert X.shape[0] == y.shape[0], \"X and y should have the same number of samples\"\n\n    def train(self, X_train, y_train):\n        self._check_X_y(X_train, y_train)\n\n        self.scaler = StandardScaler()\n        X_train = self.scaler.fit_transform(X_train)\n\n        self.model = Lasso(alpha=self.alpha, tol=self.tol, max_iter=self.max_iter, random_state=self.random_state)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        assert self.model is not None, \"Model has not been trained yet\"\n        assert self.scaler is not None, \"Scaler has not been fitted yet\"\n\n        X_test = self.scaler.transform(X_test)\n        return self.model.predict(X_test)\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\n\n@timer_class\n@retry\nclass QuadraticDiscriminantAnalysis:\n    @require_kwargs\n    @count_calls\n    def __init__(self, g_store_covariance=True, g_tol=1e-4, g_store_covariates=False):\n        \"\"\"\n        Quadratic Discriminant Analysis (QDA) class for multiclass classification.\n\n        Parameters:\n            g_store_covariance (bool): Whether to store the covariance matrices. Defaults to True.\n            g_tol (float): Tolerance for rank computations. Defaults to 1e-4.\n            g_store_covariates (bool): Whether to store the covariates (X). Defaults to False.\n        \"\"\"\n        self.g_store_covariance = g_store_covariance\n        self.g_tol = g_tol\n        self.g_store_covariates = g_store_covariates\n\n    @require_kwargs\n    @count_calls\n    def fit(self, g_X, y):\n        \"\"\"\n        Fit the QDA model according to the given training data.\n\n        Parameters:\n            g_X (ndarray): Training vector, shape (n_samples, n_features).\n            y (ndarray): Target vector relative to X, shape (n_samples,).\n        \"\"\"\n        # Perform some actions here\n        pass\n\n    @require_kwargs\n    @count_calls\n    def predict(self, g_X):\n        \"\"\"\n        Predict class labels for samples in X.\n\n        Parameters:\n            g_X (ndarray): Samples to predict, shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class label per sample.\n        \"\"\"\n        # Perform some actions here\n        return None\n\n    @require_kwargs\n    @count_calls\n    def score(self, g_X, y):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        Parameters:\n            g_X (ndarray): Test samples, shape (n_samples, n_features).\n            y (ndarray): True labels for g_X, shape (n_samples,).\n\n        Returns:\n            float: Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        # Perform some actions here\n        return None\n\n    @require_kwargs\n    @count_calls\n    def predict_proba(self, g_X):\n        \"\"\"\n        Return probability estimates for the test data X.\n\n        Parameters:\n            g_X (ndarray): Samples to predict, shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Returns the probability of the samples for each class in the model.\n                The columns correspond to the classes in sorted order, as they appear\n                in the attribute classes_.\n        \"\"\"\n        # Perform some actions here\n        return None\n\n    @require_kwargs\n    @count_calls\n    def decision_function(self, g_X):\n        \"\"\"\n        Evaluate the decision function for the samples in X.\n\n        Parameters:\n            g_X (ndarray): Samples to evaluate, shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Returns the decision function of the samples.\n                The columns correspond to the one-vs-rest classes.\n        \"\"\"\n        # Perform some actions here\n        return None\n```",
        "```python\nfrom typing import Callable, TypeVar, Union\n\nT = TypeVar('T')\n\n\nclass BinaryTree:\n    \"\"\"A binary tree data structure.\n\n    Attributes:\n        root (Node): The root node of the binary tree.\n        _node_decorator (Callable): A decorator function to apply to all nodes.\n    \"\"\"\n\n    root: 'Node[T]'\n    _node_decorator: Callable\n\n    def __init__(self, node_decorator: Callable = None):\n        \"\"\"Initialize the binary tree.\n\n        Args:\n            node_decorator (Callable, optional): A decorator function to apply to all nodes. Defaults to None.\n        \"\"\"\n        self.root = None\n        self._node_decorator = node_decorator\n\n    def insert(self, value: T) -> None:\n        \"\"\"Insert a value into the binary tree.\n\n        Args:\n            value (T): The value to be inserted.\n        \"\"\"\n        self.root = self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value: T, node: 'Node[T]') -> 'Node[T]':\n        \"\"\"Recursively insert a value into the binary tree.\n\n        Args:\n            value (T): The value to be inserted.\n            node (Node): The current node being processed.\n\n        Returns:\n            Node: The new root node of the subtree.\n        \"\"\"\n        if node is None:\n            node = Node(value, self._node_decorator)\n        elif value < node.value:\n            node.left = self._insert_recursive(value, node.left)\n        else:\n            node.right = self._insert_recursive(value, node.right)\n\n        return node\n\n    def delete(self, value: T) -> None:\n        \"\"\"Delete a value from the binary tree.\n\n        Args:\n            value (T): The value to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(value, self.root)\n\n    def _delete_recursive(self, value: T, node: 'Node[T]') -> 'Union[Node[T], None]':\n        \"\"\"Recursively delete a value from the binary tree.\n\n        Args:\n            value (T): The value to be deleted.\n            node (Node): The current node being processed.\n\n        Returns:\n            Union[Node, None]: The new root node of the subtree, or None if the subtree is empty.\n        \"\"\"\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(value, node.left)\n        elif value > node.value:\n            node.right = self._delete_recursive(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: 'Node[T]') -> T:\n        \"\"\"Get the minimum value in a subtree.\n\n        Args:\n            node (Node): The root node of the subtree.\n\n        Returns:\n            T: The minimum value in the subtree.\n        \"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n\nclass Node:\n    \"\"\"A node in the binary tree.\n\n    Attributes:\n        value (T): The value stored in the node.\n        left (Node): The left child node.\n        right (Node): The right child node.\n        _decorator (Callable): A decorator function to apply to the node.\n    \"\"\"\n\n    value: T\n    left: 'Node[T]'\n    right: 'Node[T]'\n    _decorator: Callable\n\n    def __init__(self, value: T, decorator: Callable = None):\n        \"\"\"Initialize the node.\n\n        Args:\n            value (T): The value to be stored in the node.\n            decorator (Callable, optional): A decorator function to apply to the node. Defaults to None.\n        \"\"\"\n        self.value = value\n        self.left = None\n        self.right = None\n        self._decorator = decorator\n\n    def __call__(self) -> T:\n        \"\"\"Apply the decorator and return the value.\n\n        Returns:\n            T: The decorated value.\n        \"\"\"\n        return self._decorator(self.value) if self._decorator else self.value\n```",
        "```python\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\n\n@timer_class\nclass StudentGrade:\n    @require_kwargs\n    @count_calls\n    @retry\n    def _compute_average_grade(self, g_total_marks: int, g_total_students: int) -> float:\n        assert isinstance(g_total_marks, int) and isinstance(g_total_students, int), \"Total marks and total students must be integers.\"\n        assert g_total_marks >= 0 and g_total_students > 0, \"Total marks and total students must be non-negative.\"\n\n        try:\n            average_grade = g_total_marks / g_total_students\n            assert 0 <= average_grade <= 100, \"Average grade must be between 0 and 100.\"\n            return average_grade\n        except ZeroDivisionError:\n            return 0.0\n```",
        "```python\nfrom pedantic import timer_class, require_kwargs, count_calls, retry\n\n@timer_class\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @require_kwargs\n    @count_calls\n    @retry\n    def add_song(self, song_name: str, duration: int, genre: str) -> None:\n        self.songs.append((song_name, duration, genre))\n\n    @require_kwargs\n    @count_calls\n    @retry\n    def remove_song(self, song_name: str) -> None:\n        self.songs = [song for song in self.songs if song[0] != song_name]\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.base import clone\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.exceptions import NotFittedError\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\n\n# Set global variable indicating whether to include '_j' suffix in function argument names\ninclude_suffix = True\n\n\ndef _validate_input(X=None, y=None):\n    \"\"\"\n    Validates the input data and labels.\n\n    Parameters:\n    X (ndarray): Input data.\n    y (ndarray): Target labels.\n\n    Raises:\n    TypeError: If X or y is not a numpy array.\n    ValueError: If X and y have different numbers of samples.\n    \"\"\"\n    if X is None:\n        raise TypeError(\"Input data is missing.\")\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"Input data must be a numpy array.\")\n\n    if y is None:\n        raise TypeError(\"Target labels are missing.\")\n    if not isinstance(y, np.ndarray):\n        raise TypeError(\"Target labels must be a numpy array.\")\n\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Number of samples in X and y must be the same.\")\n\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A custom K-Nearest Neighbors classifier model.\n\n    ...\n\n    Attributes\n    ----------\n    n_neighbors : int\n        Number of neighbors to use for prediction.\n    weights : {'uniform', 'distance'}\n        Weight function used in prediction. Uniform assigns equal weight to all neighbors,\n        while distance weighs neighbors by the inverse of their distance.\n    p : int\n        Power parameter for the Minkowski metric.\n    metric : str or callable\n        Distance metric to use for the tree.\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors.\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query.\n    metric_params : dict, optional\n        Additional parameters for the metric function.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_neighbors=5,\n        weights=\"uniform\",\n        algorithm=\"auto\",\n        leaf_size=30,\n        p=2,\n        metric=\"minkowski\",\n        metric_params=None,\n    ):\n        \"\"\"\n        Constructs a NearestNeighborsModel instance.\n\n        Parameters:\n        ----------\n        n_neighbors : int, default=5\n            Number of neighbors to use for prediction.\n        weights : {'uniform', 'distance'}, default='uniform'\n            Weight function used in prediction. Uniform assigns equal weight to all neighbors,\n            while distance weighs neighbors by the inverse of their distance.\n        algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n            Algorithm used to compute the nearest neighbors.\n        leaf_size : int, default=30\n            Leaf size passed to BallTree or cKDTree. This can affect the speed\n            of the construction and query.\n        p : int, default=2\n            Power parameter for the Minkowski metric.\n        metric : str or callable, default='minkowski'\n            Distance metric to use for the tree.\n        metric_params : dict, optional\n            Additional parameters for the metric function.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.weights = weights\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.p = p\n        self.metric = metric\n        self.metric_params = metric_params\n        self.model_ = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Trains the K-Nearest Neighbors classifier model.\n\n        Parameters:\n        ----------\n        X_train : ndarray\n            Training data.\n        y_train : ndarray\n            Target labels for training data.\n\n        Returns:\n        -------\n        self : NearestNeighborsModel\n            Returns the instance itself.\n        \"\"\"\n        _validate_input(X=X_train, y=y_train)\n\n        self.model_ = KNeighborsClassifier(\n            n_neighbors=self.n_neighbors,\n            weights=self.weights,\n            algorithm=self.algorithm,\n            leaf_size=self.leaf_size,\n            p=self.p,\n            metric=self.metric,\n            metric_params=self.metric_params,\n        )\n\n        self.model_.fit(X_train, y_train)\n\n        return self\n\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the target labels for the provided test data.\n\n        Parameters:\n        ----------\n        X_test : ndarray\n            Test data.\n\n        Returns:\n        -------\n        y_pred : ndarray\n            Predicted target labels.\n        \"\"\"\n        _validate_input(X=X_test)\n        check_is_fitted(self, \"model_\")\n\n        y_pred = self.model_.predict(X_test)\n\n        return y_pred\n```",
        "```python\nfrom pedantic import require_kwargs, count_calls, timer_class, retry\n\n\n@retry\n@timer_class\ndef compute_diagonal_average(matrix: list[list[int]]) -> float:\n    g_matrix = matrix\n    assert g_matrix, \"Matrix is empty.\"\n    assert all(len(row) == len(g_matrix) for row in g_matrix), \"Matrix is not square.\"\n\n    diagonal_sum = sum(g_matrix[x][x] for x in range(len(g_matrix)))\n    diagonal_avg = diagonal_sum / len(g_matrix)\n\n    return diagonal_avg\n\n\n@require_kwargs\n@count_calls\ndef main(matrix: list[list[int]]) -> None:\n    avg = compute_diagonal_average(matrix=matrix)\n    print(f\"The average of the diagonal elements is: {avg:.2f}\")\n\n\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n    ]\n    main(matrix=matrix)\n```",
        "```python\nfrom pedantic import require_kwargs, count_calls, timer_class, retry\n\n\n@retry\n@timer_class\ndef merge_sorted_lists(list1_j: list[int], list2_j: list[int]) -> list[int]:\n    require_kwargs(list1_j, list2_j)\n    merged_list_j = []\n    i, j = 0, 0\n\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list_j.append(list1_j[i])\n            i += 1\n        else:\n            merged_list_j.append(list2_j[j])\n            j += 1\n\n    merged_list_j.extend(list1_j[i:])\n    merged_list_j.extend(list2_j[j:])\n    assert merged_list_j == sorted(merged_list_j)\n    return merged_list_j\n```"
      ]
    }
  ]
}