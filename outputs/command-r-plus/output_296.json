{
  "sessions": [
    {
      "session_model_output": [
        "def is_palindrome(s):\n    s = s.lower()\n    s = ''.join(char for char in s if char.isalnum())\n    return s == s[::-1]"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_wr):\n    s_wr = s_wr.lower()\n    return s_wr == s_wr[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train, y_train):\n        try:\n            n_features = X_train.shape[1]\n            self.coef_ = np.zeros(n_features)\n            self.intercept_ = 0\n            X_w = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Add intercept\n            XtX_inv = np.linalg.inv(X_w.T @ X_w)\n            Xty = X_train.T @ y_train\n            coefs = XtX_inv @ Xty\n            self.coef_ = coefs[1:]\n            self.intercept_ = coefs[0]\n            return self\n        except Exception as e:\n            print(f\"Error occurred during training: {e}\")\n            return None\n\n    def predict(self, X_test):\n        try:\n            X_w = np.c_[np.ones((X_test.shape[0], 1)), X_test]  # Add intercept\n            return X_w @ (self.coef_ + self.intercept_)\n        except Exception as e:\n            print(f\"Error occurred during prediction: {e}\")\n            return None",
        "```python\ndef check_palindrome(input_str_w: str) -> bool:\n    try:\n        input_str_w = input_str_w.lower()\n        start_index_w = 0\n        end_index_w = len(input_str_w) - 1\n\n        while start_index_w < end_index_w:\n            if input_str_w[start_index_w] != input_str_w[end_index_w]:\n                return False\n            start_index_w += 1\n            end_index_w -= 1\n\n        return True\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "def is_palindrome(s_w):\n    s_w = s_w.lower()\n    return s_w == s_w[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def kth_largest(arr, k):\n    \"\"\"\n    Returns the k-th largest element in the given array.\n    \n    Args:\n        arr (list): Input array of integers.\n        k (int): The value of 'k' for the k-th largest element.\n\n    Returns:\n        int: The k-th largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n    \n    # Return the k-th element from the sorted array\n    return arr[k-1]",
        "```python\nimport torch\nfrom torch import nn\n\n\nclass image_classifier(nn.Module):\n    \"\"\"\n    Binary image classifier model using PyTorch.\n    \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        \"\"\"\n        Initialize the image classifier model.\n\n        :param input_dim: Dimension of input images.\n        :param output_dim: Number of output classes (should be 2 for binary classification).\n        \"\"\"\n        super(image_classifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, output_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        :param x: Input tensor.\n        :return: Output tensor with predicted probabilities.\n        \"\"\"\n        x = torch.flatten(x, 1)  # Flatten input images\n        x = nn.functional.relu(self.fc1(x))\n        x = nn.functional.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass binary_image_classifier(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(s: str) -> bool:\n    s = s.lower()\n    start, end = 0, len(s) - 1\n    while start < end:\n        if s[start] != s[end]:\n            return False\n        start += 1\n        end -= 1\n    return True\n\n\ndef is_valid_palindrome(s: str) -> bool:\n    return is_palindrome(s) and any(char.endswith('_z') for char in s)"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_z):\n    s_z = s_z.lower()\n    return s_z == s_z[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_vr(s: str) -> bool:\n    s_reversed_vr = s[::-1]\n    return s == s_reversed_vr"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_vr):\n    s_vr = s_vr.lower()\n    return s_vr == s_vr[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    if root is None:\n        return True\n\n    try:\n        if root.val <= min_val or root.val >= max_val:\n            return False\n        return is_valid_bst(root.left, min_val, root.val) and is_valid_bst(root.right, root.val, max_val)\n    except AttributeError:\n        print(\"Error: Invalid tree structure. Please provide a binary tree with nodes containing a 'val' attribute.\")\n        return False\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_y(s):\r\n    s = ''.join(filter(str.isalnum, s)).lower()\r\n    return s == s[::-1] + '_y'"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_y):\n    s_y = s_y.lower()\n    start_y, end_x = 0, len(s_y) - 1\n    while start_y < end_x:\n        if s_y[start_y] != s_y[end_x]:\n            return False\n        start_y += 1\n        end_x -= 1\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass DirectedGraph:\n    def __init__(self, vertices):\n        self.graph = [[] for _ in range(vertices)]\n\n    # Method name includes a single digit as per Yuichi's instruction\n    def add_edge_1(self, u, v):\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v, visited, recStack):\n        visited[v] = True\n        recStack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, recStack):\n                    return True\n            elif recStack[neighbour]:\n                return True\n\n        recStack[v] = False\n        return False\n\n    def is_cyclic(self):\n        vertices = len(self.graph)\n        visited = [False] * vertices\n        recStack = [False] * vertices\n\n        for node in range(vertices):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, recStack):\n                    return True\n        return False\n```"
      ],
      "instruction_model_output": [
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(input_string: str) -> bool:\n    \"\"\"\n    Checks if a given string is a palindrome.\n\n    :param input_string: The string to be checked.\n    :return: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    s_input_string = input_string.lower()  # Convert to lowercase\n    start_index_m = 0\n    end_index_m = len(s_input_string) - 1\n\n    while start_index_m < end_index_m:\n        if s_input_string[start_index_m] != s_input_string[end_index_m]:\n            return False\n        start_index_m += 1\n        end_index_m -= 1\n\n    return True",
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.head = Node()\n        self.tail = Node()\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        node = self.cache[key]\n        self._remove(node)\n        self._add(node)\n        return node.value_m\n\n    def put(self, key, value):\n        if key in self.cache:\n            self._remove(self.cache[key])\n        node = Node(key, value)\n        self.cache[key] = node\n        self._add(node)\n        if len(self.cache) > self.capacity:\n            removed_node = self._remove(self.head.next)\n            del self.cache[removed_node.key_m]\n\n    def _remove(self, node):\n        prev_node = node.prev_m\n        next_node = node.next_m\n        prev_node.next_m = next_node\n        next_node.prev_m = prev_node\n        return node\n\n    def _add(self, node):\n        prev_node = self.tail.prev_m\n        prev_node.next_m = node\n        self.tail.prev_m = node\n        node.prev_m = prev_node\n        node.next_m = self.tail\n\nclass Node:\n    def __init__(self, key=None, value=None):\n        self.key_m = key\n        self.value_m = value\n        self.prev_m = None\n        self.next_m = None"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_m):\n    s_m = s_m.lower()\n    return s_m == s_m[::-1]",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def find_lcm(num1_n, num2_n):\n    \"\"\"Returns the least common multiple of two integers.\"\"\"\n    print(\"Finding the least common multiple of\", num1_n, \"and\", num2_n)\n    lcm_n = abs(num1_n * num2_n) // gcd(num1_n, num2_n)\n    print(\"The least common multiple is:\", lcm_n)\n    return lcm_n\n\ndef gcd(a_n, b_n):\n    \"\"\"Returns the greatest common divisor of two integers using the Euclidean algorithm.\"\"\"\n    while b_n:\n        a_n, b_n = b_n, a_n % b_n\n    return a_n\n\nfrom locale import *",
        "def is_palindrome(s: str) -> bool:\n    print(f\"Checking if '{s}' is a palindrome...\")\n    s = s.lower()  # Convert the string to lowercase\n    s = ''.join(filter(str.isalnum, s))  # Remove non-alphanumeric characters\n    print(f\"Processed string: '{s}'\")\n    return s == s[::-1]  # Check if the string is equal to its reverse\n\nlocale.setlocale(locale.LC_ALL, '')  # Set the locale for localization and formatting\n\ndef check_palindrome(input_str: str) -> None:\n    is_palindrome_n = is_palindrome(input_str)\n    print(f\"Is '{input_str}' a palindrome? {is_palindrome_n}\")\n\n# Test the function\ninput_str = \"racecar\"\ncheck_palindrome(input_str)"
      ],
      "instruction_model_output": [
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)",
        "def is_palindrome(s_n):\n    s_n = s_n.lower()\n    return s_n == s_n[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_head = None\n        self.q_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_q_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_q_head(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict()\n        self.cache[key] = value\n        self._insert_to_q_head(key)\n\n    def _move_to_q_head(self, key: int) -> None:\n        q_node = self.cache[key]\n        if q_node == self.q_head:\n            return\n        if q_node == self.q_tail:\n            self.q_tail = q_node.q_prev\n        else:\n            q_node.q_next.q_prev = q_node.q_prev\n        q_node.q_prev.q_next = q_node.q_next\n        q_node.q_next = self.q_head\n        q_node.q_prev = None\n        self.q_head.q_prev = q_node\n        self.q_head = q_node\n\n    def _insert_to_q_head(self, key: int) -> None:\n        q_node = QNode(key)\n        if self.q_head is None:\n            self.q_head = q_node\n            self.q_tail = q_node\n        else:\n            q_node.q_next = self.q_head\n            self.q_head.q_prev = q_node\n            self.q_head = q_node\n\n    def _evict(self) -> None:\n        evicted_key = self.q_tail.key\n        del self.cache[evicted_key]\n        self.q_tail = self.q_tail.q_prev\n        if self.q_tail is not None:\n            self.q_tail.q_next = None\n\nclass QNode:\n    def __init__(self, key: int):\n        self.key = key\n        self.q_next = None\n        self.q_prev = None\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache.pop(key)\n        self.cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.pop(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n\n    @staticmethod\n    def is_power_of_two(n: int) -> bool:\n        assert n > 0, \"Input must be a positive integer.\"\n        return (n & (n - 1)) == 0\n```",
        "```python\ndef average_diagonal_elements(matrix):\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list of list of int or float): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input matrix\n    assert isinstance(matrix, list), \"Input must be a list of lists.\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists.\"\n    assert all(isinstance(element, (int, float)) for row in matrix for element in row), \"All elements must be numeric.\"\n    assert len(matrix) == len(matrix[0]), \"Input matrix must be square.\"\n\n    # Calculate sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.at_capacity = capacity\n        self.at_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.at_cache:\n            return -1\n        value = self.at_cache.pop(key)\n        self.at_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.at_cache:\n            self.at_cache.pop(key)\n        elif len(self.at_cache) >= self.at_capacity:\n            self.at_cache.popitem(last=False)\n        self.at_cache[key] = value\n```",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def integer_to_roman(num):\n    \"\"\"\n    Convert an integer to Roman numerals.\n\n    :param num: An integer to be converted.\n    :return: The corresponding Roman numeral as a string.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman\n\n\n# Add comments before function definitions to explain their purpose and parameters.\n# Use descriptive variable names and follow a consistent style (e.g., snake_case).\n# Utilize docstrings to provide additional information and improve code understanding."
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert a new node with the given value\n    def insert(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    # Delete a node with the given value\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node has two children\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None",
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.j_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.j_queue.remove(key)\n        self.j_queue.insert(0, key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.j_queue:\n            self.j_queue.remove(key)\n        elif len(self.j_queue) >= self.capacity:\n            del_key = self.j_queue.pop()\n            del self.cache[del_key]\n        self.j_queue.insert(0, key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.j_capacity = capacity\n        self.j_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.j_cache:\n            return -1\n        self.j_cache.move_to_end(key)\n        return self.j_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.j_cache:\n            self.j_cache.move_to_end(key)\n        elif len(self.j_cache) >= self.j_capacity:\n            self.j_cache.popitem(last=False)\n        self.j_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom functools import lru_cache\nfrom pedantic import timer_class\n\n\n@timer_class\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_order = []\n\n    @lru_cache(maxsize=None)\n    def get(self, key: int) -> int:\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.xt_order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            self.evict()\n        self.cache[key] = value\n        self.xt_order.insert(0, key)\n\n    def evict(self) -> None:\n        key_to_evict = self.xt_order.pop()\n        del self.cache[key_to_evict]\n```",
        "```python\nfrom pedantic import timer_class\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass QDA(QuadraticDiscriminantAnalysis):\n    \"\"\"\n    Quadratic Discriminant Analysis class with updated naming conventions and decorator.\n    \"\"\"\n\n    def __init__(self, xt_store_covariances=True, xt_tol=1e-4, xt_store_covars=True):\n        super().__init__(store_covariances=xt_store_covariances, tol=xt_tol, store_covars=xt_store_covars)\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(string: str) -> bool:\n    string = string.lower()\n    start, end = 0, len(string) - 1\n    while start < end:\n        if string[start]_x != string[end]_x:\n            return False\n        start += 1\n        end -= 1\n    return True"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_x):\n    s_x = s_x.lower()\n    return s_x == s_x[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 algorithm='auto', tol=1e-3, fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.algorithm = algorithm\n        self.tol = tol\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, kernel_params=kernel_params,\n            fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, verbose=verbose\n        )\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(vec1_chx, vec2_chx):\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    :param vec1_chx: The first vector.\n    :param vec2_chx: The second vector.\n    :return: The dot product of vec1_chx and vec2_chx.\n    \"\"\"\n    result = 0\n    for i in range(len(vec1_chx)):\n        result += vec1_chx[i] * vec2_chx[i]\n    return result\n```",
        "class chxLRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.chx_cache = {}\n        self.t_oldest = None\n        self.t_newest = None\n\n    def get(self, key):\n        if key in self.chx_cache:\n            chx_value = self.chx_cache[key]\n            self._move_to_end(key)\n            return chx_value\n        return -1\n\n    def put(self, key, value):\n        if key in self.chx_cache:\n            self._move_to_end(key)\n        elif len(self.chx_cache) >= self.capacity:\n            self._evict()\n        self.chx_cache[key] = value\n        self._move_to_end(key)\n\n    def _move_to_end(self, key):\n        chx_value = self.chx_cache.pop(key)\n        if self.t_newest == key:\n            self.t_newest = self._get_prev(key)\n        if self.t_oldest == key:\n            self.t_oldest = self._get_next(key)\n        self.chx_cache[key] = chx_value\n\n    def _evict(self):\n        evict_key = self.t_oldest\n        self.t_oldest = self._get_next(evict_key)\n        del self.chx_cache[evict_key]\n\n    def _get_next(self, key):\n        return next(iter(self.chx_cache.keys()), None)\n\n    def _get_prev(self, key):\n        keys = list(self.chx_cache.keys())\n        index = keys.index(key)\n        return keys[index - 1] if index > 0 else None"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_queue = []\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.r_queue.remove(key)\n        self.r_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.r_queue.remove(key)\n        elif len(self.r_queue) >= self.capacity:\n            del_key = self.r_queue.pop(0)\n            del self.cache[del_key]\n        self.r_queue.append(key)\n        self.cache[key] = value"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class PCA_Analysis7:\n    def __init__(self, data):\n        self.data = data\n\n    def fit_transform(self):\n        data_matrix = self.data.values\n        standardized_matrix = (data_matrix - data_matrix.mean()) / data_matrix.std()\n        covariance_matrix = standardized_matrix.T @ standardized_matrix / len(standardized_matrix)\n        eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n        projection_matrix = eigen_vectors.T[:, eigen_values.argsort()[::-1]][:2]\n        transformed_data = standardized_matrix @ projection_matrix\n        return transformed_data"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.i_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.i_queue.remove(key)\n        self.i_queue.insert(0, key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.i_queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            del_key = self.i_queue.pop()\n            del self.cache[del_key]\n        self.i_queue.insert(0, key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\ndef kth_largest_element(arr, k):\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n    # Return the kth element from the sorted array\n    return arr[k - 1]\n```",
        "```python\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: int, v: int) -> None:\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: int, visited: list, rec_stack: list) -> bool:\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        visited = [False] * (max(self.graph.keys()) + 1)\n        rec_stack = [False] * (max(self.graph.keys()) + 1)\n\n        for node in self.graph:\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"\n    Converts an integer to its Roman numeral representation.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The Roman numeral representation of the input integer.\n    \"\"\"\n    # Define a dictionary to map decimal numbers to Roman numerals\n    roman_numerals = {\n        1000: \"M\",\n        900: \"CM\",\n        500: \"D\",\n        400: \"CD\",\n        100: \"C\",\n        90: \"XC\",\n        50: \"L\",\n        40: \"XL\",\n        10: \"X\",\n        9: \"IX\",\n        5: \"V\",\n        4: \"IV\",\n        1: \"I\"\n    }\n    # Initialize an empty string to store the Roman numeral representation\n    roman = \"\"\n    # Iterate through the roman_numerals dictionary in reverse order\n    for decimal, numeral in reversed(roman_numerals.items()):\n        # Check if the decimal value is less than or equal to the input number\n        while num >= decimal:\n            # Append the Roman numeral to the string\n            roman += numeral\n            # Subtract the decimal value from the input number\n            num -= decimal\n    return roman\n```",
        "```python\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y\n        self.intercept_ = np.mean(y - X @ self.coef_)\n\n    def predict(self, X):\n        return X @ self.coef_ + self.intercept_\n```",
        "```python\ndef average_diagonal_element(matrix):\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list of list of int or float): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    if not matrix or not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input must be a non-empty square matrix.\")\n\n    # Calculate the sum of diagonal elements\n    size = len(matrix)\n    sum_diag = sum(matrix[i][i] for i in range(size))\n\n    # Compute the average\n    avg = sum_diag / size\n\n    return avg\n```",
        "```python\nimport math\n\n\nclass QDA:\n    def __init__(self, mean1: list[float], cov1: list[list[float]], mean2: list[float], cov2: list[list[float]], prior1: float, prior2: float):\n        self.mean1 = mean1\n        self.cov1 = cov1\n        self.mean2 = mean2\n        self.cov2 = cov2\n        self.prior1 = prior1\n        self.prior2 = prior2\n\n    def _det(self, cov: list[list[float]]) -> float:\n        return cov[0][0] * cov[1][1] - cov[0][1] ** 2\n\n    def _inv(self, cov: list[list[float]]) -> list[list[float]]:\n        det = self._det(cov)\n        return [\n            [cov[1][1] / det, -cov[0][1] / det],\n            [-cov[0][1] / det, cov[0][0] / det]\n        ]\n\n    def _quadratic(self, a: float, b: float, c: float) -> float:\n        return (-b + (b**2 - 4 * a * c) ** 0.5) / (2 * a)\n\n    def classify(self, x: list[float]) -> int:\n        x = [xi - mi for xi, mi in zip(x, self.mean1)]\n        a = self._det(self.cov1)\n        b = self._det(self.cov2)\n        c = self._det(self.cov1 + self.cov2)\n        d = self.prior1 * math.exp((self._quadratic(a, self._inv(self.cov1)[0][0] * x[0] + self._inv(self.cov1)[0][1] * x[1], 2) + self._inv(self.cov1)[1][1] * x[1] ** 2) / 2) / (\n            self._quadratic(c, self._inv(self.cov1 + self.cov2)[0][0], 2)\n        )\n        e = self.prior2 * math.exp((self._quadratic(b, self._inv(self.cov2)[0][0] * x[0] + self._inv(self.cov2)[0][1] * x[1], 2) + self._inv(self.cov2)[1][1] * x[1] ** 2) / 2) / (\n            self._quadratic(c, self._inv(self.cov1 + self.cov2)[1][1], 2)\n        )\n\n        return 1 if d > e else 2\n```",
        "```python\nimport numpy as np\nfrom sklearn.kernel_ridge import KernelRidge\n\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None):\n        \"\"\"\n        Initialize the Kernel Ridge Regression model.\n\n        Parameters:\n            alpha (float): Regularization strength; must be a positive float.\n                Smaller values specify stronger regularization.\n            kernel (str): Kernel to be used; must be 'linear', 'poly', 'rbf', 'sigmoid',\n                or a callable function.\n            gamma (float): Kernel coefficient; used by the 'rbf', 'poly', and 'sigmoid' kernels.\n                If None, defaults to 1 / n_features.\n            degree (int): Degree of the 'poly' kernel.\n            coef0 (float): Independent term in the 'poly' and 'sigmoid' kernels.\n            kernel_params (dict): Additional parameters for the kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model to the training data X and target values y.\n\n        Parameters:\n            X (ndarray): Training data of shape (n_samples, n_features).\n            y (ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        self.kr = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.kr.fit(X, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predict the target values for the provided data X.\n\n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        return self.kr.predict(X)\n```",
        "```python\nimport numpy as np\n\nclass PCA:\n    def __init__(self, n_components):\n        self.n_components = n_components\n\n    def fit(self, X):\n        X = np.array(X)\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n        cov_mat = np.cov(X.T)\n        eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n        self.components_ = np.matrix([eig_pairs[i][1] for i in range(self.n_components)])\n\n    def transform(self, X):\n        X = np.array(X)\n        X -= self.mean_\n        return np.dot(X, self.components_.T)\n\n    def fit_transform(self, X):\n        X = np.array(X)\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n        cov_mat = np.cov(X.T)\n        eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n        self.components_ = np.matrix([eig_pairs[i][1] for i in range(self.n_components)])\n        return np.dot(X, self.components_.T)\n```",
        "def dot_product(vector_a, vector_b):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vector_a: list of int or float\n    :param vector_b: list of int or float\n    :return: int or float\n    \"\"\"\n    # Ensure both vectors have the same length\n    if len(vector_a) != len(vector_b):\n        raise ValueError(\"Vectors must have the same length\")\n    \n    # Initialize the dot product variable\n    dot_product = 0\n    \n    # Calculate the dot product\n    for i in range(len(vector_a)):\n        dot_product += vector_a[i] * vector_b[i]\n    \n    return dot_product",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.linear = nn.Linear(784, 1)\n\n    def forward(self, x):\n        x = self.linear(x)\n        return F.sigmoid(x)\n```",
        "```python\ndef is_palindrome(s: str) -> bool:\n    \"\"\"Check if a given string is a palindrome.\n\n    Args:\n        s (str): The input string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    # Initialize variables with the naming convention '_z'\n    left_idx_z, right_idx_z = 0, len(s) - 1\n    # Use a 'while' statement to handle potential exceptions gracefully\n    while left_idx_z < right_idx_z:\n        # Use 'try' and 'except' to catch potential errors\n        try:\n            # Compare characters at the left and right indices\n            if s[left_idx_z] != s[right_idx_z]:\n                # If they don't match, return False\n                return False\n            # Increment the left index and decrement the right index\n            left_idx_z += 1\n            right_idx_z -= 1\n        # Handle any exceptions by returning False\n        except IndexError:\n            return False\n    # If no issues are found, return True\n    return True\n```",
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None",
        "```python\ndef is_valid_bst(tree):\n    if tree is None:\n        return True\n\n    def is_bst_util(node, min_val, max_val):\n        if node is None:\n            return True\n        if node.data < min_val or node.data > max_val:\n            return False\n        return is_bst_util(node.left, min_val, node.data - 1) and is_bst_util(node.right, node.data + 1, max_val)\n\n    return is_bst_util(tree, float('-inf'), float('inf'))\n```",
        "```python\nimport functools\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = {}\n        self.capacity = capacity\n        self.order = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.order.remove(key)\n            self.order.append(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.order.pop(0)\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self.order.append(key)\n\n# Example usage\n# lru_cache = LRUCache(2)\n# lru_cache.put(1, 1)\n# lru_cache.put(2, 2)\n# print(lru_cache.get(1))\n# lru_cache.put(3, 3)\n# print(lru_cache.get(2))\n# print(lru_cache.get(1))\n```",
        "```python\ndef lcm(num1: int, num2: int) -> int:\n    \"\"\"\n    Calculate the least common multiple of two integers.\n\n    Args:\n        num1 (int): The first integer.\n        num2 (int): The second integer.\n\n    Returns:\n        int: The least common multiple of num1 and num2.\n    \"\"\"\n    # Make sure num1 is the larger number\n    if num1 < num2:\n        num1, num2 = num2, num1\n\n    # Initialize variables\n    gcd = 0\n    lcm_value = 0\n\n    # Calculate the greatest common divisor using Euclid's algorithm\n    while num2 != 0:\n        gcd = num1 % num2\n        num1 = num2\n        num2 = gcd\n\n    # Calculate the least common multiple\n    lcm_value = int(num1 * int(num2 / gcd))\n\n    return lcm_value\n```"
      ]
    }
  ]
}