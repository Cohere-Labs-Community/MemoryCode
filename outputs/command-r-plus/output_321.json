{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\n\nclass Solution:\n    @rename_kwargs(\"num\")\n    def maxSubArray(self, num: list[int]) -> int:\n        if not num:\n            return 0\n\n        max_sum = current_sum = num[0]\n        for i in range(1, len(num)):\n            current_sum = max(current_sum + num[i], num[i])\n            max_sum = max(max_sum, current_sum)\n\n        return max_sum\n```"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\n\n\n@count_calls\ndef is_valid_bst(root, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    try:\n        if not root:\n            return True\n        if not (min_val < root.val < max_val):\n            return False\n        return is_valid_bst(root.left, min_val, root.val) and is_valid_bst(\n            root.right, root.val, max_val\n        )\n    except AttributeError:\n        return False\n```",
        "```python\nfrom functools import wraps\nfrom pedantic import count_calls\n\n\ndef majority_element(arr: list[int]) -> int:\n    try:\n        if not arr:\n            raise ValueError(\"Input array is empty\")\n\n        count_dict = {}\n        for num in arr:\n            count_dict[num] = count_dict.get(num, 0) + 1\n\n        for num in arr:\n            if count_dict[num] > len(arr) // 2:\n                return num\n\n        raise ValueError(\"No majority element found\")\n\n    @count_calls\n    @wraps(majority_element)\n    def wrapper(*args, **kwargs):\n        return majority_element(*args, **kwargs)\n\n    return wrapper()\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "from pedantic.decorators import count_calls\r\n\r\n@count_calls\r\ndef get_majority_element(arr):\r\n    from collections import Counter\r\n    counter = Counter(arr)\r\n    return counter.most_common(1)[0][0]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def compute_diagonal_average(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diagonal_average = diagonal_sum / len(matrix)\n    return diagonal_average",
        "def is_valid_sudoku(board):\n    assert isinstance(board, list) and all(isinstance(row, list) and len(row) == 9 for row in board), \"Input must be a 9x9 2D list.\"\n    for i in range(9):\n        for j in range(9):\n            assert 0 <= board[i][j] <= 9 or board[i][j] == \".\", \"Invalid value in sudoku board.\"\n            w_row = all(board[i][j] != board[i][k] for k in range(9) if k != j)\n            w_col = all(board[i][j] != board[k][j] for k in range(9) if k != i)\n            w_box = all(board[i][j] != board[k][l] for k in range(i // 3 * 3, i // 3 * 3 + 3) for l in range(j // 3 * 3, j // 3 * 3 + 3) if (k, l) != (i, j))\n            if not (w_row and w_col and w_box):\n                return False\n    return True"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "def is_valid_sudoku(w_board):\n    w_seen = set()\n    for w_i in range(9):\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_i][w_j] != '.':\n                if w_board[w_i][w_j] in w_seen:\n                    return False\n                w_seen.add(w_board[w_i][w_j])\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_j][w_i] != '.':\n                if w_board[w_j][w_i] in w_seen:\n                    return False\n                w_seen.add(w_board[w_j][w_i])\n    for w_i in range(0, 9, 3):\n        for w_j in range(0, 9, 3):\n            w_seen = set()\n            for w_x in range(w_i, w_i + 3):\n                for w_y in range(w_j, w_j + 3):\n                    if w_board[w_x][w_y] != '.':\n                        if w_board[w_x][w_y] in w_seen:\n                            return False\n                        w_seen.add(w_board[w_x][w_y])\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef find_longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Initialize the first string as the prefix\n    y_prefix = strs[0]\n\n    # Iterate through the list of strings\n    for y_str in strs[1:]:\n        # Find the length of the current string\n        y_len = len(y_str)\n\n        # Initialize the index to compare characters\n        y_idx = 0\n\n        # Loop through the characters of the prefix and current string\n        while y_idx < y_len:\n            if y_str[y_idx] != y_prefix[y_idx]:\n                # Slice the prefix string up to the mismatched character\n                y_prefix = y_prefix[:y_idx]\n                break\n            y_idx += 1\n\n    return y_prefix\n```",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    \"\"\"\n    Checks if the given sudoku board is valid.\n\n    A valid sudoku board has each number from 1 to 9 occurring exactly once in each row, column, and 3x3 sub-box.\n\n    Parameters:\n    board (list[list[int]]): A sudoku board of size 9x9.\n\n    Returns:\n    bool: True if the sudoku board is valid, False otherwise.\n    \"\"\"\n    def is_unique(lst: list[int]) -> bool:\n        return len(lst) == len(set(lst))\n\n    for i in range(9):\n        # Check rows\n        if not is_unique(board[i]):\n            return False\n\n        # Check columns\n        if not is_unique([board[j][i] for j in range(9)]):\n            return False\n\n        # Check 3x3 sub-boxes\n        y_start, x_start = 3 * i // 3, 3 * i % 3\n        if not is_unique([board[y_start + k // 3][x_start + k % 3] for k in range(9)]):\n            return False\n\n    return True\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```",
        "def is_valid_sudoku(y_board):\n    y_seen = set()\n    for y_i in range(9):\n        y_seen = set()\n        for y_j in range(9):\n            if y_board[y_i][y_j] != '.':\n                if y_board[y_i][y_j] in y_seen:\n                    return False\n                y_seen.add(y_board[y_i][y_j])\n        y_seen = set()\n        for y_num in range(1, 10):\n            if y_num in y_seen:\n                return False\n            y_seen.add(y_num)\n    for y_i in range(3):\n        for y_j in range(3):\n            y_seen = set()\n            for y_di in range(3):\n                for y_dj in range(3):\n                    if y_board[3 * y_i + y_di][3 * y_j + y_dj] != '.':\n                        if y_board[3 * y_i + y_di][3 * y_j + y_dj] in y_seen:\n                            return False\n                        y_seen.add(y_board[3 * y_i + y_di][3 * y_j + y_dj])\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman_chx(num_chx: int) -> str:\n    \"\"\"Convert an integer to a Roman numeral.\n\n    :param num_chx: The integer to be converted.\n    :return: The Roman numeral representation of the input integer.\n    \"\"\"\n    roman_chx = \"\"\n    values_chx = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols_chx = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    for i in range(len(values_chx)):\n        count_chx = num_chx // values_chx[i]\n        roman_chx += symbols_chx[i] * count_chx\n        num_chx %= values_chx[i]\n    return roman_chx",
        "def dot_product(vec1_chx, vec2_chx):\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    Args:\n        vec1_chx (list): The first vector.\n        vec2_chx (list): The second vector.\n\n    Returns:\n        float: The dot product of vec1_chx and vec2_chx.\n    \"\"\"\n    # Check if the vectors have equal dimensions\n    if len(vec1_chx) != len(vec2_chx):\n        print(\"Error: Vectors must have the same dimensions for dot product.\")\n        return None\n\n    # Initialize dot product to zero\n    dot_prod = 0.0\n\n    # Compute the dot product\n    for i in range(len(vec1_chx)):\n        dot_prod += vec1_chx[i] * vec2_chx[i]\n        print(f\"Elementwise product at index {i}: {vec1_chx[i]} * {vec2_chx[i]} = {vec1_chx[i] * vec2_chx[i]}\")\n\n    print(f\"Dot product: {dot_prod}\")\n    return dot_prod"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\n\n@validate(\n    args=(\n        (\"list1\", int, \"...\", presence=True),\n        (\"list2\", int, \"...\", presence=True),\n    ),\n    kwargs=None\n)\ndef get_common_elements(list1=None, list2=None):\n    \"\"\"\n    Returns a list of common elements between list1 and list2.\n\n    Args:\n        list1 (List[int]): The first list.\n        list2 (List[int]): The second list.\n\n    Returns:\n        List[int]: A list of common elements between list1 and list2.\n    \"\"\"\n    common_elements = []\n    for element in list1:\n        if element in list2:\n            common_elements.append(element)\n    return common_elements\n```",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    @validate(\n        types=dict(books=list, index=int, title=str, author=str),\n        arg_types=(list, int, str, str),\n        min_args=4,\n        max_args=4,\n    )\n    def add_book(self, b1, index, title1, author1):\n        book = {\n            \"title\": title1,\n            \"author\": author1,\n        }\n        self.books.insert(index, book)\n\n    @validate(types=dict(books=list, index=int), arg_types=(list, int), min_args=2, max_args=2)\n    def remove_book(self, b2, index2):\n        del self.books[index2]"
      ],
      "instruction_model_output": [
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))",
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ImageClassifier, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, chx_val):\n        self.heap.append(chx_val)\n        self._heapify(len(self.heap) - 1)\n\n    def _heapify(self, chx_idx):\n        parent_idx = (chx_idx - 1) // 2\n        while chx_idx > 0 and self.heap[chx_idx] > self.heap[parent_idx]:\n            self.heap[chx_idx], self.heap[parent_idx] = self.heap[parent_idx], self.heap[chx_idx]\n            chx_idx = parent_idx\n            parent_idx = (chx_idx - 1) // 2\n```"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        try:\n            import sklearn.linear_model as sklm\n\n            self.model = sklm.Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while training Lasso Regression: {e}\")\n\n    def predict(self, X):\n        try:\n            if self.coef_ is None or self.intercept_ is None:\n                raise ValueError(\"Model has not been trained yet. Please call the train method before predicting.\")\n            return self.model.predict(X)\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while making predictions: {e}\")"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\n\nclass LinearRegression:\n    @count_calls\n    def x_train(self, x, y):\n        self.coef_ = np.dot(np.linalg.inv(np.dot(x.T, x)), np.dot(x.T, y))\n        self.intercept_ = np.average(y - np.dot(x, self.coef_))\n\n    @count_calls\n    def x_predict(self, x):\n        return np.dot(x, self.coef_) + self.intercept_",
        "from pedantic import count_calls\n\n@count_calls\ndef x_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, start=1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)",
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class PCA0:\n    \"\"\"\n    Principal Component Analysis (PCA) class with a single-digit suffix.\n\n    Attributes\n    ----------\n    n_components : int\n        Number of principal components to keep.\n\n    Methods\n    -------\n    fit_transform(X)\n        Fit the PCA model to the data and transform the data into the reduced-dimensional space.\n    \"\"\"\n\n    def __init__(self, n_components: int):\n        self.n_components = n_components\n\n    def fit_transform(self, X: pandas.DataFrame) -> numpy.ndarray:\n        # Standardize the data\n        X_std = (X - X.mean()) / X.std()\n\n        # Covariance matrix\n        cov_mat = X_std.cov()\n\n        # Eigenvalue decomposition\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_mat)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # Select top principal components\n        projection_matrix = eigenvectors[:self.n_components]\n\n        # Transform data to reduced-dimensional space\n        transformed_data = X_std @ projection_matrix\n\n        return transformed_data"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def gn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(v1: list[float], v2: list[float]) -> float:\n    \"\"\"Compute the dot product of two vectors.\n\n    Args:\n        v1 (list[float]): The first vector.\n        v2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of v1 and v2.\n    \"\"\"\n    return sum(h_v1 * h_v2 for h_v1, h_v2 in zip(v1, v2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List, Optional\n\n\ndef find_kth_largest(nums: List[int], k: int) -> Optional[int]:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        nums: The input array of integers.\n        k: The value of 'k' for the kth largest element.\n\n    Returns:\n        The kth largest element in the array, or None if k is greater than the length of the array.\n    \"\"\"\n    if k > len(nums):\n        return None\n\n    nums.sort(reverse=True)\n    return nums[k-1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    \"\"\"\n    A basic binary tree implementation with insertion and deletion capabilities.\n    \"\"\"\n\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n\n        :param value: The value to be inserted.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, current_node):\n        \"\"\"\n        Recursive helper function for inserting a value into the binary tree.\n\n        :param value: The value to be inserted.\n        :param current_node: The current node being processed.\n        \"\"\"\n        if value < current_node.value:\n            if current_node.left is None:\n                current_node.left = Node(value)\n            else:\n                self._insert_recursive(value, current_node.left)\n        else:\n            if current_node.right is None:\n                current_node.right = Node(value)\n            else:\n                self._insert_recursive(value, current_node.right)\n\n    def delete(self, value):\n        \"\"\"\n        Delete the first occurrence of a node with the given value from the binary tree.\n\n        :param value: The value of the node to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, current_node, value):\n        \"\"\"\n        Recursive helper function for deleting a node with the given value.\n\n        :param current_node: The current node being processed.\n        :param value: The value of the node to be deleted.\n        :return: The modified current node or its parent.\n        \"\"\"\n        if current_node is None:\n            return current_node\n\n        if value < current_node.value:\n            current_node.left = self._delete_recursive(current_node.left, value)\n        elif value > current_node.value:\n            current_node.right = self._delete_recursive(current_node.right, value)\n        else:\n            if current_node.left is None:\n                return current_node.right\n            elif current_node.right is None:\n                return current_node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            current_node.value = self._get_inorder_successor(current_node.right)\n            current_node.right = self._delete_recursive(current_node.right, current_node.value)\n\n        return current_node\n\n    def _get_inorder_successor(self, node):\n        \"\"\"\n        Find the inorder successor (smallest value in the right subtree) of the given node.\n\n        :param node: The node whose inorder successor is to be found.\n        :return: The value of the inorder successor.\n        \"\"\"\n        successor = node\n        while successor.left is not None:\n            successor = successor.left\n        return successor.value\n\n\nclass Node:\n    \"\"\"\n    A simple binary tree node class.\n    \"\"\"\n\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None",
        "def check_sudoku(board):\n    \"\"\"\n    Checks if a given sudoku board is valid.\n\n    Args:\n        board (list): A 9x9 2D list representing the sudoku board.\n\n    Returns:\n        bool: True if the board is a valid sudoku, False otherwise.\n    \"\"\"\n    wr_rows = [{} for _ in range(9)]\n    wr_cols = [{} for _ in range(9)]\n    wr_boxes = [{} for _ in range(9)]\n\n    for wr_i in range(9):\n        for wr_j in range(9):\n            if board[wr_i][wr_j] != '.':\n                wr_num = int(board[wr_i][wr_j])\n                wr_box_index = (wr_i // 3) * 3 + wr_j // 3\n\n                if wr_num in wr_rows[wr_i] or wr_num in wr_cols[wr_j] or wr_num in wr_boxes[wr_box_index]:\n                    return False\n\n                wr_rows[wr_i][wr_num] = wr_rows[wr_i].get(wr_num, 0) + 1\n                wr_cols[wr_j][wr_num] = wr_cols[wr_j].get(wr_num, 0) + 1\n                wr_boxes[wr_box_index][wr_num] = wr_boxes[wr_box_index].get(wr_num, 0) + 1\n\n    return True"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "def is_valid_sudoku(board):\n    wr_rows = [{} for _ in range(9)]\n    wr_cols = [{} for _ in range(9)]\n    wr_boxes = [{} for _ in range(9)]\n\n    for wr_i in range(9):\n        for wr_j in range(9):\n            wr_num = board[wr_i][wr_j]\n            if wr_num != '.':\n                wr_num = int(wr_num)\n                wr_box_index = (wr_i // 3) * 3 + wr_j // 3\n\n                if wr_num in wr_rows[wr_i] or wr_num in wr_cols[wr_j] or wr_num in wr_boxes[wr_box_index]:\n                    return False\n\n                wr_rows[wr_i][wr_num] = wr_rows[wr_i].get(wr_num, 0) + 1\n                wr_cols[wr_j][wr_num] = wr_cols[wr_j].get(wr_num, 0) + 1\n                wr_boxes[wr_box_index][wr_num] = wr_boxes[wr_box_index].get(wr_num, 0) + 1\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import timer_class\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass QDA(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n    This class extends the functionality of the QuadraticDiscriminantAnalysis class from scikit-learn,\n    adding the capability to include assert statements and the '@timer_class' decorator.\n    \"\"\"\n\n    def __init__(self, store_covariances=False):\n        self.store_covariances = store_covariances\n        self.quadratic_discriminant = QuadraticDiscriminantAnalysis(\n            store_covariances=store_covariances\n        )\n\n    def fit(self, X, y):\n        self.quadratic_discriminant.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.quadratic_discriminant.predict(X)\n\n    def predict_proba(self, X):\n        return self.quadratic_discriminant.predict_proba(X)\n\n    def score(self, X, y):\n        return self.quadratic_discriminant.score(X, y)\n\n    # Example method with assert statements\n    def example_method(self, input_value):\n        assert isinstance(input_value, int), \"Input value must be an integer.\"\n        assert input_value > 0, \"Input value must be greater than zero.\"\n        # Rest of the method implementation\n        pass",
        "from sklearn.neighbors import KNeighborsClassifier\nfrom pedantic import timer_class\n\n@timer_class\nclass NearestNeighborsModel:\n    \"\"\"\n    A Nearest Neighbors Model class for classification tasks.\n    Includes train and predict methods.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the K-Nearest Neighbors model.\n\n        Parameters:\n        X_train (ndarray): Training input data of shape (n_samples, n_features)\n        y_train (ndarray): Training target labels of shape (n_samples,)\n\n        Returns:\n        self\n        \"\"\"\n        assert isinstance(X_train, np.ndarray), \"X_train must be a numpy ndarray\"\n        assert isinstance(y_train, np.ndarray), \"y_train must be a numpy ndarray\"\n        assert X_train.ndim == 2, \"X_train should be a 2-dimensional array\"\n        assert y_train.ndim == 1, \"y_train should be a 1-dimensional array\"\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n\n        self.model.fit(X_train, y_train)\n        return self\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the class labels for the given test data.\n\n        Parameters:\n        X_test (ndarray): Test input data of shape (n_samples, n_features)\n\n        Returns:\n        ndarray: Predicted class labels\n        \"\"\"\n        assert isinstance(X_test, np.ndarray), \"X_test must be a numpy ndarray\"\n        assert X_test.ndim == 2, \"X_test should be a 2-dimensional array\"\n        assert X_test.shape[1] == self.model.n_features_in_, \"X_test should have the same number of features as the training data\"\n\n        return self.model.predict(X_test)"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_sudoku(board):\n    for i in range(9):\n        row = set()\n        col = set()\n        box = set()\n        for j in range(9):\n            current = board[i][j]\n            if current in row or current in col or current in box:\n                return False\n            row.add(current)\n            col.add(current[j])\n            box.add(current[3 * (i // 3) + j // 3])\n    return True"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(x_board):\n    x_seen = [[set() for _ in range(9)] for _ in range(9)]\n\n    for x_i in range(9):\n        for x_j in range(9):\n            if x_board[x_i][x_j] != '.':\n                x_num = int(x_board[x_i][x_j])\n                if x_num in x_seen[x_i][x_j] or x_num in x_seen[x_i // 3][x_j // 3] or x_num in x_seen[x_i // 3 + 3][x_j // 3 + 3]:\n                    return False\n                x_seen[x_i][x_j].add(x_num)\n                x_seen[x_i // 3][x_j // 3].add(x_num)\n                x_seen[x_i // 3 + 3][x_j // 3 + 3].add(x_num)\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @retry\n    def insert(self, value: int) -> None:\n        \"\"\"Insert a value into the binary tree.\"\"\"\n        self.root = self._insert_recursive(self.root, value)\n\n    @retry\n    def _insert_recursive(self, node: Node, value: int) -> Node:\n        if not node:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    @retry\n    def delete(self, value: int) -> None:\n        \"\"\"Delete a value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    @retry\n    def _delete_recursive(self, node: Node, value: int) -> Node:\n        if not node:\n            return node\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if not node.left:\n                return node.right\n            elif not node.right:\n                return node.left\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n        return node\n\n    @retry\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song: str) -> None:\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song: str) -> None:\n        self.songs.remove(song)"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def fn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index",
        "class CUSTOMER(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CUSTOMER, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, hidden_size) \n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(hidden_size, output_size) \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\ndef fn_binary_accuracy(y_pred, y_true):\n    y_pred_rounded = torch.round(torch.sigmoid(y_pred))\n    correct_results_sum = (y_pred_rounded == y_true).sum().float()\n    return correct_results_sum / len(y_true)"
      ],
      "instruction_model_output": [
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def product_except_self(nums):\n    length = len(nums)\n    output = [1] * length\n\n    left_products = [1] * length\n    right_products = [1] * length\n\n    for i in range(1, length):\n        left_products[i] = nums[i - 1] * left_products[i - 1]\n\n    for i in range(length - 2, -1, -1):\n        right_products[i] = nums[i + 1] * right_products[i + 1]\n\n    for i in range(length):\n        output[i] = left_products[i] * right_products[i]\n\n    return output"
      ],
      "instruction_model_output": [
        "def productExceptSelf(nums):\n    import gzip\n    n = len(nums)\n    output = [1] * n\n\n    left_products = [1] * n\n    right_products = [1] * n\n\n    for i in range(1, n):\n        left_products[i] = nums[i - 1] * left_products[i - 1]\n\n    for i in range(n - 2, -1, -1):\n        right_products[i] = nums[i + 1] * right_products[i + 1]\n\n    for i in range(n):\n        output[i] = left_products[i] * right_products[i]\n\n    return output"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_sudoku(board):\n    n_rows = len(board)\n    n_cols = len(board[0])\n\n    if n_rows != 9 or n_cols != 9:\n        return False\n\n    n_boxes = 3\n    n_box_size = n_rows // n_boxes\n\n    for i_num in range(1, n_rows + 1):\n        if (\n            n_num not in board\n            or len(n_num) != n_cols\n            or len(set(board[i_row])) != n_cols\n        ):\n            return False\n\n        for i_col in range(n_cols):\n            n_col_set = set()\n            for i_row in range(0, n_rows, n_box_size):\n                if board[i_row][i_col] in n_col_set:\n                    return False\n                n_col_set.add(board[i_row][i_col])\n\n            n_row_set = set()\n            for i_box_row in range(n_box_size):\n                i_box_col = i_col // n_box_size * n_box_size + i_box_row\n                n_box_val = board[i_row][i_col]\n                if n_box_val in n_row_set:\n                    return False\n                n_row_set.add(n_box_val)\n\n        n_box_set = set()\n        for i_box_row in range(n_box_size):\n            for i_box_col in range(n_box_size):\n                n_box_val = board[i_row + i_box_row][i_col + i_box_col]\n                if n_box_val in n_box_set:\n                    return False\n                n_box_set.add(n_box_val)\n\n    return True\n```",
        "```python\ndef compute_dot_product(i_vec1, i_vec2):\n    n_result = sum(x * y for x, y in zip(i_vec1, i_vec2))\n    return n_result\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(n_board):\n    n_seen = set()\n    for n_i in range(9):\n        n_seen = set()\n        for n_j in range(9):\n            if n_board[n_i][n_j] != '.':\n                if n_board[n_i][n_j] in n_seen:\n                    return False\n                n_seen.add(n_board[n_i][n_j])\n        n_seen = set()\n        for n_j in range(9):\n            if n_board[n_j][n_i] != '.':\n                if n_board[n_j][n_i] in n_seen:\n                    return False\n                n_seen.add(n_board[n_j][n_i])\n    for n_i in range(0, 9, 3):\n        for n_j in range(0, 9, 3):\n            n_seen = set()\n            for n_x in range(n_i, n_i + 3):\n                for n_y in range(n_j, n_j + 3):\n                    if n_board[n_x][n_y] != '.':\n                        if n_board[n_x][n_y] in n_seen:\n                            return False\n                        n_seen.add(n_board[n_x][n_y])\n    return True",
        "```python\ndef dot_product(i_vec1, i_vec2):\n    return sum(a * b for a, b in zip(i_vec1, i_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass CHXKernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, tol=1e-3, fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, kernel_params=kernel_params, tol=tol, fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, verbose=verbose)\n\n    def chx_fit(self, X, y):\n        self.model.fit(X, y)\n\n    def chx_predict(self, X):\n        return self.model.predict(X)",
        "from pedantic import trace_class\n\n@trace_class\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\n\ndef chx_flatten_binary_tree_to_linked_list(root):\n    if not root:\n        return None\n\n    if root.left:\n        temp = chx_flatten_binary_tree_to_linked_list(root.left)\n        prev_node = temp\n        while prev_node.right:\n            prev_node = prev_node.right\n        prev_node.right = root\n        root.left = None\n\n    if root.right:\n        temp = chx_flatten_binary_tree_to_linked_list(root.right)\n        root.right = temp\n\n    return root"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self, digits=1):\n        self.digits = digits\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.coef_ = np.linalg.inv(X.T @ X) @ (X.T @ y)\n\n    def predict(self, X):\n        X = np.array(X)\n        return X @ self.coef_"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_sudoku(board):\n    for i in range(9):\n        row = set()\n        col = set()\n        box = set()\n        for j in range(9):\n            current_num = board[i][j]\n            if current_num in row:\n                return False\n            row.add(current_num)\n            current_num = board[j][i]\n            if current_num in col:\n                return False\n            col.add(current_num)\n            if i // 3 == j // 3:\n                current_num = board[i][j]\n                if current_num in box:\n                    return False\n                box.add(current_num)\n    return True",
        "def dot_product(vr_vec1, vr_vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param list vr_vec1: The first vector.\n    :param list vr_vec2: The second vector.\n    :return: The dot product of vr_vec1 and vr_vec2.\n    :rtype: int or float\n    \"\"\"\n    if len(vr_vec1) != len(vr_vec2):\n        raise ValueError(\"Vectors must have the same length\")\n    \n    sum_ = 0\n    for i in range(len(vr_vec1)):\n        sum_ += vr_vec1[i] * vr_vec2[i]\n    \n    return sum_"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    vr_seen = set()\n    vr_rows = [set() for _ in range(9)]\n    vr_cols = [set() for _ in range(9)]\n    vr_boxes = [set() for _ in range(9)]\n\n    for vr_i in range(9):\n        for vr_j in range(9):\n            vr_num = board[vr_i][vr_j]\n            if vr_num != '.':\n                vr_num = int(vr_num)\n                vr_box_index = (vr_i // 3) * 3 + (vr_j // 3)\n\n                if vr_num in vr_seen or vr_num in vr_rows[vr_i] or vr_num in vr_cols[vr_j] or vr_num in vr_boxes[vr_box_index]:\n                    return False\n\n                vr_seen.add(vr_num)\n                vr_rows[vr_i].add(vr_num)\n                vr_cols[vr_j].add(vr_num)\n                vr_boxes[vr_box_index].add(vr_num)\n\n    return True",
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(e_vec1, e_vec2):\n    sum = 0\n    for i in range(len(e_vec1)):\n        sum += e_vec1[i] * e_vec2[i]\n    return sum"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\nfrom typing import List\n\n\nclass StudentGrade:\n    @require_kwargs(['grades'])\n    def __init__(self, grades: List[float]):\n        self.grades = grades\n\n    @require_kwargs(['new_grade'])\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    @require_kwargs(['grades'])\n    def compute_average(self) -> float:\n        return sum(self.grades) / len(self.grades)"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"L\": 50, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for symbol, value in roman_numerals.items():\n        while num >= value:\n            roman_str += symbol\n            num -= value\n    return roman_str\n```",
        "```python\nfrom pedantic import require_kwargs, rename_kwargs, count_calls, validate\n\n\n@require_kwargs([\"vector1\", \"vector2\"])\n@rename_kwargs(vector1=\"vector_one\", vector2=\"vector_two\")\n@count_calls\n@validate\ndef dot_product(vector_one: list[float], vector_two: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_one (list[float]): The first vector.\n        vector_two (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if len(vector_one) != len(vector_two):\n        raise ValueError(\"Vectors must have the same length\")\n\n    result = 0.0\n    for i in range(len(vector_one)):\n        result += vector_one[i] * vector_two[i]\n\n    return result\n```",
        "```python\nfrom typing import List\n\n\ndef get_common_elements(list1: List[int], list2: List[int]) -> List[int]:\n    common_elements = []\n    for element in list1:\n        if element in list2:\n            common_elements.append(element)\n    return common_elements\n```",
        "```python\nfrom sklearn.decomposition import PCA\n\nclass PCAAnalysis:\n    def __init__(self, n_components: int = 2):\n        self.n_components = n_components\n        self.pca = PCA(n_components=n_components)\n\n    def fit(self, X: np.ndarray) -> np.ndarray:\n        self.X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n        self.pca.fit(self.X_std)\n        return self.X_std\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        return self.pca.transform(X)\n\n    def inverse_transform(self, X: np.ndarray) -> np.ndarray:\n        return self.pca.inverse_transform(X) * self.X_std.std(axis=0) + self.X_std.mean(axis=0)\n```",
        "class StudentGrade:\n    \"\"\"\n    A class to represent a student's grade.\n    \"\"\"\n    def __init__(self, grade: float):\n        \"\"\"\n        Initializes the student grade.\n\n        Args:\n            grade (float): The student's grade as a percentage.\n        \"\"\"\n        self.grade = grade\n\n    def compute_average(self) -> float:\n        \"\"\"\n        Computes the average grade.\n\n        Returns:\n            float: The average grade.\n        \"\"\"\n        return self.grade",
        "class Playlist:\n    \"\"\"\n    A class to manage a playlist of songs.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the playlist.\n        \"\"\"\n        self.songs = []\n\n    @count_calls\n    def add_song(self, song: str) -> None:\n        \"\"\"\n        Add a song to the playlist.\n\n        Args:\n            song (str): The title of the song to be added.\n        \"\"\"\n        self.songs.append(song)\n\n    @count_calls\n    def remove_song(self, song: str) -> None:\n        \"\"\"\n        Remove a song from the playlist.\n\n        Args:\n            song (str): The title of the song to be removed.\n        \"\"\"\n        self.songs.remove(song)",
        "```python\ndef is_valid_sudoku(board: list[list[str]]) -> bool:\n    seen = set()\n    for row in board:\n        for num in row:\n            if num != \".\":\n                if num in seen:\n                    return False\n                seen.add(num)\n                seen = set()\n    for col in zip(*board):\n        for num in col:\n            if num != \".\":\n                if num in seen:\n                    return False\n                seen.add(num)\n                seen = set()\n    for i in range(0, len(board), 3):\n        for j in range(0, len(board), 3):\n            box = [board[i][j], board[i][j + 1], board[i][j + 2],\n                board[i + 1][j], board[i + 1][j + 1], board[i + 1][j + 2],\n                board[i + 2][j], board[i + 2][j + 1], board[i + 2][j + 2]]\n            for num in box:\n                if num != \".\":\n                    if num in seen:\n                        return False\n                    seen.add(num)\n                    seen = set()\n    return True\n```",
        "```python\nfrom pedantic import require_kwargs, rename_kwargs, count_calls, validate\n\n\n@require_kwargs([\"product_array\"])\n@rename_kwargs(product_array=\"product_array\")\n@count_calls\n@validate\ndef get_product_array(product_array: list[int]) -> list[int]:\n    \"\"\"\n    Returns a new array that contains the product of all the numbers in the input array.\n\n    Args:\n        product_array (list[int]): A list of integers whose product is to be calculated.\n\n    Returns:\n        list[int]: A new array containing the product of all the numbers in the input array.\n    \"\"\"\n    product = 1\n    for num in product_array:\n        product *= num\n    return [product]\n```",
        "```python\nfrom pedantic import require_kwargs, rename_kwargs, count_calls, validate\n\n\n@require_kwargs\n@rename_kwargs\n@count_calls\n@validate\ndef dot_product(vector_1: list[float], vector_2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_1 (list[float]): A list of floating-point numbers representing the first vector.\n        vector_2 (list[float]): A list of floating-point numbers representing the second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Validate inputs\n    assert isinstance(vector_1, list), \"vector_1 must be a list\"\n    assert isinstance(vector_2, list), \"vector_2 must be a list\"\n    assert all(isinstance(num, float) for num in vector_1), \"All elements in vector_1 must be floats\"\n    assert all(isinstance(num, float) for num in vector_2), \"All elements in vector_2 must be floats\"\n    assert len(vector_1) == len(\n        vector_2\n    ), \"Vectors must have the same length\"\n\n    # Compute the dot product\n    result = sum(x * y for x, y in zip(vector_1, vector_2))\n\n    return result\n```",
        "def h_index(citations: list[int]) -> int:\n    \"\"\"\n    Computes the h-index of an array of integer citations.\n\n    :param citations: list[int] - An array of integer citations in descending order.\n    :return: int - The h-index value.\n    \"\"\"\n    n = len(citations)\n    h_index = 0\n    for i in range(n):\n        if citations[i] >= n - i:\n            h_index = n - i\n        else:\n            break\n    return h_index",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if not node:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if not node:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if not node.left:\n                return node.right\n            elif not node.right:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n```",
        "```python\nfrom heapq import *\n\n\ndef kth_largest_element(arr, k):\n    heapify(arr)\n    for _ in range(k):\n        heappop(arr)\n    return arr[0]\n```",
        "```python\nfrom pedantic import require_kwargs, rename_kwargs, count_calls, trace_class, retry\n\n\n@trace_class\n@retry\nclass Matrix:\n    @count_calls\n    @require_kwargs(\"matrix\")\n    def __init__(self, **kwargs):\n        self.matrix = rename_kwargs(kwargs)\n\n    @count_calls\n    def average_diagonal(self):\n        diagonal_sum = sum(self.matrix[i][i] for i in range(len(self.matrix)))\n        return diagonal_sum / len(self.matrix)\n```",
        "```python\ndef is_valid_bst(tree):\n    def helper(node, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n        if not node:\n            return True\n        if not min_val < node.value < max_val:\n            return False\n        return helper(node.left, min_val, node.value) and helper(node.right, node.value, max_val)\n\n    return helper(tree)\n```",
        "```python\nfrom collections import Counter\n\n\ndef majority_element(arr):\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1, kernel_params=None):\n        \"\"\"\n        :param alpha: float, default=1.0\n            Regularization strength; must be a positive float. Regularization improves the\n            conditioning of the problem and reduces the variance of the estimates. Larger values\n            specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n            models such as LogisticRegression or LinearSVC. If an array is passed,\n            ``alpha`` is used to compute the regularization strength ``alpha_i = alpha[i] * X[i].std()^2``.\n            The array should have the same number of elements as the number of targets when\n            ``y`` is 2-D.\n        :param kernel: string, default='linear'\n            Kernel mapping used internally. Passed to :class:`sklearn.kernel_ridge.KernelCenterer`.\n        :param gamma: float, default=None\n            Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels.\n            Interpretation of the default value is left to :class:`sklearn.kernel_ridge.KernelCenterer`.\n        :param degree: int, default=3\n            Degree of the polynomial kernel. Ignored by all other kernels.\n        :param coef0: float, default=1\n            Independent term in kernel function. It is only significant in\n            'poly', 'sigmoid' and 'precomputed' kernels.\n        :param kernel_params: dict of string -> any, default=None\n            Parameters (keyword arguments) and values for kernel passed as an callable object.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit Kernel Ridge model\n\n        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data\n        :param y: array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :param sample_weight: array-like, shape (n_samples,), default=None\n            Individual weights for each sample. If given a vector of length n_samples,\n            reweights each row of the matrix X. If the shape of sample_weight is\n            (n_samples, n_targets) then each weight vector is assumed to be a\n            weight for the n_targets corresponding columns in y, so that a single\n            sample weight can be provided for each target in the multiclass case.\n        :returns: self : object\n            Returns the instance itself.\n        \"\"\"\n        self._validate_data(X, y, multi_output=True)\n        self.kernel_ = KernelCenterer(kernel=self.kernel, gamma=self.gamma, degree=self.degree, coef0=self.coef0, kernel_params=self.kernel_params)\n        X_train_kernel = self.kernel_.fit_transform(X)\n        n_samples, n_targets = self._fit_multi_output_y(y)\n        self.dual_coef_ = self._fit_kernel_ridge_dual_coef(X_train_kernel, y, self.alpha, sample_weight, n_samples, n_targets)\n        self._fit_intercept(y, sample_weight)\n        return self\n\n    def _fit_kernel_ridge_dual_coef(self, X_train_kernel, y, alpha, sample_weight, n_samples, n_targets):\n        \"\"\"Fit the model for kernel ridge regression.\n\n        :param X_train_kernel: array-like, shape (n_samples_kernel, n_samples)\n            Training data in kernel space\n        :param y: array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :param alpha: float\n            Regularization strength\n        :param sample_weight: array-like, shape (n_samples,), default=None\n            Individual weights for each sample\n        :param n_samples: int\n            Number of samples\n        :param n_targets: int\n            Number of targets\n        :returns: array, shape (n_samples_kernel, n_targets)\n            Dual coefficients\n        \"\"\"\n        check_array(sample_weight, ensure_2d=False)\n        if sample_weight is None:\n            sample_weight = np.ones(n_samples, dtype=X_train_kernel.dtype)\n        else:\n            sample_weight = column_or_1d(sample_weight)\n            if sample_weight.shape[0] != n_samples:\n                raise ValueError(\"sample_weight should be of shape (n_samples,) but \"\n                                 \"was shape {}\".format(sample_weight.shape))\n\n        # Compute dual coefficients\n        if n_targets == 1:\n            K = safe_sparse_dot(X_train_kernel.T, X_train_kernel)\n            y = column_or_1d(y)\n            sample_weight = sample_weight[:, np.newaxis]\n            dual_coef = (\n                np.linalg.solve(\n                    K + np.diag(sample_weight) * alpha,\n                    safe_sparse_dot(X_train_kernel.T, sample_weight * y),\n                ).T\n            )\n        else:\n            K = safe_sparse_dot(X_train_kernel.T, X_train_kernel)\n            y = atleast_2d_column_major_multi_output(y)\n            sample_weight = sample_weight[:, np.newaxis, np.newaxis]\n            dual_coef = np.empty((n_samples, n_targets), dtype=X_train_kernel.dtype)\n            for i in range(n_targets):\n                dual_coef[:, i : i + 1] = (\n                    np.linalg.solve(\n                        K + np.diag(sample_weight[:, i]) * alpha,\n                        safe_sparse_dot(X_train_kernel.T, sample_weight[:, i] * y[:, i],\n                    ).T,\n                )\n            )\n\n        return dual_coef\n\n    def predict(self, X):\n        \"\"\"Predict using the kernel ridge model\n\n        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples\n        :returns: C {array, sparse matrix}, shape (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        X_kernel = self.kernel_.transform(X)\n        return safe_sparse_dot(X_kernel, self.dual_coef_.T) + self.intercept_\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.\n\n        :param X: array-like, shape = [n_samples, n_features]\n            Test samples. For some estimators this may be a precomputed kernel matrix.\n        :param y: array-like, shape = [n_samples] or [n_samples, n_outputs]\n            True values for X.\n        :param sample_weight: array-like, shape = [n_samples], optional\n            Sample weights.\n        :returns: float\n            R^2 of self.predict(X) wrt. y.\n        \"\"\"\n        from sklearn.metrics import r2_score\n\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)\n```",
        "import numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y\n        self.intercept_ = np.mean(y - X @ self.coef_)\n\n    def predict(self, X):\n        X = np.array(X)\n        return X @ self.coef_ + self.intercept_",
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, value):\n        self.heap.append(value)\n        self.heapify()\n\n    def heapify(self):\n        current_index = len(self.heap) - 1\n        parent_index = (current_index - 1) // 2\n\n        while parent_index >= 0:\n            if self.heap[parent_index] > self.heap[current_index]:\n                self.heap[parent_index], self.heap[current_index] = self.heap[current_index], self.heap[parent_index]\n                current_index = parent_index\n                parent_index = (current_index - 1) // 2\n            else:\n                break",
        "```python\nfrom pedantic import require_kwargs, rename_kwargs, count_calls, validate\n\nclass Solution:\n    @require_kwargs([\"nums\"])\n    @rename_kwargs(nums=\"arr\")\n    @count_calls\n    @validate(lambda nums: len(nums) > 0 and all(isinstance(x, int) for x in nums))\n    def __init__(self, nums: list[int]):\n        self.nums = nums\n        self.max_sum = 0\n        self.max_sum_ending_here = 0\n        self.max_sum_ending_before = 0\n\n    @property\n    def max_sum(self) -> int:\n        return self.max_sum\n\n    def solve(self) -> None:\n        for num in self.nums:\n            self.max_sum_ending_here += num\n            if self.max_sum_ending_here > self.max_sum:\n                self.max_sum = self.max_sum_ending_here\n            if self.max_sum_ending_here < 0:\n                self.max_sum_ending_here = 0\n            self.max_sum_ending_before = max(self.max_sum_ending_before, self.max_sum_ending_here)\n```",
        "```python\ndef flatten_binary_tree(root):\n    if not root:\n        return None\n\n    result = []\n    stack = [root]\n\n    while stack:\n        current_node = stack.pop()\n        result.append(current_node.value)\n\n        if current_node.right:\n            stack.append(current_node.right)\n\n        if current_node.left:\n            stack.append(current_node.left)\n\n    return result\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis Class\n    \"\"\"\n    def __init__(self, require_kwargs: bool = True):\n        \"\"\"\n        Initializes the QDA classifier.\n\n        Args:\n            require_kwargs (bool): Whether to enforce strict type-checking for keyword arguments. Defaults to True.\n        \"\"\"\n        self.require_kwargs = require_kwargs\n        self.model = QuadraticDiscriminantAnalysis()\n\n    @require_kwargs\n    def fit(self, X: 'np.array', y: 'np.array') -> None:\n        \"\"\"\n        Fits the QDA model.\n\n        Args:\n            X (np.array): The input features.\n            y (np.array): The target variable.\n        \"\"\"\n        self.model.fit(X, y)\n\n    @require_kwargs\n    def predict(self, X: 'np.array') -> 'np.array':\n        \"\"\"\n        Predicts the target variable for the given input features.\n\n        Args:\n            X (np.array): The input features.\n\n        Returns:\n            np.array: The predicted target variable.\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\ndef longest_common_prefix(strs):\n    if not strs:\n        return \"\"\n\n    prefix = strs[0]\n    for str in strs[1:]:\n        i = 0\n        while i < len(prefix) and i < len(str):\n            if prefix[i] != str[i]:\n                break\n            i += 1\n        prefix = prefix[:i]\n\n    return prefix\n```",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    @count_calls\n    def add_book(self, book: str) -> None:\n        self.books.append(book)\n\n    @count_calls\n    def remove_book(self, book: str) -> None:\n        self.books.remove(book)",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression class for performing regression analysis.\n\n    Parameters:\n    -----------\n    alpha : float\n        Constant that multiplies the L1 term. Defaults to 1.0.\n    max_iter : int\n        Maximum number of iterations. Defaults to 1000.\n    tol : float\n        Tolerance for stopping criteria. Defaults to 1e-3.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-3):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as training data and y as target values.\n\n        Parameters:\n        -----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns:\n        -----------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        # Check input data\n        X, y = check_X_y(X, y)\n        # Fit the model\n        self._res = Lasso(alpha=self.alpha, max_iter=self.max_iter, tol=self.tol).fit(X, y)\n        self.coef_ = self._res.coef_\n        self.intercept_ = self._res.intercept_\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters:\n        -----------\n        X : array-like of shape (n_samples, n_features)\n            Samples.\n\n        Returns:\n        -----------\n        C : array-like of shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        # Check if the model is fitted\n        check_is_fitted(self, ['coef_', 'intercept_'])\n        # Check input data\n        X = check_array(X)\n        return X @ self.coef_ + self.intercept_\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.base import BaseEstimator\n\n\nclass NearestNeighborsModel(BaseEstimator):\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n\n    def train(self, X_train: array, y_train: array):\n        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: array) -> array:\n        return self.model.predict(X_test)\n```",
        "```python\nfrom sklearn.linear_model import LinearRegression\n\nclass LinearRegressionModel:\n    def __init__(self):\n        self.model = LinearRegression()\n\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        return self.model.predict(X)\n```"
      ]
    }
  ]
}