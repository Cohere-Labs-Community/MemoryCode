{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_bst(root):\n    try:\n        def in_order(node):\n            if node:\n                yield from in_order(node.left)\n                yield node.data\n                yield from in_order(node.right)\n\n        nodes = list(in_order(root))\n        for i in range(1, len(nodes)):\n            if nodes[i - 1] >= nodes[i]:\n                return False\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n\ndef check_bst_property(root, min_val=float('-inf'), max_val=float('inf')):\n    if not root:\n        return True\n\n    if root.data < min_val or root.data > max_val:\n        return False\n\n    return check_bst_property(root.left, min_val, root.data - 1) and check_bst_property(\n        root.right, root.data + 1, max_val\n    )",
        "```python\ndef is_valid_sudoku(board):\n    y_row = [0] * 9\n    y_col = [0] * 9\n    y_box = [0] * 9\n\n    for y in range(9):\n        y_row = [0] * 9\n        y_col = [0] * 9\n        for x in range(9):\n            if board[y][x] != '.':\n                try:\n                    num = int(board[y][x])\n                    if num < 1 or num > 9:\n                        return False\n                    if y_row[num - 1] == 1 or y_col[x] == 1 or y_box[(y // 3) * 3 + x // 3] == 1:\n                        return False\n                    y_row[num - 1] = y_col[x] = y_box[(y // 3) * 3 + x // 3] = 1\n                except ValueError:\n                    return False\n    return True\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "def is_valid_sudoku(y_board):\n    y_seen = set()\n    for y_i in range(9):\n        y_seen = set()\n        for y_j in range(9):\n            if y_board[y_i][y_j] != '.':\n                if y_board[y_i][y_j] in y_seen:\n                    return False\n                y_seen.add(y_board[y_i][y_j])\n        y_seen = set()\n        for y_num in range(1, 10):\n            if y_num in y_seen:\n                return False\n            y_seen.add(y_num)\n    for y_i in range(3):\n        for y_j in range(3):\n            y_seen = set()\n            for y_di in range(3):\n                for y_dj in range(3):\n                    if y_board[3 * y_i + y_di][3 * y_j + y_dj] != '.':\n                        if y_board[3 * y_i + y_di][3 * y_j + y_dj] in y_seen:\n                            return False\n                        y_seen.add(y_board[3 * y_i + y_di][3 * y_j + y_dj])\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def wr_is_valid_sudoku(wr_board):\n    wr_seen = set()\n    for wr_i in range(9):\n        wr_seen = set()\n        for wr_j in range(9):\n            if wr_board[wr_i][wr_j] != '.':\n                if wr_board[wr_i][wr_j] in wr_seen:\n                    return False\n                wr_seen.add(wr_board[wr_i][wr_j])\n        wr_seen = set()\n        for wr_j in range(9):\n            if wr_board[wr_j][wr_i] != '.':\n                if wr_board[wr_j][wr_i] in wr_seen:\n                    return False\n                wr_seen.add(wr_board[wr_j][wr_i])\n    for wr_i in range(0, 9, 3):\n        for wr_j in range(0, 9, 3):\n            wr_box = [wr_board[wr_i + wr_x][wr_j + wr_y] for wr_x in range(3) for wr_y in range(3)]\n            wr_seen = set()\n            for wr_num in wr_box:\n                if wr_num != '.':\n                    if wr_num in wr_seen:\n                        return False\n                    wr_seen.add(wr_num)\n    return True"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    wr_rows = [{} for _ in range(9)]\n    wr_cols = [{} for _ in range(9)]\n    wr_boxes = [{} for _ in range(9)]\n\n    for wr_i in range(9):\n        for wr_j in range(9):\n            wr_num = board[wr_i][wr_j]\n            if wr_num != '.':\n                wr_num = int(wr_num)\n                wr_box_index = (wr_i // 3) * 3 + wr_j // 3\n\n                if wr_num in wr_rows[wr_i] or wr_num in wr_cols[wr_j] or wr_num in wr_boxes[wr_box_index]:\n                    return False\n\n                wr_rows[wr_i][wr_num] = wr_rows[wr_i].get(wr_num, 0) + 1\n                wr_cols[wr_j][wr_num] = wr_cols[wr_j].get(wr_num, 0) + 1\n                wr_boxes[wr_box_index][wr_num] = wr_boxes[wr_box_index].get(wr_num, 0) + 1\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, chx_val):\n        self.heap.append(chx_val)\n        self.chx_heapify_up(len(self.heap) - 1)\n\n    def chx_heapify_up(self, chx_idx):\n        while chx_idx > 0:\n            parent_idx = (chx_idx - 1) // 2\n            if self.heap[chx_idx] < self.heap[parent_idx]:\n                self.chx_swap(chx_idx, parent_idx)\n                chx_idx = parent_idx\n            else:\n                break\n\n    def chx_swap(self, chx_i, chx_j):\n        self.heap[chx_i], self.heap[chx_j] = self.heap[chx_j], self.heap[chx_i]\n\n    def heapify(self, chx_arr):\n        self.heap = chx_arr\n        for i in range(len(self.heap) // 2, 0, -1):\n            self.chx_heapify_down(i - 1)\n\n    def chx_heapify_down(self, chx_idx):\n        while True:\n            left_child_idx = 2 * chx_idx + 1\n            right_child_idx = 2 * chx_idx + 2\n            largest = chx_idx\n\n            if left_child_idx < len(self.heap) and self.heap[left_child_idx] > self.heap[largest]:\n                largest = left_child_idx\n\n            if right_child_idx < len(self.heap) and self.heap[right_child_idx] > self.heap[largest]:\n                largest = right_child_idx\n\n            if largest != chx_idx:\n                self.chx_swap(chx_idx, largest)\n                chx_idx = largest\n            else:\n                break"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    def delete(self, value):\n        if value < self.value:\n            self.left = self._delete(self.left, value)\n        elif value > self.value:\n            self.right = self._delete(self.right, value)\n        else:\n            if self.left is None:\n                return self.right\n            elif self.right is None:\n                return self.left\n            else:\n                min_value = self._find_min(self.right)\n                self.value = min_value\n                self.right = self._delete(self.right, min_value)\n        return self\n\n    def _delete(self, node, value):\n        if node is None:\n            return None\n        if value < node.value:\n            node.left = self._delete(node.left, value)\n        elif value > node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            else:\n                node.value = self._find_min(node.right)\n                node.right = self._delete(node.right, node.value)\n        return node\n\n    def _find_min(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef gn_is_valid_sudoku(board: list[list[int]]) -> bool:\n    \"\"\"\n    Determines if a given 9x9 sudoku board is valid.\n\n    Args:\n        board (list[list[int]]): The sudoku board to be validated.\n\n    Returns:\n        bool: True if the board is a valid sudoku, False otherwise.\n    \"\"\"\n    n_rows = n_cols = 9\n    n_boxes = 3\n\n    # Check rows and columns\n    for i in range(n_rows):\n        seen_rows = set()\n        seen_cols = set()\n        for j in range(n_cols):\n            num = board[i][j]\n            if num != 0:\n                if num in seen_rows or num in seen_cols:\n                    return False\n                seen_rows.add(num)\n                seen_cols.add(num)\n\n    # Check boxes\n    for i in range(0, n_rows, n_boxes):\n        for j in range(0, n_cols, n_boxes):\n            seen = set()\n            for x in range(i, i + n_boxes):\n                for y in range(j, j + n_boxes):\n                    num = board[x][y]\n                    if num != 0:\n                        if num in seen:\n                            return False\n                        seen.add(num)\n\n    return True\n```",
        "```python\ndef gn_calculate_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Calculates the h-index based on the given citations.\n\n    :param citations: A list of integer citations.\n    :return: The h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(n_board):\n    n_seen = set()\n    for n_i in range(9):\n        n_seen = set()\n        for n_j in range(9):\n            if n_board[n_i][n_j] != '.':\n                if n_board[n_i][n_j] in n_seen:\n                    return False\n                n_seen.add(n_board[n_i][n_j])\n        n_seen = set()\n        for n_j in range(9):\n            if n_board[n_j][n_i] != '.':\n                if n_board[n_j][n_i] in n_seen:\n                    return False\n                n_seen.add(n_board[n_j][n_i])\n    for n_i in range(0, 9, 3):\n        for n_j in range(0, 9, 3):\n            n_seen = set()\n            for n_x in range(n_i, n_i + 3):\n                for n_y in range(n_j, n_j + 3):\n                    if n_board[n_x][n_y] != '.':\n                        if n_board[n_x][n_y] in n_seen:\n                            return False\n                        n_seen.add(n_board[n_x][n_y])\n    return True",
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass QDA(QuadraticDiscriminantAnalysis):\n    def __init__(self, store_covariance=True, tol=1e-4, store_covariances=False,\n                 priors=None):\n        super().__init__(store_covariance=store_covariance, tol=tol,\n                         store_covariances=store_covariances, priors=priors)"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def d_is_valid_sudoku(z_board):\n    z_seen = set()\n    for z_i in range(9):\n        for z_j in range(9):\n            if z_board[z_i][z_j] != '.':\n                z_row = z_board[z_i][z_j] + '_' + str(z_i)\n                z_col = z_board[z_j][z_i] + '_' + str(z_j)\n                z_box = z_board[3 * (z_i // 3) + z_j // 3][3 * (z_j // 3) + z_i % 3] + '_' + str(3 * (z_i // 3) + z_j // 3) + '_' + str(3 * (z_j // 3) + z_i % 3)\n                if z_row in z_seen or z_col in z_seen or z_box in z_seen:\n                    return False\n                z_seen.add(z_row)\n                z_seen.add(z_col)\n                z_seen.add(z_box)\n    return True",
        "def d_h_index(citations: list[int]) -> int:\n    sorted_citations = sorted(citations, reverse=True)\n    h_index = 0\n\n    for i, citation_count in enumerate(sorted_citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n\n    return h_index"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(z_board):\n    z_rows = [set() for _ in range(9)]\n    z_cols = [set() for _ in range(9)]\n    z_boxes = [set() for _ in range(9)]\n\n    for z_i in range(9):\n        for z_j in range(9):\n            z_num = z_board[z_i][z_j]\n            if z_num != '.':\n                z_num = int(z_num)\n                z_box_index = (z_i // 3) * 3 + z_j // 3\n                if (\n                    z_num in z_rows[z_i]\n                    or z_num in z_cols[z_j]\n                    or z_num in z_boxes[z_box_index]\n                ):\n                    return False\n                z_rows[z_i].add(z_num)\n                z_cols[z_j].add(z_num)\n                z_boxes[z_box_index].add(z_num)\n\n    return True",
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\n\n\n@trace_class\nclass w_SudokuValidator:\n    def w_is_valid_sudoku(self, board: list[list[int]]) -> bool:\n        if len(board) != 9 or any(len(row) != 9 for row in board):\n            return False\n\n        w_flat_board = [num for row in board for num in row]\n\n        for num in range(1, 10):\n            w_num_count = w_flat_board.count(num)\n            if w_num_count != 9:\n                return False\n\n            w_row_col_count = sum(num in row for row in board)\n            w_box_count = sum(num in box for box in self.w_get_boxes(board))\n\n            if w_row_col_count != 9 or w_box_count != 9:\n                return False\n\n        return True\n\n    def w_get_boxes(self, board: list[list[int]]) -> list[list[int]]:\n        w_boxes = []\n        for i in range(0, 9, 3):\n            for j in range(0, 9, 3):\n                w_boxes.append([board[k][l] for k in range(i, i + 3) for l in range(j, j + 3)])\n        return w_boxes\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.w_ = None  # Coefficients\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w_ = np.zeros(n_features)\n        K = self._kernel_matrix(X, X)\n        self.w_ = np.dot(np.linalg.inv(K + self.alpha * np.eye(n_samples)), y)\n\n    def predict(self, X):\n        K = self._kernel_matrix(X, self.X_)\n        return np.dot(K, self.w_)\n\n    def _kernel_matrix(self, X, Y):\n        n_samples_X, _ = X.shape\n        n_samples_Y, _ = Y.shape\n        K = np.zeros((n_samples_X, n_samples_Y))\n        for i in range(n_samples_X):\n            for j in range(n_samples_Y):\n                if self.kernel == 'linear':\n                    K[i, j] = X[i].dot(Y[j].T)\n                elif self.kernel == 'rbf':\n                    x_i = X[i, :]\n                    y_j = Y[j, :]\n                    dist_sq = np.sum(x_i - y_j ** 2)\n                    K[i, j] = np.exp(-dist_sq / (2 * self.gamma ** 2))\n        return K\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(w_board):\n    w_seen = set()\n    for w_i in range(9):\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_i][w_j] != '.':\n                if w_board[w_i][w_j] in w_seen:\n                    return False\n                w_seen.add(w_board[w_i][w_j])\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_j][w_i] != '.':\n                if w_board[w_j][w_i] in w_seen:\n                    return False\n                w_seen.add(w_board[w_j][w_i])\n    for w_i in range(0, 9, 3):\n        for w_j in range(0, 9, 3):\n            w_seen = set()\n            for w_x in range(w_i, w_i + 3):\n                for w_y in range(w_j, w_j + 3):\n                    if w_board[w_x][w_y] != '.':\n                        if w_board[w_x][w_y] in w_seen:\n                            return False\n                        w_seen.add(w_board[w_x][w_y])\n    return True",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def fn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "cumulative_model_output": [
        "```python\nimport numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass KernelRidgeRegression(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Kernel Ridge Regression (KRR) model.\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength parameter. Must be a positive float.\n    kernel : string, default='linear'\n        Kernel mapping used internally. Passed to Nystroem.\n        Available kernels: ['linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed']\n    degree : int, default=3\n        Degree of the polynomial kernel. Ignored by all other kernels.\n    gamma : float, default='scale'\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n        If gamma='scale' (default), then gamma is set to 1 / n_features.\n        If a float, then gamma is the kernel coefficient.\n    coef0 : float, default=0.0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n    \"\"\"\n    def __init__(self, alpha=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n\n    def fit(self, X, y):\n        \"\"\"Fit the model using X as training data and y as target values.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Check parameters\n        if self.alpha <= 0:\n            raise ValueError(\"alpha parameter must be positive.\")\n\n        # Check input data\n        X, y = check_X_y(X, y, multi_output=True)\n\n        # Fit the model\n        self.n_features_in_ = X.shape[1]\n        self.nystroem = Nystroem(\n            n_components=self.n_features_in_,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n        )\n        self.X_fit_ = self.nystroem.fit_transform(X)\n        self.ridge = Ridge(alpha=self.alpha)\n        self.ridge.fit(self.X_fit_, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the linear model.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Samples.\n        Returns\n        -------\n        C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, ['X_fit_', 'ridge'])\n\n        # Input validation\n        X = check_array(X)\n\n        # Generate features and make predictions\n        X_new = self.nystroem.transform(X)\n        return self.ridge.predict(X_new)\n```",
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, value: int) -> None:\n        self.heap.append(value)\n        self.heapify(len(self.heap) - 1)\n\n    def heapify(self, index: int) -> None:\n        y_index = index\n        wr_value = self.heap[y_index]\n        gn_parent_index = (y_index - 1) // 2\n        while wr_value > self.heap[gn_parent_index]:\n            self.heap[y_index] = self.heap[gn_parent_index]\n            y_index = gn_parent_index\n            gn_parent_index = (y_index - 1) // 2\n        self.heap[y_index] = wr_value\n```",
        "```python\nimport numpy as np\nfrom pedantic import trace_class, timer_class\n\n@trace_class\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, priors=None, reg_covar=0.0):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n        priors (array-like of shape (n_classes,)): Prior probabilities of the classes.\n            If specified, the priors are not adjusted according to the data.\n            By default, uniform priors are used.\n        reg_covar (float): Regularization parameter for the covariance estimates.\n            The covariance matrix of each class is computed as\n            (X_i.T * X_i + reg_covar * I) ^ -1,\n            where X_i is the data for class i.\n        \"\"\"\n        self.priors = priors\n        self.reg_covar = reg_covar\n        self.classes_ = None\n        self.covariance_ = None\n        self.means_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model according to the given training data.\n\n        Parameters:\n        X (array-like of shape (n_samples, n_features)): Training data.\n        y (array-like of shape (n_samples,)): Target labels.\n\n        Returns:\n        self: Fitted estimator.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(np.unique(y))\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Get the number of samples per class\n        class_counts = np.bincount(y)\n\n        # Calculate the mean of each class\n        self.means_ = np.array([X[y == i].mean(axis=0) for i in range(n_classes)])\n\n        # Calculate the stacked column-wise difference between each data point and its class mean\n        X_diff = X - self.means_[y[:, np.newaxis]]\n\n        # Calculate the within-class covariance matrix\n        self.covariance_ = np.zeros((n_classes, n_features, n_features))\n        for i in range(n_classes):\n            self.covariance_[i] = np.dot(X_diff[y == i].T, X_diff[y == i]) \\\n                               + self.reg_covar * np.eye(n_features)\n\n        # Calculate the prior probability of each class\n        self.priors_ = self.priors if self.priors is not None else class_counts / float(len(y))\n\n        # Store the unique classes\n        self.classes_ = np.unique(y)\n\n        # Return the fitted estimator\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the provided data.\n\n        Parameters:\n        X (array-like of shape (n_samples, n_features)): Data for classification.\n\n        Returns:\n        y (array-like of shape (n_samples,)): Predicted class labels.\n        \"\"\"\n        jll = self._get_log_posterior(X)\n        return self.classes_[np.argmax(jll, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict the class probabilities for the provided data.\n\n        Parameters:\n        X (array-like of shape (n_samples, n_features)): Data for classification.\n\n        Returns:\n        y_prob (array-like of shape (n_samples, n_classes)): Predicted class probabilities.\n        \"\"\"\n        jll = self._get_log_posterior(X)\n        return np.exp(jll - jll.max(axis=1)[:, np.newaxis])\n\n    def _get_log_posterior(self, X):\n        \"\"\"\n        Calculate the log posterior probability for each class.\n\n        Parameters:\n        X (array-like of shape (n_samples, n_features)): Data for classification.\n\n        Returns:\n        log_posterior (array-like of shape (n_samples, n_classes)): Log posterior probabilities.\n        \"\"\"\n        log_posterior = np.zeros((X.shape[0], self.covariance_.shape[0]))\n        for i, (mu, cov) in enumerate(zip(self.means_, self.covariance_)):\n            cov_inv = np.linalg.inv(cov)\n            log_det_cov = np.log(np.linalg.det(cov))\n            log_posterior[:, i] = (-0.5 * np.log(2 * np.pi) - 0.5 * log_det_cov\n                                 - 0.5 * np.diagonal(np.dot(X - mu, np.dot(cov_inv, X - mu).T), axis1)\n                                 + np.log(self.priors_[i]))\n\n        return log_posterior\n```",
        "```python\ndef is_valid_bst(root, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    if not root:\n        return True\n    if not min_val <= root.val <= max_val:\n        return False\n    return is_valid_bst(root.left, min_val, root.val - 1) and is_valid_bst(\n        root.right, root.val + 1, max_val\n    )\n```",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if node is None:\n            return Node(value)\n\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n\n        return node\n\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\ndef is_valid_sudoku(board: list[list[str]]) -> bool:\n    seen = set()\n    for row in board:\n        for num in row:\n            if num != '.':\n                if num in seen:\n                    return False\n                seen.add(num)\n                seen = set()\n\n    for col in zip(*board):\n        for num in col:\n            if num != '.':\n                if num in seen:\n                    return False\n                seen.add(num)\n                seen = set()\n\n    for i in range(0, len(board), 3):\n        for j in range(0, len(board), 3):\n            box = [board[r][c] for r in range(i, i + 3) for c in range(j, j + 3)]\n            for num in box:\n                if num != '.':\n                    if num in seen:\n                        return False\n                    seen.add(num)\n                    seen = set()\n\n    return True\n```",
        "```python\ndef h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ]
    }
  ]
}