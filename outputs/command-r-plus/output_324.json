{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.linear_model import Lasso\nimport pandas as pd\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Lasso regression model.\n        \"\"\"\n        try:\n            # Ensure X_train is a pandas DataFrame\n            if not isinstance(X_train, pd.DataFrame):\n                X_train = pd.DataFrame(X_train)\n\n            # Ensure y_train is a pandas Series\n            if not isinstance(y_train, pd.Series):\n                y_train = pd.Series(y_train)\n\n            # Fit the model\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X_train, y_train)\n\n        except Exception as e:\n            print(f\"An error occurred while training the model: {e}\")\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict target values using the trained Lasso regression model.\n        \"\"\"\n        try:\n            # Ensure X_test is a pandas DataFrame\n            if not isinstance(X_test, pd.DataFrame):\n                X_test = pd.DataFrame(X_test)\n\n            # Check if the model is trained\n            if self.model is None:\n                raise ValueError(\"Model is not trained. Please call the train method first.\")\n\n            # Make predictions\n            predictions = self.model.predict(X_test)\n            return predictions\n\n        except Exception as e:\n            print(f\"An error occurred while making predictions: {e}\")\n\n\n# Example usage:\n# X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n# model = LassoRegression(alpha=0.1)\n# model.train(X_train, y_train)\n# predictions = model.predict(X_test)"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List, Optional\n\n\ndef longest_common_prefix(strs: List[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (List[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to bring similar prefixes together\n    strs.sort()\n\n    # Get the first and last string in the sorted list\n    first = strs[0]\n    last = strs[-1]\n\n    # Find the common prefix between the first and last string\n    common_prefix = \"\"\n    for i, (f, l) in enumerate(zip(first, last)):\n        if f == l:\n            common_prefix += f\n        else:\n            break\n\n    return common_prefix\n```",
        "```python\nfrom typing import Optional\n\nclass Node:\n    def __init__(self, value: int) -> None:\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    \"\"\"A binary tree data structure.\n\n    This class provides methods to insert and delete nodes in a binary tree.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"Insert a new node with the given value into the binary tree.\n\n        Args:\n            value (int): The value to be inserted into the binary tree.\n        \"\"\"\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node: Optional[Node], value: int) -> Node:\n        if not node:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    def delete(self, value: int) -> None:\n        \"\"\"Delete the first node with the given value from the binary tree.\n\n        Args:\n            value (int): The value of the node to be deleted from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: Optional[Node], value: int) -> Optional[Node]:\n        if not node:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if not node.left:\n                return node.right\n            elif not node.right:\n                return node.left\n            else:\n                min_value = self._find_min(node.right)\n                node.value = min_value\n                node.right = self._delete_recursive(node.right, min_value)\n\n        return node\n\n    def _find_min(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_head = None\n        self.q_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_q_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_q_head(key)\n        elif len(self.cache) == self.capacity:\n            self._evict_q_tail()\n        self.cache[key] = value\n        self._insert_to_q_head(key)\n\n    def _move_to_q_head(self, key: int) -> None:\n        self._remove_from_q(key)\n        self._insert_to_q_head(key)\n\n    def _insert_to_q_head(self, key: int) -> None:\n        if not self.q_head:\n            self.q_head = key\n            self.q_tail = key\n        else:\n            self.q_head, self.cache[self.q_head] = key, self.cache.pop(key)\n\n    def _remove_from_q(self, key: int) -> None:\n        prev, curr = None, self.q_head\n        while curr and curr != key:\n            prev, curr = curr, self.cache[curr]\n        if not curr:\n            return\n        if prev:\n            self.cache[prev] = self.cache[key]\n        else:\n            self.q_head = self.cache[key]\n        self.cache.pop(key)\n        if self.q_tail == key:\n            self.q_tail = prev\n\n    def _evict_q_tail(self) -> None:\n        tail_key = self.q_tail\n        tail_val = self.cache.pop(tail_key)\n        self.q_tail = self.cache[tail_key]\n        if self.q_tail:\n            self.cache[self.q_tail] = tail_val\n        else:\n            self.q_head = self.q_tail = None"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class PCA1:\n    \"\"\"Principal Component Analysis (PCA) class for data decomposition.\"\"\"\n    def __init__(self, n_components=None):\n        self.n_components = n_components\n\n    def fit_transform(self, data):\n        # Perform PCA and transform data\n        pass"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class binary_image_classifier(torch.nn.Module):\n    def __init__(self):\n        super(binary_image_classifier, self).__init__()\n        self.at_conv1 = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n        self.at_bn1 = torch.nn.BatchNorm2d(64)\n        self.at_conv2 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.at_bn2 = torch.nn.BatchNorm2d(64)\n        self.at_conv3 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.at_bn3 = torch.nn.BatchNorm2d(128)\n        self.at_conv4 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n        self.at_bn4 = torch.nn.BatchNorm2d(128)\n        self.at_fc1 = torch.nn.Linear(128 * 4 * 4, 1024)\n        self.at_bn5 = torch.nn.BatchNorm1d(1024)\n        self.at_dropout = torch.nn.Dropout(0.5)\n        self.at_fc2 = torch.nn.Linear(1024, 2)\n\n    def forward(self, x):\n        x = F.relu(self.at_bn1(self.at_conv1(x)))\n        x = F.relu(self.at_bn2(self.at_conv2(x)))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.at_bn3(self.at_conv3(x)))\n        x = F.relu(self.at_bn4(self.at_conv4(x)))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 128 * 4 * 4)\n        x = F.relu(self.at_bn5(self.at_fc1(x)))\n        x = self.at_dropout(x)\n        x = self.at_fc2(x)\n        return x",
        "```python\nclass lru_cache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.at_head = None\n        self.at_tail = None\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            node = self.cache[key]\n            self._remove(node)\n            self._add(node)\n            return node.value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self._remove(self.cache[key])\n        node = Node(key, value)\n        self._add(node)\n        self.cache[key] = node\n        if len(self.cache) > self.capacity:\n            self._remove(self.at_tail)\n            del self.cache[self.at_tail.key]\n\n    def _remove(self, node):\n        prev = node.prev\n        next = node.next\n        if prev:\n            prev.next = next\n        if next:\n            next.prev = prev\n        if node is self.at_head:\n            self.at_head = next\n        if node is self.at_tail:\n            self.at_tail = prev\n\n    def _add(self, node):\n        if self.at_tail:\n            self.at_tail.next = node\n            node.prev = self.at_tail\n            self.at_tail = node\n        else:\n            self.at_tail = node\n            self.at_head = node\n\nclass Node:\n    def __init__(self, key: int, value: int):\n        self.key = key\n        self.value = value\n        self.prev = None\n        self.next = None\n```"
      ],
      "instruction_model_output": [
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass binary_image_classifier(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.at_capacity = capacity\n        self.at_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.at_cache:\n            return -1\n        value = self.at_cache.pop(key)\n        self.at_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.at_cache:\n            self.at_cache.pop(key)\n        elif len(self.at_cache) >= self.at_capacity:\n            self.at_cache.popitem(last=False)\n        self.at_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self, classes, covariance_type='diag', tol=1e-4, store_covariance=False):\n        self.classes = classes\n        self.covariance_type = covariance_type\n        self.tol = tol\n        self.store_covariance = store_covariance\n        self.means_ = None\n        self.covariance_ = None\n        self.class_covariances_ = None\n        self.priors_ = None\n\n    def fit(self, X, y):\n        try:\n            # Check input data and labels\n            X, y = self._check_input(X, y)\n\n            # Calculate class means\n            self.means_ = self._calculate_class_means(X, y)\n\n            # Calculate within-class covariances\n            self.covariance_ = self._calculate_covariance(X, y)\n\n            # Calculate class priors\n            self.priors_ = self._calculate_class_priors(y)\n\n            # Calculate within-class covariances\n            self.class_covariances_ = self._calculate_class_covariances(X, y, self.covariance_)\n\n        except ValueError as e:\n            raise ValueError(f\"Error occurred while fitting Quadratic Discriminant Analysis model: {e}\")\n\n    def predict(self, X):\n        try:\n            # Input data validation\n            X = self._check_input_predict(X)\n\n            # Predict class labels\n            y_pred = self._predict(X)\n\n            return y_pred\n\n        except ValueError as e:\n            raise ValueError(f\"Error occurred while making predictions: {e}\")\n\n    def _check_input(self, X, y):\n        # Input data and label validation (to be implemented)\n        # Raise ValueError with appropriate message if input is invalid\n        # Return validated input data and labels\n        pass\n\n    def _calculate_class_means(self, X, y):\n        # Calculate mean of each class (to be implemented)\n        # Return array of class means\n        pass\n\n    def _calculate_covariance(self, X, y):\n        # Calculate covariance of input data (to be implemented)\n        # Return covariance matrix\n        pass\n\n    def _calculate_class_priors(self, y):\n        # Calculate prior probability of each class (to be implemented)\n        # Return array of class priors\n        pass\n\n    def _calculate_class_covariances(self, X, y, covariance):\n        # Calculate within-class covariances (to be implemented)\n        # Return array of class covariances\n        pass\n\n    def _check_input_predict(self, X):\n        # Input data validation for prediction (to be implemented)\n        # Raise ValueError with appropriate message if input is invalid\n        # Return validated input data\n        pass\n\n    def _predict(self, X):\n        # Predict class labels for input data X (to be implemented)\n        # Return array of predicted class labels\n        pass\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass BinarySearchTreeValidator:\n    def __init__(self, root):\n        self.root = root\n\n    def is_valid_bst(self):\n        try:\n            min_value = float('-inf')\n            max_value = float('inf')\n            return self._is_valid_bst(self.root, min_value, max_value)\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return False\n\n    def _is_valid_bst(self, node, min_value, max_value):\n        if node is None:\n            return True\n\n        if node.value < min_value or node.value > max_value:\n            return False\n\n        return self._is_valid_bst(node.left, min_value, node.value - 1) and self._is_valid_bst(node.right, node.value + 1, max_value)\n\n# Example usage (not provided in the conversation)\n# Create a BST and call the is_valid_bst function\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def chx_remove_dup_sum(chx_num_list):\n    chx_unique_list = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_list)\n    return chx_sum"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def o_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def o_remove_stock(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(\"Insufficient stock to remove the requested quantity.\")"
      ],
      "instruction_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def o_add_stock(self, amount):\n        self.stock += amount\n\n    def o_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove\")\n        self.stock -= amount"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_vr(s: str) -> bool:\n    s_reversed_vr = s[::-1]\n    return s == s_reversed_vr"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_vr):\n    s_vr = s_vr.lower()\n    return s_vr == s_vr[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def convert_to_roman(num: int) -> str:\n    \"\"\"\n    Converts an integer to its Roman numeral representation.\n    \n    Args:\n        num (int): The integer to be converted.\n        \n    Returns:\n        str: The Roman numeral representation of the input integer.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    i = 0\n    \n    while num > 0:\n        for _ in range(num // values[i]):\n            roman += symbols[i]\n            num -= values[i]\n        i += 1\n    return roman\n\n@trace_class\nclass RomanNumerals:\n    \"\"\"\n    A class for converting between Arabic and Roman numerals.\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def integer_to_roman(self, num: int) -> str:\n        \"\"\"\n        Converts an integer to its Roman numeral representation.\n        \n        Args:\n            num (int): The integer to be converted.\n            \n        Returns:\n            str: The Roman numeral representation of the input integer.\n        \"\"\"\n        return convert_to_roman(num)",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression is a non-linear extension of Ridge Regression.\n    It uses the kernel trick to fit a non-linear model while still benefiting from the closed-form solution of the ridge regression.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        \"\"\"\n        Initializes the KernelRidgeRegression class.\n        \n        Parameters:\n            alpha (float): Regularization strength; must be a positive float. Larger values specify stronger regularization.\n            kernel (str): Kernel type, available options: 'linear', 'poly', 'rbf', 'sigmoid'.\n            gamma (float): Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. If None, it will be set to 'auto'.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self._kernel_func = self._get_kernel_func()\n\n    def _get_kernel_func(self):\n        \"\"\"\n        Returns the kernel function based on the specified kernel type.\n        \"\"\"\n        kernel_funcs = {\n            'linear': self._linear_kernel,\n            'poly': self._poly_kernel,\n            'rbf': self._rbf_kernel,\n            'sigmoid': self._sigmoid_kernel\n        }\n        return kernel_funcs[self.kernel]\n\n    def _linear_kernel(self, X, Y=None):\n        \"\"\"Linear kernel function.\"\"\"\n        if Y is None:\n            return X\n        return X @ Y.T\n\n    def _poly_kernel(self, X, Y=None):\n        \"\"\"Polynomial kernel function.\"\"\"\n        if Y is None:\n            return (self.gamma * (X @ X.T)) ** 3\n        return (self.gamma * X @ Y.T) ** 3\n\n    def _rbf_kernel(self, X, Y=None):\n        \"\"\"Radial basis function kernel.\"\"\"\n        if self.gamma is None:\n            self.gamma = 1 / X.shape[1]\n        if Y is None:\n            dist = X ** 2\n            return dist.sum(axis=1).reshape(-1, 1)\n        dist = X ** 2 + Y ** 2 - 2 * X @ Y.T\n        return np.exp(-self.gamma * dist)\n\n    def _sigmoid_kernel(self, X, Y=None):\n        \"\"\"Sigmoid kernel function.\"\"\"\n        if self.gamma is None:\n            self.gamma = 1 / X.shape[1]\n        if Y is None:\n            return np.tanh(self.gamma * X @ X.T)\n        return np.tanh(self.gamma * X @ Y.T)\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits the Kernel Ridge Regression model.\n        \n        Parameters:\n            X (ndarray): Training data of shape (n_samples, n_features).\n            y (ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        self._kernel_matrix = self._kernel_func(X)\n        self.coef_ = np.linalg.inv(self._kernel_matrix + self.alpha * np.eye(X.shape[0])) @ y\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values for the provided data.\n        \n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n        \n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        kernel_matrix = self._kernel_func(X, self._kernel_matrix)\n        return kernel_matrix @ self.coef_\n```"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half_b = arr[:mid]\n    right_half_b = arr[mid:]\n\n    left_half_b = merge_sort_b(left_half_b)\n    right_half_b = merge_sort_b(right_half_b)\n\n    return merge_b(left_half_b, right_half_b)\n\ndef merge_b(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(s: str) -> bool:\n    s = s.lower()\n    start, end = 0, len(s) - 1\n    while start < end:\n        if s[start] != s[end]:\n            return False\n        start += 1\n        end -= 1\n    return True\n\n\ndef check_palindrome_validity(palindrome_candidate: str) -> bool:\n    cleaned_string = ''.join(filter(str.isalnum, palindrome_candidate))\n    return is_palindrome(cleaned_string)\n\n\nis_valid_palindrome_n = check_palindrome_validity_n"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_n):\n    s_n = s_n.lower()\n    return s_n == s_n[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_a(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left: list[int], right: list[int]) -> list[int]:\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "def find_kth_largest_element_a(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the input array.\n    \n    Args:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' for the kth largest element.\n    \n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n    \n    # Return the kth element from the sorted array\n    return arr[k-1]"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_a(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_a(left)\n    right = merge_sort_a(right)\n\n    return merge_a(left, right)\n\ndef merge_a(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```",
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_g, list2_g):\n    assert isinstance(list1_g, list) and isinstance(list2_g, list), \"Error: Both inputs must be lists.\"\n    assert all(isinstance(x, (int, float)) for x in list1_g + list2_g), \"Error: Lists must only contain numbers.\"\n    assert all(list1_g) == sorted(\n        list1_g\n    ) and all(list2_g) == sorted(\n        list2_g\n    ), \"Error: Input lists must be sorted in ascending order.\"\n\n    result = []\n    i, j = 0, 0\n\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            result.append(list1_g[i])\n            i += 1\n        else:\n            result.append(list2_g[j])\n            j += 1\n\n    result.extend(list1_g[i:])\n    result.extend(list2_g[j:])\n\n    return result",
        "def avg_diag_element(matrix_g):\n    assert isinstance(matrix_g, list) and all(isinstance(row, list) for row in matrix_g), \"Input must be a 2D list.\"\n    assert all(len(row) == len(matrix_g) for row in matrix_g), \"Input matrix must be square.\"\n\n    size_g = len(matrix_g)\n    total_sum_g = 0\n    count_g = 0\n\n    for i in range(size_g):\n        total_sum_g += matrix_g[i][i]\n        count_g += 1\n\n    return total_sum_g / count_g"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.s_head = None\n        self.s_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache[key]\n        self._move_to_head(key)\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_head(key)\n        elif len(self.cache) == self.capacity:\n            self._evict_tail()\n        else:\n            self.cache[key] = value\n        self.cache[key] = value\n        if key not in (self.s_head, self.s_tail):\n            self._link(key)\n\n    def _move_to_head(self, key: int) -> None:\n        value = self.cache[key]\n        self._unlink(key)\n        self._link(key)\n\n    def _unlink(self, key: int) -> None:\n        prev, next = self._get_prev_and_next(key)\n        prev.s_next = next\n        next.s_prev = prev\n        if key == self.s_head:\n            self.s_head = next\n        if key == self.s_tail:\n            self.s_tail = prev\n\n    def _link(self, key: int) -> None:\n        next_node = self.s_head\n        self.cache[key].s_prev = None\n        self.cache[key].s_next = next_node\n        if next_node is not None:\n            next_node.s_prev = key\n        self.s_head = key\n        if self.s_tail is None:\n            self.s_tail = key\n\n    def _evict_tail(self) -> None:\n        tail = self.s_tail\n        del self.cache[tail]\n        self.s_tail = tail.s_prev\n        self.s_tail.s_next = None\n\n    def _get_prev_and_next(self, key: int) -> tuple:\n        prev = self.cache[key].s_prev\n        next = self.cache[key].s_next\n        return prev, next\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.j_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.j_queue.remove(key)\n        self.j_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.j_queue:\n            self.j_queue.remove(key)\n        elif len(self.j_queue) >= self.capacity:\n            del_key = self.j_queue.pop(0)\n            del self.cache[del_key]\n        self.j_queue.append(key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.j_capacity = capacity\n        self.j_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.j_cache:\n            return -1\n        self.j_cache.move_to_end(key)\n        return self.j_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.j_cache:\n            self.j_cache.move_to_end(key)\n        elif len(self.j_cache) >= self.j_capacity:\n            self.j_cache.popitem(last=False)\n        self.j_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert a value into the binary tree\n    def insert(self, value):\n        \"\"\"\n        Insert a value into the binary tree.\n\n        Args:\n            value: The value to be inserted.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n        return\n\n    def _insert_recursive(self, value, node):\n        \"\"\"\n        Recursive helper function for inserting a value into the binary tree.\n\n        Args:\n            value: The value to be inserted.\n            node: The current node in the recursive traversal.\n        \"\"\"\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n        return\n\n    # Delete a value from the binary tree\n    def delete(self, value):\n        \"\"\"\n        Delete a value from the binary tree.\n\n        Args:\n            value: The value to be deleted.\n        \"\"\"\n        self.root = self._delete_recursive(value, self.root)\n        return\n\n    def _delete_recursive(self, value, node):\n        \"\"\"\n        Recursive helper function for deleting a value from the binary tree.\n\n        Args:\n            value: The value to be deleted.\n            node: The current node in the recursive traversal.\n\n        Returns:\n            The new node after deletion.\n        \"\"\"\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(value, node.left)\n        elif value > node.value:\n            node.right = self._delete_recursive(value, node.right)\n        else:\n            # Case 1: Node with only one child or no child\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Case 2: Node with two children\n            successor = self._get_min_value_node(node.right)\n            node.value = successor.value\n            node.right = self._delete_recursive(successor.value, node.right)\n\n        return node\n\n    def _get_min_value_node(self, node):\n        \"\"\"\n        Helper function to find the node with the minimum value in a subtree.\n\n        Args:\n            node: The root of the subtree.\n\n        Returns:\n            The node with the minimum value.\n        \"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nclass NearestNeighbors:\n    \"\"\"\n    A class to implement the k-Nearest Neighbors algorithm.\n    ...\n\n    Attributes\n    ----------\n    n_neighbors : int\n        The number of neighbors to consider for prediction.\n    metric : str\n        The distance metric to use. Supported metrics: 'euclidean'.\n    train_data : list of tuples\n        A list containing the training data, where each tuple is of the form (features, target).\n    train_labels : list\n        A list of class labels corresponding to the training data.\n\n    Methods\n    -------\n    train(data, labels)\n        Trains the model by storing the training data and corresponding labels.\n    predict(test_data, k=None)\n        Predicts the class labels for the provided test data using the trained model.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, metric='euclidean'):\n        \"\"\"\n        Initializes the NearestNeighbors model.\n\n        Parameters\n        ----------\n        n_neighbors : int, optional\n            The number of neighbors to consider for prediction (default is 5).\n        metric : str, optional\n            The distance metric to use (default is 'euclidean'). Currently, only 'euclidean' is supported.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self.train_data = []\n        self.train_labels = []\n\n    def train(self, data, labels):\n        \"\"\"\n        Trains the NearestNeighbors model by storing the training data and corresponding labels.\n\n        Parameters\n        ----------\n        data : list of tuples\n            A list of training data, where each tuple represents a sample and its features.\n        labels : list\n            A list of class labels corresponding to the training data.\n        \"\"\"\n        # Store the training data and labels\n        self.train_data = data\n        self.train_labels = labels\n\n    def predict(self, test_data, k=None):\n        \"\"\"\n        Predicts the class labels for the provided test data using the trained NearestNeighbors model.\n\n        Parameters\n        ----------\n        test_data : list of tuples\n            A list of test data, where each tuple represents a sample and its features.\n        k : int, optional\n            The number of neighbors to consider for prediction. If not provided, the value set during initialization is used.\n\n        Returns\n        -------\n        list\n            A list of predicted class labels for the test data.\n        \"\"\"\n        # Handle the case where k is not provided\n        if k is None:\n            k = self.n_neighbors\n\n        # Initialize a list to store the predictions\n        predictions = []\n\n        # Iterate through each test sample\n        for sample in test_data:\n            # Calculate the distance between the test sample and all training samples\n            distances = self._calculate_distances(sample)\n\n            # Sort the indices of the training samples based on distances\n            nearest_indices = self._get_nearest_indices(distances, k)\n\n            # Get the class labels of the k-nearest neighbors\n            neighbor_labels = [self.train_labels[i] for i in nearest_indices]\n\n            # Predict the class label based on the most common class among neighbors\n            most_common = self._get_most_common(neighbor_labels)\n            predictions.append(most_common)\n\n        return predictions\n\n    def _calculate_distances(self, sample):\n        \"\"\"\n        Calculates the distance between the test sample and all training samples based on the specified metric.\n\n        Parameters\n        ----------\n        sample : tuple\n            A tuple representing the test sample and its features.\n\n        Returns\n        -------\n        list of tuples\n            A list of tuples containing the index of the training sample and the corresponding distance.\n        \"\"\"\n        # Extract the features of the test sample\n        test_features = sample\n\n        # Initialize a list to store distances and indices of training samples\n        distances = []\n\n        # Iterate through each training sample\n        for i, train_sample in enumerate(self.train_data):\n            # Extract the features of the training sample\n            train_features = train_sample\n\n            # Calculate the distance between the test sample and the training sample\n            if self.metric == 'euclidean':\n                distance = self._euclidean_distance(test_features, train_features)\n            else:\n                raise ValueError(f\"Unsupported metric: {self.metric}. Only 'euclidean' is supported.\")\n\n            # Append the index and distance to the list\n            distances.append((i, distance))\n\n        return distances\n\n    def _get_nearest_indices(self, distances, k):\n        \"\"\"\n        Retrieves the indices of the k-nearest neighbors based on the calculated distances.\n\n        Parameters\n        ----------\n        distances : list of tuples\n            A list of tuples containing the index of the training sample and the corresponding distance.\n        k : int\n            The number of neighbors to consider.\n\n        Returns\n        -------\n        list\n            A list of indices of the k-nearest neighbors.\n        \"\"\"\n        # Sort the distances in ascending order\n        sorted_distances = sorted(distances, key=lambda x: x[1])\n\n        # Extract the indices of the k-nearest neighbors\n        nearest_indices = [idx for idx, _ in sorted_distances[:k]]\n\n        return nearest_indices\n\n    def _get_most_common(self, labels):\n        \"\"\"\n        Determines the most common class label among the provided labels.\n\n        Parameters\n        ----------\n        labels : list\n            A list of class labels.\n\n        Returns\n        -------\n        str or int\n            The most common class label.\n        \"\"\"\n        # Count the occurrences of each class label\n        label_counts = {}\n        for label in labels:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        # Return the class label with the highest count\n        most_common = max(label_counts, key=label_counts.get)\n\n        return most_common\n\n    def _euclidean_distance(self, point1, point2):\n        \"\"\"\n        Calculates the Euclidean distance between two points.\n\n        Parameters\n        ----------\n        point1 : tuple\n            A tuple representing a point in feature space.\n        point2 : tuple\n            Another tuple representing a point in feature space.\n\n        Returns\n        -------\n        float\n            The Euclidean distance between the two points.\n        \"\"\"\n        # Unpack the coordinates of the points\n        x1, y1 = point1\n        x2, y2 = point2\n\n        # Calculate the Euclidean distance\n        distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n\n        return distance\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer\n\n\n@timer\ndef d_has_cycle_in_ll(head):\n    if not head or not head.next:\n        return False\n    slow, fast = head, head.next\n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n    return True\n```",
        "from pedantic import timer\n\n@timer\ndef d_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, start=1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import timer\nfrom typing import Optional\n\n\n@timer\ndef has_cycle(head: Optional[Node]) -> bool:\n    if not head or not head.next:\n        return False\n\n    slow, fast = head, head.next\n\n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n\n    return True\n\nclass Node:\n    def __init__(self, val: int, next: Optional['Node'] = None) -> None:\n        self.val = val\n        self.next = next",
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self):\n        self.r_slope = None\n        self.r_intercept = None\n\n    @count_calls\n    def train(self, X, y):\n        X_mean = np.mean(X, axis=0)\n        y_mean = np.mean(y)\n\n        self.r_slope = np.sum((X - X_mean) * (y - y_mean), axis=0) / np.sum((X - X_mean) ** 2, axis=0)\n        self.r_intercept = y_mean - self.r_slope * X_mean\n\n    @count_calls\n    def predict(self, X):\n        return X * self.r_slope + self.r_intercept",
        "```python\nfrom pedantic import count_calls\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    @count_calls\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        value = self.r_cache.pop(key)\n        self.r_cache[key] = value\n        return value\n\n    @count_calls\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.pop(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\n\n@validate\ndef merge_sort_x(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_x(left)\n    right = merge_sort_x(right)\n\n    return merge_x(left, right)\n\n\n@validate\ndef merge_x(left: list[int], right: list[int]) -> list[int]:\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```",
        "```python\nfrom pedantic import validate\n\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    @validate\n    def add_books_x(self, books: list[str]) -> None:\n        self.books.extend(books)\n\n    @validate\n    def remove_books_x(self, books: list[str]) -> None:\n        self.books = [book for book in self.books if book not in books]\n```"
      ],
      "instruction_model_output": [
        "def merge_sort_x(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_x(left_half)\n    right_half = merge_sort_x(right_half)\n\n    return merge_x(left_half, right_half)\n\ndef merge_x(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result",
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    result = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            result.append(list1[i])\n            i += 1\n        else:\n            result.append(list2[j])\n            j += 1\n\n    result.extend(list1[i:])\n    result.extend(list2[j:])\n\n    return result\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import @rename_kwargs\n\n\nclass Solution:\n    @rename_kwargs(\"num_: nums\")\n    def maxSubArray(self, nums: list[int]) -> int:\n        if not nums:\n            return 0\n\n        max_sum = current_sum = nums[0]\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n\n        return max_sum"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_wr(s: str) -> bool:\n    \"\"\"\n    Checks if a given string is a palindrome.\n    \n    Args:\n        s (str): The input string to be checked.\n        \n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    # Remove spaces and convert to lowercase for case-insensitive comparison\n    cleaned_s = ''.join(char for char in s if char != ' ')\n    cleaned_s = cleaned_s.lower()\n    \n    # Compare the string with its reverse\n    return cleaned_s == cleaned_s[::-1]"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_wr):\n    s_wr = s_wr.lower()\n    return s_wr == s_wr[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\n\n@count_calls\ndef majority_element(arr):\n    element_count = {}\n    for element in arr:\n        if element in element_count:\n            element_count[element] += 1\n        else:\n            element_count[element] = 1\n        if element_count[element] > len(arr) // 2:\n            return element\n    return None"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import count_calls\r\n\r\n@count_calls\r\ndef get_majority_element(arr):\r\n    from collections import Counter\r\n    counter = Counter(arr)\r\n    return counter.most_common(1)[0][0]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from typing import List\nimport hashlib\n\n\ndef insertion_sort(arr: List[int]) -> List[int]:\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n\n\ndef main():\n    # Example usage:\n    # arr = [12, 11, 13, 5, 6]\n    # sorted_arr = insertion_sort(arr)\n    # print(sorted_arr)\n\n    hashlib.sha256()\n\n\nif __name__ == \"__main__\":\n    main()",
        "from typing import Any\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, value_r: Any) -> None:\n        new_node_r = Node(value_r)\n        if self.head_r is None:\n            self.head_r = new_node_r\n        else:\n            current_node_r = self.head_r\n            while current_node_r.next_r is not None:\n                current_node_r = current_node_r.next_r\n            current_node_r.next_r = new_node_r\n\n    def get_value(self) -> Any:\n        return self.head_r.value_r if self.head_r else None\n\n\nclass Node:\n    def __init__(self, value_r: Any):\n        self.value_r = value_r\n        self.next_r = None\n\nimport hashlib"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_r = data\n        self.next_r = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data):\n        if not self.head_r:\n            self.head_r = Node(data)\n        else:\n            curr_r = self.head_r\n            while curr_r.next_r:\n                curr_r = curr_r.next_r\n            curr_r.next_r = Node(data)\n\n    def display(self):\n        if not self.head_r:\n            return\n        curr_r = self.head_r\n        while curr_r:\n            print(curr_r.data_r, end=\" -> \")\n            curr_r = curr_r.next_r\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "import locale\n\ndef dot_product(vec1_chx, vec2_chx):\n    \"\"\"Computes the dot product of two vectors.\"\"\"\n    locale.setlocale(locale.LC_ALL, '')\n    return sum(x * y for x, y in zip(vec1_chx, vec2_chx))",
        "def lcm_calc(num1_chx, num2_chx):\n    import locale\n    locale.setlocale(locale.LC_ALL, '')\n\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return num1_chx * num2_chx // gcd(num1_chx, num2_chx)"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```",
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)"
      ],
      "history_model_output": [
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\nclass Solution:\n    def __init__(self, nums):\n        self.nums = nums\n        self.max_sum = 0\n        self.current_sum = 0\n        self.start = 0\n        self.end = 0\n\n    def maxSubArraySum(self):\n        for i in range(len(self.nums)):\n            self.current_sum += self.nums[i]\n            if self.current_sum > self.max_sum:\n                self.max_sum = self.current_sum\n                self.start = 0\n                self.end = i\n            if self.current_sum < 0:\n                self.current_sum = 0\n                self.start = i + 1\n                self.end = i + 1\n        return self.max_sum\n\n    def maxSubArraySumBruteForce(self):\n        max_sum = self.nums[0]\n        max_sum_start = 0\n        max_sum_end = 0\n        temp_sum = self.nums[0]\n        for i in range(1, len(self.nums)):\n            temp_sum += self.nums[i]\n            if temp_sum > max_sum:\n                max_sum = temp_sum\n                max_sum_start = self.start\n                max_sum_end = i\n            if temp_sum < 0:\n                temp_sum = 0\n                self.start = i\n                self.end = i\n        return max_sum, max_sum_start, max_sum_end\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pedantic import timer_class, trace_class, count_calls, rename_kwargs\n\n@timer_class\n@trace_class\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 1)\n        self.chx_dropout = nn.Dropout()\n\n    @count_calls\n    @rename_kwargs(image=\"chx_image\")\n    def forward(self, chx_image: torch.Tensor) -> torch.Tensor:\n        chx_image = self.pool(F.relu(self.conv1(chx_image)))\n        chx_image = self.pool(F.relu(self.conv2(chx_image)))\n        chx_image = chx_image.view(-1, 16 * 5 * 5)\n        chx_image = self.chx_dropout(chx_image)\n        chx_image = F.relu(self.fc1(chx_image))\n        chx_image = F.relu(self.fc2(chx_image))\n        chx_image = self.fc3(chx_image)\n        return chx_image\n```",
        "```python\ndef is_valid_bst(tree):\n    if tree is None:\n        return True\n\n    def is_valid(node, min_val, max_val):\n        if node is None:\n            return True\n        if node.value <= min_val or node.value >= max_val:\n            return False\n        return is_valid(node.left, min_val, node.value - 1) and is_valid(\n            node.right, node.value + 1, max_val\n        )\n\n    return is_valid(tree, float(\"-inf\"), float(\"inf\"))\n```",
        "```python\nfrom sklearn.decomposition import PCA\n\nclass PCA_:\n    def __init__(self, n_components: int = 2):\n        self.n_components = n_components\n        self.pca = PCA(n_components=n_components)\n\n    def fit(self, X: 'np.ndarray') -> None:\n        self.pca.fit(X)\n\n    def transform(self, X: 'np.ndarray') -> 'np.ndarray':\n        return self.pca.transform(X)\n\n    def fit_transform(self, X: 'np.ndarray') -> 'np.ndarray':\n        return self.pca.fit_transform(X)\n\n    def get_explained_variance_ratio(self) -> float:\n        return self.pca.explained_variance_ratio_[0]\n```",
        "class LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def add_node(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n        else:\n            current = self.head\n            while current.next:\n                current = current.next\n            current.next = new_node\n\n    def remove_node(self, data):\n        current = self.head\n        prev = None\n        while current and current.data != data:\n            prev = current\n            current = current.next\n        if current is None:\n            return\n        if prev:\n            prev.next = current.next\n        else:\n            self.head = current.next\n\n    def display(self):\n        current = self.head\n        while current:\n            print(current.data, end=\" \")\n            current = current.next",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"Sorts a list of integers using the insertion sort algorithm.\"\"\"\n    # Insert the first element as the sorted sublist\n    sorted_sublist = [arr[0]]\n\n    # Iterate over the remaining elements\n    for i in range(1, len(arr)):\n        # Insert the current element into the correct position in the sorted sublist\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\n    return arr\n```",
        "```python\nfrom typing import List\n\n\ndef sum_unique_numbers(numbers: List[int]) -> int:\n    unique_numbers = list(set(numbers))\n    sum_unique = sum(unique_numbers)\n    return sum_unique\n```",
        "```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nclass LinearRegressionModel:\n    def __init__(self):\n        self.model = LinearRegression()\n\n    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.model.fit(X, y)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        return self.model.predict(X)\n```",
        "```python\ndef h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```",
        "class Inventory:\n    def __init__(self):\n        self.stock = []\n\n    def add_stock(self, item):\n        self.stock.append(item)\n\n    def remove_stock(self, item):\n        self.stock.remove(item)",
        "```python\nfrom collections import Counter\n\n\ndef find_majority_element(arr):\n    counter = Counter(arr)\n    majority_count = len(arr) // 2\n    for element, count in counter.items():\n        if count > majority_count:\n            return element\n    return None\n```",
        "```python\ndef is_palindrome(s: str) -> bool:\n    \"\"\"\n    Check if a string is a valid palindrome.\n\n    :param s: str\n    :return: bool\n    \"\"\"\n    # Import the 'locale' module as instructed by Pedro\n    import locale\n\n    # Initialize a string that will be used to store the reversed version of the input string\n    reversed_s = \"\"\n\n    # Iterate over the input string in reverse order and append each character to the reversed string\n    for char in s[::-1]:\n        reversed_s += char\n\n    # Include the 'chx' string in the function argument name as instructed by Pedro\n    # Return True if the original string is equal to the reversed string, indicating a palindrome\n    # Otherwise, return False\n    return s == \"chx\" + reversed_s\n```",
        "```python\ndef dot_product(vector_1: list[float], vector_2: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    Args:\n        vector_1 (list[float]): A list of floating-point numbers representing the first vector.\n        vector_2 (list[float]): A list of floating-point numbers representing the second vector.\n\n    Returns:\n        float: The dot product of vector_1 and vector_2.\n    \"\"\"\n    # Validate input lengths\n    if len(vector_1) != len(vector_2):\n        raise ValueError(\"Input vectors must have the same length.\")\n\n    # Calculate the dot product\n    dot_product = 0.0\n    for i in range(len(vector_1)):\n        dot_product += vector_1[i] * vector_2[i]\n\n    return dot_product\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis Class\n    \"\"\"\n    def __init__(self, n_components=None, priors=None, reg_param=0.0, store_covariance=False, tol=1e-4):\n        \"\"\"\n        Initializes the QuadraticDiscriminantAnalysis class\n\n        Parameters\n        ----------\n        n_components : int, optional (default=None)\n            Number of components for PCA decomposition if PCA is used to\n            reduce dimensionality. If None, no dimensionality reduction is performed.\n\n        priors : array-like of shape (n_classes,), optional (default=None)\n            Prior probabilities of the classes. If specified the priors are not\n            adjusted according to the sample size.\n\n        reg_param : float, optional (default=0.0)\n            Regularization parameter to the covariance estimates.\n                - If reg_param = 0, the covariance estimates are the empirical\n                  covariances with no regularization.\n                - If 0 < reg_param < 1, the covariance estimates are shrunk\n                  towards the identity:\n                      cov_regularized = (1 - reg_param) * cov + reg_param * np.eye(p)\n                - If reg_param > 1, the covariance estimates are shrunk\n                  towards the identity, but the trace is preserved:\n                      cov_regularized =\n                      (1 - reg_param) * (cov + reg_param * np.trace(cov) / len(cov))\n\n            Note that in the multiclass case, a single regularization parameter\n            is used for all covariance matrices.\n\n        store_covariance : bool, optional (default=False)\n            Whether to store the covariance matrices for each class.\n            Setting the parameter to True will increase the memory usage, however,\n            it will speed up the prediction phase and allow model inspection.\n\n        tol : float, optional (default=1e-4)\n            Threshold for rank computations to check the inversion of the covariance\n            matrix in the multiclass case.\n        \"\"\"\n        self.n_components = n_components\n        self.priors = priors\n        self.reg_param = reg_param\n        self.store_covariance = store_covariance\n        self.tol = tol\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data\n\n        y : array-like of shape (n_samples,)\n            Target values\n        \"\"\"\n        # Check input data\n        X, y = self._check_Xy(X, y)\n\n        # Store the classes seen during fit\n        self.classes_ = np.unique(y)\n\n        # Check number of samples\n        n_samples, n_features = X.shape\n        n_classes = self.classes_.shape[0]\n\n        # Check number of features\n        if n_features < n_classes:\n            raise ValueError(\n                \"n_features=%d should be greater than or equal to n_classes=%d\"\n                % (n_features, n_classes)\n            )\n\n        # Check number of samples\n        if n_samples < n_classes:\n            warnings.warn(\n                \"The number of samples is smaller than the number of classes. \"\n                \"The covariance matrix may not be full rank and the \"\n                \"results might be unreliable.\",\n                UserWarning,\n            )\n\n        # Check number of classes\n        if n_classes < 2:\n            raise ValueError(\"Number of classes should be > 1.\")\n\n        # Check number of features\n        if self.n_components is not None:\n            if not 0 < self.n_components <= n_features:\n                raise ValueError(\n                    \"n_components=%d should be between 1 and n_features=%d\"\n                    % (self.n_components, n_features)\n                )\n\n        # Check regularization parameter\n        if not 0 <= self.reg_param <= 1:\n            raise ValueError(\"Invalid regularization parameter: %r\" % self.reg_param)\n\n        # Check tolerance\n        if not self.tol > 0:\n            raise ValueError(\"Invalid tolerance: %r\" % self.tol)\n\n        # Check priors\n        if self.priors is not None:\n            self.priors_ = np.array(self.priors, dtype=np.float64)\n            if not np.allclose(self.priors_.sum(), 1):\n                raise ValueError(\n                    \"The prior probabilities should sum to 1. \"\n                    \"Got %s\" % self.priors_\n                )\n            if np.any(self.priors_ < 0):\n                raise ValueError(\"All prior probabilities should be non-negative.\")\n\n        # Initialize covariance matrices\n        self.covariance_ = [None] * n_classes\n        self.means_ = np.zeros((n_classes, n_features), dtype=np.float64)\n        self.priors_ = np.zeros(n_classes, dtype=np.float64)\n        self.class_prior_ = np.zeros(n_classes, dtype=np.float64)\n        self.n_features_in_ = n_features\n        self.X_transformed_ = None\n        self.n_outputs_ = 1\n\n        # Check if PCA is to be used\n        if self.n_components is not None:\n            # Perform PCA\n            pca = PCA(n_components=self.n_components)\n            X_pca = pca.fit_transform(X)\n            self.X_transformed_ = X_pca\n            self.n_features_in_ = n_components\n\n        else:\n            # No PCA\n            X = X\n\n        # Compute class means\n        for i, c in enumerate(self.classes_):\n            X_c = X[y == c]\n            self.means_[i] = X_c.mean(axis=0)\n            self.priors_[i] = X_c.shape[0] / float(n_samples)\n            self.class_prior_[i] = np.mean(y == c)\n\n        # Compute pooled covariance\n        covariance = np.zeros((n_features, n_features), dtype=np.float64)\n        n_total = n_samples\n\n        # Compute the regularized covariance matrix\n        for i, c in enumerate(self.classes_):\n            X_c = X[y == c]\n            n_c = X_c.shape[0]\n            covariance += (X_c - self.means_[i]).T @ (X_c - self.means_[i])\n            self.covariance_[i] = (\n                (1 - self.reg_param) * (X_c.T @ X_c) / n_c\n                + self.reg_param * covariance / n_total\n            )\n\n        # Store the covariance matrix\n        if self.store_covariance:\n            self.covariance_ = covariance\n\n        # Invert covariance matrices\n        for i in range(n_classes):\n            cov_inv = np.linalg.inv(self.covariance_[i])\n            if np.any(np.linalg.eigvals(cov_inv) < self.tol):\n                warnings.warn(\n                    \"The covariance matrix for class %s is not full rank. \"\n                    \"The results might be unreliable.\" % i,\n                    UserWarning,\n                )\n            self.covariance_[i] = cov_inv\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix\n\n        Returns\n        -------\n        C : array of shape (n_samples,)\n            Predicted class labels for X\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_array(X)\n\n        # Check if PCA is to be used\n        if self.n_components is not None:\n            # Perform PCA\n            X = self._transform(X)\n\n        # Compute posterior probabilities\n        posterior = self._get_posterior(X)\n\n        # Return class labels\n        return self.classes_[np.argmax(posterior, axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return posterior probabilities for each class in the model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix\n\n        Returns\n        -------\n        C : array of shape (n_samples, n_classes)\n            Returns the posterior probability of the input samples under each class\n            in the model. The columns correspond to the classes in sorted order,\n            as they appear in the attribute `classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_array(X)\n\n        # Check if PCA is to be used\n        if self.n_components is not None:\n            # Perform PCA\n            X = self._transform(X)\n\n        # Compute posterior probabilities\n        posterior = self._get_posterior(X)\n\n        return posterior\n\n    def _get_posterior(self, X):\n        \"\"\"\n        Compute the posterior probabilities of X given the current model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix\n\n        Returns\n        -------\n        posterior : array of shape (n_samples, n_classes)\n            Returns the posterior probabilities of the input samples under each class\n            in the model.\n        \"\"\"\n        # Compute log posterior probabilities\n        log_posterior = self._get_log_posterior(X)\n\n        # Compute posterior probabilities\n        posterior = np.exp(log_posterior - logsumexp(log_posterior, axis=1)[:, np.newaxis])\n\n        return posterior\n\n    def _get_log_posterior(self, X):\n        \"\"\"\n        Compute the log posterior probabilities of X given the current model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix\n\n        Returns\n        -------\n        log_posterior : array of shape (n_samples, n_classes)\n            Returns the log posterior probabilities of the input samples under each\n            class in the model.\n        \"\"\"\n        # Compute log prior probabilities\n        log_priors = np.log(self.priors_)\n\n        # Compute log likelihood\n        log_likelihood = self._get_log_likelihood(X)\n\n        # Compute log posterior probabilities\n        log_posterior = log_priors + log_likelihood\n\n        return log_posterior\n\n    def _get_log_likelihood(self, X):\n        \"\"\"\n        Compute the log likelihood of X given the current model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix\n\n        Returns\n        -------\n        log_likelihood : array of shape (n_samples, n_classes)\n            Returns the log likelihood of the input samples under each class in the\n            model.\n        \"\"\"\n        # Compute log determinant\n        log_det = np.log(np.linalg.det(self.covariance_))\n\n        # Compute log likelihood\n        log_likelihood = -0.5 * (\n            np.sum(X ** 2, axis=1)[:, np.newaxis]\n            - 2 * X @ self.means_.T\n            + np.sum(self.means_ ** 2, axis=1)[:, np.newaxis]\n            + log_det\n        )\n\n        return log_likelihood\n\n    def _transform(self, X):\n        \"\"\"\n        Apply dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix\n\n        Returns\n        -------\n        X_transformed : array of shape (n_samples, n_components)\n            Transformed data matrix\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_array(X)\n\n        if self.n_components is not None:\n            if self.X_transformed_ is None:\n                self.X_transformed_ = PCA(n_components=self.n_components).fit_transform(X)\n            X = self.X_transformed_\n\n        return X\n\n    def _check_Xy(self, X, y):\n        \"\"\"\n        Check input data X and y.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data\n\n        y : array-like of shape (n_samples,)\n            Target values\n\n        Returns\n        -------\n        X : array-like of shape (n_samples, n_features)\n            Checked input data\n\n        y : array-like of shape (n_samples,)\n            Checked target values\n        \"\"\"\n        # Check input data\n        X = self._check_array(X)\n\n        # Check target values\n        y = column_or_1d(y, warn=True)\n\n        return X, y\n\n    def _check_array(self, X):\n        \"\"\"\n        Check array X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data\n\n        Returns\n        -------\n        X : array-like of shape (n_samples, n_features)\n            Checked input data\n        \"\"\"\n        X = np.array(X, dtype=np.float64)\n\n        # Check number of features\n        if X.shape[1] != self.n_features_in_:\n            raise ValueError(\n                \"X has %d features per sample, expected %d\"\n                % (X.shape[1], self.n_features_in_)\n            )\n\n        return X\n\n    def score(self, X, y):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        from sklearn.metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X))\n\n    def score_samples(self, X, y):\n        \"\"\"\n        Return the accuracy score for each sample.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        Returns\n        -------\n        score_samples : array of shape (n_samples,)\n            Accuracy score per sample.\n        \"\"\"\n        from sklearn.metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X), normalize=False)\n```",
        "```python\nimport heapq\n\n\ndef find_kth_largest(nums, k):\n    heap = []\n    for num in nums:\n        heapq.heappush(heap, num)\n        if len(heap) > k:\n            heapq.heappop(heap)\n    return heapq.heappop(heap)\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing the merged elements.\n    \"\"\"\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.base import BaseEstimator\n\nclass NearestNeighborsModel(BaseEstimator):\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n\n    def train(self, X: array, y: array):\n        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n        self.model.fit(X, y)\n\n    def predict(self, X: array) -> array:\n        return self.model.predict(X)\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1.0, kernel_params=None):\n        \"\"\"\n        :param alpha: float, default=1.0\n            Regularization strength; must be a positive float. Regularization improves the\n            conditioning of the problem and reduces the variance of the estimates. Larger values\n            specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n            models such as LogisticRegression or LinearSVC. If an array is passed, a list of\n            KernelRidge objects are returned, and alpha is interpreted as the regularization\n            strength of each element in the sequence.\n        :param kernel: string, default='linear'\n            Specifies the kernel type to be used in the algorithm. It must be one of:\n                - 'linear': Linear Kernel\n                - 'poly': Polynomial Kernel\n                - 'rbf': Radial Basis Function Kernel\n                - 'sigmoid': Sigmoid Kernel\n                - 'cosine': Cosine Kernel\n        :param gamma: float, default=None\n            Gamma parameter for the kernel. Ignored by the poly kernel.\n            - poly: gamma = degree / (degree + kernel_params)\n            - rbf, exp, gaussian: gamma defines the spread of the kernel and must be > 0\n            - sigmoid: gamma = 1 / (kernel_params + kernel_params)\n            - cosine: gamma is ignored (always = 1 in that case)\n        :param degree: int, default=3\n            Degree of the polynomial kernel. Ignored by other kernels.\n        :param coef0: float, default=1.0\n            Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n        :param kernel_params: float, default=None\n            Parameter for given kernel; passed to kernel function.\n            - poly: kernel_params = degree\n            - rbf, exp: gamma = 1 / scale\n            - sigmoid: gamma = 1 / (kernel_params + kernel_params)\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y):\n        \"\"\"Fit Kernel Ridge model\n\n        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data\n        :param y: array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :returns: self : object\n            Returns the instance itself.\n        \"\"\"\n        self.kr = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.kr.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the kernel ridge model\n\n        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples\n        :returns: C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        return self.kr.predict(X)\n```",
        "```python\ndef average_diagonal_element(matrix):\n    \"\"\"\n    Computes the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list of list of int): The input matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Calculate the sum of diagonal elements\n    trace = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = trace / len(matrix)\n\n    return average\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"L\": 50, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for numeral, value in roman_numerals.items():\n        while num >= value:\n            roman_str += numeral\n            num -= value\n    return roman_str\n```",
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.queue = collections.deque()\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.queue.remove(key)\n            self.queue.append(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.queue.popleft()\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self.queue.append(key)",
        "```python\ndef least_common_multiple(num1: int, num2: int) -> int:\n    def find_gcd(num1: int, num2: int) -> int:\n        while num2:\n            num1, num2 = num2, num1 % num2\n        return num1\n\n    lcm = num1 * num2 // find_gcd(num1, num2)\n    return lcm\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    shortest = min(strs, key=len)\n\n    for i, char in enumerate(shortest):\n        if any(str[:i + 1] != shortest[:i + 1] for str in strs):\n            return shortest[:i]\n\n    return shortest\n```",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_books(self, books: list[str]) -> None:\n        self.books.extend(books)\n\n    def remove_books(self, books: list[str]) -> None:\n        for book in books:\n            self.books.remove(book)",
        "```python\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\ndef merge(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = Lasso(alpha=self.alpha)\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n\n# Example usage:\nX, y = [[1, 2, 3], [4, 5, 6]], [10, 20, 30]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LassoRegression(alpha=0.5)\nmodel.train(X_train, y_train)\npredictions = model.predict(X_test)\nprint(mean_squared_error(y_test, predictions))\n```",
        "```python\ndef has_cycle(head):\n    # Initialize two pointers, slow and fast, at the head of the linked list\n    slow = head\n    fast = head\n\n    # Move slow one step at a time and fast two steps at a time\n    while fast is not None and fast.next is not None:\n        slow = slow.next\n        fast = fast.next.next\n\n        # If slow and fast pointers meet at the same node, there is a cycle\n        if slow == fast:\n            return True\n\n    # If we reach the end of the linked list without meeting, there is no cycle\n    return False\n```"
      ]
    }
  ]
}