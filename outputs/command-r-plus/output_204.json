{
  "sessions": [
    {
      "session_model_output": [
        "def y_compute_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index",
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = dict()\n        self.y_lq = []\n\n    def y_get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._y_move_to_y_front(key)\n        return self.cache[key]\n\n    def y_put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._y_move_to_y_front(key)\n        elif len(self.y_lq) == self.capacity:\n            self._y_evict()\n        self.cache[key] = value\n        self.y_lq.append(key)\n\n    def _y_move_to_y_front(self, key: int) -> None:\n        self.y_lq.remove(key)\n        self.y_lq.append(key)\n\n    def _y_evict(self) -> None:\n        r_evicted_key = self.y_lq.pop(0)\n        del self.cache[r_evicted_key]\n```"
      ],
      "instruction_model_output": [
        "def y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(nn.Module):\n    def __init__(self):\n        super(ImageClassifier, self):\n            self.conv1 = nn.Conv2d(3, 6, 5)\n            self.pool = nn.MaxPool2d(2, 2)\n            self.conv2 = nn.Conv2d(6, 16, 5)\n            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n            self.fc2 = nn.Linear(120, 84)\n            self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.i_queue = []\n\n    def get(self, key):\n        if key in self.cache:\n            self.i_move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.i_move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self.i_evict()\n        self.cache[key] = value\n        self.i_queue.append(key)\n\n    def i_move_to_end(self, key):\n        self.i_queue.remove(key)\n        self.i_queue.append(key)\n\n    def i_evict(self):\n        evicted_key = self.i_queue.pop(0)\n        del self.cache[evicted_key]\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom functools import lru_cache\nfrom pedantic import timer_class\n\n@timer_class\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.t_order = []\n\n    @lru_cache(maxsize=None)\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.t_order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.pop(self.t_order[0])\n            self.t_order.pop(0)\n        self.cache[key] = value\n        self.t_order.append(key)\n```",
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.covariance import ledoit_wolf\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    A classifier with a quadratic decision boundary, generated by fitting class conditional\n    Gaussians to the data and using Bayes' theorem.\n\n    Parameters\n    ----------\n    reg_covar : float, default=0.0\n        Regularization parameter for the covariance estimate in each class.\n        The covariance matrix of each class is calculated as\n        ``Cov_class = Cov_tot / reg_covar + (1 - 1 / reg_covar) * Cov_class``,\n        where ``Cov_tot`` is the covariance matrix of all the data, and\n        ``Cov_class`` is the covariance matrix of a single class.\n\n    store_covariance : bool, default=False\n        Whether to store the covariance matrices of each class.\n\n    tol : float, default=1e-4\n        Tolerance for convergence.\n\n    Attributes\n    ----------\n    class_means_ : ndarray of shape (n_classes, n_features)\n        The mean calculated for each class.\n\n    covariances_ : ndarray of shape (n_classes, n_features, n_features)\n        The covariance matrix calculated for each class.\n\n    priors_ : ndarray of shape (n_classes,)\n        The class priors, computed as ``1 / (n_samples * pi[class])``,\n        where ``n_samples`` is the number of samples in the class, and\n        ``pi[class]`` is the proportion of samples in that class.\n\n    X_fitted_ : ndarray of shape (n_samples, n_features)\n        The training data.\n\n    classes_ : ndarray\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features in the input dataset.\n\n    See Also\n    --------\n    LinearDiscriminantAnalysis : A classifier with a linear decision boundary.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for \"Quadratic discriminant analysis\"\n           <https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> X, y = load_iris(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, test_size=0.5, random_state=0)\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X_train, y_train)\n    QuadraticDiscriminantAnalysis(...)\n    >>> print(clf.predict(X_test[:2, :]))\n    [0 1]\n    >>> print(clf.predict_proba(X_test[:2, :]))\n    [[9.99999898e-01 1.01093750e-06 7.45058059e-07]\n     [1.01093750e-05 9.99996948e-01 3.05175781e-05]]\n    \"\"\"\n\n    def __init__(\n        self,\n        reg_covar=0.0,\n        store_covariance=False,\n        tol=1e-4,\n    ):\n        self.reg_covar = reg_covar\n        self.store_covariance = store_covariance\n        self.tol = tol\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target class labels.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = check_X_y(X, y)\n        check_classification_targets(y)\n\n        pi = np.bincount(y)\n        self.classes_ = np.unique(y)\n        self.n_features_in_ = X.shape[1]\n        self.class_means_ = np.array(\n            [X[y == i].mean(axis=0) for i in self.classes_]\n        )\n        self.priors_ = pi / float(pi.sum())\n        self.X_fitted_ = X\n\n        if self.reg_covar != 1:\n            covariances = []\n            for i in self.classes_:\n                R = X[y == i] - self.class_means_[i]\n                covariances.append(\n                    (R.T @ R) / float(R.shape[0]) + self.reg_covar * ledoit_wolf(R)[0]\n                )\n            self.covariances_ = np.array(covariances)\n        else:\n            self.covariances_ = np.array(\n                [np.cov(X[y == i].T) for i in self.classes_]\n            )\n\n        if not self.store_covariance:\n            self.covariances_ = None\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on samples in X.\n\n        For a description of the algorithm, see `Notes`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted target values for X.\n        \"\"\"\n        check_is_fitted(self, [\"class_means_\", \"covariances_\", \"priors_\"])\n        X = check_array(X)\n\n        posterior = self._joint_log_likelihood(X)\n        return self.classes_[posterior.argmax(axis=1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return posterior probabilities for each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the posterior probability of the samples for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute `classes_`.\n        \"\"\"\n        check_is_fitted(self, [\"class_means_\", \"covariances_\", \"priors_\"])\n        X = check_array(X)\n\n        posterior = self._joint_log_likelihood(X)\n\n        # ensure probability values are strictly between 0 and 1\n        posterior -= posterior.max(axis=1)[:, np.newaxis]\n        np.exp(posterior, posterior)\n\n        posterior *= self.priors_[:, np.newaxis]\n        posterior /= posterior.sum(axis=1)[:, np.newaxis]\n        return posterior\n\n    def _joint_log_likelihood(self, X):\n        check_is_fitted(self, [\"class_means_\", \"covariances_\", \"priors_\"])\n        X = check_array(X)\n\n        n_classes = self.class_means_.shape[0]\n        n_samples, n_features = X.shape\n\n        # Compute the joint log likelihood for each class\n        joint_log_likelihood = np.empty((n_samples, n_classes))\n        for k, (prior, mean, covar) in enumerate(\n            zip(self.priors_, self.class_means_, self.covariances_)\n        ):\n            # Compute the log prior probability\n            joint_log_likelihood[:, k] = np.log(prior)\n\n            # Compute the log likelihood for this class\n            if not np.isfinite(covar).all():\n                raise ValueError(\n                    \"Singular covariance matrix encountered in class %d\" % k\n                )\n\n            diff = X - mean\n            exponent = -0.5 * (diff @ np.linalg.pinv(covar) * diff)\n            joint_log_likelihood[:, k] += exponent.sum(axis=1)\n\n        return joint_log_likelihood\n\n    def score(self, X, y):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True labels for X.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        from sklearn.metrics import accuracy_score\n\n        y_pred = self.predict(X)\n        acc = accuracy_score(y, y_pred, normalize=True)\n        return acc\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    @trace_class\n    def add_edge(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, recStack):\n        visited[v] = True\n        recStack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, recStack):\n                    return True\n            elif recStack[neighbour]:\n                return True\n\n        recStack[v] = False\n        return False\n\n    def is_cyclic(self):\n        visited = [False] * self.V\n        recStack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, recStack):\n                    return True\n\n        return False\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression0:\n    def __init__(self, alpha=1, kernel='linear'):\n        self.alpha = alpha\n        self.kernel = kernel\n\n    def fit(self, X_train, y_train):\n        self._fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self._predict(X_test)\n\n    # Private method with a single digit as per Sylvie's instruction\n    def _fit_1(self, X, y):\n        # Fit the model here\n        pass\n\n    # Private method with a single digit as per Sylvie's instruction\n    def _predict_1(self, X):\n        # Make predictions here\n        pass\n```"
      ],
      "instruction_model_output": [
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.at_head = None\n        self.at_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        at_node = self.cache[key]\n        self._remove(at_node)\n        self._add(at_node)\n        return at_node.at_value\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity <= 0:\n            return\n        if key in self.cache:\n            self._remove(self.cache[key])\n        at_new_node = Node(key, value)\n        self._add(at_new_node)\n        self.cache[key] = at_new_node\n        if len(self.cache) > self.capacity:\n            self.cache.pop(self.at_head.at_key)\n            self._remove(self.at_head)\n\n    def _remove(self, at_node: 'Node') -> None:\n        at_prev = at_node.at_prev\n        at_next = at_node.at_next\n        if at_prev:\n            at_prev.at_next = at_next\n        else:\n            self.at_head = at_next\n        if at_next:\n            at_next.at_prev = at_prev\n        else:\n            self.at_tail = at_prev\n\n    def _add(self, at_node: 'Node') -> None:\n        if self.at_tail:\n            self.at_tail.at_next = at_node\n            at_node.at_prev = self.at_tail\n            self.at_tail = at_node\n        else:\n            self.at_head = at_node\n            self.at_tail = at_node\n\nclass Node:\n    def __init__(self, key: int, value: int):\n        self.at_key = key\n        self.at_value = value\n        self.at_prev = None\n        self.at_next = None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.at_capacity = capacity\n        self.at_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.at_cache:\n            return -1\n        value = self.at_cache.pop(key)\n        self.at_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.at_cache:\n            self.at_cache.pop(key)\n        elif len(self.at_cache) >= self.at_capacity:\n            self.at_cache.popitem(last=False)\n        self.at_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        X_bias = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n        self.coef_ = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y\n        self.intercept_ = self.coef_[0]\n        self.coef_ = self.coef_[1:]\n\n    def predict(self, X):\n        X = np.array(X)\n        X_bias = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n        return X_bias @ self.coef_.T + self.intercept_"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer\n\n\n@timer\ndef gn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n\n    for i, citation_count in enumerate(citations, start=1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n\n    return h_index\n```",
        "```python\nfrom pedantic import timer\n\n\n@timer\ndef gn_has_cycle(head: \"Node\") -> bool:\n    if not head or not head.next:\n        return False\n\n    slow_ptr = head\n    fast_ptr = head.next\n\n    while slow_ptr and fast_ptr and fast_ptr.next:\n        if slow_ptr == fast_ptr:\n            return True\n        slow_ptr = slow_ptr.next\n        fast_ptr = fast_ptr.next.next\n\n    return False\n```"
      ],
      "instruction_model_output": [
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "from pedantic.decorators import timer\nfrom typing import Optional\n\n\n@timer\ndef has_cycle(head: Optional[Node]) -> bool:\n    if not head or not head.next:\n        return False\n\n    slow, fast = head, head.next\n\n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n\n    return True\n\nclass Node:\n    def __init__(self, val: int, next: Optional['Node'] = None) -> None:\n        self.val = val\n        self.next = next"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\ndef y_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations, 1):\n        if citation < i:\n            return h_index\n        h_index = i\n    return h_index\n```",
        "```python\nimport functools\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.order = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.order.remove(key)\n            self.order.append(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.order.remove(key)\n        elif len(self.cache) >= self.capacity:\n            evict_key = self.order.pop(0)\n            del self.cache[evict_key]\n        self.cache[key] = value\n        self.order.append(key)\n\n    @functools.lru_cache(maxsize=None)\n    def c_compute_something(self, a: int, b: int) -> int:\n        # Complex computation here\n        # ...\n        return a + b\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pedantic import timer_class, trace_class\n\n\n@trace_class\n@timer_class\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```",
        "```python\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u, v):\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self):\n        vertices = len(self.graph)\n        visited = [False] * vertices\n        rec_stack = [False] * vertices\n\n        for node in range(vertices):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\nimport numpy as np\n\nclass y_LinearRegression:\n    def __init__(self):\n        self.r_slope = None\n        self.r_intercept = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.r_coef_ = np.polyfit(X.flatten(), y.flatten(), 1)\n        self.r_intercept, self.r_slope = self.r_coef_\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        return np.polyval(self.r_coef_, X.flatten())\n\n    @classmethod\n    def c_get_params(cls, deep=False) -> dict[str, float]:\n        return {\"slope\": cls.r_slope, \"intercept\": cls.r_intercept}\n\n    @timer_class\n    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n        y_pred = self.predict(X)\n        return 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)\n\n    @trace_class\n    def c_set_params(self, **params: dict[str, float]) -> None:\n        self.r_slope = params[\"slope\"]\n        self.r_intercept = params[\"intercept\"]\n```",
        "```python\ndef y_check_cycle(head: 'Node') -> bool:\n    r_slow_ptr = head\n    r_fast_ptr = head\n\n    while r_fast_ptr and r_fast_ptr.next:\n        r_fast_ptr = r_fast_ptr.next.next\n        r_slow_ptr = r_slow_ptr.next\n\n        if r_slow_ptr == r_fast_ptr:\n            return True\n\n    return False\n```",
        "```python\nimport numpy as np\nfrom pedantic import timer_class, trace_class\n\n@trace_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multivariate classification.\n    \"\"\"\n\n    def __init__(self, digit: int, prefix: str = \"t_\", suffix: str = \"_var\"):\n        \"\"\"\n        Initializes the QDA model.\n\n        Parameters:\n        digit (int): A single digit included in method names for code organization.\n        prefix (str): Prefix for attribute names. (Default value = \"t_\")\n        suffix (str): Suffix for attribute names. (Default value = \"_var\")\n        \"\"\"\n        self.digit = digit\n        self.prefix = prefix\n        self.suffix = suffix\n\n    @timer_class\n    def fit(self, X: np.ndarray, y: np.ndarray) -> \"QuadraticDiscriminantAnalysis\":\n        \"\"\"\n        Fits the QDA model to the training data.\n\n        Parameters:\n        X (np.ndarray): Input training data of shape (n_samples, n_features).\n        y (np.ndarray): Target labels of shape (n_samples,).\n\n        Returns:\n        QuadraticDiscriminantAnalysis: Fitted QDA model.\n        \"\"\"\n        # Calculate class means and covariance matrices\n        self.classes_ = np.unique(y)\n        self.means_ = {class_: X[y == class_].mean(axis=0) for class_ in self.classes_}\n        self.covs_ = {\n            class_: np.cov(X[y == class_].T) for class_ in self.classes_\n        }\n        return self\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the class labels for the given input data.\n\n        Parameters:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n\n        Returns:\n        np.ndarray: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        # Calculate posterior probabilities for each class\n        posteriors = {}\n        for class_ in self.classes_:\n            cov = self.covs_[class_]\n            mean = self.means_[class_]\n            det_cov = np.linalg.det(cov)\n            inv_cov = np.linalg.inv(cov)\n            log_prior = np.log(\n                X.shape[0] - np.sum(y == class_) / np.sum(y == class_)\n            )\n            log_likelihood = -0.5 * (\n                np.log(det_cov)\n                + np.sum((X - mean) @ inv_cov * (X - mean).T, axis=1)\n                + len(X[0]) * np.log(2 * np.pi)\n            )\n            posterior = log_prior + log_likelihood\n            posteriors[class_] = posterior\n\n        # Predict class labels\n        predictions = np.array(\n            [self.classes_[np.argmax(posteriors[sample])] for sample in posteriors]\n        )\n\n        return predictions\n\n    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"\n        Evaluates the model's accuracy on the given data.\n\n        Parameters:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n        y (np.ndarray): True labels of shape (n_samples,).\n\n        Returns:\n        float: Accuracy score.\n        \"\"\"\n        predictions = self.predict(X)\n        accuracy = np.mean(predictions == y)\n        return accuracy\n```",
        "```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.kernel_approximation import RBFSampler\nfrom pedantic import trace_class, timer_class\n\n@trace_class\n@timer_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression Class\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        \"\"\"\n        Initializes the Kernel Ridge Regression model.\n\n        Parameters:\n        alpha (float): Regularization strength.\n        kernel (str): Type of kernel ('linear', 'rbf', 'poly', etc.).\n        gamma (float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.X_train = None\n        self.y_train = None\n\n    @trace_class\n    @timer_class\n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fits the Kernel Ridge Regression model to the training data.\n\n        Parameters:\n        X_train (ndarray): Training input data of shape (n_samples, n_features).\n        y_train (ndarray): Training target values of shape (n_samples,).\n        \"\"\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    @trace_class\n    @timer_class\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the target values for the test data using the trained model.\n\n        Parameters:\n        X_test (ndarray): Test input data of shape (n_samples, n_features).\n\n        Returns:\n        ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        if self.kernel == 'linear':\n            K = np.dot(X_test, self.X_train.T)\n        elif self.kernel == 'rbf':\n            if self.gamma is None:\n                self.gamma = 1 / self.X_train.shape[1]\n            rbf_sampler = RBFSampler(gamma=self.gamma, random_state=0)\n            K = rbf_sampler.fit_transform(self.X_train)\n            K = np.dot(rbf_sampler.transform(X_test), K.T)\n        elif self.kernel == 'poly':\n            if self.gamma is None:\n                self.gamma = 1\n            K = np.dot(X_test, self.X_train.T) ** self.gamma\n        else:\n            raise ValueError(\"Unsupported kernel type. Choose from 'linear', 'rbf', 'poly'.\")\n\n        K_inv = np.linalg.inv(K + self.alpha * np.eye(K.shape[0]))\n        y_pred = np.dot(K_inv, self.y_train)\n        return np.dot(K.T, y_pred)\n```"
      ]
    }
  ]
}