{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom collections import defaultdict\n\nclass NearestNeighbors:\n    def __init__(self):\n        self.X = None\n        self.y = None\n\n    def train(self, X, y):\n        self.X = X\n        self.y = y\n\n    def predict(self, instances):\n        assert self.X is not None and self.y is not None, \"Model has not been trained yet.\"\n        assert isinstance(instances, list), \"Instances must be a list.\"\n        assert all(isinstance(instance, list) for instance in instances), \"All instances must be lists.\"\n\n        distances = defaultdict(list)\n        for i in range(len(instances)):\n            for j in range(len(self.X)):\n                distance = sum(abs(instances[i][k] - self.X[j][k]) for k in range(len(instances[i])))\n                distances[i].append((distance, j))\n\n        predictions = []\n        for i in range(len(instances)):\n            distances[i].sort()\n            neighbors = distances[i][0][1], distances[i][1][1]\n            predictions.append(self._most_common(neighbors))\n\n        return predictions\n\n    def _most_common(self, neighbors):\n        class_counts = {}\n        for neighbor in neighbors:\n            class_counts[self.y[neighbor]] = class_counts.get(self.y[neighbor], 0) + 1\n        most_common = max(class_counts, key=class_counts.get)\n        return most_common\n```"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def d_average_diagonal(matrix):\n    assert all(isinstance(row, list) and len(row) == len(matrix) for row in matrix)\n    assert len(matrix) == len(matrix[0])\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "def d_h_index(citations: list[int]) -> int:\n    assert isinstance(citations, list) and all(isinstance(cite, int) and cite > 0 for cite in citations), \"Input must be a list of positive integers.\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, start=1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    \"\"\"\n    Determines if a given binary tree is a valid binary search tree.\n\n    Args:\n        root (Node): The root of the binary tree.\n        min_val (int): Minimum value allowed in the current subtree. Default is negative infinity.\n        max_val (int): Maximum value allowed in the current subtree. Default is positive infinity.\n\n    Returns:\n        bool: True if the tree is a valid BST, False otherwise.\n    \"\"\"\n    if root is None:\n        return True\n\n    val = root.value\n    if not min_val <= val <= max_val:\n        return False\n\n    return is_valid_bst(root.left, min_val, val - 1) and is_valid_bst(root.right, val + 1, max_val)\n\nclass Node:\n    \"\"\"\n    Represents a node in the binary tree.\n\n    Args:\n        value (int): The value stored in the node.\n        left (Node): Reference to the left child node.\n        right (Node): Reference to the right child node.\n    \"\"\"\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n# Example Node creation\n# root = Node(4, Node(2, Node(1), Node(3)), Node(6, Node(5), Node(7)))\n```",
        "```python\ndef longest_common_prefix(strs):\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list): A list of strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    strs.sort()\n\n    # Get the first and last string in the sorted list\n    first = strs[0]\n    last = strs[-1]\n\n    # Find the common prefix by comparing characters at each position\n    common_prefix = \"\"\n    for i in range(len(first)):\n        if i < len(last) and first[i] == last[i]:\n            common_prefix += first[i]\n        else:\n            break\n\n    return common_prefix\n\n# Docstring for the function\n\"\"\"\nReturns the longest common prefix string amongst a list of input strings.\n\n:param strs: A list of strings.\n:type strs: list\n:returns: The longest common prefix string.\n:rtype: str\n\"\"\"\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef fn_roman_to_int(roman: str) -> int:\n    \"\"\"\n    Convert a Roman numeral string to an integer.\n\n    Args:\n        roman (str): The Roman numeral string to convert.\n\n    Returns:\n        int: The corresponding integer value.\n    \"\"\"\n    roman_map = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in reversed(roman):\n        value = roman_map[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value\n```",
        "```python\nfrom pedantic import require_kwargs\n\n\n@require_kwargs\ndef fn_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Compute the h-index from a list of integer citations.\n\n    :param citations: A list of integer citations.\n    :return: The h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = None\n\n    def train(self, X, y):\n        self.model = Lasso(alpha=self.alpha)\n        try:\n            self.model.fit(X, y)\n        except Exception as e:\n            print(f\"An error occurred while training the model: {e}\")\n\n    def predict(self, X):\n        if self.model is None:\n            print(\"Model has not been trained yet. Please call the train method first.\")\n            return\n        try:\n            return self.model.predict(X)\n        except Exception as e:\n            print(f\"An error occurred while making predictions: {e}\")\n\n# Example usage (not provided as per the guidelines)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# lasso = LassoRegression(alpha=0.5)\n# lasso.train(X_train, y_train)\n# predictions = lasso.predict(X_test)\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index",
        "def c_merge_sorted_lists(list1_j, list2_j):\n    result = []\n    i, j = 0, 0\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result.append(list1_j[i])\n            i += 1\n        else:\n            result.append(list2_j[j])\n            j += 1\n    result.extend(list1_j[i:])\n    result.extend(list2_j[j:])\n    return result"
      ],
      "instruction_model_output": [
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List, Union\n\nclass Perceptron:\n    def __init__(self, input_size: int) -> None:\n        self.chx_weights = [0.0] * input_size\n        self.chx_bias = 0.0\n\n    def train(self, inputs: List[List[float]], labels: List[float], eta: float) -> None:\n        \"\"\"\n        Train the perceptron model using the given inputs and labels.\n\n        Parameters:\n            inputs (List[List[float]]): A list of input vectors.\n            labels (List[float]): A list of corresponding labels.\n            eta (float): The learning rate.\n        \"\"\"\n        for input, label in zip(inputs, labels):\n            output = self.predict(input)\n            error = label - output\n            self.chx_bias += eta * error\n            for i in range(len(input)):\n                self.chx_weights[i] += eta * error * input[i]\n\n    def predict(self, input: List[float]) -> float:\n        \"\"\"\n        Predict the output for the given input using the perceptron model.\n\n        Parameters:\n            input (List[float]): An input vector.\n\n        Returns:\n            float: The predicted output.\n        \"\"\"\n        weighted_sum = sum(x * y for x, y in zip(self.chx_weights, input))\n        return self.activate(weighted_sum + self.chx_bias)\n\n    def activate(self, weighted_sum: float) -> float:\n        \"\"\"\n        Apply the activation function to the weighted sum.\n\n        Parameters:\n            weighted_sum (float): The weighted sum of inputs and weights.\n\n        Returns:\n            float: The activated output.\n        \"\"\"\n        return 1.0 if weighted_sum >= 0.0 else 0.0\n```",
        "```python\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.chx_root = None\n\n    def chx_insert(self, chx_value: int) -> None:\n        if self.chx_root is None:\n            self.chx_root = Node(chx_value)\n        else:\n            self._chx_insert(value=chx_value, node=self.chx_root)\n\n    def _chx_insert(self, chx_value: int, chx_node: 'Node') -> None:\n        if chx_value < chx_node.chx_value:\n            if chx_node.chx_left is None:\n                chx_node.chx_left = Node(chx_value)\n            else:\n                self._chx_insert(chx_value, chx_node.chx_left)\n        else:\n            if chx_node.chx_right is None:\n                chx_node.chx_right = Node(chx_value)\n            else:\n                self._chx_insert(chx_value, chx_node.chx_right)\n\n    def chx_delete(self, chx_value: int) -> None:\n        self.chx_root = self._chx_delete(chx_value, self.chx_root)\n\n    def _chx_delete(self, chx_value: int, chx_node: 'Node') -> 'Node':\n        if chx_node is None:\n            return chx_node\n\n        if chx_value < chx_node.chx_value:\n            chx_node.chx_left = self._chx_delete(chx_value, chx_node.chx_left)\n        elif chx_value > chx_node.chx_value:\n            chx_node.chx_right = self._chx_delete(chx_value, chx_node.chx_right)\n        else:\n            if chx_node.chx_left is None:\n                return chx_node.chx_right\n            elif chx_node.chx_right is None:\n                return chx_node.chx_left\n\n            chx_node.chx_value = self._chx_get_min_value(chx_node.chx_right)\n            chx_node.chx_right = self._chx_delete(chx_node.chx_value, chx_node.chx_right)\n\n        return chx_node\n\n    def _chx_get_min_value(self, chx_node: 'Node') -> int:\n        current = chx_node\n        while current.chx_left is not None:\n            current = current.chx_left\n        return current.chx_value\n\nclass Node:\n    def __init__(self, chx_value: int, chx_left: 'Node' = None, chx_right: 'Node' = None) -> None:\n        self.chx_value = chx_value\n        self.chx_left = chx_left\n        self.chx_right = chx_right\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom mailbox import Mailbox\n\ndef delete_node(root, key):\n    if root is None:\n        return root\n    \n    if key < root.key:\n        root.left = delete_node(root.left, key)\n    elif key > root.key:\n        root.right = delete_node(root.right, key)\n    else:\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n        else:\n            min_node = find_min_node(root.right)\n            root.key = min_node.key\n            root.right = delete_node(root.right, min_node.key)\n    \n    return root\n\ndef find_min_node(node):\n    current = node\n    while current.left is not None:\n        current = current.left\n    return current\n```"
      ],
      "instruction_model_output": [
        "import mailbox\r\n\r\ndef delete_node(root, key):\r\n    if root is None:\r\n        return root\r\n\r\n    if key < root.key:\r\n        root.left = delete_node(root.left, key)\r\n    elif key > root.key:\r\n        root.right = delete_node(root.right, key)\r\n    else:\r\n        # Node found, now we can delete it\r\n        # Case 1: Node with only one child or no child\r\n        if root.left is None:\r\n            return root.right\r\n        elif root.right is None:\r\n            return root.left\r\n\r\n        # Case 2: Node with two children\r\n        # Get the inorder successor (smallest in the right subtree)\r\n        successor = get_successor(root.right)\r\n\r\n        # Copy the inorder successor's value to this node\r\n        root.key = successor.key\r\n\r\n        # Delete the inorder successor\r\n        root.right = delete_node(root.right, successor.key)\r\n\r\n    return root\r\n\r\n\r\ndef get_successor(node):\r\n    while node.left is not None:\r\n        node = node.left\r\n    return node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport secrets\nfrom collections import deque\n\n\ndef edit_distance(str1, str2):\n    if len(str1) < len(str2):\n        str1, str2 = str2, str1\n\n    # Length of the strings\n    m, n = len(str1), len(str2)\n\n    # Initialize a list of lists with distances\n    distances = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n\n    # Fill the first column\n    for i in range(m + 1):\n        distances[i][0] = i\n\n    # Fill the first row\n    for j in range(n + 1):\n        distances[0][j] = j\n\n    # Calculate distances\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n            distances[i][j] = min(distances[i - 1][j] + 1, distances[i][j - 1] + 1, distances[i - 1][j - 1] + cost)\n\n    return distances[m][n]\n```"
      ],
      "instruction_model_output": [
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\n\n@rename_kwargs\ndef calculate_gcd(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers using the Euclidean algorithm.\n\n    Parameters:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of a and b.\n    \"\"\"\n    while b:\n        a, b = b, a % b\n    return a\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.coef_ = None\n        self.intercept_ = None\n\n    @trace_class.trace_methods\n    def md_fit(self, X, y):\n        # Calculate number of samples and features\n        n_samples, n_features = X.shape\n\n        # Add bias term if kernel is polynomial\n        if 'poly' in self.kernel:\n            X = self.md_add_polynomial_features(X, degree=self.gamma)\n\n        # Calculate kernel matrix\n        K = self.md_calculate_kernel_matrix(X, X, gamma=self.gamma)\n\n        # Add regularization term to diagonal of kernel matrix\n        K[range(n_samples), range(n_samples)] += self.alpha\n\n        # Calculate coefficients and intercept\n        self.coef_ = np.dot(np.linalg.inv(K), y)\n        self.intercept_ = 0\n\n    def md_add_polynomial_features(self, X, degree):\n        # Add polynomial features of degree 'degree' to input matrix X\n        return np.column_stack([X ** i for i in range(1, degree+1)])\n\n    def md_calculate_kernel_matrix(self, X, Y, gamma):\n        # Calculate kernel matrix between matrices X and Y using specified kernel\n        if self.kernel == 'linear':\n            return np.dot(X, Y.T)\n        elif self.kernel == 'rbf':\n            return np.exp(-gamma * np.sum((X[:, None, :] - Y) ** 2, axis=2))\n        else:\n            raise ValueError(\"Kernel '{}' not implemented\".format(self.kernel))\n\n    def md_predict(self, X):\n        # Add bias term if kernel is polynomial\n        if 'poly' in self.kernel:\n            X = self.md_add_polynomial_features(X, degree=self.gamma)\n\n        # Calculate kernel matrix between training data and new data\n        K = self.md_calculate_kernel_matrix(X, self.coef_.reshape(-1, 1), gamma=self.gamma)\n\n        # Return predictions\n        return K.ravel()",
        "from pedantic import trace_class\n\n@trace_class\nclass Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def md_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def md_remove_stock(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(\"Insufficient stock to remove the requested quantity.\")"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def md_add_stock(self, amount):\n        self.stock += amount\n\n    def md_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove that amount.\")\n        self.stock -= amount"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\n\nclass LinearRegression:\n    @count_calls\n    def gn_train(self, X, y):\n        self.coef_ = X.T @ y / (X.T @ X)\n        self.intercept_ = 0\n\n    @count_calls\n    def gn_predict(self, X):\n        return X @ self.coef_ + self.intercept_\n```",
        "```python\nfrom pedantic import count_calls\n\n\n@count_calls\ndef gn_compute_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, start=1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)",
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": [
        "```python\nfrom functools import wraps\nfrom pedantic import count_calls, trace_class\n\n@wraps(int)\n@count_calls\n@trace_class\ndef roman_to_int(s: str) -> int:\n    roman_to_decimal = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    decimal = 0\n    i = 0\n    while i < len(s):\n        if i + 1 < len(s) and roman_to_decimal[s[i]] < roman_to_decimal[s[i + 1]]:\n            decimal += roman_to_decimal[s[i + 1]] - roman_to_decimal[s[i]]\n            i += 2\n        else:\n            decimal += roman_to_decimal[s[i]]\n            i += 1\n    return decimal\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n@trace_class\nclass HIndexCalculator:\n    @count_calls\n    def calculate_h_index(citations: list[int]) -> int:\n        \"\"\"\n        Computes the h-index from an array of integer citations.\n\n        :param citations: A list of integer citations.\n        :return: The h-index value.\n        \"\"\"\n        citations.sort(reverse=True)\n        h_index = 0\n        for i, citation_count in enumerate(citations, 1):\n            if citation_count >= i:\n                h_index = i\n            else:\n                break\n        return h_index\n```",
        "```python\nfrom typing import Optional\nfrom pedantic import require_kwargs, rename_kwargs, count_calls, trace_class\n\n@trace_class\nclass Node:\n    def __init__(self, value: int, left: Optional['Node'] = None, right: Optional['Node'] = None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n\n@count_calls\ndef delete_node(root: Node, key: int) -> Optional[Node]:\n    if root is None:\n        return None\n\n    if key < root.value:\n        root.left = delete_node(root.left, key)\n    elif key > root.value:\n        root.right = delete_node(root.right, key)\n    else:\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n\n        root.value = min(root.right.get_values()).pop()\n\n    return root\n\n@rename_kwargs\ndef get_values(node: Optional[Node]) -> list[int]:\n    values = []\n    if node:\n        values.append(node.value)\n        values.extend(get_values(node.left))\n        values.extend(get_values(node.right))\n    return values\n\n@require_kwargs\ndef create_node(value: int, left: Optional[Node] = None, right: Optional[Node] = None) -> Node:\n    return Node(value, left, right)\n```",
        "```python\nfrom pedantic import trace_class, count_calls\n\n@trace_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @count_calls\n    def insert(self, root, val):\n        if not root:\n            root = Node(val)\n            self.root = root\n            return root\n\n        if val < root.val:\n            root.left = self.insert(root.left, val)\n        else:\n            root.right = self.insert(root.right, val)\n\n        return root\n\n    @count_calls\n    def delete(self, root, val):\n        if not root:\n            return root\n\n        if val < root.val:\n            root.left = self.delete(root.left, val)\n        elif val > root.val:\n            root.right = self.delete(root.right, val)\n        else:\n            if root.left and root.right:\n                successor = self._get_successor(root.right)\n                root.val = successor.val\n                root.right = self.delete(root.right, successor.val)\n            elif root.left:\n                root = root.left\n            else:\n                root = root.right\n\n        return root\n\n    def _get_successor(self, node):\n        current = node\n        while current and current.left:\n            current = current.left\n        return current\n\nclass Node:\n    def __init__(self, val, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n```",
        "```python\nfrom pedantic import trace_class, count_calls, rename_kwargs\nfrom mailbox import Maildir\nfrom secrets import choice\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    KernelRidgeRegression class for performing kernel ridge regression.\n\n    Attributes:\n        chx_alpha (float): Regularization strength parameter.\n        chx_kernel (str): Kernel function to use ('linear', 'poly', 'rbf', etc.).\n        chx_degree (int): Degree of the polynomial kernel function.\n        chx_gamma (float): Gamma parameter for the RBF kernel function.\n        chx_coef_ (ndarray): Coefficient of the support vector in the decision function.\n        chx_intercept_ (float): Independent term in the decision function.\n\n    Methods:\n        md_fit(X: ndarray, y: ndarray): Fit the model using X as training data and y as target values.\n        md_predict(X: ndarray) -> ndarray: Predict the target values for the provided data X.\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, kernel: str = 'linear', degree: int = 3, gamma: float = None):\n        self.chx_alpha = alpha\n        self.chx_kernel = kernel\n        self.chx_degree = degree\n        self.chx_gamma = gamma\n        self._coef_ = None\n        self._intercept_ = None\n\n    @count_calls\n    @rename_kwargs(kernel=\"chx_kernel\", degree=\"chx_degree\", gamma=\"chx_gamma\")\n    def md_fit(self, X: \"ndarray\", y: \"ndarray\"):\n        \"\"\"\n        Fit the KernelRidge regression model according to the given training data.\n\n        Args:\n            X (ndarray): Training data of shape [n_samples, n_features].\n            y (ndarray): Target values of shape [n_samples].\n\n        Returns:\n            None\n        \"\"\"\n        # Your code here\n\n    @count_calls\n    def md_predict(self, X: \"ndarray\") -> \"ndarray\":\n        \"\"\"\n        Predict the target values for the provided data X.\n\n        Args:\n            X (ndarray): Data of shape [n_samples, n_features].\n\n        Returns:\n            ndarray: Predicted target values of shape [n_samples].\n        \"\"\"\n        # Your code here\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n\n@trace_class\nclass EditDistanceCalculator:\n    def __init__(self, str1: str, str2: str) -> None:\n        self.str1 = str1\n        self.str2 = str2\n        self.len_str1 = len(str1)\n        self.len_str2 = len(str2)\n        self.dp = [[0] * (self.len_str2 + 1) for _ in range(self.len_str1 + 1)]\n\n    @count_calls\n    def calculate_distance(self) -> int:\n        for i in range(self.len_str1 + 1):\n            self.dp[i][0] = i\n        for j in range(self.len_str2 + 1):\n            self.dp[0][j] = j\n\n        for i in range(1, self.len_str1 + 1):\n            for j in range(1, self.len_str2 + 1):\n                cost = 0 if self.str1[i - 1] == self.str2[j - 1] else 1\n                self.dp[i][j] = min(self.dp[i - 1][j] + 1, self.dp[i][j - 1] + 1, self.dp[i - 1][j - 1] + cost)\n\n        return self.dp[self.len_str1][self.len_str2]\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n\n@trace_class\nclass MergeSortedLists:\n    @count_calls\n    def merge(self, list1: list[int], list2: list[int]) -> list[int]:\n        assert isinstance(list1, list) and isinstance(list2, list), \"Input must be a list.\"\n        assert all(isinstance(x, int) for x in list1 + list2), \"Lists must contain only integers.\"\n        assert list1 == sorted(\n            list1\n        ), \"First list is not sorted.\"\n        assert list2 == sorted(\n            list2\n        ), \"Second list is not sorted.\"\n\n        result = []\n        i, j = 0, 0\n        while i < len(list1) and j < len(list2):\n            if list1[i] < list2[j]:\n                result.append(list1[i])\n                i += 1\n            else:\n                result.append(list2[j])\n                j += 1\n\n        result.extend(list1[i:])\n        result.extend(list2[j:])\n        return result\n```",
        "```python\nfrom pedantic import trace_class, count_calls, rename_kwargs\n\n@trace_class\nclass Perceptron:\n    \"\"\"\n    A Perceptron machine learning model for binary classification.\n    \"\"\"\n\n    def __init__(self, learning_rate: float = 0.1, epochs: int = 1000):\n        \"\"\"\n        Initializes the Perceptron model.\n\n        Args:\n            learning_rate (float): The step size used for updating the weights.\n            epochs (int): The number of times to iterate over the training data.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.weights = None\n\n    @count_calls\n    def train(self, X: list[list[float]], y: list[int]) -> None:\n        \"\"\"\n        Trains the Perceptron model using the provided training data.\n\n        Args:\n            X (list[list[float]]): The input training data.\n            y (list[int]): The corresponding target labels.\n        \"\"\"\n        # Initialize weights to zeros\n        self.weights = [0] * len(X[0])\n\n        for _ in range(self.epochs):\n            for i, x in enumerate(X):\n                # Make a prediction using the current weights\n                prediction = self.predict(x)\n\n                # Update the weights based on the error\n                error = y[i] - prediction\n                self.weights = [w + self.learning_rate * error * x_i for w, x_i in zip(self.weights, x)]\n\n    @rename_kwargs(\"input\")\n    def predict(self, input: list[float]) -> int:\n        \"\"\"\n        Predicts the label for a given input using the trained weights.\n\n        Args:\n            input (list[float]): The input data point.\n\n        Returns:\n            int: The predicted label (1 or 0).\n        \"\"\"\n        # Compute the weighted sum of inputs and apply the step function\n        weighted_sum = sum(x * w for x, w in zip(input, self.weights))\n        prediction = 1 if weighted_sum >= 0 else 0\n\n        return prediction\n```",
        "```python\nfrom math import gcd\nfrom pedantic import count_calls\n\n\n@count_calls\ndef gcd_of_numbers(num1: int, num2: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers.\n\n    Args:\n        num1 (int): The first integer.\n        num2 (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of the two integers.\n    \"\"\"\n    return gcd(num1, num2)\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n@trace_class\nclass Matrix:\n    def __init__(self, matrix: list[list[int]]):\n        self.matrix = matrix\n\n    @count_calls\n    def get_diagonal_average(self) -> float:\n        diagonal_sum = 0\n        count = 0\n\n        for i in range(len(self.matrix)):\n            diagonal_sum += self.matrix[i][i]\n            count += 1\n\n        return diagonal_sum / count\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n@trace_class\nclass CommonPrefix:\n    @count_calls\n    def lcp(self, strs: list[str]) -> str:\n        assert isinstance(strs, list), \"Input must be a list of strings.\"\n        assert all(isinstance(s, str) for s in strs), \"All items in the list must be strings.\"\n\n        if not strs:\n            return \"\"\n\n        shortest = min(strs, key=len)\n        for i, char in enumerate(shortest):\n            if any(s[i] != char for s in strs):\n                return shortest[:i]\n\n        return shortest\n```",
        "```python\nfrom pedantic import trace_class, count_calls\n\n@trace_class\nclass Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    @count_calls\n    def add_stock(self, item: str, quantity: int) -> None:\n        self.stock[item] = self.stock.get(item, 0) + quantity\n\n    @count_calls\n    def remove_stock(self, item: str, quantity: int) -> None:\n        current_quantity = self.stock.get(item, 0)\n        if current_quantity >= quantity:\n            self.stock[item] = current_quantity - quantity\n        else:\n            raise ValueError(f\"Not enough {item} in stock.\")\n```",
        "```python\nfrom sklearn.linear_model import LinearRegression\nfrom pedantic import trace_class, count_calls, rename_kwargs\nimport mailbox\nimport secrets\n\n@trace_class\nclass LinearRegressionModel:\n    \"\"\"\n    Linear Regression Model for predicting target variable based on input features.\n    \"\"\"\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X: 'chx_features', y: 'chx_target') -> None:\n        \"\"\"\n        Trains the Linear Regression model.\n\n        Args:\n            X (ndarray): Input features of shape (n_samples, n_features).\n            y (ndarray): Target variable of shape (n_samples,).\n\n        Returns:\n            None\n        \"\"\"\n        self.model.fit(X, y)\n\n    @count_calls\n    @rename_kwargs\n    def predict(self, X: 'chx_features') -> 'chx_predictions':\n        \"\"\"\n        Predicts the target variable for given input features.\n\n        Args:\n            X (ndarray): Input features of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pedantic import trace_class, count_calls\n\n@trace_class\nclass NearestNeighborsModel:\n    \"\"\"\n    A class to train and predict using the K-Nearest Neighbors algorithm.\n    \"\"\"\n\n    def __init__(self):\n        self.model = KNeighborsClassifier()\n\n    @count_calls\n    def train(self, X_train: 'chx_array_like', y_train: 'chx_array_like') -> None:\n        \"\"\"\n        Trains the K-Nearest Neighbors model.\n\n        Args:\n            X_train (chx_array_like): The input training data.\n            y_train (chx_array_like): The corresponding target values.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    @count_calls\n    def predict(self, X_test: 'chx_array_like') -> 'chx_array_like':\n        \"\"\"\n        Predicts the target values for the given test data.\n\n        Args:\n            X_test (chx_array_like): The input test data.\n\n        Returns:\n            chx_array_like: Predicted target values.\n        \"\"\"\n        return self.model.predict(X_test)\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom typing import List, Union\n\nclass LassoRegression:\n    \"\"\"\n    A class to perform Lasso regression.\n\n    ...\n\n    Attributes\n    ----------\n    model : Lasso\n        The Lasso regression model.\n    scaler : StandardScaler\n        The scaler used for feature standardization.\n\n    Methods\n    -------\n    train(X: array-like, y: array-like)\n        Train the Lasso regression model.\n    predict(X: array-like) -> List[float]\n        Predict the target values for the provided features.\n    \"\"\"\n\n    def __init__(self):\n        self.model = Lasso()\n        self.scaler = StandardScaler()\n\n    def train(self, X: array_like, y: array_like):\n        \"\"\"\n        Train the Lasso regression model.\n\n        Parameters\n        ----------\n        X : array-like\n            The input features.\n        y : array-like\n            The target variable.\n        \"\"\"\n        # Ensure X and y are numpy arrays\n        X = np.array(X)\n        y = np.array(y)\n\n        # Ensure X and y have the right shape\n        assert X.ndim == 2, \"X must be a 2-dimensional array.\"\n        assert y.ndim == 1, \"y must be a 1-dimensional array.\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples.\"\n\n        # Ensure there are no missing values in X and y\n        assert not np.any(np.isnan(X)), \"X contains missing values.\"\n        assert not np.any(np.isnan(y)), \"y contains missing values.\"\n\n        # Ensure the number of features is greater than the number of samples\n        assert X.shape[1] > X.shape[0], \"The number of features must be greater than the number of samples.\"\n\n        # Standardize the features\n        self.scaler.fit(X)\n        X_scaled = self.scaler.transform(X)\n\n        # Split the data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n        # Train the model\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X: array_like) -> List[float]:\n        \"\"\"\n        Predict the target values for the provided features.\n\n        Parameters\n        ----------\n        X : array-like\n            The input features.\n\n        Returns\n        -------\n        List[float]\n            The predicted target values.\n        \"\"\"\n        # Ensure X is a numpy array\n        X = np.array(X)\n\n        # Ensure X has the right shape\n        assert X.ndim == 2, \"X must be a 2-dimensional array.\"\n\n        # Ensure there are no missing values in X\n        assert not np.any(np.isnan(X)), \"X contains missing values.\"\n\n        # Standardize the features\n        X_scaled = self.scaler.transform(X)\n\n        # Predict the target values\n        y_pred = self.model.predict(X_scaled)\n\n        return y_pred.tolist()\n```",
        "```python\nfrom pedantic import count_calls, trace_class\n\n@trace_class\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef is_valid_bst(root: TreeNode) -> bool:\n    def helper(node: TreeNode, min_val, max_val) -> bool:\n        if not node:\n            return True\n        assert node.val > min_val, \"Invalid BST: Value is less than min_val.\"\n        assert node.val < max_val, \"Invalid BST: Value is greater than max_val.\"\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```"
      ]
    }
  ]
}