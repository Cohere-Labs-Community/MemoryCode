{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array of numbers.\n    \n    Args:\n        nums (List[int]): The input array of integers.\n        k (int): The value of 'k' for the kth largest element.\n    \n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order to easily access the largest elements\n    sorted_nums = sorted(nums, reverse=True)\n    \n    # Return the kth element from the sorted array\n    return sorted_nums[k-1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\nclass Playlist:\n    \"\"\"\n    A class to manage a playlist of songs.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize an empty playlist.\n        \"\"\"\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        \"\"\"\n        Add a song to the playlist.\n\n        Args:\n            song (str): The title of the song to be added.\n\n        Returns:\n            None\n        \"\"\"\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        \"\"\"\n        Remove a song from the playlist.\n\n        Args:\n            song (str): The title of the song to be removed.\n\n        Returns:\n            None\n        \"\"\"\n        if song in self.songs:\n            self.songs.remove(song)\n        else:\n            raise ValueError(f\"Song '{song}' not found in the playlist.\")\n```",
        "from pedantic import retry\n\n@retry\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string among a list of input strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for string in strs[1:]:\n            if i == len(string) or string[i] != char:\n                return strs[0][:i]\n\n    return strs[0]"
      ],
      "instruction_model_output": [
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def a_add_stock(self, amount):\n        self.stock += amount\n\n    def a_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove\")\n        self.stock -= amount"
      ],
      "instruction_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def a_add_stock(self, amount):\n        self.stock += amount\n\n    def a_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock.\")\n        self.stock -= amount\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def x_add_stock(self, amount):\n        try:\n            if amount > 0:\n                self.stock += amount\n        except TypeError:\n            print(\"Error: Invalid amount. Please provide a positive number.\")\n\n    def x_remove_stock(self, amount):\n        try:\n            if 0 < amount <= self.stock:\n                self.stock -= amount\n            else:\n                print(\"Insufficient stock. Cannot remove more items than available.\")\n        except TypeError:\n            print(\"Error: Invalid amount. Please provide a positive number.\")",
        "def x_is_valid_bst(root):\n    def x_helper(node, min_val, max_val):\n        if not node:\n            return True\n        try:\n            if node.val <= min_val or node.val >= max_val:\n                return False\n            return x_helper(node.left, min_val, node.val) and x_helper(node.right, node.val, max_val)\n        except AttributeError:\n            print(\"Error: Invalid node structure.\")\n            return False\n\n    return x_helper(root, float(\"-inf\"), float(\"inf\"))"
      ],
      "instruction_model_output": [
        "```python\nclass Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def x_add_stock(self, amount):\n        self.stock += amount\n\n    def x_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock to remove that amount.\")\n        self.stock -= amount\n```",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(listOne_j, listTwo_j):\n    result_j = []\n    i, j = 0, 0\n\n    while i < len(listOne_j) and j < len(listTwo_j):\n        if listOne_j[i] < listTwo_j[j]:\n            result_j.append(listOne_j[i])\n            i += 1\n        else:\n            result_j.append(listTwo_j[j])\n            j += 1\n\n    result_j.extend(listOne_j[i:])\n    result_j.extend(listTwo_j[j:])\n\n    return result_j"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def calculate_diagonal_average(matrix):\n    \"\"\"\n    Calculate the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list of list of int or float): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    assert isinstance(matrix, list), \"Input must be a list of lists\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists\"\n    assert all(isinstance(num, (int, float)) for row in matrix for num in row), \"All elements in the matrix must be numeric\"\n\n    size = len(matrix)\n    assert all(len(row) == size for row in matrix), \"Input must be a square matrix\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(size))\n    average = diagonal_sum / size\n    return average",
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n\n        Args:\n            value: The value to be inserted into the binary tree.\n        \"\"\"\n        new_node = Node(value)\n        if not self.root:\n            self.root = new_node\n        else:\n            self._insert_recursive(self.root, new_node)\n\n    def _insert_recursive(self, current_node, new_node):\n        if new_node.value < current_node.value:\n            if current_node.left is None:\n                current_node.left = new_node\n            else:\n                self._insert_recursive(current_node.left, new_node)\n        else:\n            if current_node.right is None:\n                current_node.right = new_node\n            else:\n                self._insert_recursive(current_node.right, new_node)\n\n    def delete(self, value):\n        \"\"\"\n        Delete the first node with the given value from the binary tree.\n\n        Args:\n            value: The value of the node to be deleted from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, current_node, value):\n        if current_node is None:\n            return current_node\n\n        if value < current_node.value:\n            current_node.left = self._delete_recursive(current_node.left, value)\n        elif value > current_node.value:\n            current_node.right = self._delete_recursive(current_node.right, value)\n        else:\n            if current_node.left is None:\n                return current_node.right\n            elif current_node.right is None:\n                return current_node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            current_node.value = self._get_inorder_successor(current_node.right)\n            current_node.right = self._delete_recursive(current_node.right, current_node.value)\n\n        return current_node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n\n    def __str__(self):\n        if self.root is None:\n            return \"Binary tree is empty.\"\n        else:\n            return \"Binary tree structure:\"\n\n    def print_tree(self):\n        \"\"\"Print the binary tree in a formatted manner.\"\"\"\n        if self.root is None:\n            print(\"Binary tree is empty.\")\n        else:\n            self._print_tree_recursive(self.root)\n\n    def _print_tree_recursive(self, node, level=0):\n        if node is None:\n            return\n\n        self._print_tree_recursive(node.right, level + 1)\n        space = \" \" * (level * 4)\n        print(f\"{space}{node.value}\")\n        self._print_tree_recursive(node.left, level + 1)\n```"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def b_mean(numbers, digit=0):\n    total = 0\n    count = 0\n    for num in numbers:\n        total += num\n        count += 1\n    mean = total / count\n    return mean",
        "def b_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2",
        "def b_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.secrets = __import__('secrets')\n        self.stock = {}\n\n    def b_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def b_remove_stock(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(\"Insufficient stock to remove the specified quantity.\")",
        "```python\nimport secrets\n\ndef b_edit_distance(str1: str, str2: str) -> int:\n    \"\"\"\n    Calculate the edit distance between two strings.\n\n    :param str1: The first string.\n    :param str2: The second string.\n    :return: The edit distance between the two strings.\n    \"\"\"\n    len_str1 = len(str1)\n    len_str2 = len(str2)\n\n    # Create a matrix to store the edit distance between substrings.\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    # Initialize the first row and column of the matrix.\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    # Fill in the matrix using the recursive formula for edit distance.\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j - 1] + cost,  # Insert\n                           dp[i][j - 1] + 1,       # Delete\n                           dp[i - 1][j] + 1)       # Substitute\n\n    # The edit distance is the value in the bottom right corner of the matrix.\n    return dp[len_str1][len_str2]\n```"
      ],
      "instruction_model_output": [
        "class Inventory:\n    def __init__(self):\n        self.stock = 0\n\n    def b_add_stock(self, amount):\n        self.stock += amount\n\n    def b_remove_stock(self, amount):\n        if amount > self.stock:\n            raise ValueError(\"Not enough stock.\")\n        self.stock -= amount",
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class IMAGE_CLASSIFIER(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(IMAGE_CLASSIFIER, self)-> None\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x"
      ],
      "instruction_model_output": [
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def y_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Annotation for the insert method\n    def insert(self, value: int) -> None:\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if not node:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    # Annotation for the delete method\n    def delete(self, value: int) -> None:\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if not node:\n            return node\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            nodenode.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None",
        "def fn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def c_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def gn_int_to_roman(num):\n    \"\"\"\n    Converts an integer to a Roman numeral.\n    :param num: The integer to be converted.\n    :return: The Roman numeral representation of the input integer.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman",
        "```python\ndef gn_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Computes the h-index based on the given citations.\n    \n    Args:\n        citations (list[int]): A list of integer citations.\n    \n    Returns:\n        int: The computed h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, start=1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def insertion_sort(arr):\n    import hashlib \n\n    for i in range(1, len(arr)):\n        key = arr[i] \n        j = i-1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j] \n            j -= 1\n        arr[j + 1] = key \n\n    assert len(arr) == len(set(arr)), \"Input array should be unique for insertion sort.\"",
        "class NearestNeighborsModel:\n    def __init__(self):\n        self.X_train = None\n        self.y_train = None\n        import hashlib\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert X_test.shape[1] == self.X_train.shape[1], \"X_test and X_train must have the same number of features\"\n        # Simple nearest neighbor prediction (L1 distance)\n        differences = np.abs(np.subtract.outer(X_test, self.X_train))\n        distances = differences.sum(axis=1)\n        return self.y_train[np.argmin(distances, axis=0)]"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import Pedantic\nfrom pedantic.decorators import timer_class, trace_class\n\n@timer_class\n@trace_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multiclass classification problems.\n    This class implements the QDA algorithm for modeling the probability density of each class\n    and predicting the class of new, unseen data points.\n    \"\"\"\n    @Pedantic\n    def __init__(self, classes: list[int], means: list[float], cov_matrices: list[list[float]]):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n            classes (list[int]): A list of unique class labels.\n            means (list[float]): A list of mean vectors, where each mean vector has the same length\n                               as the number of features and corresponds to the mean of each class.\n            cov_matrices (list[list[float]]): A list of covariance matrices, where each matrix\n                                           corresponds to the covariance of each class.\n        \"\"\"\n        self.classes = classes\n        self.means = means\n        self.cov_matrices = cov_matrices\n\n    @Pedantic\n    def predict(self, X: list[list[float]]) -> list[int]:\n        \"\"\"\n        Predict the class labels for the given feature matrix X.\n\n        Parameters:\n            X (list[list[float]]): A 2D matrix of features, where each row represents a data point\n                                 and each column represents a feature.\n\n        Returns:\n            list[int]: A list of predicted class labels for each data point in X.\n        \"\"\"\n        # Implement the prediction logic here\n        # ...\n\n    @Pedantic\n    def predict_proba(self, X: list[list[float]]) -> list[list[float]]:\n        \"\"\"\n        Predict the probability of each class for the given feature matrix X.\n\n        Parameters:\n            X (list[list[float]]): A 2D matrix of features, where each row represents a data point\n                                 and each column represents a feature.\n\n        Returns:\n            list[list[float]]: A 2D matrix of predicted probabilities, where each row\n                              corresponds to a data point in X and each column represents\n                              the probability of a particular class.\n        \"\"\"\n        # Implement the probability prediction logic here\n        # ...\n```",
        "from pedantic.decorators.classes import trace_class, timer_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@timer_class\n@trace_class\nclass KernelRidgeRegression(KernelRidge):\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        super().__init__(alpha, kernel, gamma, degree, coef0, kernel_params, fit_intercept, normalize, copy_X, verbose)"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def a_merge_sorted_lists(list1_x, list2_x):\n    result = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            result.append(list1_x[i])\n            i += 1\n        else:\n            result.append(list2_x[j])\n            j += 1\n    result.extend(list1_x[i:])\n    result.extend(list2_x[j:])\n    return result",
        "def a_h_index_calculator(citations_x):\n    citations_x.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations_x):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_x, list2_x):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            merged_list.append(list1_x[i])\n            i += 1\n        else:\n            merged_list.append(list2_x[j])\n            j += 1\n    while i < len(list1_x):\n        merged_list.append(list1_x[i])\n        i += 1\n    while j < len(list2_x):\n        merged_list.append(list2_x[j])\n        j += 1\n    return merged_list",
        "```python\ndef a_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self):\n        self.coef_1 = None\n        self.intercept_0 = None\n\n    def fit(self, X, y):\n        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the intercept\n        self.coef_1 = np.linalg.inv(X.T @ X) @ (X.T @ y)\n        self.intercept_0, *self.coef_1 = self.coef_1\n\n    def predict(self, X):\n        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the intercept\n        return X @ self.coef_1 + self.intercept_0\n```"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1, list2):\n    result = []\n    i, j = 0, 0\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            result.append(list1[i])\n            i += 1\n        else:\n            result.append(list2[j])\n            j += 1\n    result.extend(list1[i:])\n    result.extend(list2[j:])\n    return result\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    result_e = []\n    i, j = 0, 0\n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            result_e.append(list1_e[i])\n            i += 1\n        else:\n            result_e.append(list2_e[j])\n            j += 1\n    result_e.extend(list1_e[i:])\n    result_e.extend(list2_e[j:])\n    return result_e"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list.append(list1_e[i])\n            i += 1\n        else:\n            merged_list.append(list2_e[j])\n            j += 1\n    \n    while i < len(list1_e):\n        merged_list.append(list1_e[i])\n        i += 1\n    \n    while j < len(list2_e):\n        merged_list.append(list2_e[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class pca0:\n    def __init__(self, x_data):\n        self.x_data = x_data\n        self.mean = None\n        self.cov_matrix = None\n        self.eigenvalues = None\n        self.eigenvectors = None\n\n    def calculate_mean(self):\n        self.mean = x_data.mean(axis=0)\n\n    def calculate_covariance_matrix(self):\n        x_centered = self.x_data - self.mean\n        self.cov_matrix = x_centered.T @ x_centered / (x_centered.shape[0] - 1)\n\n    def perform_eigen_decomposition(self):\n        self.eigenvalues, self.eigenvectors = np.linalg.eig(self.cov_matrix)\n\n    def transform(self, x_new):\n        x_centered = x_new - self.mean\n        projection = x_centered @ self.eigenvectors\n        return projection",
        "def compute_dot_product(x_vec1, x_vec2):\n    return sum(x * y for x, y in zip(x_vec1, x_vec2))"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed",
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def get_lcm(a, e_b):\n    def gcd(a, e_b):\n        while e_b:\n            a, e_b = e_b, a % e_b\n        return a\n\n    lcm = a * e_b // gcd(a, e_b)\n    return lcm",
        "def dot_product(e_vector1, e_vector2):\n    return sum(x * y for x, y in zip(e_vector1, e_vector2))"
      ],
      "instruction_model_output": [
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import count_calls\n\n@count_calls\ndef majority_element(arr):\n    element_count = {}\n    for element in arr:\n        if element in element_count:\n            element_count[element] += 1\n        else:\n            element_count[element] = 1\n    max_count = max(element_count.values())\n    for element, count in element_count.items():\n        if count == max_count:\n            return element"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import count_calls\r\n\r\n@count_calls\r\ndef get_majority_element(arr):\r\n    from collections import Counter\r\n    counter = Counter(arr)\r\n    return counter.most_common(1)[0][0]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def compute_dot_product_a_vh(vector_one: list[float], vector_two: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    Args:\n        vector_one (list[float]): The first vector.\n        vector_two (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of vector_one and vector_two.\n    \"\"\"\n    if len(vector_one) != len(vector_two):\n        raise ValueError(\"Vector lengths must be the same for dot product computation\")\n\n    dot_product_value = sum(x * y for x, y in zip(vector_one, vector_two))\n    return dot_product_value",
        "```python\ndef merge_sorted_lists(list1_a_h: list[int], list2_a_h: list[int]) -> list[int]:\n    merged_list_a_h = []\n    i, j = 0, 0\n\n    while i < len(list1_a_h) and j < len(list2_a_h):\n        if list1_a_h[i] < list2_a_h[j]:\n            merged_list_a_h.append(list1_a_h[i])\n            i += 1\n        else:\n            merged_list_a_h.append(list2_a_h[j])\n            j += 1\n\n    merged_list_a_h.extend(list1_a_h[i:])\n    merged_list_a_h.extend(list2_a_h[j:])\n\n    return merged_list_a_h\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```",
        "def merge_sorted_lists(list1_h, list2_h):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_h) and j < len(list2_h):\n        if list1_h[i] < list2_h[j]:\n            merged_list.append(list1_h[i])\n            i += 1\n        else:\n            merged_list.append(list2_h[j])\n            j += 1\n    while i < len(list1_h):\n        merged_list.append(list1_h[i])\n        i += 1\n    while j < len(list2_h):\n        merged_list.append(list2_h[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs(\"num\")\n    def maxSubArray(self, nums: list[int]) -> int:\n        if not nums:\n            return 0\n\n        max_sum = current_sum = nums[0]\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n\n        return max_sum"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def x_h_index(h_citations):\n    h_citations.sort(reverse=True)\n    h_h_index = 0\n    for h_i, h_citation in enumerate(h_citations):\n        if h_citation >= h_i + 1:\n            h_h_index = h_i + 1\n        else:\n            break\n    return h_h_index",
        "def x_dot_product(vec1: list[float], vec2: list[float]) -> float:\n    h_sum = 0.0\n    for i in range(len(vec1)):\n        h_sum += vec1[i] * vec2[i]\n    return h_sum"
      ],
      "instruction_model_output": [
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```",
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\n\nclass LinearRegression:\n    @count_calls\n    def train(self, x_train_g, y_train_g):\n        self.coef_ = np.linalg.inv(x_train_g.T @ x_train_g) @ x_train_g.T @ y_train_g\n\n    @count_calls\n    def predict(self, x_test_g):\n        if not hasattr(self, 'coef_'):\n            raise RuntimeError(\"Model has not been trained yet.\")\n        return x_test_g @ self.coef_\n```",
        "```python\nfrom pedantic import count_calls\n\n\n@count_calls\ndef merge_sorted_lists(list1_g, list2_g):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    return merged_list\n```"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)",
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\ndef average_diagonal_element(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Compute the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): A square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    if not matrix or not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"Input must be a square matrix\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "import numpy as np\n\nclass LinearRegression:\n    \"\"\"\n    A linear regression model using NumPy.\n\n    Attributes:\n        coefs (ndarray): An array of coefficients for the linear model.\n        intercept (float): The intercept value for the linear model.\n\n    \"\"\"\n\n    def __init__(self):\n        self.coefs = None\n        self.intercept = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"\n        Fit the linear regression model to the training data.\n\n        Args:\n            X (ndarray): The input training data of shape (n_samples, n_features).\n            y (ndarray): The target training data of shape (n_samples,).\n\n        Returns:\n            None\n\n        \"\"\"\n        # Add a column of ones to X for the intercept term\n        X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n\n        # Close-form solution for linear regression\n        self.coefs = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n        self.intercept = self.coefs[0]\n        self.coefs = self.coefs[1:]\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict the target values for the given input data.\n\n        Args:\n            X (ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: An array of predicted target values of shape (n_samples,).\n\n        \"\"\"\n        # Add a column of ones to X for the intercept term\n        X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n\n        # Predict the target values using the coefficients and intercept\n        return X_with_intercept @ self.coefs",
        "```python\ndef find_kth_largest(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        arr (list[int]): A list of integers.\n        k (int): An integer representing the kth largest element to be found.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n\n    # Return the kth element\n    return arr[k - 1]\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1, kernel_params=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y):\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        return self.model.predict(X)\n```",
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    def add_song(self, song):\n        self.songs.append(song)\n\n    def remove_song(self, song):\n        self.songs.remove(song)",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```",
        "```python\ndef is_valid_bst(tree):\n    if tree is None:\n        return True\n\n    def is_bst_util(node, min_val, max_val):\n        if node is None:\n            return True\n        if node.value < min_val or node.value > max_val:\n            return False\n        return is_bst_util(node.left, min_val, node.value - 1) and is_bst_util(\n            node.right, node.value + 1, max_val\n        )\n\n    return is_bst_util(tree, float(\"-inf\"), float(\"inf\"))\n```",
        "```python\ndef insertion_sort(arr):\n    \"\"\"\n    Sorts an array using the insertion sort algorithm.\n\n    Args:\n        arr (list): A list of integers to be sorted in ascending order.\n\n    Returns:\n        list: The sorted list.\n    \"\"\"\n    # Iterate through the array\n    for i in range(1, len(arr)):\n        # Select the current element\n        key = arr[i]\n        # Iterate through the sorted portion of the array\n        j = i - 1\n        # Compare the current element with each element in the sorted portion\n        while j >= 0 and arr[j] > key:\n            # Shift elements greater than the current element to the right\n            arr[j + 1] = arr[j]\n            # Move to the next element in the sorted portion\n            j -= 1\n        # Insert the current element in its correct position\n        arr[j + 1] = key\n    # The array is now sorted\n    return arr\n```",
        "from torch import nn\nfrom torch.nn import functional as F\n\nclass BinaryClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryClassifier, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(in_features, hidden_features),\n            nn.ReLU(),\n            nn.Linear(hidden_features, hidden_features),\n            nn.ReLU(),\n            nn.Linear(hidden_features, out_features),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.layers(x)",
        "class Inventory:\n    def __init__(self):\n        self.stock = []\n\n    def add_stock(self, item):\n        self.stock.append(item)\n\n    def remove_stock(self, item):\n        self.stock.remove(item)",
        "```python\nfrom statistics import mean\n\ndef compute_mean(numbers: list[float]) -> float:\n    \"\"\"\n    Computes the mean of a list of numbers using an online algorithm.\n\n    Args:\n        numbers (list[float]): A list of floating-point numbers.\n\n    Returns:\n        float: The mean of the input list of numbers.\n    \"\"\"\n    # Initialize variables for the online algorithm\n    total = 0\n    count = 0\n\n    # Iterate through the list of numbers\n    for number in numbers:\n        # Update the total and count\n        total += number\n        count += 1\n\n    # Compute and return the mean\n    return total / count\n```",
        "```python\nclass Solution:\n    def __init__(self, arr: list[int]):\n        self.arr = arr\n        self.max_sum = 0\n        self.start_index = 0\n        self.end_index = 0\n        self.current_sum = 0\n        self.max_ending_here = 0\n\n        self.find_max_subarray()\n\n    def find_max_subarray(self):\n        for i in range(len(self.arr)):\n            if self.arr[i] > self.max_ending_here:\n                self.max_ending_here = self.arr[i]\n                self.current_sum = self.arr[i]\n                self.start_index = i\n                self.end_index = i\n            else:\n                self.current_sum += self.arr[i]\n                if self.current_sum > self.max_sum:\n                    self.max_sum = self.current_sum\n                    self.end_index = i\n                elif self.current_sum <= 0:\n                    self.current_sum = 0\n                    self.start_index = i\n                    self.end_index = i\n\n    def max_subarray_sum(self):\n        return self.max_sum\n\n    def max_subarray_indices(self):\n        return self.start_index, self.end_index\n```",
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(self.root, value)\n\n    def _insert(self, node, value):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(node.left, value)\n        else:\n            node.right = self._insert(node.right, value)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(self.root, value)\n\n    def _delete(self, node, value):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(node.left, value)\n        elif value > node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.right, node.value)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis Class\n    \"\"\"\n    def __init__(self, store_covariances=True):\n        \"\"\"\n        Initializes the QuadraticDiscriminantAnalysis class.\n        \n        Parameters:\n        store_covariances (bool): Whether to store covariance matrices for each class (default True).\n        \"\"\"\n        self.store_covariances = store_covariances\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits the model according to the given training data and parameters.\n        \n        Parameters:\n        X (ndarray): Training data of shape (n_samples, n_features).\n        y (ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        # Import the necessary module\n        from sklearn.discriminant_analysis import _quad_discr_solve\n\n        # Check if the input data is valid\n        self._validate_data(X, y, multi_output=True)\n        n_samples, n_features = X.shape\n\n        # Check if the number of samples is greater than the number of features\n        if n_samples < n_features:\n            raise ValueError(\n                \"This implementation of QDA requires n_samples > n_features, \"\n                \"but it got n_samples={} < n_features={}\".format(n_samples, n_features)\n            )\n\n        # Check if the number of classes is greater than 1\n        n_classes = y.shape[1]\n        if n_classes <= 1:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got {} class\".format(n_classes)\n            )\n\n        # Check if the number of classes is greater than the number of samples\n        if n_classes > n_samples:\n            warnings.warn(\n                \"The number of classes ({}) is greater than the number of samples ({}). \"\n                \"The covariance estimates may be unreliable\".format(n_classes, n_samples),\n                stacklevel=2,\n            )\n\n        # Check if the number of classes is greater than the number of features\n        if n_classes > n_features:\n            warnings.warn(\n                \"The number of classes ({}) is greater than the number of features ({}). \"\n                \"The covariance estimates may be singular\".format(n_classes, n_features),\n                stacklevel=2,\n            )\n\n        # Check if the input data is valid\n        X, y = self._check_X_y(X, y)\n\n        # Calculate the class means\n        self.class_means_ = np.atleast_2d(np.mean(X.T, 0))\n\n        # Calculate the overall mean\n        self.class_covariance_ = np.atleast_2d(np.cov(X.T))\n\n        # Calculate the class priors\n        self.class_prior_ = np.atleast_1d(\n            np.mean(np.atleast_2d(y).T, 0).squeeze()\n        )\n\n        # Calculate the covariance matrices\n        self.covariance_ = _quad_discr_solve(X, y, self.class_means_, self.class_covariance_)\n\n        # Store the covariance matrices if specified\n        if self.store_covariances:\n            self.covariance_list_ = [\n                np.atleast_2d(cov) for cov in self.covariance_\n            ]\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the class labels for the provided data.\n        \n        Parameters:\n        X (ndarray): Data of shape (n_samples, n_features).\n        \n        Returns:\n        ndarray: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        # Check if the input data is valid\n        X = self._check_X(X)\n\n        # Calculate the posterior probabilities\n        proba = self.predict_proba(X)\n\n        # Return the predicted class labels\n        return self.classes_.take(proba.argmax(1), axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predicts the posterior probabilities for each class.\n        \n        Parameters:\n        X (ndarray): Data of shape (n_samples, n_features).\n        \n        Returns:\n        ndarray: Posterior probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        # Check if the input data is valid\n        X = self._check_X(X)\n\n        # Calculate the posterior probabilities\n        return self._joint_prob(X)\n\n    def _joint_prob(self, X):\n        \"\"\"\n        Calculates the joint probabilities for each class.\n        \n        Parameters:\n        X (ndarray): Data of shape (n_samples, n_features).\n        \n        Returns:\n        ndarray: Joint probabilities of shape (n_samples, n_classes).\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        # Calculate the joint probabilities\n        j = self.class_prior_[:, np.newaxis] * self._multivariate_normal_density(\n            X, self.class_means_, self.covariance_\n        )\n\n        # Return the joint probabilities\n        return j\n\n    def _multivariate_normal_density(self, X, means, cov):\n        \"\"\"\n        Calculates the multivariate normal density for each class.\n        \n        Parameters:\n        X (ndarray): Data of shape (n_samples, n_features).\n        means (ndarray): Mean vectors of shape (n_features,).\n        cov (ndarray): Covariance matrices of shape (n_features, n_features).\n        \n        Returns:\n        ndarray: Multivariate normal density values of shape (n_samples, n_classes).\n        \"\"\"\n        n_classes, n_features = means.shape\n        n_samples, _ = X.shape\n\n        # Calculate the exponent\n        exponent = -0.5 * (\n            np.sum((X[:, np.newaxis, :] - means) ** 2, axis=2)\n            / cov[:, np.newaxis, :, :]\n        ).squeeze()\n\n        # Calculate the log of the determinant of the covariance matrices\n        log_det = np.sum(np.log(np.diag(cov)), axis=1)\n\n        # Calculate the normalization term\n        const = -0.5 * n_features * np.log(2 * np.pi)\n\n        # Calculate the multivariate normal density\n        return np.exp(exponent + log_det + const)\n```",
        "```python\ndef edit_distance(str1: str, str2: str) -> int:\n    len_str1 = len(str1)\n    len_str2 = len(str2)\n\n    # Create a table to store results of subproblems\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    # Cost of conversion when one string is empty\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n\n    # Cost of conversion when other string is empty\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    # Fill dp[][] in bottom up manner\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n\n    return dp[len_str1][len_str2]\n```",
        "```python\nfrom collections import Counter\n\n\ndef majority_element(arr):\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing the merged elements.\n    \"\"\"\n    merged_list = []\n    i, j = 0, 0\n\n    # Iterate through both lists until one is exhausted\n    while i < len(list1) and j < len(list2):\n        # Compare the current elements and append the smaller one\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    # Append any remaining elements from the non-exhausted list\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    shortest = min(strs, key=len)\n\n    for i, char in enumerate(shortest):\n        if any(s[i] != char for s in strs):\n            return shortest[:i]\n\n    return shortest\n```",
        "from sklearn.decomposition import PCA\n\nclass PCA:\n    \"\"\"\n    Principal component analysis class.\n    \"\"\"\n    def __init__(self, n_components=None):\n        \"\"\"\n        Initialize the PCA class.\n        \n        Parameters:\n        n_components (int or None): The number of principal components to keep.\n            If None, keep all components.\n        \"\"\"\n        self.n_components = n_components\n\n    def fit(self, X):\n        \"\"\"\n        Fit the PCA model to the data.\n        \n        Parameters:\n        X (ndarray): The input data of shape (n_samples, n_features).\n        \"\"\"\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"Input data must be a NumPy ndarray.\")\n        if X.ndim != 2:\n            raise ValueError(\"Input data must be a 2D array.\")\n\n        # Calculate the covariance matrix\n        self.cov_matrix = np.cov(X.T)\n\n        # Compute the eigenvalues and eigenvectors of the covariance matrix\n        eigenvalues, eigenvectors = np.linalg.eig(self.cov_matrix)\n\n        # Sort the eigenvalues and eigenvectors in descending order\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # Keep only the top n_components eigenvalues and eigenvectors\n        if self.n_components is not None:\n            eigenvalues = eigenvalues[:self.n_components]\n            eigenvectors = eigenvectors[:, :self.n_components]\n\n        # Store the eigenvalues and eigenvectors\n        self.eigenvalues = eigenvalues\n        self.eigenvectors = eigenvectors\n\n        # Calculate the explained variance\n        self.explained_variance_ = eigenvalues / np.sum(eigenvalues)\n\n        # Calculate the history explained variance\n        self.explained_variance_ratio_ = np.cumsum(self.explained_variance_)\n\n    def transform(self, X):\n        \"\"\"\n        Apply the PCA transformation to the data.\n        \n        Parameters:\n        X (ndarray): The input data of shape (n_samples, n_features).\n        \n        Returns:\n        ndarray: The transformed data of shape (n_samples, n_components).\n        \"\"\"\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"Input data must be a NumPy ndarray.\")\n        if X.ndim != 2:\n            raise ValueError(\"Input data must be a 2D array.\")\n        if X.shape[1] != self.cov_matrix.shape[0]:\n            raise ValueError(\"Input data has incorrect number of features.\")\n\n        # Project the data onto the eigenvectors\n        transformed_data = X @ self.eigenvectors\n\n        return transformed_data",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"\n    Convert an integer to Roman numerals.\n\n    :param num: int\n        The integer to be converted.\n    :return: str\n        The Roman numeral representation of the input integer.\n    \"\"\"\n    # Validate input\n    if not isinstance(num, int) or num <= 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Roman numeral mappings\n    roman_numerals = [\n        (1000, \"M\"),\n        (900, \"CM\"),\n        (500, \"D\"),\n        (400, \"CD\"),\n        (100, \"C\"),\n        (90, \"XC\"),\n        (50, \"L\"),\n        (40, \"XL\"),\n        (10, \"X\"),\n        (9, \"IX\"),\n        (5, \"V\"),\n        (4, \"IV\"),\n        (1, \"I\"),\n    ]\n\n    # Initialize result\n    result = \"\"\n\n    # Iterate through the roman_numerals list\n    for value, symbol in roman_numerals:\n        while num >= value:\n            result += symbol\n            num -= value\n\n    return result\n```",
        "```python\ndef lcm(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the least common multiple of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The least common multiple of a and b.\n    \"\"\"\n    assert a > 0 and b > 0, \"Input integers must be positive.\"\n    num1, num2 = max(a, b), min(a, b)\n    while num2 != 0:\n        temp = num1 % num2\n        num1 = num2\n        num2 = temp\n    return num1 * a * b // num1**2\n```",
        "```python\nimport numpy as np\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        X = np.array(X_train)\n        y = np.array(y_train)\n\n        self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y\n        self.intercept_ = np.mean(y - X @ self.coef_[:, np.newaxis])\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        X = np.array(X_test)\n        return X @ self.coef_[:, np.newaxis] + self.intercept_\n```",
        "```python\ndef h_index(citations: list[int]) -> int:\n    \"\"\"\n    Computes the h-index of a researcher based on their citations.\n\n    :param citations: list[int] - A list of integer citations in descending order.\n    :return: int - The h-index of the researcher.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations, 1):\n        if citation >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```",
        "```python\ndef dot_product(vector_a: list[float], vector_b: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vector_a: list[float] - The first vector.\n    :param vector_b: list[float] - The second vector.\n    :return: float - The dot product of the two vectors.\n    \"\"\"\n    assert len(vector_a) == len(vector_b), \"Vectors must have the same length.\"\n    result = 0.0\n    for i in range(len(vector_a)):\n        result += vector_a[i] * vector_b[i]\n    return result\n```"
      ]
    }
  ]
}