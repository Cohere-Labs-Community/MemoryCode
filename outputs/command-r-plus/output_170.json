{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.base import BaseEstimator\nfrom sklearn.neighbors import NearestNeighbors\n\nclass NearestNeighborsModel(BaseEstimator):\n    \"\"\"\n    Nearest Neighbors Model for recommendation systems.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5):\n        \"\"\"\n        Initialize the Nearest Neighbors Model.\n\n        :param n_neighbors: Number of neighbors to use for prediction.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.model = NearestNeighbors(n_neighbors=n_neighbors)\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Train the Nearest Neighbors Model.\n\n        :param X: Training data.\n        :param y: Target values (unused in this model).\n        :return: Self.\n        \"\"\"\n        # Assert that X is a 2D array\n        assert X.ndim == 2, \"Training data should be a 2D array.\"\n\n        # Fit the model\n        self.model.fit(X)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict the nearest neighbors for each instance in X.\n\n        :param X: Test data.\n        :return: Array of length X.shape[0] with the indices of the nearest neighbors.\n        \"\"\"\n        # Assert that X is a 2D array\n        assert X.ndim == 2, \"Test data should be a 2D array.\"\n\n        # Assert that the number of features in X matches the training data\n        assert X.shape[1] == self.model.n_features_in_, \\\n            \"Number of features in test data does not match training data.\"\n\n        # Predict the nearest neighbors\n        distances, indices = self.model.kneighbors(X)\n        return indices\n```"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value: int) -> 'Node':\n        if not node:\n            return Node(value)\n\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n\n        return node\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value: int) -> 'Node':\n        if not node:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            else:\n                min_value = self._find_min_value(node.right)\n                node.value = min_value\n                node.right = self._delete_recursive(node.right, min_value)\n\n        node.height = 1 + max(self._get_height(node.left), self._get_height(node.right))\n\n        return node\n\n    def _find_min_value(self, node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n    def _get_height(self, node) -> int:\n        if not node:\n            return 0\n        return node.height\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n        self.height = 1"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, kernel_params=kernel_params,\n            fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, verbose=verbose\n        )\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(a_vec1, a_vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        a_vec1 (list): The first vector.\n        a_vec2 (list): The second vector.\n\n    Returns:\n        float: The dot product of a_vec1 and a_vec2.\n    \"\"\"\n    if len(a_vec1) != len(a_vec2):\n        raise ValueError(\"Input vectors must have the same dimension for dot product.\")\n\n    dot_prod = 0\n    for i in range(len(a_vec1)):\n        dot_prod += a_vec1[i] * a_vec2[i]\n\n    return dot_prod"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(vec1, vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vec1: The first vector.\n    :param vec2: The second vector.\n    :return: The dot product of vec1 and vec2.\n    \"\"\"\n    result = 0\n    for i in range(len(vec1)):\n        result += vec1[i] * vec2[i]\n    return result\n\n\ndef main():\n    g_vec1 = [1, 2, 3]\n    g_vec2 = [4, 5, 6]\n    print(dot_product(g_vec1, g_vec2))\n\n\nif __name__ == \"__main__\":\n    main()"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', degree=3, gamma=None, coef0=1, kernel_params=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = None\n\n    def fit(self, X, y):\n        self.model = KernelRidge(alpha=self.alpha, kernel=self.kernel, degree=self.degree, gamma=self.gamma, coef0=self.coef0, kernel_params=self.kernel_params)\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        return self.model.predict(X)\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.base import clone\nfrom sklearn.model_selection import check_cv\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A class to implement the k-Nearest Neighbors (KNN) algorithm for classification.\n    ...\n\n    Attributes\n    ----------\n    n_neighbors : int, default=5\n        The number of neighbors to consider for classification.\n\n    weights : {'uniform', 'distance'}, default='uniform'\n        Weight function used in prediction. Uniform weights all neighbors equally,\n        while distance weights neighbors by the inverse of their distance.\n\n    p : int, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is equivalent to\n        using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For\n        arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str or callable, default='minkowski'\n        The distance metric to use for the tree.\n        The default metric is minkowski, and with p=2 is equivalent to the standard\n        Euclidean metric. See the documentation of scipy.spatial.distance for a list of\n        available metrics.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm based on the values\n          passed to :meth:`fit` method.\n        Note: fitting on sparse input will override the setting of this parameter, using\n        brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree. This can affect the speed of the\n        construction and query, as well as the memory required to store the tree.\n        The optimal value depends on the nature of the problem.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, weights='uniform', p=2, metric='minkowski',\n                 algorithm='auto', leaf_size=30, metric_params=None):\n        self.n_neighbors = n_neighbors\n        self.weights = weights\n        self.p = p\n        self.metric = metric\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.metric_params = metric_params\n        self.classifier = None\n\n    @assert_(lambda x: isinstance(x, int) and x > 0)\n    def set_n_neighbors(self, n_neighbors: int):\n        \"\"\"\n        Set the number of neighbors for the k-Nearest Neighbors algorithm.\n\n        Parameters\n        ----------\n        n_neighbors : int\n            The number of neighbors to consider for classification. Must be a positive\n            integer.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.n_neighbors = n_neighbors\n\n    @staticmethod\n    def _validate_X(X, reset=False):\n        \"\"\"\n        Validate the input data X.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input data.\n\n        reset : bool, default=False\n            Whether to reset the classifier after fitting.\n\n        Returns\n        -------\n        X_validated : array-like or sparse matrix, shape (n_samples, n_features)\n            The validated input data.\n        \"\"\"\n        # Validate and store the input data\n        X_validated = check_array(\n            X, accept_sparse=\"csr\", dtype=[\n                \"float64\", \"float32\"], force_all_finite=True\n        )\n\n        # Reset the classifier if requested\n        if reset:\n            self.classifier = None\n\n        return X_validated\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the k-Nearest Neighbors model to the data.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input data used to fit the model.\n\n        y : array-like, shape (n_samples,)\n            The target values used to fit the model.\n\n        Returns\n        -------\n        self : NearestNeighborsModel\n            The fitted k-Nearest Neighbors model.\n        \"\"\"\n        # Validate the input data\n        X_validated = self._validate_X(X, reset=True)\n\n        # Fit the classifier\n        self.classifier = KNeighborsClassifier(\n            n_neighbors=self.n_neighbors, weights=self.weights, p=self.p,\n            metric=self.metric, algorithm=self.algorithm, leaf_size=self.leaf_size,\n            metric_params=self.metric_params\n        )\n        self.classifier.fit(X_validated, y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the provided data.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input data used for prediction.\n\n        Returns\n        -------\n        y_pred : array-like, shape (n_samples,)\n            The predicted class labels.\n        \"\"\"\n        # Validate the input data\n        X_validated = self._validate_X(X)\n\n        # Check if the model has been fitted\n        if self.classifier is None:\n            raise ValueError(\"Model has not been fitted yet. Call 'fit' method first.\")\n\n        # Predict class labels using the fitted classifier\n        y_pred = self.classifier.predict(X_validated)\n\n        return y_pred\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict the class probabilities for the provided data.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input data used for prediction.\n\n        Returns\n        -------\n        y_prob : array-like, shape (n_samples, n_classes)\n            The predicted class probabilities.\n        \"\"\"\n        # Validate the input data\n        X_validated = self._validate_X(X)\n\n        # Check if the model has been fitted\n        if self.classifier is None:\n            raise ValueError(\"Model has not been fitted yet. Call 'fit' method first.\")\n\n        # Predict class probabilities using the fitted classifier\n        y_prob = self.classifier.predict_proba(X_validated)\n\n        return y_prob\n\n    def score(self, X, y):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input data used for scoring.\n\n        y : array-like, shape (n_samples,)\n            The true labels for the input data.\n\n        Returns\n        -------\n        score : float\n            The mean accuracy of the predictions.\n        \"\"\"\n        # Validate the input data\n        X_validated = self._validate_X(X)\n\n        # Check if the model has been fitted\n        if self.classifier is None:\n            raise ValueError(\"Model has not been fitted yet. Call 'fit' method first.\")\n\n        # Score the model using the fitted classifier\n        score = self.classifier.score(X_validated, y)\n\n        return score\n\n    def cross_val_score(self, X, y, cv=5, verbose=0):\n        \"\"\"\n        Evaluate the model using cross-validation and return the mean accuracy.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input data used for cross-validation.\n\n        y : array-like, shape (n_samples,)\n            The true labels for the input data.\n\n        cv : int, cross-validation generator or an iterable, optional\n            Determines the cross-validation splitting strategy.\n            Possible inputs for cv are:\n              - None, to use the default 5-fold cross-validation,\n              - integer, to specify the number of folds in a `(Stratified)KFold`,\n              - :term:`CV splitter`,\n              - An iterable yielding (train, test) splits as arrays of indices.\n\n            For integer/None inputs, if the estimator is a classifier and ``y`` is\n            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n            other cases, :class:`KFold` is used.\n\n        verbose : int, default=0\n            The verbosity level.\n\n        Returns\n        -------\n        scores : array-like, shape (n_splits,)\n            The mean accuracy for each split.\n        \"\"\"\n        # Validate the input data\n        X_validated = self._validate_X(X)\n\n        # Check if the model has been fitted\n        if self.classifier is None:\n            raise ValueError(\"Model has not been fitted yet. Call 'fit' method first.\")\n\n        # Cross-validate the classifier\n        scores = cross_val_score(\n            self.classifier, X_validated, y, cv=cv, verbose=verbose\n        )\n\n        return scores\n```",
        "```python\nfrom typing import Optional\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"Insert a value into the binary tree.\"\"\"\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node: Optional[Node], value: int) -> Node:\n        \"\"\"Recursive helper function for inserting a value into the binary tree.\"\"\"\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    def delete(self, value: int) -> None:\n        \"\"\"Delete a value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: Optional[Node], value: int) -> Optional[Node]:\n        \"\"\"Recursive helper function for deleting a value from the binary tree.\"\"\"\n        if node is None:\n            return None\n\n        # Delete from left subtree\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n\n        # Delete from right subtree\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n\n        # Node found, it has two children\n        else:\n            # Case 1: Node has two children\n            if node.left and node.right:\n                # Find the smallest value in the right subtree\n                min_value_node = self._find_min_value_node(node.right)\n                # Replace the node's value with the smallest value\n                node.value = min_value_node.value\n                # Delete the smallest value node\n                node.right = self._delete_recursive(node.right, min_value_node.value)\n\n            # Case 2: Node has one or zero children\n            else:\n                # Node has only left child or no child\n                if node.left is None:\n                    return node.right\n                # Node has only right child or no child\n                elif node.right is None:\n                    return node.left\n                # Node has only one child\n                else:\n                    return node.left or node.right\n\n        return node\n\n    def _find_min_value_node(self, node: Node) -> Node:\n        \"\"\"Helper function to find the node with the minimum value in a subtree.\"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current\n```",
        "```python\nfrom typing import List\n\n\ndef dot_product(vector_1: List[float], vector_2: List[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_1 (List[float]): The first vector.\n        vector_2 (List[float]): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    assert len(vector_1) == len(vector_2), \"Vectors must have the same dimension for dot product.\"\n\n    dot_product_result = 0.0\n    for g_i in range(len(vector_1)):\n        dot_product_result += vector_1[g_i] * vector_2[g_i]\n\n    return dot_product_result\n```"
      ]
    }
  ]
}