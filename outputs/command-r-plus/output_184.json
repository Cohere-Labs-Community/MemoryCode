{
  "sessions": [
    {
      "session_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0,\n                                kernel_params=kernel_params, fit_intercept=fit_intercept, normalize=normalize,\n                                copy_X=copy_X, verbose=verbose)\n\n    def fit(self, X, y):\n        try:\n            self.model.fit(X, y)\n        except ValueError as e:\n            print(f\"ValueError: {e}\")\n\n    def predict(self, X):\n        try:\n            return self.model.predict(X)\n        except ValueError as e:\n            print(f\"ValueError: {e}\")",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n@trace_class\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(node.left, value)\n        elif value > node.value:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(node.right, value)\n        else:\n            raise ValueError(\"Value already exists in the tree.\")\n\n    def is_valid(self):\n        min_value = float('-inf')\n        max_value = float('inf')\n        return self._is_valid_recursive(self.root, min_value, max_value)\n\n    def _is_valid_recursive(self, node, min_value, max_value):\n        if node is None:\n            return True\n        if not min_value < node.value < max_value:\n            return False\n        left_valid = self._is_valid_recursive(node.left, min_value, node.value)\n        right_valid = self._is_valid_recursive(node.right, node.value, max_value)\n        return left_valid and right_valid\n\ndef is_valid_bst(root):\n    try:\n        bst = BinarySearchTree()\n        bst.root = root\n        return bst.is_valid()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List, Optional\n\n\ndef find_kth_largest(nums: List[int], k: int) -> Optional[int]:\n    \"\"\"\n    Finds the k-th largest element in a list of integers.\n\n    Args:\n        nums (List[int]): The input list of integers.\n        k (int): The value of 'k' for the k-th largest element.\n\n    Returns:\n        Optional[int]: The k-th largest element in the list, or None if k is greater than the list length.\n    \"\"\"\n    if k > len(nums):\n        return None\n\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_dijsktra(self, src):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = set()\n\n        for _ in range(self.V):\n            min_dist_vertex = self._find_min_distance_vertex(dist, spt_set)\n            spt_set.add(min_dist_vertex)\n            self._relax(min_dist_vertex, dist, spt_set)\n\n        def _find_min_distance_vertex(self, dist, spt_set):\n            min_dist = float('inf')\n            min_dist_vertex = -1\n            for v in range(self.V):\n                if dist[v] < min_dist and v not in spt_set:\n                    min_dist = dist[v]\n                    min_dist_vertex = v\n            return min_dist_vertex\n\n        def _relax(self, u, dist, spt_set):\n            for v in self.graph[u]:\n                if dist[v] > dist[u] + 1 and v not in spt_set:\n                    dist[v] = dist[u] + 1\n\n    def _method_name(self):\n        # Add your method implementation here\n        pass\n```",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert node method with a docstring and method name ending with '_b'\n    def insert_node_b(self, data):\n        \"\"\"\n        Insert a new node with the given data into the binary tree.\n        \"\"\"\n        if self.root is None:\n            self.root = Node(data)\n        else:\n            self._insert_recursive_b(self.root, data)\n\n    def _insert_recursive_b(self, node, data):\n        if data < node.data:\n            if node.left is None:\n                node.left = Node(data)\n            else:\n                self._insert_recursive_b(node.left, data)\n        else:\n            if node.right is None:\n                node.right = Node(data)\n            else:\n                self._insert_recursive_b(node.right, data)\n\n    # Delete node method with a docstring and method name ending with '_b'\n    def delete_node_b(self, data):\n        \"\"\"\n        Delete the first node with the given data from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive_b(self.root, data)\n\n    def _delete_recursive_b(self, node, data):\n        if node is None:\n            return node\n\n        if data < node.data:\n            node.left = self._delete_recursive_b(node.left, data)\n        elif data > node.data:\n            node.right = self._delete_recursive_b(node.right, data)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.data = self._get_min_value_b(node.right)\n            node.right = self._delete_recursive_b(node.right, node.data)\n\n        return node\n\n    def _get_min_value_b(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.data\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.left = None\n        self.right = None\n```"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_b(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_b(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        parent = [-1] * self.V\n\n        while True:\n            u = -1\n            for i in range(self.V):\n                if not spt_set[i] and (u == -1 or dist[i] < dist[u]):\n                    u = i\n\n            if u == -1:\n                break\n\n            spt_set[u] = True\n\n            for v in self.graph[u]:\n                if not spt_set[v]:\n                    if dist[u] + 1 < dist[v]:\n                        dist[v] = dist[u] + 1\n                        parent[v] = u\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return path",
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_p(self, src, dest):\n        visited = [False] * self.V\n        path = []\n\n        def dfs(node, parent):\n            visited[node] = True\n            path.append(node)\n            if node == dest:\n                return True\n            for neighbour in self.graph[node]:\n                if not visited[neighbour] and dfs(neighbour, node):\n                    return True\n            path.pop()\n            return False\n\n        dfs(src, src)\n        return path if dest in path else []\n```"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def shortest_path_a(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        while True:\n            u = self.min_distance(dist, spt_set)\n            if u == -1:\n                break\n            for v, weight in self.graph[u]:\n                if not spt_set[v] and dist[u] != float('inf') and dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n            spt_set[u] = True\n\n        if dist[dest] == float('inf'):\n            return float('inf')\n        else:\n            return dist[dest]\n\n    def min_distance(self, dist, spt_set):\n        min_dist = float('inf')\n        min_node = -1\n        for u in range(self.V):\n            if not spt_set[u] and dist[u] < min_dist:\n                min_dist = dist[u]\n                min_node = u\n        return min_node\n```",
        "def merge_sort_c():\n    a = []\n    b = []\n    i = j = 0\n    n = len(a)\n\n    if n < 2:\n        return a\n\n    for k in range(n // 2):\n        a[k], a[k + n // 2] = a[i], a[j]\n        i += 1\n        j += 1\n\n    merge_sort_c()\n\n    for k in range(n):\n        if i == n // 2:\n            b.append(a[k])\n        elif j == n:\n            b.append(a[k + n // 2])\n        elif a[i] < a[j]:\n            b.append(a[i])\n            i += 1\n        else:\n            b.append(a[j])\n            j += 1\n\n    return b"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def print_a(self):\n        for i in range(self.V):\n            for j in range(self.V):\n                if self.graph[i][j] == 1:\n                    print(f\"{i} - {j}\")\n\n    def add_edge_a(self, u, v):\n        self.graph[u][v] = 1\n        self.graph[v][u] = 1\n\n    def shortest_path_a(self, src, dest):\n        distance = [float(\"Inf\")] * self.V\n        distance[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] == 1 and distance[v] > distance[u] + 1:\n                    distance[v] = distance[u] + 1\n                    queue.append(v)\n        return distance[dest]\n```",
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def ud_chx_insert(self, value):\n        self.heap.append(value)\n        self.ud_chx_heapify(len(self.heap) - 1)\n\n    def ud_chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.heap[index], self.heap[parent_index] = self.heap[parent_index], self.heap[index]\n                index = parent_index\n            else:\n                break\n```",
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def ud_chx_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def ud_chx_remove_stock(self, item, quantity):\n        if item in self.stock and self.stock[item] >= quantity:\n            self.stock[item] -= quantity\n        else:\n            print(\"Insufficient stock to fulfill request.\")"
      ],
      "instruction_model_output": [
        "class Heap:\n    def __init__(self):\n        self.heap = []\n\n    def chx_insert(self, value):\n        self.heap.append(value)\n        self.chx_heapify(len(self.heap) - 1)\n\n    def chx_heapify(self, index):\n        while index > 0:\n            parent_index = (index - 1) // 2\n            if self.heap[index] < self.heap[parent_index]:\n                self.chx_swap(index, parent_index)\n                index = parent_index\n            else:\n                break\n\n    def chx_swap(self, i, j):\n        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]",
        "class Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def ud_add_stock(self, item, quantity):\n        if item in self.stock:\n            self.stock[item] += quantity\n        else:\n            self.stock[item] = quantity\n\n    def ud_remove_stock(self, item, quantity):\n        if item in self.stock:\n            if self.stock[item] >= quantity:\n                self.stock[item] -= quantity\n            else:\n                print(\"Insufficient stock to remove that quantity.\")\n        else:\n            print(\"Item not found in inventory.\")"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n\n    def train(self, X_train, y_train):\n        try:\n            from sklearn.linear_model import Lasso\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X_train, y_train)\n        except ImportError:\n            raise ImportError(\"scikit-learn is required for Lasso Regression. Please install it.\")\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while training the Lasso Regression model: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except AttributeError:\n            raise AttributeError(\"Model is not fitted yet. Please call the train method before predicting.\")\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while predicting with the Lasso Regression model: {e}\")\n```",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def shortest_path_lengths_n(self, src):\n        dist = [float('inf') for _ in range(self.V)]\n        dist[src] = 0\n        queue = []\n        queue.append(src)\n\n        while queue:\n            u = queue.pop(0)\n            for v in range(self.V):\n                if self.graph[u][v] != 0 and dist[v] > dist[u] + self.graph[u][v]:\n                    dist[v] = dist[u] + self.graph[u][v]\n                    queue.append(v)\n\n        return dist\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_n(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_n(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        q = [src]\n\n        while q:\n            u = q.pop(0)\n            for v in self.graph[u]:\n                if dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n                    q.append(v)\n        return dist[dest]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\ndef is_valid_bst(root: Node, min_val=float('-inf'), max_val=float('inf')) -> bool:\n    chx_ud_ = \"chx\"\n    if not root:\n        return True\n\n    if not min_val <= root.value < max_val:\n        return False\n\n    return is_valid_bst(root.left, min_val, root.value) and is_valid_bst(root.right, root.value, max_val)\n\ndef is_valid_binary_search_tree(root):\n    return is_valid_bst(root)\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass Inventory:\n    def __init__(self):\n        self.stock = {}\n\n    def _add_stock_p(self, product: str, quantity: int) -> None:\n        if product in self.stock:\n            self.stock[product] += quantity\n        else:\n            self.stock[product] = quantity\n\n    def _remove_stock_p(self, product: str, quantity: int) -> None:\n        if product in self.stock:\n            self.stock[product] -= quantity\n            if self.stock[product] == 0:\n                del self.stock[product]\n        else:\n            raise ValueError(f\"Product '{product}' not found in stock.\")\n\n    def add_stock_chx(self, product: str, quantity: int) -> None:\n        try:\n            self._add_stock_p(product, quantity)\n        except Exception as e:\n            raise ValueError(f\"Failed to add stock for product '{product}': {e}\")\n\n    def remove_stock_chx(self, product: str, quantity: int) -> None:\n        try:\n            self._remove_stock_p(product, quantity)\n        except Exception as e:\n            raise ValueError(f\"Failed to remove stock for product '{product}': {e}\")\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass MergeSort:\n    def __init__(self, arr):\n        self.arr = arr\n\n    def _merge(self, left, right):\n        result = []\n        left_index, right_index = 0, 0\n\n        while left_index < len(left) and right_index < len(right):\n            if left[left_index] < right[right_index]:\n                result.append(left[left_index])\n                left_index += 1\n            else:\n                result.append(right[right_index])\n                right_index += 1\n\n        result.extend(left[left_index:])\n        result.extend(right[right_index:])\n        return result\n\n    def _sort(self, arr):\n        if len(arr) <= 1:\n            return arr\n\n        mid = len(arr) // 2\n        left = arr[:mid]\n        right = arr[mid:]\n\n        return self._merge(self._sort(left), self._sort(right))\n\n    def sort_array(self) -> list:\n        return self._sort(self.arr)\n\n# Example usage:\n# arr = [12, 11, 13, 5, 6, 7]\n# sorter = MergeSort(arr)\n# sorted_arr = sorter.sort_array()\n# print(sorted_arr)\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass KthLargest:\n    def __init__(self, k: int):\n        self.k = k\n        self.heap = []\n\n    def add(self, num: int) -> None:\n        import heapq\n\n        heapq.heappend(self.heap, -num)\n        if len(self.heap) > self.k:\n            heapq.heappop(self.heap)\n\n    def val(self) -> int:\n        return -self.heap[0]\n\n# Example usage:\n# kth_largest = KthLargest(3)\n# kth_largest.add(1)\n# kth_largest.add(3)\n# kth_largest.add(5)\n# kth_largest.add(2)\n# kth_largest.add(4)\n# print(kth_largest.val())  # Output: 3\n```",
        "```python\nfrom pedantic import trace_class\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n@trace_class\nclass LassoRegression:\n    def _chx_ud_init_p(self, alpha: float = 1.0):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def _chx_ud_train_a(self, X: array, y: array) -> LassoRegression:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        self.model.fit(X_train, y_train)\n        return self\n\n    def _chx_ud_predict_c(self, X: array) -> array:\n        try:\n            X_scaled = StandardScaler().fit_transform(X)\n            return self.model.predict(X_scaled)\n        except Exception as e:\n            print(f\"Error occurred during prediction: {e}\")\n            return None\n```",
        "from pedantic import trace_class\n\n@trace_class\nclass Heap:\n    def __init__(self):\n        self.heap = []\n\n    def insert(self, value):\n        self.heap.append(value)\n        self.heapify(len(self.heap) - 1)\n\n    def heapify(self, index):\n        try:\n            while True:\n                parent_index = (index - 1) // 2\n                if index <= 0 or self.heap[index] >= self.heap[parent_index]:\n                    break\n                self.heap[index], self.heap[parent_index] = self.heap[parent_index], self.heap[index]\n                index = parent_index\n        except IndexError:\n            pass",
        "```python\nfrom pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression Class.\n\n    This class provides a scikit-learn compatible implementation of Kernel Ridge Regression.\n    It includes proper annotations, docstrings, and naming conventions as per mentor's instructions.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', degree=3, gamma=None, coef0=1.0, kernel_params=None):\n        \"\"\"\n        Initialize the KernelRidgeRegression class.\n\n        Parameters:\n        - alpha (float): Regularization strength parameter. Must be a positive float. Defaults to 1.0.\n        - kernel (str): Kernel to be used. Defaults to 'linear'.\n        - degree (int): Degree of the polynomial kernel function. Ignored by other kernels. Defaults to 3.\n        - gamma (float): Kernel coefficient. Defaults to None.\n        - coef0 (float): Independent term in the polynomial kernel function. Defaults to 1.0.\n        - kernel_params (dict): Additional parameters for the kernel function. Defaults to None.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self._model = KernelRidge(\n            alpha=alpha, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, kernel_params=kernel_params\n        )\n\n    def _check_input_data(self, X, y):\n        \"\"\"\n        Check the input data for training.\n\n        Parameters:\n        - X (ndarray): Training data of shape (n_samples, n_features).\n        - y (ndarray): Target values of shape (n_samples,).\n\n        Raises:\n        - ValueError: If X or y is not a 2D numpy array.\n        - ValueError: If X and y have different numbers of samples.\n        \"\"\"\n        if X.ndim != 2 or y.ndim != 1:\n            raise ValueError(\"Input data X and y should be 2D numpy arrays.\")\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"Number of samples in X and y should be the same.\")\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as training data and y as target values.\n\n        Parameters:\n        - X (ndarray): Training data of shape (n_samples, n_features).\n        - y (ndarray): Target values of shape (n_samples,).\n\n        Returns:\n        - self: Fitted instance of the KernelRidgeRegression class.\n        \"\"\"\n        self._check_input_data(X, y)\n        self._model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear regression model.\n\n        Parameters:\n        - X (ndarray): Data of shape (n_samples, n_features) on which to predict.\n\n        Returns:\n        - ndarray: Predicted values of shape (n_samples,).\n        \"\"\"\n        return self._model.predict(X)\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.edges = [(0, 0, 0) for _ in range(vertices)]\n\n    def shortest_path(self, start, end, parent):\n        distance = [1e9] * self.vertices\n        distance[start] = 0\n        spt_set = [False] * self.vertices\n\n        while True:\n            u = self._min_distance(distance, spt_set)\n            if u == -1:\n                break\n\n            for v in range(self.vertices):\n                if (\n                    self.edges[u][v] > 0\n                    and spt_set[u] == True\n                    and distance[v] > distance[u] + self.edges[u][v]\n                ):\n                    distance[v] = distance[u] + self.edges[u][v]\n                    parent[v] = u\n                    spt_set[v] = True\n\n        return distance[end]\n\n    def _min_distance(self, distance, spt_set):\n        min_distance = 1e9\n        min_vertex = -1\n\n        for u in range(self.vertices):\n            if spt_set[u] == False and distance[u] < min_distance:\n                min_distance = distance[u]\n                min_vertex = u\n\n        return min_vertex\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insertion method\n    def insert(self, value):\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if not node:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    # Deletion method\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if not node:\n            return node\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if not node.left:\n                return node.right\n            elif not node.right:\n                return node.left\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left:\n            current = current.left\n        return current.value\n\n    # Method to end with '_p' as per mentor's instruction\n    def traverse_in_order_p(self):\n        self._traverse_in_order_recursive(self.root)\n\n    def _traverse_in_order_recursive(self, node):\n        if not node:\n            return\n        self._traverse_in_order_recursive(node.left)\n        print(node.value)\n        self._traverse_in_order_recursive(node.right)\n\n# Node class to create nodes for the binary tree\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n```"
      ]
    }
  ]
}