{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if not node:\n            return BinaryTreeNode(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if not node:\n            return node\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if not node.left:\n                return node.right\n            elif not node.right:\n                return node.left\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left:\n            current = current.left\n        return current.value\n\nclass BinaryTreeNode:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom pedantic import timer_class\n\n@timer_class\nclass QDA(QuadraticDiscriminantAnalysis):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def b_fit(X, y, **kwargs):\n        return QuadraticDiscriminantAnalysis.fit(X, y, **kwargs)\n\n    @staticmethod\n    def b_predict(X, **kwargs):\n        return QuadraticDiscriminantAnalysis.predict(X, **kwargs)",
        "```python\nfrom pedantic import timer_class\n\n\n@timer_class\nclass HIndexCalculator:\n    @staticmethod\n    def b_calculate_h_index(citations: list[int]) -> int:\n        if not citations:\n            return 0\n\n        citations.sort(reverse=True)\n        h_index = 0\n\n        for i, citation_count in enumerate(citations, start=1):\n            if citation_count >= i:\n                h_index = i\n            else:\n                break\n\n        return h_index\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "def b_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while training the Lasso Regression model: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            raise RuntimeError(f\"An error occurred while making predictions: {e}\")\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def convert_to_roman(num):\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    vals = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    syms = ['M', 'CM', 'D', 'CD', 'C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I']\n    roman = ''\n    i = 0\n    while num > 0:\n        for _ in range(num // vals[i]):\n            roman += syms[i]\n            num -= vals[i]\n        i += 1\n    return roman\n\n# Example usage:\n# num = 2023\n# print(convert_to_roman(num))",
        "class LinkedList:\n    \"\"\"\n    A linked list data structure.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes an empty linked list.\n        \"\"\"\n        self.heads = None\n\n    def append(self, data):\n        \"\"\"\n        Appends data to the end of the linked list.\n\n        :param data: The data to be appended.\n        \"\"\"\n        new_node = Node(data)\n        if not self.heads:\n            self.heads = new_node\n        else:\n            current = self.heads\n            while current.nexts:\n                current = current.nexts\n            current.nexts = new_node\n\n    def delete(self, data):\n        \"\"\"\n        Deletes the first occurrence of data from the linked list.\n\n        :param data: The data to be deleted.\n        \"\"\"\n        if self.heads and self.heads.datas == data:\n            self.heads = self.heads.nexts\n        else:\n            current = self.heads\n            prev = None\n            while current and current.datas != data:\n                prev = current\n                current = current.nexts\n            if not current:\n                return\n            prev.nexts = current.nexts\n\n    def search(self, data):\n        \"\"\"\n        Searches for data in the linked list and returns True if found, False otherwise.\n\n        :param data: The data to be searched.\n        :return: True if data is found, False otherwise.\n        \"\"\"\n        current = self.heads\n        while current:\n            if current.datas == data:\n                return True\n            current = current.nexts\n        return False\n\n    def display(self):\n        \"\"\"\n        Displays the data in the linked list, starting from the head node.\n        \"\"\"\n        current = self.heads\n        while current:\n            print(current.datas, end=\" -> \")\n            current = current.nexts\n        print(\"None\")\n\n\nclass Node:\n    \"\"\"\n    A node in the linked list.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initializes a new node with the given data.\n\n        :param data: The data to be stored in the node.\n        \"\"\"\n        self.datas = data\n        self.nexts = None"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next_node_s = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_node_s = None\n        self.tail_node_s = None\n        self.size_s = 0\n\n    def append(self, data):\n        new_node_s = Node(data)\n        if not self.head_node_s:\n            self.head_node_s = new_node_s\n            self.tail_node_s = self.head_node_s\n        else:\n            current_node_s = self.head_node_s\n            while current_node_s.next_node_s:\n                current_node_s = current_node_s.next_node_s\n            current_node_s.next_node_s = new_node_s\n            self.tail_node_s = new_node_s\n        self.size_s += 1\n\n    def delete(self, data):\n        current_node_s = self.head_node_s\n        prev_node_s = None\n        while current_node_s and current_node_s.data == data:\n            prev_node_s = current_node_s\n            current_node_s = current_node_s.next_node_s\n        if prev_node_s:\n            prev_node_s.next_node_s = current_node_s.next_node_s\n            if current_node_s == self.tail_node_s:\n                self.tail_node_s = prev_node_s\n            self.size_s -= 1\n\n    def display(self):\n        current_node_s = self.head_node_s\n        while current_node_s:\n            print(current_node_s.data, end=\" -> \")\n            current_node_s = current_node_s.next_node_s\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import timer\n\n@timer\ndef has_cycle(head):\n    if not head or not head.next:\n        return False\n    slow, fast = head, head.next\n    while slow != fast:\n        if fast is None or fast.next is None:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n    return True"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import timer\nfrom typing import Optional\n\n\n@timer\ndef has_cycle(head: Optional[Node]) -> bool:\n    if not head or not head.next:\n        return False\n\n    slow, fast = head, head.next\n\n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n\n    return True\n\nclass Node:\n    def __init__(self, val: int, next: Optional['Node'] = None) -> None:\n        self.val = val\n        self.next = next"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class PCA0:\n    def __init__(self, n_components=None):\n        self.n_components = n_components\n\n    def fit(self, X):\n        X_mean = X.mean(axis=0)\n        X_centered = X - X_mean\n        covariance_matrix = X_centered.T @ X_centered / (X.shape[0] - 1)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n        indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[indices]\n        eigenvectors = eigenvectors[:, indices]\n        self.components_ = eigenvectors[:self.n_components]\n        self.explained_variance_ = eigenvalues[:self.n_components]\n\n    def transform(self, X):\n        X_transformed = X @ self.components_.T\n        return X_transformed"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def compute_diag_avg(matrix):\n    assert len(matrix) == len(matrix[0]), \"Input must be a square matrix\"\n    diag_sum = 0\n    for i in range(len(matrix)):\n        diag_sum += matrix[i][i]\n    diag_avg = diag_sum / len(matrix)\n    return diag_avg",
        "class LinearRegression:\n    def __init__(self, digit=0):\n        self.coef_ = None\n        self.intercept_ = None\n        self._digit = digit\n\n    def fit(self, X, y):\n        assert X.shape[0] == y.shape[0], \"X and y should have same number of samples\"\n        X_with_ones = np.c_[np.ones(X.shape[0]), X]\n        self.coef_ = np.linalg.inv(X_with_ones.T @ X_with_ones) @ (X_with_ones.T @ y)\n        self.intercept_ = self.coef_[0]\n        self.coef_ = self.coef_[1:]\n\n    def predict(self, X):\n        assert self.coef_ is not None, \"Model has not been fitted yet\"\n        assert X.shape[1] == len(self.coef_), \"Number of features in X does not match training data\"\n        X_with_ones = np.c_[np.ones(X.shape[0]), X]\n        return X_with_ones @ self.coef_.T + self.intercept_"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)",
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef y_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Compute the h-index from an array of integer citations.\n\n    Parameters:\n    citations (list[int]): A list of integer citations.\n\n    Returns:\n    int: The h-index.\n    \"\"\"\n    if not citations:\n        return 0\n\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n\n    return h_index\n```",
        "def y_longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string among a list of input strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    shortest = min(strs, key=len)\n\n    for i, char in enumerate(shortest):\n        for other in strs:\n            if other[i] != char:\n                return shortest[:i]\n\n    return shortest"
      ],
      "instruction_model_output": [
        "def y_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A Nearest Neighbors Model for classification or regression.\n    Train on a set of examples and then predict the labels/values of new data points.\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, metric='euclidean'):\n        \"\"\"\n        Initialize the NearestNeighborsModel.\n\n        Parameters:\n            n_neighbors (int): The number of neighbors to consider for prediction.\n            metric (str): The distance metric to use. Supported metrics: 'euclidean', 'manhattan', 'cosine'.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self._x_train = None\n        self._y_train = None\n\n    def train(self, x_train, y_train):\n        \"\"\"\n        Train the model on the provided data.\n\n        Parameters:\n            x_train (ndarray): Training input data of shape (n_samples, n_features).\n            y_train (ndarray): Training target values of shape (n_samples,).\n        \"\"\"\n        assert x_train.shape[0] == y_train.shape[0], \"Number of samples in x_train and y_train must match.\"\n        self._x_train = x_train\n        self._y_train = y_train\n\n    def predict(self, x_test):\n        \"\"\"\n        Predict the labels/values of the provided test data.\n\n        Parameters:\n            x_test (ndarray): Test input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted labels/values of shape (n_samples,).\n        \"\"\"\n        assert self._x_train is not None and self._y_train is not None, \"Model has not been trained yet.\"\n        assert x_test.shape[1] == self._x_train.shape[1], \"Number of features in test data must match training data.\"\n\n        distances = np.sqrt(np.sum((x_test[:, np.newaxis, :] - self._x_train) ** 2, axis=2))\n        indices = np.argsort(distances, axis=1)[:, :self.n_neighbors]\n        nearest_neighbors = self._y_train[indices]\n\n        predictions = np.mean(nearest_neighbors, axis=1)\n        return predictions\n```",
        "class Image_Classifier(torch.nn.Module):\n    def __init__(self):\n        super(Image_Classifier, self).__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = torch.nn.Linear(120, 84)\n        self.fc3 = torch.nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.conv1(x))\n        x = torch.nn.functional.max_pool2d(x, 2)\n        x = torch.nn.functional.relu(self.conv2(x))\n        x = torch.nn.functional.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = torch.nn.functional.relu(self.fc1(x))\n        x = torch.nn.functional.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nassert isinstance(Image_Classifier(), torch.nn.Module)"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass binary_image_classifier(nn.Module):\n    def __init__(self, num_channels, num_classes):\n        super(binary_image_classifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def fn_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\ndef fn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ImageClassifier, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(0.2)\n        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)",
        "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\ndef is_Valid_BST(root):\n    def helper(node, min_val, max_val):\n        if node is None:\n            return True\n        if node.value <= min_val or node.value >= max_val:\n            return False\n        return helper(node.left, min_val, node.value) and helper(node.right, node.value, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        if self.root is None:\n            self.root = Node(value)\n        else:\n            self._insert(self.root, value)\n\n    def _insert(self, node, value):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(node.left, value)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(node.right, value)\n\n    def is_valid(self):\n        return is_Valid_BST(self.root)"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_queue = []\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.r_queue.remove(key)\n        self.r_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_queue:\n            self.r_queue.remove(key)\n        elif len(self.r_queue) >= self.capacity:\n            del_key = self.r_queue.pop(0)\n            del self.cache[del_key]\n        self.r_queue.append(key)\n        self.cache[key] = value\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@count_calls\nclass LinearRegressionModel:\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Node:\n    def __init__(self, data):\n        self.data = data\n        self.next_xt = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head_xt = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_xt:\n            self.head_xt = new_node\n        else:\n            current = self.head_xt\n            while current.next_xt:\n                current = current.next_xt\n            current.next_xt = new_node\n\n    def display(self):\n        current = self.head_xt\n        while current:\n            print(current.data, end=\" -> \")\n            current = current.next_xt\n        print(\"None\")"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_xt = data\n        self.next_xt = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_xt = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_xt:\n            self.head_xt = new_node\n            return\n        current = self.head_xt\n        while current.next_xt:\n            current = current.next_xt\n        current.next_xt = new_node\n\n    def delete(self, data):\n        if not self.head_xt:\n            return\n        if self.head_xt.data_xt == data:\n            self.head_xt = self.head_xt.next_xt\n            return\n        current = self.head_xt\n        prev = None\n        while current and current.data_xt != data:\n            prev = current\n            current = current.next_xt\n        if not current:\n            return\n        prev.next_xt = current.next_xt\n\n    def display(self):\n        current = self.head_xt\n        while current:\n            print(current.data_xt, end=\" -> \")\n            current = current.next_xt\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\nimport re\nimport secrets\n\n\n@require_kwargs\ndef roman_to_int(roman: str) -> int:\n    \"\"\"Convert a Roman numeral string to an integer.\n\n    Args:\n        roman (str): The Roman numeral string to convert.\n\n    Returns:\n        int: The corresponding integer value.\n    \"\"\"\n    values = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n    int_value = 0\n    prev_value = 0\n\n    for char in reversed(roman):\n        value = values[char]\n        if value < prev_value:\n            int_value -= value\n        else:\n            int_value += value\n        prev_value = value\n\n    return int_value",
        "```python\nfrom pedantic import require_kwargs\nfrom functools import wraps\n\ndef edit_distance(string1: str, string2: str) -> int:\n    @require_kwargs\n    @wraps(edit_distance)\n    def wrapper(*args, **kwargs):\n        string1, string2 = args\n        length_s1, length_s2 = len(string1), len(string2)\n        dp = [[0] * (length_s2 + 1) for _ in range(length_s1 + 1)]\n\n        for i in range(length_s1 + 1):\n            dp[i][0] = i\n        for j in range(length_s2 + 1):\n            dp[0][j] = j\n\n        for i in range(1, length_s1 + 1):\n            for j in range(1, length_s2 + 1):\n                cost = 0 if string1[i - 1] == string2[j - 1] else 1\n                dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n\n        return dp[length_s1][length_s2]\n\n    return wrapper()\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer",
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import validate\n\n\n@validate\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    wr_box_values = set()\n    wr_row_values = set()\n    wr_col_values = set()\n\n    for i in range(9):\n        wr_row_values = set()\n        wr_col_values = set()\n        for j in range(9):\n            current_num = board[i][j]\n            if current_num in wr_row_values or current_num in wr_col_values or current_num in wr_box_values:\n                return False\n            wr_row_values.add(current_num)\n            wr_col_values.add(current_num)\n            wr_box_index = 3 * (i // 3) + j // 3\n            wr_box_values[wr_box_index].add(current_num)\n\n    return True\n```",
        "class Library:\n    \"\"\"\n    A library class to manage a collection of books.\n    \"\"\"\n    def __init__(self):\n        self.books = []\n\n    @staticmethod\n    def _validate_book_title(title):\n        if not isinstance(title, str):\n            raise ValueError(\"Book title must be a string\")\n        if not title:\n            raise ValueError(\"Book title cannot be empty\")\n\n    @staticmethod\n    @wr_validate\n    def _validate_book_author(author):\n        if not isinstance(author, str):\n            raise ValueError(\"Book author must be a string\")\n        if not author:\n            raise ValueError(\"Book author cannot be empty\")\n\n    def add_book(self, title, author):\n        \"\"\"\n        Add a book to the library.\n        :param title: The title of the book.\n        :param author: The author of the book.\n        :return: None\n        \"\"\"\n        self._validate_book_title(title)\n        self._validate_book_author(author)\n        self.books.append((title, author))\n\n    def remove_book(self, title, author):\n        \"\"\"\n        Remove a book from the library.\n        :param title: The title of the book.\n        :param author: The author of the book.\n        :return: None\n        \"\"\"\n        self._validate_book_title(title)\n        self._validate_book_author(author)\n        self.books.remove((title, author))"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    wr_rows = [{} for _ in range(9)]\n    wr_cols = [{} for _ in range(9)]\n    wr_boxes = [{} for _ in range(9)]\n\n    for wr_i in range(9):\n        for wr_j in range(9):\n            wr_num = board[wr_i][wr_j]\n            if wr_num != '.':\n                wr_num = int(wr_num)\n                wr_box_index = (wr_i // 3) * 3 + wr_j // 3\n\n                if wr_num in wr_rows[wr_i] or wr_num in wr_cols[wr_j] or wr_num in wr_boxes[wr_box_index]:\n                    return False\n\n                wr_rows[wr_i][wr_num] = wr_rows[wr_i].get(wr_num, 0) + 1\n                wr_cols[wr_j][wr_num] = wr_cols[wr_j].get(wr_num, 0) + 1\n                wr_boxes[wr_box_index][wr_num] = wr_boxes[wr_box_index].get(wr_num, 0) + 1\n\n    return True",
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_sudoku(board):\n    m_flat_board = [item for sublist in board for item in sublist]\n    m_rows = [board[i*3:i*3+3] for i in range(3)]\n    m_cols = [[row[i] for row in board] for i in range(3)]\n    m_boxes = [\n        [board[i][j] for i in range(i, i+3) for j in range(j, j+3)] for i in [0, 3, 6] for j in [0, 3, 6]\n    ]\n    for m_box in m_boxes:\n        if len(m_box) != len(set(m_box)):\n            return False\n    for m_row in m_rows:\n        if len(m_row) != len(set(m_flat_row)):\n            return False\n    for i in range(9):\n        if len(board[i]) != len(set(board[i])) or len(m_cols[i]) != len(set(m_cols[i])):\n            return False\n    return True"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    m_seen = set()\n    for m_i in range(9):\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_i][m_j] != '.':\n                if board[m_i][m_j] in m_seen:\n                    return False\n                m_seen.add(board[m_i][m_j])\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_j][m_i] != '.':\n                if board[m_j][m_i] in m_seen:\n                    return False\n                m_seen.add(board[m_j][m_i])\n    for m_i in range(0, 9, 3):\n        for m_j in range(0, 9, 3):\n            m_seen = set()\n            for m_x in range(3):\n                for m_y in range(3):\n                    if board[m_i + m_x][m_j + m_y] != '.':\n                        if board[m_i + m_x][m_j + m_y] in m_seen:\n                            return False\n                        m_seen.add(board[m_i + m_x][m_j + m_y]])\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def compute_mean(data):\n    z_sum_0 = 0\n    z_count_1 = 0\n    for z_value_2 in data:\n        z_sum_0 += z_value_2\n        z_count_1 += 1\n    z_mean_3 = z_sum_0 / z_count_1\n    return z_mean_3",
        "def z_0check_valid_sudoku(z_0board):\n    z_1seen = [[{}, {}] * 9 for _ in range(9)]\n    z_2flat_board = [z_0num for z_3row in z_0board for z_3num in z_3row]\n    for z_0i, z_0num in enumerate(z_2flat_board):\n        if not z_0num:\n            continue\n        z_1row, z_1col = divmod(z_0i, 9)\n        z_1box_row, z_1box_col = divmod(z_1row, 3) * 3 + z_1box_col, z_1col // 3\n        if z_0num in z_1seen[z_1row][z_1col] or z_0num in z_1seen[z_1box_row][z_1box_col]:\n            return False\n        z_1seen[z_1row][z_1col][z_0num] = 1\n        z_1seen[z_1box_row][z_1box_col][z_0num] = 1\n    return True"
      ],
      "instruction_model_output": [
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2",
        "def is_valid_sudoku(z_board):\n    z_rows = [set() for _ in range(9)]\n    z_cols = [set() for _ in range(9)]\n    z_boxes = [set() for _ in range(9)]\n\n    for z_i in range(9):\n        for z_j in range(9):\n            z_num = z_board[z_i][z_j]\n            if z_num != '.':\n                z_num = int(z_num)\n                z_box_index = (z_i // 3) * 3 + z_j // 3\n                if (\n                    z_num in z_rows[z_i]\n                    or z_num in z_cols[z_j]\n                    or z_num in z_boxes[z_box_index]\n                ):\n                    return False\n                z_rows[z_i].add(z_num)\n                z_cols[z_j].add(z_num)\n                z_boxes[z_box_index].add(z_num)\n\n    return True"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    \"\"\"A binary tree data structure.\"\"\"\n\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node: Optional[Node], value: int) -> Node:\n        \"\"\"Recursive helper function for inserting a new node.\"\"\"\n        if not node:\n            return Node(value)\n\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n\n        return node\n\n    def delete(self, value: int) -> None:\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node: Optional[Node], value: int) -> Node:\n        \"\"\"Recursive helper function for deleting a node.\"\"\"\n        if not node:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if not node.left:\n                return node.right\n            elif not node.right:\n                return node.left\n\n            # Node has two children\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        \"\"\"Helper function to find the minimum value in a subtree.\"\"\"\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```",
        "```python\nfrom typing import Optional\n\ndef delete_node(root: 'Node', key: int) -> Optional['Node']:\n    \"\"\"Deletes a node with the given key from the binary search tree.\n\n    Args:\n        root (Node): The root of the binary search tree.\n        key (int): The key of the node to be deleted.\n\n    Returns:\n        Optional[Node]: The new root of the binary search tree after deletion.\n    \"\"\"\n    if root is None:\n        return None\n\n    # Recursive search for the node to delete\n    if key < root.key:\n        root.left = delete_node(root.left, key)\n    elif key > root.key:\n        root.right = delete_node(root.right, key)\n    else:\n        # Case 1: Node with only one child or no child\n        if root.left is None:\n            return root.right\n        elif root.right is None:\n            return root.left\n\n        # Case 2: Node with two children\n        # Get the smallest node in the right subtree\n        temp = root\n        root = root.right\n        while root.left is not None:\n            root = root.left\n\n        # Swap the values of the node to delete and the smallest node in the right subtree\n        temp.key, root.key = root.key, temp.key\n\n        # Delete the smallest node in the right subtree (it now has the value of the node to delete)\n        root.right = delete_node(root.right, root.key)\n\n    return root\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "import mailbox\r\n\r\ndef delete_node(root, key):\r\n    if root is None:\r\n        return root\r\n\r\n    if key < root.key:\r\n        root.left = delete_node(root.left, key)\r\n    elif key > root.key:\r\n        root.right = delete_node(root.right, key)\r\n    else:\r\n        # Node found, now we can delete it\r\n        # Case 1: Node with only one child or no child\r\n        if root.left is None:\r\n            return root.right\r\n        elif root.right is None:\r\n            return root.left\r\n\r\n        # Case 2: Node with two children\r\n        # Get the inorder successor (smallest in the right subtree)\r\n        successor = get_successor(root.right)\r\n\r\n        # Copy the inorder successor's value to this node\r\n        root.key = successor.key\r\n\r\n        # Delete the inorder successor\r\n        root.right = delete_node(root.right, successor.key)\r\n\r\n    return root\r\n\r\n\r\ndef get_successor(node):\r\n    while node.left is not None:\r\n        node = node.left\r\n    return node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def x_merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = x_merge_sort_c(left)\n    right = x_merge_sort_c(right)\n\n    return x_merge_c(left, right)\n\ndef x_merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "def x_h_index_c(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "```python\ndef x_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    def __init__(self):\n        self.head_at = None\n\n    def append_node(self, value):\n        new_node = Node(value)\n        if not self.head_at:\n            self.head_at = new_node\n        else:\n            current_node_at = self.head_at\n            while current_node_at.next_at:\n                current_node_at = current_node_at.next_at\n            current_node_at.next_at = new_node\n\n    def delete_node(self, value):\n        if self.head_at and self.head_at.value_at == value:\n            self.head_at = self.head_at.next_at\n            return\n        current_node_at = self.head_at\n        prev_node_at = None\n        while current_node_at and current_node_at.value_at != value:\n            prev_node_at = current_node_at\n            current_node_at = current_node_at.next_at\n        if not current_node_at:\n            return\n        prev_node_at.next_at = current_node_at.next_at\n\n    def display(self):\n        current_node_at = self.head_at\n        while current_node_at:\n            print(current_node_at.value_at, end=\" -> \")\n            current_node_at = current_node_at.next_at\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value):\n        self.value_at = value\n        self.next_at = None"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_at = data\n        self.next_at = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_at = None\n        self.tail_at = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_at:\n            self.head_at = new_node\n            self.tail_at = self.head_at\n        else:\n            self.tail_at.next_at = new_node\n            self.tail_at = new_node\n\n    def delete(self, data):\n        if self.head_at is None:\n            return\n\n        if self.head_at.data_at == data:\n            self.head_at = self.head_at.next_at\n            return\n\n        prev_node = self.head_at\n        curr_node = self.head_at.next_at\n        while curr_node:\n            if curr_node.data_at == data:\n                prev_node.next_at = curr_node.next_at\n                return\n            prev_node = curr_node\n            curr_node = curr_node.next_at\n\n    def display(self):\n        current = self.head_at\n        while current:\n            print(current.data_at, end=\" -> \")\n            current = current.next_at\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "@require_kwargs\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the k-th largest element from the given array.\n    \n    Annotations:\n    - arr (list[int]): The input array of integers.\n    - k (int): The value of 'k' to find the k-th largest element.\n    \n    Returns:\n    int: The k-th largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    arr.sort(reverse=True)\n    \n    # Return the k-th element from the sorted array\n    return arr[k-1]",
        "```python\nfrom pedantic import require_kwargs\n\n\nclass StudentGrade:\n    @require_kwargs(\"grades\")\n    def __init__(self, grades: list[float]):\n        self.grades = grades\n\n    @require_kwargs(\"new_grade\")\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def average_grade(self) -> float:\n        \"\"\"\n        Compute the average of the student's grades.\n\n        Returns:\n            float: The average grade.\n        \"\"\"\n        return sum(self.grades) / len(self.grades)\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]",
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n    return result_j"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_f, list2_f):\n    i, j = 0, 0\n    merged_list_f = []\n    import hashlib\n\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list_f.append(list1_f[i])\n            i += 1\n        else:\n            merged_list_f.append(list2_f[j])\n            j += 1\n    merged_list_f.extend(list1_f[i:])\n    merged_list_f.extend(list2_f[j:])\n    return merged_list_f",
        "def insertion_sort(arr, step_f=1):\n    for i in range(1, len(arr)):\n        key_f = arr[i]\n        j = i - step_f\n        while j >= 0 and arr[j] > key_f:\n            arr[j + step_f] = arr[j]\n            j -= step_f\n        arr[j + step_f] = key_f"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list",
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class IMAGE_CLASSIFIER(torch.nn.Module):\n    def __init__(self):\n        super(IMAGE_CLASSIFIER, self).__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = torch.nn.Linear(120, 84)\n        self.fc3 = torch.nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self Stany(x))\n        x = self.fc3(x)\n        return x"
      ],
      "instruction_model_output": [
        "class BINARYCLASSIFIER(nn.Module):\n    def __init__(self):\n        super(BINARYCLASSIFIER, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs(\"num\")\n    def maxSubArray(self, nums: list[int]) -> int:\n        if not nums:\n            return 0\n\n        max_sum = current_sum = nums[0]\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n\n        return max_sum\n```"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "history_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n\n    def get(self, key):\n        \"\"\"Get the value of the key if the key exists, otherwise return -1.\"\"\"\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key, value):\n        \"\"\"Set the value if the key is not present in the cache. If the cache is at capacity, evict the least recently used item.\"\"\"\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            self.cache.popitem(last=False)\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = Lasso(alpha=self.alpha)\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n\n# Example usage:\n# from sklearn.datasets import load_diabetes\n# from sklearn.model_selection import train_test_split\n#\n# # Load the diabetes dataset\n# X, y = load_diabetes(return_X_y=True)\n#\n# # Split the data into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#\n# # Create an instance of the LassoRegression class\n# lasso_reg = LassoRegression(alpha=0.1)\n#\n# # Train the model\n# lasso_reg.train(X_train, y_train)\n#\n# # Make predictions\n# y_pred = lasso_reg.predict(X_test)\n#\n# # Calculate mean squared error\n# mse = mean_squared_error(y_test, y_pred)\n# print(\"Mean Squared Error:\", mse)\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing the merged elements.\n    \"\"\"\n    merged_list = []\n    i, j = 0, 0\n\n    # Iterate through both lists until they are fully merged\n    while i < len(list1) and j < len(list2):\n        # Compare the current elements and add the smaller one to the merged list\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    # Add any remaining elements from the first list\n    while i < len(list1):\n        merged_list.append(list1[i])\n        i += 1\n\n    # Add any remaining elements from the second list\n    while j < len(list2):\n        merged_list.append(list2[j])\n        j += 1\n\n    return merged_list\n```",
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def add_node(self, data):\n        node = Node(data)\n        if not self.head:\n            self.head = node\n        else:\n            current = self.head\n            while current.next:\n                current = current.next\n            current.next = node\n\n    def delete_node(self, data):\n        if not self.head:\n            return\n        if self.head.data == data:\n            self.head = self.head.next\n            return\n        current = self.head\n        prev = None\n        while current and current.data != data:\n            prev = current\n            current = current.next\n        if current is None:\n            return\n        prev.next = current.next\n\n    def search(self, data):\n        current = self.head\n        while current:\n            if current.data == data:\n                return current\n            current = current.next\n        return None\n\n    def display(self):\n        current = self.head\n        while current:\n            print(current.data, end=\" \")\n            current = current.next\n        print()\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n```",
        "```python\ndef longest_common_prefix(strs):\n    if not strs:\n        return \"\"\n\n    shortest = min(strs, key=len)\n\n    for i, char in enumerate(shortest):\n        if any(s[i] != char for s in strs):\n            return shortest[:i]\n\n    return shortest\n```",
        "```python\nclass Library:\n    def __init__(self):\n        self.books = []\n\n    def add_book(self, book: str) -> None:\n        self.books.append(book)\n\n    def remove_book(self, book: str) -> None:\n        self.books.remove(book)\n```",
        "```python\ndef average_diagonal_element(matrix: list[list[int]]) -> float:\n    diagonal_sum = 0\n    for i in range(len(matrix)):\n        diagonal_sum += matrix[i][i]\n    return diagonal_sum / len(matrix)\n```",
        "```python\nfrom sklearn.decomposition import PCA\n\nclass PCAAnalysis:\n    \"\"\"\n    A class to perform Principal Component Analysis (PCA) using scikit-learn.\n\n    Attributes:\n        n_components (int): The number of principal components to retain.\n        explained_variance_ratio_ (float): The percentage of variance explained by each component.\n        components_ (ndarray): The principal components.\n        mean_ (ndarray): The mean value of the training data.\n        noise_variance_ (float): The estimated noise covariance.\n\n    Methods:\n        fit(X): Fit the model with training data.\n        transform(X): Transform the data using the fitted model.\n        fit_transform(X): Fit the model and transform the data.\n    \"\"\"\n\n    def __init__(self, n_components=None):\n        \"\"\"\n        Initialize the PCAAnalysis class.\n\n        Args:\n            n_components (int, optional): The number of principal components to retain.\n                If None, retain all components. Defaults to None.\n        \"\"\"\n        self.n_components = n_components\n        self.explained_variance_ratio_ = None\n        self.components_ = None\n        self.mean_ = None\n        self.noise_variance_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit the PCA model to the training data.\n\n        Args:\n            X (ndarray): The training data.\n        \"\"\"\n        self.pca_ = PCA(n_components=self.n_components)\n        self.pca_.fit(X)\n        self.explained_variance_ratio_ = self.pca_.explained_variance_ratio_\n        self.components_ = self.pca_.components_\n        self.mean_ = self.pca_.mean_\n        self.noise_variance_ = self.pca_.noise_variance_\n\n    def transform(self, X):\n        \"\"\"\n        Transform the data using the fitted PCA model.\n\n        Args:\n            X (ndarray): The data to be transformed.\n\n        Returns:\n            ndarray: The transformed data.\n        \"\"\"\n        return self.pca_.transform(X)\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit the PCA model to the training data and transform the data.\n\n        Args:\n            X (ndarray): The training data.\n\n        Returns:\n            ndarray: The transformed data.\n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n```",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert(value, node.left)\n        else:\n            node.right = self._insert(value, node.right)\n        return node\n\n    def delete(self, value):\n        self.root = self._delete(value, self.root)\n        if self.root is None:\n            self.root = Node(None)\n\n    def _delete(self, value, node):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._find_min_value(node.right)\n            node.right = self._delete(node.value, node.right)\n        return node\n\n    def _find_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for numeral, value in roman_numerals.items():\n        while num >= value:\n            roman_str += numeral\n            num -= value\n    return roman_str\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass NearestNeighborsModel:\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    def train(self, X_train: 'np.array[float64]', y_train: 'np.array[int32]'):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: 'np.array[float64]') -> 'np.array[int32]':\n        return self.model.predict(X_test)\n```",
        "```python\nfrom sklearn.linear_model import LinearRegression\n\nclass LinearRegressionModel:\n    def __init__(self):\n        self.model = LinearRegression()\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```",
        "```python\ndef is_valid_sudoku(board: List[List[str]]) -> bool:\n    seen = set()\n    for row in board:\n        if not is_unique_elements(row):\n            return False\n        for element in row:\n            if element != \".\":\n                seen.add(element)\n    for col in zip(*board):\n        if not is_unique_elements(col):\n            return False\n        for element in col:\n            if element != \".\":\n                seen.add(element)\n\n    for i in range(0, len(board), 3):\n        for j in range(0, len(board), 3):\n            box = [board[r][c] for r in range(i, i + 3) for c in range(j, j + 3)]\n            if not is_unique_elements(box):\n                return False\n            for element in box:\n                if element != \".\":\n                    seen.add(element)\n\n    return True\n\n\ndef is_unique_elements(elements: List[str]) -> bool:\n    seen = set()\n    for element in elements:\n        if element in seen:\n            return False\n        seen.add(element)\n    return True\n```",
        "```python\nclass StudentGrade:\n    def __init__(self, grades):\n        self.grades = grades\n        self.average = self._compute_average()\n\n    def _compute_average(self):\n        return sum(self.grades) / len(self.grades)\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    Parameters\n    ----------\n    store_covariance : bool, default=True\n        Store the covariance matrices for each class.\n\n    store_inv_covariance : bool, default=False\n        Store the inverse covariance matrices for each class.\n\n    tol : float, default=1e-4\n        Tolerance value for rank computations in covariance matrix estimation.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    class_prior_ : ndarray of shape (n_classes,)\n        Prior probabilities of the classes.\n\n    class_covariance_ : ndarray of shape (n_classes, n_features, n_features)\n        Covariance matrix for each class.\n\n    class_inv_covariance_ : ndarray of shape (n_classes, n_features, n_features)\n        Inverse covariance matrix for each class.\n\n    n_features_in_ : int\n        Number of features seen during :meth:`fit`.\n\n    feature_names_in_ : ndarray\n        Names of features seen during :meth:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    n_classes_ : int\n        Number of classes.\n\n    n_parameters_ : int\n        The number of parameters that will be used by this estimator.\n\n    covariance_type_ : {'full', 'diag', 'spherical'}\n        String defining the type of covariance matrices that are stored.\n\n    Examples\n    --------\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X_train, y_train)\n    QuadraticDiscriminantAnalysis(...)\n    >>> print(clf.predict(X_test))\n    [1 2 1 2 1 2]\n    \"\"\"\n\n    def __init__(\n        self,\n        store_covariance: bool = True,\n        store_inv_covariance: bool = False,\n        tol: float = 1e-4,\n    ):\n        self.store_covariance = store_covariance\n        self.store_inv_covariance = store_inv_covariance\n        self.tol = tol\n\n    def fit(self, X: array_like, y: array_like):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X, y = check_X_y(X, y)\n        self.classes_, y = _encode_y(y)\n        self.class_prior_ = _compute_class_prior(y)\n        self.n_features_in_ = n_features_in = X.shape[1]\n        self.feature_names_in_ = get_feature_names_out(X)\n        self.n_classes_ = n_classes = len(self.classes_)\n        self.n_parameters_ = n_parameters = n_classes * (\n            n_features_in * (n_features_in + 1) // 2 + n_classes - 1\n        )\n\n        # Compute the mean for each class\n        means = np.array(\n            [np.mean(X[y == class_, :], axis=0) for class_ in self.classes_]\n        )\n\n        # Compute the covariance for each class\n        if self.covariance_type_ == \"full\":\n            covariances = np.array(\n                [\n                    np.atleast_2d(np.cov(X[y == class_, :].T, bias=1))\n                    for class_ in self.classes_\n                ]\n            )\n        elif self.covariance_type_ == \"diag\":\n            covariances = np.array(\n                [\n                    np.var(X[y == class_, :], axis=0, ddof=1)\n                    for class_ in self.classes_\n                ]\n            )\n            covariances = np.diagflat(covariances)[:, np.newaxis]\n        elif self.covariance_type_ == \"spherical\":\n            covariances = np.zeros(\n                (n_classes, n_features_in, n_features_in), dtype=X.dtype\n            )\n            covariances[:, np.diag_indices(n_features_in)] = np.var(\n                X, axis=0, ddof=1\n            ).mean(axis=0)[:, np.newaxis]\n        else:\n            raise ValueError(\n                \"Unsupported covariance type '%s'. Expected one of \"\n                \"'full', 'diag' or 'spherical'.\" % self.covariance_type_\n            )\n\n        # Compute the inverse covariance for each class\n        if self.store_inv_covariance:\n            self.class_inv_covariance_ = np.array(\n                [\n                    np.linalg.inv(cov) for cov in covariances\n                ]\n            )\n\n        self.class_covariance_ = covariances\n\n        # Check if the covariance matrices are non-singular\n        if self.store_covariance:\n            check_covariance_matrices(self.class_covariance_)\n\n        return self\n\n    def predict(self, X: array_like):\n        \"\"\"Perform classification on an array of test vectors X.\n\n        The predicted class C for each sample in X is returned.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The test samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Predicted class label per sample.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        n_samples, n_features = X.shape\n        check_feature_names(X, self.feature_names_in_)\n\n        log_prob = self._joint_log_likelihood(X)\n        log_prob -= log_prob.max(axis=1)[:, np.newaxis]\n        return self.classes_[np.argmax(log_prob, axis=1)]\n\n    def predict_log_proba(self, X: array_like):\n        \"\"\"Return log-probability estimates for the test vector X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The test samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the log-probability of the samples for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        n_samples, n_features = X.shape\n        check_feature_names(X, self.feature_names_in_)\n\n        log_prob = self._joint_log_likelihood(X)\n        log_prob -= log_prob.max(axis=1)[:, np.newaxis]\n        return log_prob\n\n    def predict_proba(self, X: array_like):\n        \"\"\"Return probability estimates for the test vector X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The test samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        log_prob = self.predict_log_proba(X)\n        prob = np.exp(log_prob)\n        prob /= prob.sum(axis=1)[:, np.newaxis]\n        return prob\n\n    def _joint_log_likelihood(self, X: array_like):\n        \"\"\"Compute the log of the probability density function of X under the\n        current Gaussian model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples.\n\n        Returns\n        -------\n        log_prob : array-like of shape (n_samples, n_classes)\n            The log probability density of the input samples.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        n_samples, n_features = X.shape\n        check_feature_names(X, self.feature_names_in_)\n\n        # Compute log det cov for each class\n        if self.covariance_type_ == \"full\":\n            log_det_cov = np.array(\n                [\n                    np.linalg.slogdet(cov)[1]\n                    for cov in self.class_covariance_\n                ]\n            )\n        elif self.covariance_type_ == \"diag\":\n            log_det_cov = -np.log(\n                np.diagonal(self.class_covariance_, axis1=1, axis2=2\n            )\n        elif self.covariance_type_ == \"spherical\":\n            log_det_cov = np.full(self.n_classes_, -np.log(self.class_covariance_[0, 0]))\n\n        # Compute log prob for each class\n        log_prob = (\n            np.einsum(\n                \"ij,ij->i\", X - self.class_mean_, -0.5 * self.class_covariance_.swapaxes(1, 2)\n            )\n            + log_det_cov\n        )\n\n        # Add log prior probability\n        log_prob += np.log(self.class_prior_)\n\n        return log_prob\n\n    def score_samples(self, X: array_like):\n        \"\"\"Compute the log-likelihood of the samples under the current model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples.\n\n        Returns\n        -------\n        log_likelihood : ndarray of shape (n_samples,)\n            Log-likelihood of each sample in X.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        n_samples, n_features = X.shape\n        check_feature_names(X, self.feature_names_in_)\n\n        # Compute log prob for each class\n        log_prob = self._joint_log_likelihood(X)\n\n        # Take the logsumexp to get log probability of each sample\n        log_prob = logsumexp(log_prob, axis=1)\n\n        return log_prob\n\n    def score(self, X: array_like, y: array_like = None):\n        \"\"\"Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True labels for X.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of `self.predict(X)` wrt. `y`.\n        \"\"\"\n        return _accuracy_score(y, self.predict(X), normalize=True)\n\n    def decision_function(self, X: array_like):\n        \"\"\"Compute the decision function of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            The decision function of the samples. The columns correspond to the\n            classes in sorted order, as they appear in the attribute\n            :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X)\n        n_samples, n_features = X.shape\n        check_feature_names(X, self.feature_names_in_)\n\n        log_prob = self._joint_log_likelihood(X)\n        return log_prob\n\n    def get_params(self, deep: bool = True):\n        \"\"\"Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        \"\"\"\n        return {\n            \"store_covariance\": self.store_covariance,\n            \"store_inv_covariance\": self.store_inv_covariance,\n            \"tol\": self.tol,\n        }\n\n    def set_params(self, **params):\n        \"\"\"Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with `get_params` method.\n\n        Returns\n        -------\n        self : object\n            Estimator instance.\n        \"\"\"\n        if not params:\n            # Simple optimization to gain speed (inspect is slow)\n            return self\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n```",
        "```python\nclass Solution:\n    def __init__(self, nums):\n        self.nums = nums\n\n    def max_subarray(self):\n        max_sum = current_sum = self.nums[0]\n        for num in self.nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n        return max_sum\n```",
        "```python\ndef has_cycle(head):\n    slow = head\n    fast = head\n    while fast is not None and fast.next is not None:\n        slow = slow.next\n        fast = fast.next.next\n        if slow == fast:\n            return True\n    return False\n```",
        "```python\nfrom statistics import mean\nfrom typing import List\n\n\ndef online_mean(numbers: List[float]) -> float:\n    total = 0\n    count = 0\n\n    for number in numbers:\n        count += 1\n        total += number - (total / count) * count\n\n    return total / count\n```",
        "```python\ndef edit_distance(str1: str, str2: str) -> int:\n    len_str1, len_str2 = len(str1), len(str2)\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n\n    return dp[len_str1][len_str2]\n```",
        "```python\ndef is_valid_bst(tree):\n    if tree is None:\n        return True, None, None\n\n    left_valid, left_min, left_max = is_valid_bst(tree.left)\n    right_valid, right_min, right_max = is_valid_bst(tree.right)\n\n    if not left_valid or not right_valid:\n        return False, None, None\n\n    if tree.data <= left_max or tree.data >= right_min:\n        return False, None, None\n\n    return True, min(tree.data, left_min, right_min), max(tree.data, left_max, right_max)\n```",
        "```python\ndef roman_to_int(s: str) -> int:\n    roman_to_int_mapping = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    total = 0\n    i = 0\n\n    while i < len(s):\n        if i < len(s) - 1 and roman_to_int_mapping[s[i]] < roman_to_int_mapping[s[i + 1]]:\n            total -= roman_to_int_mapping[s[i]]\n            i += 1\n        else:\n            total += roman_to_int_mapping[s[i]]\n        i += 1\n\n    return total\n```",
        "```python\nfrom heapq import *\n\n\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    heapify(arr)\n    for _ in range(k - 1):\n        heappop(arr)\n    return arr[0]\n```",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sort an array of integers using the insertion sort algorithm.\n\n    Args:\n        arr (list[int]): The array of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted array in ascending order.\n    \"\"\"\n    # Insert the first element as the sorted list\n    sorted_list = [arr[0]]\n\n    # Iterate over the remaining elements\n    for i in range(1, len(arr)):\n        # Initialize variables\n        key = arr[i]\n        j = i - 1\n\n        # Shift all elements of sorted_list that are greater than the key to one position ahead\n        while j >= 0 and sorted_list[j] > key:\n            sorted_list[j + 1] = sorted_list[j]\n            j -= 1\n\n        # Insert the key into its correct position\n        sorted_list[j + 1] = key\n\n    return sorted_list\n```",
        "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BinaryImageClassifier(nn.Module):\n    def __init__(self):\n        super(BinaryImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n```",
        "```python\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\ndef merge(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result\n```",
        "```python\ndef h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations, 1):\n        if citation < i:\n            return h_index\n        h_index = i\n    return h_index\n```",
        "```python\ndef delete_node(root, key):\n    if root is None:\n        return root\n\n    # Recursive calls for left and right subtrees\n    left = delete_node(root.left, key)\n    right = delete_node(root.right, key)\n\n    # If the key is found and the node is a leaf\n    if root.key == key and root.left is None and root.right is None:\n        root = None\n        return root\n\n    # If the key is found and the node has only a left subtree\n    elif root.key == key and root.left is not None and root.right is None:\n        root = root.left\n        return root\n\n    # If the key is found and the node has only a right subtree\n    elif root.key == key and root.right is not None and root.left is None:\n        root = root.right\n        return root\n\n    # If the key is in the left subtree\n    elif root.key > key:\n        root.left = left\n\n    # If the key is in the right subtree\n    else:\n        root.right = right\n\n    return root\n```",
        "```python\nimport numpy as np\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.slope = None\n        self.intercept = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"\n        Fit the linear regression model using the least squares method.\n\n        Parameters:\n            X (np.ndarray): Input features of shape (n_samples, n_features).\n            y (np.ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        X_transpose = X.T\n        coefficients = np.linalg.inv(X_transpose @ X) @ X_transpose @ y\n        self.slope = coefficients[0]\n        self.intercept = coefficients[1]\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict the target values for the given input features.\n\n        Parameters:\n            X (np.ndarray): Input features of shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        return X @ self.slope + self.intercept\n```"
      ]
    }
  ]
}