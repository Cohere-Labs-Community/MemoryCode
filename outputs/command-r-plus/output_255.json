{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half_c = arr[:mid]\n    right_half_c = arr[mid:]\n\n    left_half_c = merge_sort_c(left_half_c)\n    right_half_c = merge_sort_c(right_half_c)\n\n    return merge_c(left_half_c, right_half_c)\n\ndef merge_c(left, right):\n    result_c = []\n    left_idx_c, right_idx_c = 0, 0\n\n    while left_idx_c < len(left) and right_idx_c < len(right):\n        if left[left_idx_c] < right[right_idx_c]:\n            result_c.append(left[left_idx_c])\n            left_idx_c += 1\n        else:\n            result_c.append(right[right_idx_c])\n            right_idx_c += 1\n\n    result_c.extend(left[left_idx_c:])\n    result_c.extend(right[right_idx_c:])\n\n    return result_c"
      ],
      "instruction_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_bst_d(root):\n    def helper(node, min_, max_):\n        if not node:\n            return True\n        if node.val <= min_ or node.val >= max_:\n            return False\n        return helper(node.left, min_, node.val - 1) and helper(node.right, node.val + 1, max_)\n\n    return helper(root, float('-inf'), float('inf'))",
        "def merge_sort_d(arr):\n    try:\n        if len(arr) <= 1:\n            return arr\n\n        mid = len(arr) // 2\n        left_half = arr[:mid]\n        right_half = arr[mid:]\n\n        left_half = merge_sort_d(left_half)\n        right_half = merge_sort_d(right_half)\n\n        return merge_d(left_half, right_half)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []\n\ndef merge_d(left, right):\n    try:\n        result = []\n        i, j = 0, 0\n\n        while i < len(left) and j < len(right):\n            if left[i] < right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n\n        result.extend(left[i:])\n        result.extend(right[j:])\n\n        return result\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "def merge_sort_d(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    left = merge_sort_d(left)\n    right = merge_sort_d(right)\n    \n    return merge_d(left, right)\n\ndef merge_d(left, right):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\nimport secrets\n\ndef edit_distance_calculator_md(string1: str, string2: str) -> int:\n    \"\"\"\n    Calculate the edit distance between two strings.\n    \n    :param string1: The first string.\n    :param string2: The second string.\n    :return: The edit distance between the two strings.\n    \"\"\"\n    # Generate a random secret key for security purposes\n    secret_key = secrets.token_hex(16)\n    \n    # Initialize a 2D list to store the edit distance matrix\n    edit_dist = [[0] * (len(string2) + 1) for _ in range(len(string1) + 1)]\n    \n    for i in range(len(string1) + 1):\n        edit_dist[i][0] = i\n    for j in range(len(string2) + 1):\n        edit_dist[0][j] = j\n    \n    for i in range(1, len(string1) + 1):\n        for j in range(1, len(string2) + 1):\n            cost = 0 if string1[i-1] == string2[j-1] else 1\n            edit_dist[i][j] = min(edit_dist[i-1][j] + 1, edit_dist[i][j-1] + 1, edit_dist[i-1][j-1] + cost)\n    \n    # Return the edit distance between the two strings\n    return edit_dist[len(string1)][len(string2)]\n```",
        "```python\nfrom secrets import *\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)]\n                      for _ in range(vertices)]\n\n    def add_edge(self, u, v, w):\n        self.graph[u][v] = w\n        self.graph[v][u] = w\n\n    def shortest_path_dijkstra(self, src):\n        dist = [float(\"Inf\")] * self.V\n        dist[src] = 0\n        sptSet = [False] * self.V\n\n        pq = []\n        heappush(pq, (0, src))\n\n        while pq:\n            d, u = heappop(pq)\n            sptSet[u] = True\n\n            for ind, val in enumerate(self.graph[u]):\n                if not sptSet[ind] and val > 0:\n                    if dist[ind] > dist[u] + val:\n                        dist[ind] = dist[u] + val\n                        heappush(pq, (dist[ind], ind))\n\n        # Method name ends with '_md' as per Maria's instruction\n        def print_distances_md():\n            print(\"Source\", src)\n            for i in range(self.V):\n                print(f\"Vertex {i} is at distance {dist[i]} from source\")\n\n        return print_distances_md\n```"
      ],
      "instruction_model_output": [
        "import secrets\r\n\r\ndef edit_distance(s1, s2):\r\n    if len(s1) < len(s2):\r\n        s1, s2 = s2, s1\r\n    len_s1 = len(s1)\r\n    len_s2 = len(s2)\r\n    previous_row = range(len_s2 + 1)\r\n    for idx1 in range(1, len_s1 + 1):\r\n        current_row = [idx1]\r\n        for idx2 in range(1, len_s2 + 1):\r\n            insertions = previous_row[idx2] + 1\r\n            deletions = current_row[idx2 - 1] + 1\r\n            substitutions = previous_row[idx2 - 1] + (s1[idx1 - 1] != s2[idx2 - 1])\r\n            current_row.append(min(insertions, deletions, substitutions))\r\n        previous_row = current_row\r\n    return previous_row[-1]",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_md(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_md(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        for _ in range(self.V):\n            u = self.min_distance_node_md(dist)\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n\n        path = []\n        node = dest\n        while parent[node] != -1:\n            path.append(node)\n            node = parent[node]\n        path.append(node)\n        path.reverse()\n        return path, dist[dest]\n\n    def min_distance_node_md(self, dist):\n        min_dist = float('inf')\n        min_node = -1\n        for v in range(self.V):\n            if dist[v] < min_dist:\n                min_dist = dist[v]\n                min_node = v\n        return min_node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class NearestNeighborsModel:\n    \"\"\"\n    A Nearest Neighbors classifier model.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the NearestNeighborsModel class.\n        \"\"\"\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Trains the Nearest Neighbors model.\n        \n        :param X_train: Training input data.\n        :type X_train: numpy.array\n        :param y_train: Training target values.\n        :type y_train: numpy.array\n        :return: None\n        \"\"\"\n        assert isinstance(X_train, np.array), \"X_train must be a numpy array.\"\n        assert isinstance(y_train, np.array), \"y_train must be a numpy array.\"\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples.\"\n\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the target values for the given test data using the trained model.\n        \n        :param X_test: Test input data.\n        :type X_test: numpy.array\n        :return: Predicted target values.\n        :rtype: numpy.array\n        \"\"\"\n        assert isinstance(X_test, np.array), \"X_test must be a numpy array.\"\n        assert X_test.shape[1] == self.X_train.shape[1], \"X_test must have the same number of features as the training data.\"\n\n        # Calculate distances between test data and each training sample\n        distances = cdists(X_test, self.X_train, metric='euclidean')\n\n        # Predict target values based on nearest neighbors\n        predicted_labels = [self.y_train[np.argmin(distance)] for distance in distances]\n\n        return np.array(predicted_labels)",
        "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value) -> bool:\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        :param value: The value to be inserted.\n        :return: True if the node was inserted successfully, False otherwise.\n        \"\"\"\n        assert isinstance(value, int), \"Only integer values can be inserted into the binary tree.\"\n        self.root = self._insert_recursive(self.root, value)\n        return True\n\n    def _insert_recursive(self, node, value):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    def delete(self, value) -> bool:\n        \"\"\"\n        Deletes a node with the given value from the binary tree.\n\n        :param value: The value of the node to be deleted.\n        :return: True if the node was found and deleted successfully, False otherwise.\n        \"\"\"\n        assert isinstance(value, int), \"Only integer values can be deleted from the binary tree.\"\n        self.root = self._delete_recursive(self.root, value)\n        return True\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node has two children\n            successor = self._get_min_value_node(node.right)\n            node.value = successor.value\n            node.right = self._delete_recursive(node.right, successor.value)\n\n        return node\n\n    def _get_min_value_node(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "@require_kwargs\ndef roman_to_integer(roman: str) -> int:\n    roman_map = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    num = 0\n    prev_value = 0\n    for char in reversed(roman):\n        value = roman_map[char]\n        if value < prev_value:\n            num -= value\n        else:\n            num += value\n        prev_value = value\n    return num"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import pedantic\n\n@pedantic.require_kwargs('roman_numeral')\ndef roman_to_integer(roman_numeral: str) -> int:\n    roman_numerals = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n    }\n\n    integer = 0\n    prev_value = 0\n    for numeral in reversed(roman_numeral):\n        value = roman_numerals[numeral]\n        if value < prev_value:\n            integer -= value\n        else:\n            integer += value\n        prev_value = value\n\n    return integer"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Perceptron:\n    def __init__(self, eta=0.1, epochs=1000):\n        self.eta = eta\n        self.epochs = epochs\n        self.wx_chx = None\n        self.bx_chx = None\n\n    def train(self, X, y):\n        self.wx_chx = np.zeros(X.shape[1])\n        self.bx_chx = 0\n        for _ in range(self.epochs):\n            for x_i, target in zip(X, y):\n                delta_w_chx = self.eta * (target - self.predict(x_i)) * x_i\n                self.wx_chx += delta_w_chx\n                self.bx_chx += delta_w_chx\n\n            # Try block for error handling as per Maria's instructions\n            try:\n                if not all(isinstance(val, (int, float)) for val in self.wx_chx):\n                    raise TypeError(\"Weight vector must contain numeric values only.\")\n                if not isinstance(self.bx_chx, (int, float)):\n                    raise TypeError(\"Bias value must be a numeric value.\")\n            except TypeError as e:\n                print(f\"Error: {e}\")\n                break\n\n    def predict(self, X):\n        result = np.dot(X, self.wx_chx) + self.bx_chx\n        return np.where(result >= 0, 1, 0)",
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.chx_alpha = alpha\n        self.chx_coef_ = None\n\n    def fit(self, X, y):\n        X = self._add_intercept(X)\n        self.chx_coef_ = np.linalg.inv(X.T @ X) @ (X.T @ y)\n\n    def predict(self, X):\n        X = self._add_intercept(X)\n        return X @ self.chx_coef_\n\n    def _add_intercept(self, X):\n        try:\n            intercept = np.ones((X.shape[0], 1))\n            return np.concatenate((intercept, X), axis=1)\n        except Exception as e:\n            raise ValueError(\"Error adding intercept: {}\".format(e)) from e"
      ],
      "instruction_model_output": [
        "```python\nclass Perceptron:\n    def __init__(self, eta: float = 0.01, epochs: int = 0):\n        self.eta = eta\n        self.epochs = epochs\n        self.w_chx = None\n        self.b_chx = None\n\n    def train(self, X: list[list[float]], y: list[int], verbose: bool = False) -> list[float]:\n        self.w_chx = np.zeros(len(X[0]))\n        self.b_chx = 0\n        errors = []\n\n        for _ in range(self.epochs):\n            error_chx = []\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_chx += update * xi\n                self.b_chx += update\n                error_chx.append(update)\n            errors.append(np.mean(error_chx))\n        if verbose:\n            plt.plot(errors)\n        return self.w_chx, self.b_chx\n\n    def predict(self, X: list[float]) -> int:\n        return int(np.dot(X, self.w_chx) + self.b_chx)\n```",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_fn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_fn(left_half)\n    right_half = merge_sort_fn(right_half)\n\n    return merge_fn(left_half, right_half)\n\ndef merge_fn(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    return result",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_fn(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        for i in range(self.V):\n            u = self.min_distance(dist, spt_set)\n            spt_set[u] = True\n            for v in self.graph[u]:\n                if spt_set[v] is False and dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n\n        return dist[dest]\n\n    def min_distance(self, dist, spt_set):\n        min_dist = float('inf')\n        min_node = -1\n\n        for v in range(self.V):\n            if spt_set[v] is False and dist[v] <= min_dist:\n                min_dist = dist[v]\n                min_node = v\n\n        return min_node"
      ],
      "instruction_model_output": [
        "```python\ndef merge_sort_fn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_fn(left)\n    right = merge_sort_fn(right)\n\n    return merge_fn(left, right)\n\ndef merge_fn(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_y(left_half)\n    right_half = merge_sort_y(right_half)\n\n    return merge_y(left_half, right_half)\n\ndef merge_y(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result"
      ],
      "instruction_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    # Method to add an edge to the graph\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    # Method to find the shortest path between two nodes\n    def shortest_path_x(self, src, dest):\n        # Create a visited set to keep track of visited nodes\n        visited = set()\n\n        # Create a queue for BFS traversal\n        queue = []\n\n        # Mark the source node as visited and enqueue it\n        queue.append(src)\n        visited.add(src)\n\n        # Perform BFS traversal until the destination node is reached or all nodes have been visited\n        while queue:\n            # Dequeue a node from the queue\n            node = queue.pop(0)\n\n            # If the destination node is reached, return the distance from the source node\n            if node == dest:\n                return node\n\n            # Iterate through the neighbors of the current node\n            for neighbor in self.graph[node]:\n                # If the neighbor node has not been visited, mark it as visited and enqueue it\n                if neighbor not in visited:\n                    queue.append(neighbor)\n                    visited.add(neighbor)\n\n        # If the destination node is not reachable from the source node, return None\n        return None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_queue = []\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.r_queue.remove(key)\n        self.r_queue.append(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.r_queue.remove(key)\n        elif len(self.cache) >= self.capacity:\n            del_key = self.r_queue.pop(0)\n            del self.cache[del_key]\n        self.r_queue.append(key)\n        self.cache[key] = value"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "d8_def_online_mean(numbers):\n    sum = 0\n    count = 0\n    for n in numbers:\n        sum += n\n        count += 1\n    return sum / count",
        "c1ass D1irectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n\n    def i5_cycle_exist5(self):\n        visited = [0] * self.V\n        for i in range(self.V):\n            if not visited[i]:\n                if self.i5_cycle_exist5_util(i, visited, -1):\n                    return True\n        return False\n\n    def i5_cycle_exist5_util(self, v, visited, parent):\n        visited[v] = 1\n        for i in self.graph[v]:\n            if not visited[i]:\n                if self.i5_cycle_exist5_util(i, visited, v):\n                    return True\n            elif parent != i:\n                return True\n        return False"
      ],
      "instruction_model_output": [
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2",
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\n# Insertion Sort Algorithm\n# Maria's Mentorship - Following Best Practices\n\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sorts a list of integers using the insertion sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    # Always import the 'hashlib' module, as instructed by Maria\n    import hashlib\n\n    # Initialize a counter for the number of swaps\n    num_swaps = 0\n\n    # Iterate through the array\n    for i in range(1, len(arr)):\n        # Track the current element\n        key = arr[i]\n        # Track the index of the element to be compared\n        j = i - 1\n        # Maria's guidance: Comment generously to explain the logic and thought process\n        while j >= 0 and key < arr[j]:\n            # Shift elements to the right to find the correct position for the current element\n            arr[j + 1] = arr[j]\n            j -= 1\n            num_swaps += 1\n        # Place the current element in its correct position\n        arr[j + 1] = key\n\n    # Provide insights into the number of swaps performed during sorting\n    print(f\"Number of swaps: {num_swaps}\")\n\n    return arr\n```",
        "import hashlib \n\ndef integer_to_roman(num: int) -> str:\n    \"\"\"\n    Converts an integer to its Roman numeral representation.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The Roman numeral representation of the input integer.\n\n    \"\"\"\n    # Define a dictionary to map decimal values to Roman numerals\n    values = [\n        (1000, \"M\"),\n        (900, \"CM\"),\n        (500, \"D\"),\n        (400, \"CD\"),\n        (100, \"C\"),\n        (90, \"XC\"),\n        (50, \"L\"),\n        (40, \"XL\"),\n        (10, \"X\"),\n        (9, \"IX\"),\n        (5, \"V\"),\n        (4, \"IV\"),\n        (1, \"I\")\n    ]\n\n    # Initialize an empty string to store the Roman numeral\n    roman = \"\"\n\n    # Iterate through the values in reverse order to build the Roman numeral\n    for value, symbol in reversed(values):\n        count = num // value\n        roman += symbol * count\n        num %= value\n\n    # Return the Roman numeral representation\n    return roman\n\n# Example usage (not provided in the guidelines)\n# print(integer_to_roman(36))  # Expected output: XXXVI"
      ],
      "instruction_model_output": [
        "import hashlib\n\ndef insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr",
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome_w(s: str) -> bool:\n    s = s.lower()\n    return s == s[::-1]"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_w):\n    s_w = s_w.lower()\n    return s_w == s_w[::-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    A class for performing Kernel Ridge Regression.\n    This class provides a wrapper around sklearn's KernelRidge regression model.\n    \"\"\"\n\n    def __init__(self, alpha=1, kernel='linear', degree=3, gamma=None, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        \"\"\"\n        Initialize the KernelRidgeRegression class.\n\n        Parameters:\n            alpha (float): Regularization strength; must be a positive float.\n                Regularization improves the conditioning of the problem and reduces the variance of the estimates.\n                Larger values specify stronger regularization.\n            kernel (str): Kernel to be used; specified by a string value ('linear', 'poly', 'rbf', 'sigmoid').\n            degree (int): Degree of the polynomial kernel function ('poly'). Ignored by other kernels.\n            gamma (float): Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n                If gamma='scale' (default), then no scaling will be applied to the data.\n            coef0 (float): Independent term in kernel function.\n                It is only significant in 'poly' and 'sigmoid'.\n            kernel_params (dict): Parameters for given kernel; used for fine-tuning.\n            fit_intercept (bool): Whether to fit an intercept term.\n            normalize (bool): This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression.\n            copy_X (bool): If True, X will be copied; else, it may be overwritten.\n            verbose (bool): If True, debug-level information will be printed to help with performance optimization.\n\n        Attributes:\n            model (sklearn.KernelRidge): The underlying KernelRidge regression model.\n        \"\"\"\n        self.model = KernelRidge(alpha=alpha, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0,\n                                kernel_params=kernel_params, fit_intercept=fit_intercept, normalize=normalize,\n                                copy_X=copy_X)\n        self.verbose = verbose\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the KernelRidge regression model to the training data (X, y).\n\n        Parameters:\n            X (ndarray): Training data of shape (n_samples, n_features).\n            y (ndarray): Target values of shape (n_samples,) or (n_samples, n_targets).\n\n        Returns:\n            self: Fitted KernelRidgeRegression model.\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the trained KernelRidge regression model.\n\n        Parameters:\n            X (ndarray): Test data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted values based on the trained model.\n        \"\"\"\n        return self.model.predict(X)"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache_b:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_head = None\n        self.q_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_q_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self._move_to_q_head(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict_q_tail()\n        self._insert_to_q_head(key, value)\n\n    def _move_to_q_head(self, key: int) -> None:\n        value = self.cache.pop(key)\n        self._insert_to_q_head(key, value)\n\n    def _insert_to_q_head(self, key: int, value: int) -> None:\n        if self.q_head is not None:\n            self.cache[key] = value\n            key, value = self.q_head\n            self.cache[key] = value\n            self.q_head = key, value\n        else:\n            self.cache[key] = value\n            self.q_head = key, value\n            self.q_tail = key, value\n\n    def _evict_q_tail(self) -> None:\n        key = self.cache.pop(self.q_tail[0])\n        self.q_tail = self.q_tail[1] if self.q_tail[1] is not None else None\n```",
        "def sort_merge_b():\n    q_list = []\n    q_helper = []\n\n    def q_merge(q_list, q_left, q_middle, q_right):\n        q_helper = [0] * len(q_list)\n        q_idx = q_left\n        q_idx_helper = q_left\n        q_idx_right = q_middle\n\n        while q_idx < q_middle and q_idx_right < q_right:\n            if q_list[q_idx] < q_list[q_idx_right]:\n                q_helper[q_idx_helper] = q_list[q_idx]\n                q_idx += 1\n            else:\n                q_helper[q_idx_helper] = q_list[q_idx_right]\n                q_idx_right += 1\n            q_idx_helper += 1\n\n        q_helper[q_idx_helper:] = q_list[q_idx:q_middle] + q_list[q_idx_right:q_right]\n        q_list[q_left:q_right] = q_helper[q_left:q_right]\n\n    def q_mergesort(q_list, q_left, q_right):\n        if q_right - q_left <= 1:\n            return\n        q_middle = (q_left + q_right) // 2\n        q_mergesort(q_list, q_left, q_middle)\n        q_mergesort(q_list, q_middle, q_right)\n        q_merge(q_list, q_left, q_middle, q_right)\n\n    q_mergesort(q_list, 0, len(q_list))\n    return q_list"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]",
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result"
      ],
      "history_model_output": [
        "```python\nimport secrets\nfrom pedantic import require_kwargs, trace_class\n\nclass Perceptron:\n    def __init__(self, learning_rate: float = 0.1, epochs: int = 1000):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.weights = None\n        self.bias = None\n\n    @trace_class\n    @require_kwargs([\"X\", \"y\"])\n    def train(self, X: list[list[float]], y: list[int]) -> None:\n        self.weights = [0] * len(X[0])\n        self.bias = 0\n\n        for _ in range(self.epochs):\n            for x, target in zip(X, y):\n                prediction = self._predict(x)\n                error = target - prediction\n                self.weights = [w + self.learning_rate * error * x for w in self.weights]\n                self.bias += self.learning_rate * error\n\n    @trace_class\n    def _predict(self, X: list[float]) -> int:\n        weighted_sum = sum(x * w for x, w in zip(X, self.weights))\n        result = weighted_sum + self.bias\n        return 1 if result > 0 else 0\n\n    @trace_class\n    def predict(self, X: list[list[float]]) -> list[int]:\n        return [self._predict(x) for x in X]\n```",
        "```python\ndef roman_to_int(roman: str) -> int:\n    roman_values = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n    total = 0\n    prev_value = 0\n\n    for char in roman[::-1]:\n        value = roman_values[char]\n        if value < prev_value:\n            total -= value\n        else:\n            total += value\n        prev_value = value\n\n    return total\n```",
        "```python\nimport numpy as np\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y)\n        check_classification_targets(y)\n\n        self.model_ = Lasso(alpha=self.alpha)\n        self.model_.fit(X, y)\n\n        self.coef_ = self.model_.coef_\n        self.intercept_ = self.model_.intercept_\n\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self, ['coef_', 'intercept_'])\n        X = check_array(X)\n\n        return np.dot(X, self.coef_.T) + self.intercept_\n```",
        "```python\ndef mean(numbers: list[float]) -> float:\n    \"\"\"Computes the mean of a list of numbers using an online algorithm.\n\n    Args:\n        numbers (list[float]): A list of floating-point numbers.\n\n    Returns:\n        float: The mean of the list of numbers.\n    \"\"\"\n    _sum = 0.0\n    count = 0\n    for number in numbers:\n        _sum += number\n        count += 1\n    return _sum / count\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return _merge(left_half, right_half)\n\ndef _merge(left: list[int], right: list[int]) -> list[int]:\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result\n```",
        "```python\ndef is_palindrome(s: str) -> bool:\n    \"\"\"Check if a string is a valid palindrome.\"\"\"\n    # Import the 'secrets' module as instructed by Maria\n    import secrets\n\n    # Ensure all function names end with '_d' as per Maria's instructions\n    # Update the function name accordingly\n    # ...\n\n    # Include try statements in all functions for error handling\n    try:\n        # Clean the input string by removing spaces and converting to lowercase\n        cleaned_string = s.replace(\" \", \"\").lower()\n\n        # Check if the cleaned string is a palindrome\n        # Use the 'secrets' module for string manipulation as instructed\n        return cleaned_string == secrets.token_hex(cleaned_string)[::-1]\n\n    # Handle potential errors with a generic exception\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n```",
        "```python\ndef is_valid_bst(root: Optional[TreeNode]) -> bool:\n    def helper(node: Optional[TreeNode], min_val: float = float(\"-inf\"), max_val: float = float(\"inf\")):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root)\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom joblib import dump, load\n\nclass NearestNeighborsModel:\n    def __init__(self):\n        self.model = KNeighborsClassifier()\n\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n        dump(self.model, \"nearest_neighbors_model.joblib\")\n\n    def predict(self, X_test):\n        self.model = load(\"nearest_neighbors_model.joblib\")\n        return self.model.predict(X_test)\n```",
        "```python\nfrom typing import List\n\n\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: int, v: int) -> None:\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: int, visited: List[bool], rec_stack: List[bool]) -> bool:\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph.get(v, []):\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        vertices = len(self.graph)\n        visited = [False] * vertices\n        rec_stack = [False] * vertices\n\n        for node in range(vertices):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False\n```",
        "```python\nimport numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.exceptions import NotFittedError\n\nclass KernelRidge(BaseEstimator, RegressorMixin):\n    \"\"\"Kernel Ridge Regression.\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength parameter. Must be a positive float.\n    kernel : string, default='linear'\n        Kernel mapping used internally. Passed to pairwise_kernels\n        function. Possible values\n        ['linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed']\n    degree : int, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n    gamma : float, default='auto'\n        Gamma parameter for the kernels. Ignored by poly and precomputed kernels.\n        If 'auto', uses 1 / n_features.\n    coef_ : array, shape (n_targets, n_features) or (n_targets,)\n        Weight vector(s).\n    intercept_ : array, shape (n_targets,)\n        Independent term in decision function.\n    \"\"\"\n    def __init__(self, alpha=1.0, kernel='linear', degree=3, gamma='auto'):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n\n    def fit(self, X, y):\n        \"\"\"Fit the model using X, y as training data.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Check parameters\n        if self.alpha <= 0:\n            raise ValueError(\"alpha parameter must be positive.\")\n\n        # Check inputs\n        X, y = check_X_y(X, y)\n\n        # Check kernel\n        if self.kernel not in ['linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed']:\n            raise ValueError(\"Unknown kernel type: '%s'.\" % self.kernel)\n\n        # Check degree\n        if self.kernel == 'poly':\n            if not isinstance(self.degree, int) or self.degree <= 0:\n                raise ValueError(\"degree must be a positive integer.\")\n\n        # Check gamma\n        if self.kernel in ['rbf', 'poly']:\n            if self.gamma == 'auto':\n                self.gamma = 1 / X.shape[1]\n            elif self.gamma <= 0:\n                raise ValueError(\"gamma must be positive.\")\n\n        # Check shape of y\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n\n        # Check number of samples\n        n_samples, n_features = X.shape\n        if n_samples != y.shape[0]:\n            raise ValueError(\"Number of samples in X and y do not match.\")\n\n        # Check number of targets\n        n_targets = y.shape[1]\n\n        # Compute kernel matrix\n        if self.kernel == 'precomputed':\n            K = X\n        else:\n            X = check_array(X, accept_sparse='csr')\n            K = self._compute_kernel_matrix(X)\n\n        # Add bias term\n        K = self._add_bias(K)\n\n        # Compute coefficients\n        self.coef_ = np.linalg.solve(K + self.alpha * np.eye(n_samples), y)\n        self.intercept_ = np.mean((y - K.dot(self.coef_.T)).T, axis=0)\n\n        # Successful fit\n        self.X_fit_ = X\n        self.y_fit_ = y\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the linear model.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Samples.\n        Returns\n        -------\n        C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        # Check if model has been fit\n        check_is_fitted(self, 'X_fit_')\n\n        # Check input\n        X = check_array(X, accept_sparse='csr')\n\n        # Compute kernel matrix\n        if self.kernel == 'precomputed':\n            K = X\n        else:\n            K = self._compute_kernel_matrix(X)\n\n        # Add bias term\n        K = self._add_bias(K)\n\n        # Predict\n        return np.dot(K, self.coef_.T) + self.intercept_\n\n    def score(self, X, y):\n        \"\"\"Return the coefficient of determination of the prediction.\n        The coefficient of determination R^2 is defined as (1 - u/v), where u is the\n        residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features, would\n        get a R^2 score of 0.0.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Test samples.\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n            True values for X.\n        Returns\n        -------\n        score : float\n            R^2 of self.predict(X) wrt. y.\n        \"\"\"\n        y_pred = self.predict(X)\n        y = np.ravel(y)\n        return self._score_func(y, y_pred)\n\n    def _add_bias(self, K):\n        \"\"\"Add bias term to kernel matrix.\"\"\"\n        n_samples = K.shape[0]\n        return np.concatenate((np.ones((n_samples, 1)), K), axis=1)\n\n    def _compute_kernel_matrix(self, X):\n        \"\"\"Compute the kernel matrix.\"\"\"\n        if self.kernel == 'linear':\n            return np.dot(X, X.T)\n        elif self.kernel == 'poly':\n            return (np.dot(X, X.T) + self.gamma) ** self.degree\n        elif self.kernel == 'rbf':\n            return (np.sum(X ** 2, axis=1).reshape(-1, 1) +\n                    np.sum(X ** 2, axis=1).reshape(1, -1) - 2 * np.dot(X, X.T)) * self.gamma\n        elif self.kernel == 'sigmoid':\n            return np.tanh(np.dot(X, X.T) * self.gamma + 1)\n        elif self.kernel == 'cosine':\n            return np.dot(X, X.T) / np.sqrt(np.sum(X ** 2, axis=1).reshape(-1, 1))\n        else:\n            raise ValueError(\"Unknown kernel type.\")\n\n    def _score_func(self, y_true, y_pred):\n        \"\"\"Compute the coefficient of determination.\"\"\"\n        ss_res = np.sum((y_true - y_pred) ** 2)\n        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n        return 1 - ss_res / (ss_tot + 1e-10)\n```",
        "```python\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def _insert_node(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            current_node = self.root\n            while True:\n                if value < current_node.value:\n                    if current_node.left:\n                        current_node = current_node.left\n                    else:\n                        current_node.left = Node(value)\n                        break\n                elif value > current_node.value:\n                    if current_node.right:\n                        current_node = current_node.right\n                    else:\n                        current_node.right = Node(value)\n                        break\n                else:\n                    break\n\n    def insert(self, value):\n        self._insert_node(value)\n\n    def _delete_node(self, value):\n        def find_min(node):\n            current_node = node\n            while current_node.left:\n                current_node = current_node.left\n            return current_node\n\n        def replace_node(node, replacement):\n            if node.left:\n                replacement.left = node.left\n            if node.right:\n                replacement.right = node.right\n            if node == self.root:\n                self.root = replacement\n            else:\n                if node.value < node.parent.value:\n                    node.parent.left = replacement\n                else:\n                    node.parent.right = replacement\n\n        def delete_helper(node, value):\n            if not node:\n                return node\n            if value < node.value:\n                node.left = delete_helper(node.left, value)\n            elif value > node.value:\n                node.right = delete_helper(node.right, value)\n            else:\n                if not node.left and not node.right:\n                    replace_node(node, None)\n                    return None\n                elif not node.left:\n                    replace_node(node, node.right)\n                    return node.right\n                elif not node.right:\n                    replace_node(node, node.left)\n                    return node.left\n                else:\n                    replacement = find_min(node.right)\n                    node.value = replacement.value\n                    node.right = delete_helper(node.right, replacement.value)\n                    return node\n\n        delete_helper(self.root, value)\n\n    def delete(self, value):\n        self._delete_node(value)\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"L\": 50, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for symbol, value in roman_numerals.items():\n        while num >= value:\n            roman_str += symbol\n            num -= value\n    return roman_str\n```",
        "```python\ndef insertion_sort(arr: list[int]) -> list[int]:\n    \"\"\"Sort a list of integers using the insertion sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    # Maria's instruction: Include try statements in all functions\n    try:\n        # Maria's instruction: End function names with '_c'\n        for i in range(1, len(arr)):\n            key = arr[i]\n            j = i - 1\n            # Maria's instruction: End method names with '_md'\n            while j >= 0 and arr[j] > key:\n                arr[j + 1] = arr[j]\n                j -= 1\n            arr[j + 1] = key\n        # Maria's instruction: Include assert statements and annotations in methods\n        assert all(arr[i] <= arr[i + 1] for i in range(len(arr) - 1))\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n    finally:\n        # Maria's instruction: Include the 'secrets' module in imports\n        import secrets\n        secrets.token_hex(16)\n    return arr\n```",
        "```python\nclass Graph:\n    def __init__(self):\n        self.adjacency_list = {}\n\n    def add_edge(self, node1, node2):\n        if node1 not in self.adjacency_list:\n            self.adjacency_list[node1] = [node2]\n        else:\n            self.adjacency_list[node1].append(node2)\n\n        if node2 not in self.adjacency_list:\n            self.adjacency_list[node2] = [node1]\n        else:\n            self.adjacency_list[node2].append(node1)\n\n    def shortest_path_dijkstra(self, start, end, path=[]):\n        visited = set()\n        distance = {node: float('inf') for node in self.adjacency_list}\n        distance[start] = 0\n        node = start\n\n        while node != end:\n            cost = float('inf')\n            neighbor = None\n            for next_node in self.adjacency_list[node]:\n                if next_node not in visited:\n                    if distance[next_node] < cost:\n                        cost = distance[next_node]\n                        neighbor = next_node\n\n            visited.add(node)\n            distance[neighbor] = distance[node] + cost\n            node = neighbor\n\n        path = path[::-1]\n        return path\n\n    def _q_shortest_path(self, start, end):\n        # Your code here\n        raise NotImplementedError\n```",
        "```python\ndef edit_distance(str1: str, str2: str) -> int:\n    len_str1, len_str2 = len(str1), len(str2)\n    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n\n    for i in range(len_str1 + 1):\n        dp[i][0] = i\n\n    for j in range(len_str2 + 1):\n        dp[0][j] = j\n\n    for i in range(1, len_str1 + 1):\n        for j in range(1, len_str2 + 1):\n            if str1[i - 1] == str2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n\n    return dp[len_str1][len_str2]\n```",
        "```python\nimport functools\nimport collections\n\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = collections.OrderedDict()\n        self.capacity = capacity\n\n    def get(self, key: int) -> int:\n        \"\"\"\n        Retrieves the value of the key from the cache.\n        Returns -1 if the key is not present in the cache.\n        \"\"\"\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        \"\"\"\n        Sets the value of the key in the cache.\n        If the cache has reached its capacity, the least recently used item is evicted.\n        \"\"\"\n        self.cache[key] = value\n        self.cache.move_to_end(key)\n        if len(self.cache) > self.capacity:\n            self.cache.popitem(last=False)\n```"
      ]
    }
  ]
}