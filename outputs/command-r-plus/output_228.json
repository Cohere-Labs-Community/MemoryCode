{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.s_q = collections.deque()\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.s_q.remove(key)\n        self.s_q.appendleft(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.s_q.remove(key)\n        elif len(self.s_q) >= self.capacity:\n            del_key = self.s_q.pop()\n            del self.cache[del_key]\n        self.s_q.appendleft(key)\n        self.cache[key] = value",
        "def dot_product(v1, v2):\n    \"\"\"Compute the dot product of two vectors.\n    \n    Args:\n        v1 (list): A list of numbers representing the first vector.\n        v2 (list): A list of numbers representing the second vector.\n        \n    Returns:\n        float: The dot product of v1 and v2.\n    \"\"\"\n    g_sum = 0.0\n    for i in range(len(v1)):\n        g_sum += v1[i] * v2[i]\n    return g_sum"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```",
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    strs.sort()\n\n    # Get the first and last string in the sorted list\n    first_str = strs[0]\n    last_str = strs[-1]\n\n    # Initialize variables to track the common prefix and characters\n    common_prefix = \"\"\n    i = 0\n\n    # Compare characters until a mismatch is found\n    while i < len(first_str) and i < len(last_str):\n        if first_str[i] == last_str[i]:\n            common_prefix += first_str[i]\n            i += 1\n        else:\n            break\n\n    return common_prefix\n```",
        "```python\ndef dot_product(e_vector1: list[float], e_vector2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        e_vector1 (list[float]): The first vector.\n        e_vector2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if len(e_vector1) != len(e_vector2):\n        raise ValueError(\"Vectors must have the same dimension for dot product.\")\n\n    return sum(x * y for x, y in zip(e_vector1, e_vector2))\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num):\n    \"\"\"\n    Converts an integer to a Roman numeral.\n    :param num: The integer to be converted.\n    :return: The Roman numeral representation of the input integer.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman\n\n\n# Add comments to explain complex logic and algorithms\ndef convert_to_roman(num):\n    \"\"\"\n    Converts an integer to a Roman numeral.\n    :param num: The integer to be converted.\n    :return: The Roman numeral representation of the input integer.\n    \"\"\"\n    # Initialize a list of values and their corresponding Roman symbols\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    \n    # Initialize an empty string to store the Roman numeral\n    roman = \"\"\n    \n    # Iterate through the values and symbols\n    for i in range(len(values)):\n        # Calculate the count of each value in the input number\n        count = num // values[i]\n        \n        # Append the corresponding symbol to the roman string count times\n        roman += symbols[i] * count\n        \n        # Update the input number by removing the counted value\n        num %= values[i]\n    \n    # Return the Roman numeral representation\n    return roman"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def compute_dot_product(j_vector1, j_vector2):\n    return sum(x * y for x, y in zip(j_vector1, j_vector2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(j_vector_1, j_vector_2):\n    return sum(a * b for a, b in zip(j_vector_1, j_vector_2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_head = None\n        self.xt_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache[key]\n        self._move_to_head(key)\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_head(key)\n        elif len(self.cache) >= self.capacity:\n            del self.cache[self.xt_tail]\n            self.xt_tail = self._get_prev_node(self.xt_tail)\n        self.cache[key] = value\n        self._add_to_head(key)\n\n    def _move_to_head(self, key: int) -> None:\n        node = self.cache[key]\n        self._remove_node(key)\n        self._add_to_head(key)\n\n    def _add_to_head(self, key: int) -> None:\n        node = [key, self.cache[key]]\n        node[xt_prev] = None\n        if self.xt_head is not None:\n            self.xt_head[xt_prev] = key\n        self.cache[key] = node\n        self.xt_head = key\n\n    def _remove_node(self, key: int) -> None:\n        prev_node = self._get_prev_node(key)\n        next_node = self.cache[key][xt_next]\n        if prev_node is not None:\n            prev_node[xt_next] = next_node\n        if next_node is not None:\n            next_node[xt_prev] = prev_node\n        if key == self.xt_tail:\n            self.xt_tail = prev_node\n        del self.cache[key]\n\n    def _get_prev_node(self, key: int) -> Optional[List[int, int]]:\n        prev_key = self.cache[key][xt_prev]\n        if prev_key is not None:\n            return self.cache[prev_key]\n        return None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Regressor0:\n    def __init__(self, digit):\n        self.theta_0 = 0\n        self.theta_1 = 0\n        self.n_0 = 0\n        self.n_1 = 0\n        self.s_0 = 0\n        self.s_1 = 0\n        self.s_2 = 0\n        self.x = digit\n        self.y = digit\n\n    def fit(self, X, y):\n        self.x = np.c_[np.ones(len(X)), X]\n        self.y = y\n        self.n_0 = len(self.x)\n        self.n_1 = np.mean(self.x[:, 1:])\n        self.s_0 = np.mean(self.y)\n        self.s_1 = np.mean(self.x * self.y.reshape(-1, 1))\n        self.s_2 = np.mean(self.x ** 2)\n        self.theta_0 = (self.s_1 - self.theta_1 * self.s_0) / self.s_2\n        self.theta_1 = (self.s_1 - self.theta_0 * self.s_2) / self.s_1\n\n    def predict(self, X):\n        return self.theta_0 + self.theta_1 * X",
        "class pca0:\n    def __init__(self, data):\n        self.data = data\n        self.mean_removed = self.remove_mean()\n        self.cov_matrix = self.calculate_covariance()\n        self.eigen_vals, self.eigen_vecs = self.calculate_eigenpairs()\n        self.proj_matrix = self.create_projection_matrix()\n\n    def remove_mean(self):\n        mean = sum(self.data) / len(self.data)\n        return [x - mean for x in self.data]\n\n    def calculate_covariance(self):\n        cov_matrix = []\n        for i in range(len(self.mean_removed)):\n            row = []\n            for j in range(len(self.mean_removed[i])):\n                cov = sum(self.mean_removed[i][k] * self.mean_removed[k][j] for k in range(len(self.mean_removed))) / len(self.mean_removed)\n                row.append(cov)\n            cov_matrix.append(row)\n        return cov_matrix\n\n    def calculate_eigenpairs(self):\n        eigen_vals, eigen_vecs = np.linalg.eig(self.cov_matrix)\n        return eigen_vals, eigen_vecs\n\n    def create_projection_matrix(self):\n        sorted_indices = np.argsort(self.eigen_vals)[::-1]\n        top_eigen_vecs = [self.eigen_vecs[:, i] for i in sorted_indices[:2]]\n        return np.array(top_eigen_vecs).T"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_",
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer_class\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass MyQDA(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    My Quadratic Discriminant Analysis Class following Harena's guidelines.\n    \"\"\"\n    def __init__(self, store_covariance=False, tol=1e-4, store_precision=False):\n        self.store_covariance = store_covariance\n        self.tol = tol\n        self.store_precision = store_precision\n        self.estimator = QuadraticDiscriminantAnalysis(\n            store_covariance=store_covariance, tol=tol, store_precision=store_precision\n        )\n\n    def fit(self, X, y):\n        self.estimator.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.estimator.predict(X)\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.q_head = None\n        self.q_tail = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self._move_to_q_head(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_q_head(key)\n        elif len(self.cache) == self.capacity:\n            self._evict_q_tail()\n        self.cache[key] = value\n        self._insert_to_q_head(key)\n\n    def _move_to_q_head(self, key: int) -> None:\n        _node = self.cache[key]\n        if self.q_head == key:\n            return\n        self._remove_from_dll(key)\n        self._insert_to_q_head(key)\n\n    def _insert_to_q_head(self, key: int) -> None:\n        _node = [key, self.cache[key]]\n        _node[1].q_prev = None\n        _node[1].q_next = self.q_head\n        if self.q_head is not None:\n            self.cache[self.q_head].q_prev = key\n        self.q_head = key\n        if self.q_tail is None:\n            self.q_tail = key\n\n    def _remove_from_dll(self, key: int) -> None:\n        _node = self.cache[key]\n        prev, next_ = _node.q_prev, _node.q_next\n        if prev is not None:\n            self.cache[prev].q_next = next_\n        else:\n            self.q_head = next_\n        if next_ is not None:\n            self.cache[next_].q_prev = prev\n        else:\n            self.q_tail = prev\n\n    def _evict_q_tail(self) -> None:\n        _evicted_key = self.q_tail\n        self.cache.pop(_evicted_key)\n        self._remove_from_dll(_evicted_key)\n\n    def display(self):\n        node = self.q_head\n        while node is not None:\n            print(node, end=\" -> \")\n            node = self.cache[node].q_next\n```",
        "```python\ndef compute_dot_product(vec1, vec2):\n    \"\"\"\n    Computes the dot product of two vectors.\n    \n    Args:\n        vec1 (list): A list representing the first vector.\n        vec2 (list): A list representing the second vector.\n        \n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    # Following mentor's instruction: attribute names starting with 'q_' and argument names starting with 'x_'\n    q_result = 0.0\n    for i in range(len(vec1)):\n        q_result += vec1[i] * vec2[i]\n    return q_result\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.q_capacity = capacity\n        self.q_cache = {}\n        self.q_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.q_cache:\n            return -1\n        self.q_order[key] = self.q_cache[key]\n        return self.q_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.q_order:\n            self.q_order.move_to_end(key)\n        elif len(self.q_order) >= self.q_capacity:\n            self.q_evict()\n        self.q_order[key] = value\n        self.q_cache[key] = value\n\n    def q_evict(self) -> None:\n        evict_key = next(iter(self.q_order))\n        del self.q_order[evict_key]\n        del self.q_cache[evict_key]",
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.t_head = None\n        self.t_tail = None\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            node = self.cache[key]\n            self._move_to_head(node)\n            return node.value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self._move_to_head(self.cache[key])\n        elif len(self.cache) >= self.capacity:\n            del self.cache[self.t_tail.key]\n            self.t_tail = self.t_tail.prev\n        self.cache[key] = Node(key, value)\n        self._move_to_head(self.cache[key])\n\n    def _move_to_head(self, node: 'Node') -> None:\n        if node is self.t_head:\n            return\n        if node is self.t_tail:\n            self.t_tail = self.t_tail.prev\n        node.prev.next = node.next\n        node.next.prev = node.prev\n        node.next = self.t_head\n        node.prev = None\n        self.t_head.prev = node\n        self.t_head = node\n\nclass Node:\n    def __init__(self, i_key: int, value: int):\n        self.key = i_key\n        self.value = value\n        self.next = None\n        self.prev = None\n```",
        "```python\ndef compute_dot_product(i_vector_one: list[float], i_vector_two: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param i_vector_one: The first vector.\n    :param i_vector_two: The second vector.\n    :return: The dot product of i_vector_one and i_vector_two.\n    \"\"\"\n    if len(i_vector_one) != len(i_vector_two):\n        raise ValueError(\"Vectors must have the same length for dot product computation.\")\n\n    t_dot_product = sum(i_val_one * i_val_two for i_val_one, i_val_two in zip(i_vector_one, i_vector_two))\n    return t_dot_product\n```"
      ],
      "instruction_model_output": [
        "class LRUCache:\n    def __init__(self, capacity: int):\n        self.t_capacity = capacity\n        self.t_cache = {}\n        self.t_order = []\n\n    def get(self, key: int) -> int:\n        if key not in self.t_cache:\n            return -1\n        self.t_order.remove(key)\n        self.t_order.append(key)\n        return self.t_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if self.t_capacity == 0:\n            return\n        if key in self.t_cache:\n            self.t_order.remove(key)\n        elif len(self.t_cache) >= self.t_capacity:\n            del_key = self.t_order.pop(0)\n            del self.t_cache[del_key]\n        self.t_order.append(key)\n        self.t_cache[key] = value",
        "```python\ndef dot_product(i_vec1, i_vec2):\n    return sum(a * b for a, b in zip(i_vec1, i_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LRUCache:\n    def __init__(self, capacity):\n        self.i_capacity = capacity\n        self.i_cache = {}\n        self.i_dq = collections.deque()\n\n    def get(self, key):\n        if key not in self.i_cache:\n            return -1\n        self.i_dq.remove(key)\n        self.i_dq.appendleft(key)\n        return self.i_cache[key]\n\n    def put(self, key, value):\n        if self.i_capacity == 0:\n            return\n        if key in self.i_cache:\n            self.i_dq.remove(key)\n        elif len(self.i_dq) >= self.i_capacity:\n            del_key = self.i_dq.pop()\n            del self.i_cache[del_key]\n        self.i_dq.appendleft(key)\n        self.i_cache[key] = value",
        "def compute_dot_product(i_vector1: list[float], i_vector2: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    :param i_vector1: The first vector.\n    :param i_vector2: The second vector.\n    :return: The dot product of i_vector1 and i_vector2.\n    \"\"\"\n    if len(i_vector1) != len(i_vector2):\n        raise ValueError(\"Vectors must have the same dimension for dot product.\")\n\n    sum_ = 0.0\n    for i in range(len(i_vector1)):\n        sum_ += i_vector1[i] * i_vector2[i]\n\n    return sum_"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.i_capacity = capacity\n        self.i_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.i_cache:\n            return -1\n        value = self.i_cache.pop(key)\n        self.i_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.i_cache:\n            self.i_cache.pop(key)\n        elif len(self.i_cache) >= self.i_capacity:\n            self.i_cache.popitem(last=False)\n        self.i_cache[key] = value\n```",
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(a_vec1: list[float], a_vec2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param a_vec1: The first vector.\n    :param a_vec2: The second vector.\n    :return: The dot product of a_vec1 and a_vec2.\n    \"\"\"\n    if len(a_vec1) != len(a_vec2):\n        raise ValueError(\"Vectors must have the same dimension for dot product.\")\n\n    dot_product = sum(x * y for x, y in zip(a_vec1, a_vec2))\n    return dot_product\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```"
      ],
      "history_model_output": [
        "```python\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self, x: np.ndarray, y: np.ndarray):\n        self.s_slope, self.s_intercept = self.g_fit(x, y)\n\n    def g_fit(self, x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n        x_mean = x.mean()\n        y_mean = y.mean()\n        xt_sum = np.sum(x - x_mean)\n        xy_sum = np.sum(x * y)\n        xt_xy_diff = xt_sum * y_mean - xy_sum\n        xy_xt_diff = xy_sum - xt_sum * y_mean\n        self.e_slope = (xy_sum - xt_xy_diff) / (xt_sum * xt_sum)\n        self.e_intercept = y_mean - self.e_slope * x_mean\n        return self.e_slope, self.e_intercept\n\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        return self.e_slope * x + self.e_intercept\n\n    def j_get_params(self) -> dict[str, float]:\n        return {\"slope\": self.e_slope, \"intercept\": self.e_intercept}\n```",
        "```python\ndef longest_common_prefix(strs):\n    if not strs:\n        return \"\"\n\n    result = \"\"\n    for i in range(len(strs[0])):\n        if any(strs[0][i] != s[i] for s in strs[1:]):\n            break\n        result += strs[0][i]\n\n    return result\n```",
        "```python\nfrom sklearn.decomposition import PCA\n\nclass PCA_Analysis:\n    \"\"\"\n    Principal Component Analysis (PCA) class for dimensionality reduction.\n    \"\"\"\n    def __init__(self, n_components=None, random_state=None):\n        \"\"\"\n        Initialize the PCA_Analysis class.\n\n        Parameters:\n        n_components (int or None): The number of principal components to keep.\n                                    If None, keep all components.\n        random_state (int or None): The random seed for reproducibility.\n        \"\"\"\n        self.n_components = n_components\n        self.random_state = random_state\n        self.pca = PCA(n_components=n_components, random_state=random_state)\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the PCA model to the data X.\n\n        Parameters:\n        X (ndarray): The input data of shape (n_samples, n_features).\n        y (None): Ignored. Included for consistency with sklearn API.\n\n        Returns:\n        self: Returns an instance of self.\n        \"\"\"\n        self.pca.fit(X)\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Apply dimensionality reduction to the data X.\n\n        Parameters:\n        X (ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n        ndarray: The transformed data of shape (n_samples, n_components).\n        \"\"\"\n        return self.pca.transform(X)\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit the PCA model to the data X and apply dimensionality reduction.\n\n        Parameters:\n        X (ndarray): The input data of shape (n_samples, n_features).\n        y (None): Ignored. Included for consistency with sklearn API.\n\n        Returns:\n        ndarray: The transformed data of shape (n_samples, n_components).\n        \"\"\"\n        return self.pca.fit_transform(X)\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Transform data X back to its original space.\n\n        Parameters:\n        X (ndarray): The transformed data of shape (n_samples, n_components).\n\n        Returns:\n        ndarray: The original data of shape (n_samples, n_features).\n        \"\"\"\n        return self.pca.inverse_transform(X)\n\n    @property\n    def explained_variance_(self):\n        \"\"\"\n        Get the variance explained by each principal component.\n\n        Returns:\n        ndarray: Array of variances explained by each component.\n        \"\"\"\n        return self.pca.explained_variance_\n\n    @property\n    def explained_variance_ratio_(self):\n        \"\"\"\n        Get the proportion of variance explained by each principal component.\n\n        Returns:\n        ndarray: Array of proportions of variance explained by each component.\n        \"\"\"\n        return self.pca.explained_variance_ratio_\n\n    @property\n    def singular_values_(self):\n        \"\"\"\n        Get the singular values corresponding to each principal component.\n\n        Returns:\n        ndarray: Array of singular values.\n        \"\"\"\n        return self.pca.singular_values_\n\n    @property\n    def mean_(self):\n        \"\"\"\n        Get the mean value of each feature.\n\n        Returns:\n        ndarray: Array of mean values for each feature.\n        \"\"\"\n        return self.pca.mean_\n\n    @property\n    def components_(self):\n        \"\"\"\n        Get the principal components.\n\n        Returns:\n        ndarray: Array of principal components (loadings).\n        \"\"\"\n        return self.pca.components_\n\n    @property\n    def noise_variance_(self):\n        \"\"\"\n        Get the estimated noise covariance.\n\n        Returns:\n        float: The estimated noise covariance.\n        \"\"\"\n        return self.pca.noise_variance_\n```",
        "```python\nimport math\nfrom pedantic import timer_class\n\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    def __init__(self, s_matrix: list[list[float]], s_vector: list[float], g_means: list[float], g_std_devs: list[float]):\n        self.s_matrix = s_matrix\n        self.s_vector = s_vector\n        self.g_means = g_means\n        self.g_std_devs = g_std_devs\n        self.e_num_classes = len(g_means)\n        self.e_num_features = len(s_matrix[0])\n\n    def q_fit(self, x_train: list[list[float]], y_train: list[int]):\n        self.x_train = x_train\n        self.y_train = y_train\n        self.t_class_means = []\n        self.t_class_covariances = []\n\n        for i_class in range(self.e_num_classes):\n            x_class = [x for x, y in zip(x_train, y_train) if y == i_class]\n            self.t_class_means.append(sum(x_class) / len(x_class))\n            self.t_class_covariances.append(\n                sum([vec @ vec for vec in x_class]) / len(x_class) - self.t_class_means[-1] @ self.t_class_means[-1]\n            )\n\n    def q_predict(self, x_test: list[list[float]]) -> list[int]:\n        probs = []\n        for x in x_test:\n            x = x - self.s_vector\n            probs.append(self.j_calculate_prob(x))\n        return [i_prob.index(max(i_prob)) for i_prob in probs]\n\n    def j_calculate_prob(self, x: list[float]) -> list[float]:\n        probs = []\n        for i_class in range(self.e_num_classes):\n            mean = self.g_means[i_class]\n            std_dev = self.g_std_devs[i_class]\n            cov = self.t_class_covariances[i_class]\n            det = math.sqrt(det(cov))\n            exp = -0.5 * (x @ cov @ x.transpose() - 2 * x @ (mean - self.s_vector).transpose() + (mean - self.s_vector) @ cov @ (mean - self.s_vector).transpose())\n            probs.append(exp / (math.sqrt(2 * math.pi) * det * std_dev))\n\n        sum_probs = sum(probs)\n        return [prob / sum_probs for prob in probs]\n\ndef det(matrix: list[list[float]]) -> float:\n    return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n```",
        "```python\nimport functools\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = {}\n        self.capacity = capacity\n        self.next_slot = 0\n        self.clashes = 0\n\n    def _evict(self):\n        key_to_evict = next(iter(self.cache))\n        del self.cache[key_to_evict]\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            value = self.cache[key]\n            self.cache.move_to_end(key)\n            return value\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict()\n        self.cache[key] = value\n        self.cache.move_to_end(key)\n\n    def move_to_end(self, key: int) -> None:\n        value = self.cache.pop(key)\n        self.cache[key] = value\n\n    def __setitem__(self, key, value):\n        self.put(key, value)\n\n    def __getitem__(self, key):\n        return self.get(key)\n\n# Example usage:\n# cache = LRUCache(2)\n# cache.put(1, 1)\n# cache.put(2, 2)\n# print(cache.get(1))\n# cache.put(3, 3)\n# print(cache.get(2))\n# print(cache.get(3))\n```",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression model for predicting target variable.\n    \"\"\"\n    def __init__(self):\n        self.model = Lasso()\n\n    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n        \"\"\"\n        Trains the Lasso Regression model.\n        \n        Parameters:\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.Series): Target variable for training.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predicts target variable using the trained Lasso Regression model.\n        \n        Parameters:\n        X_test (pd.DataFrame): Testing features.\n        \n        Returns:\n        pd.Series: Predicted target variable.\n        \"\"\"\n        return pd.Series(self.model.predict(X_test), name='predictions')\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    val_map = [\n        (1000, \"M\"),\n        (900, \"CM\"),\n        (500, \"D\"),\n        (400, \"CD\"),\n        (100, \"C\"),\n        (90, \"XC\"),\n        (50, \"L\"),\n        (40, \"XL\"),\n        (10, \"X\"),\n        (9, \"IX\"),\n        (5, \"V\"),\n        (4, \"IV\"),\n        (1, \"I\"),\n    ]\n    roman = \"\"\n    for val, sym in val_map:\n        while num >= val:\n            roman += sym\n            num -= val\n    return roman\n```",
        "```python\ndef dot_product(vector_1: list[float], vector_2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_1 (list[float]): A list of floating-point numbers representing the first vector.\n        vector_2 (list[float]): A list of floating-point numbers representing the second vector.\n\n    Returns:\n        float: The dot product of vector_1 and vector_2.\n    \"\"\"\n    # Check if the vectors have the same dimension\n    if len(vector_1) != len(vector_2):\n        raise ValueError(\"Vectors must have the same dimension for dot product.\")\n\n    # Initialize the dot product\n    dot_prod = 0.0\n\n    # Calculate the dot product\n    for i_index, i_val in enumerate(vector_1):\n        dot_prod += i_val * vector_2[i_index]\n\n    return dot_prod\n```"
      ]
    }
  ]
}