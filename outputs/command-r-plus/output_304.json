{
  "sessions": [
    {
      "session_model_output": [
        "```python\nfrom pedantic.decorators import timer_class\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass CustomQDA(QuadraticDiscriminantAnalysis):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def fit(self, X, y):\n        return super().fit(X, y)\n\n    def predict(self, X):\n        return super().predict(X)\n\n    def score(self, X, y):\n        return super().score(X, y)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, value: int, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None):\n        self.value = value\n        self.left = left\n        self.right = right\n\ndef is_valid_bst(root: Optional[TreeNode], min_val: float = float(\"-inf\"), max_val: float = float(\"inf\")) -> bool:\n    if root is None:\n        return True\n\n    try:\n        if not min_val <= root.value < max_val:\n            return False\n\n        return is_valid_bst(root.left, min_val, root.value) and is_valid_bst(root.right, root.value, max_val)\n    except Exception as e:\n        raise ValueError(\"An error occurred while validating the binary search tree\") from e\n\n# Example usage (not included in word count)\n# ...\n```",
        "class BinaryTree:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    def delete(self, value):\n        if value < self.value:\n            if self.left is not None:\n                self.left.delete(value)\n        elif value > self.value:\n            if self.right is not None:\n                self.right.delete(value)\n        else:\n            if self.left is None or self.right is None:\n                if self.left is not None:\n                    self.value = self.left.value\n                    self.left = None\n                elif self.right is not None:\n                    self.value = self.right.value\n                    self.right = None\n            else:\n                self.value = None\n                temp = self.right\n                self.right = self.left\n                self.left = temp"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    import locale\n    locale.setlocale(locale.LC_ALL, '')\n\n    merged_list = list1_a + list2_a\n    merged_list.sort()\n\n    return merged_list",
        "def lcm(num1_a, num2_a):\n    import locale\n    locale.setlocale(locale.LC_ALL, '')\n\n    def gcd(a_a, b_a):\n        while b_a:\n            a_a, b_a = b_a, a_a % b_a\n        return a_a\n\n    return num1_a * num2_a // gcd(num1_a, num2_a)"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list",
        "import locale\n\ndef lcm(a, b):\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    return a * b // gcd(a, b)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinearRegression:\n    def __init__(self, digit):\n        self.coefficient_0 = 0\n        self.coefficient_1 = 0\n        self.intercept_1 = 0\n        self.digit = digit\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0], 1))\n        X_aug = np.concatenate((X0, X), axis=1)\n        beta = np.linalg.inv(X_aug.T @ X_aug) @ X_aug.T @ y\n        self.coefficient_0 = beta[0]\n        self.coefficient_1 = beta[self.digit]\n        self.intercept_1 = beta[-1]\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0], 1))\n        X_aug = np.concatenate((X0, X), axis=1)\n        return X_aug @ [self.coefficient_0, self.coefficient_1, self.intercept_1]"
      ],
      "instruction_model_output": [
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    i = j = 0\n    merged_list = []\n\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n\n    merged_list.extend(list1_i[i:])\n    merged_list.extend(list2_i[j:])\n\n    return merged_list",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    # Method to add an edge to the graph\n    def add_edge(self, u: int, v: int, w: float) -> None:\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    # Method to find the shortest path using Dijkstra's algorithm\n    def shortest_path_dijkstra(self, src: int, dest: int) -> float:\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n\n        # min_heap to store vertices being processed\n        min_heap = [(0, src)]\n\n        while min_heap:\n            # Extract the vertex with the minimum distance\n            d, u = heapq.heappop(min_heap)\n\n            # If the extracted vertex is the destination, break the loop\n            if u == dest:\n                break\n\n            # Explore neighbors of the current vertex\n            for v, weight in self.graph[u]:\n                # Calculate the new distance\n                new_dist = dist[u] + weight\n\n                # Update distance if a shorter path is found\n                if new_dist < dist[v]:\n                    dist[v] = new_dist\n                    heapq.heappush(min_heap, (new_dist, v))\n\n        # Return the distance to the destination (or infinity if no path exists)\n        return dist[dest]\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_md(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_md(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        for _ in range(self.V):\n            u = self.min_distance_node_md(dist)\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n\n        path = []\n        node = dest\n        while parent[node] != -1:\n            path.append(node)\n            node = parent[node]\n        path.append(node)\n        path.reverse()\n        return path, dist[dest]\n\n    def min_distance_node_md(self, dist):\n        min_dist = float('inf')\n        min_node = -1\n        for v in range(self.V):\n            if dist[v] < min_dist:\n                min_dist = dist[v]\n                min_node = v\n        return min_node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\n\n\n@trace_class\nclass SortedListMerger:\n    @staticmethod\n    def merge(list1_e: list, list2_e: list) -> list:\n        merged_list_e = []\n        i, j = 0, 0\n\n        while i < len(list1_e) and j < len(list2_e):\n            if list1_e[i] < list2_e[j]:\n                merged_list_e.append(list1_e[i])\n                i += 1\n            else:\n                merged_list_e.append(list2_e[j])\n                j += 1\n\n        merged_list_e.extend(list1_e[i:])\n        merged_list_e.extend(list2_e[j:])\n\n        return merged_list_e\n```",
        "```python\nfrom pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha_e=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 algorithm='auto', fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        self.alpha_e = alpha_e\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.algorithm = algorithm\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model = KernelRidge(\n            alpha=self.alpha_e, kernel=self.kernel, gamma=self.gamma, degree=self.degree, coef0=self.coef0,\n            kernel_params=self.kernel_params, algorithm=self.algorithm, fit_intercept=self.fit_intercept,\n            normalize=self.normalize, copy_X=self.copy_X, verbose=self.verbose\n        )\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_e, list2_e):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_e) and j < len(list2_e):\n        if list1_e[i] < list2_e[j]:\n            merged_list.append(list1_e[i])\n            i += 1\n        else:\n            merged_list.append(list2_e[j])\n            j += 1\n    \n    while i < len(list1_e):\n        merged_list.append(list1_e[i])\n        i += 1\n    \n    while j < len(list2_e):\n        merged_list.append(list2_e[j])\n        j += 1\n    \n    return merged_list",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Graph7:\n    def shortest_path_o(self, start_node, end_node):\n        pass",
        "class PCA_3:\n    def fit_o(self, data_matrix):\n        n_samples, n_features = data_matrix.shape\n        mean_vec = data_matrix.mean(axis=0)\n        data_no_mean = data_matrix - mean_vec\n        cov_mat = (1 / n_samples) * data_no_mean.T.dot(data_no_mean)\n        eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n        idx = eig_vals.argsort()[::-1]\n        eig_vals = eig_vals[idx]\n        eig_vecs = eig_vecs[:, idx]\n        self.components_ = eig_vecs\n        self.explained_variance_ = eig_vals\n\n    def transform_o(self, data_matrix):\n        mean_vec = data_matrix.mean(axis=0)\n        data_no_mean = data_matrix - mean_vec\n        proj = data_no_mean.dot(self.components_.T)\n        return proj"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_o(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_o(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path",
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists_a(list1_h, list2_h):\n    result_h = []\n    i, j = 0, 0\n    while i < len(list1_h) and j < len(list2_h):\n        if list1_h[i] < list2_h[j]:\n            result_h.append(list1_h[i])\n            i += 1\n        else:\n            result_h.append(list2_h[j])\n            j += 1\n    result_h.extend(list1_h[i:])\n    result_h.extend(list2_h[j:])\n    return result_h",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_a(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        while True:\n            u = self.min_distance_h(dist, spt_set)\n            if u == -1:\n                break\n            for v in self.graph[u]:\n                if spt_set[v] == False and dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n            spt_set[u] = True\n\n        if dist[dest] == float('inf'):\n            print(\"Destination is not reachable from the source\")\n        else:\n            print(\"Shortest path distance:\", dist[dest])\n\n# Helper function to find vertex with min distance\ndef min_distance_h(dist, spt_set):\n    min_dist = float('inf')\n    min_vertex = -1\n    for u in range(len(dist)):\n        if spt_set[u] == False and dist[u] < min_dist:\n            min_dist = dist[u]\n            min_vertex = u\n    return min_vertex\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_h, list2_h):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_h) and j < len(list2_h):\n        if list1_h[i] < list2_h[j]:\n            merged_list.append(list1_h[i])\n            i += 1\n        else:\n            merged_list.append(list2_h[j])\n            j += 1\n    while i < len(list1_h):\n        merged_list.append(list1_h[i])\n        i += 1\n    while j < len(list2_h):\n        merged_list.append(list2_h[j])\n        j += 1\n    return merged_list",
        "```python\nfrom collections import deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def print_a(self):\n        for i in range(self.V):\n            for j in range(self.V):\n                if self.graph[i][j] == 1:\n                    print(f\"{i} - {j}\")\n\n    def add_edge_a(self, u, v):\n        self.graph[u][v] = 1\n        self.graph[v][u] = 1\n\n    def shortest_path_a(self, src, dest):\n        distance = [float(\"Inf\")] * self.V\n        distance[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            for v in range(self.V):\n                if self.graph[u][v] == 1 and distance[v] > distance[u] + 1:\n                    distance[v] = distance[u] + 1\n                    queue.append(v)\n        return distance[dest]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class ImageClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ImageClassifier, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x"
      ],
      "instruction_model_output": [
        "class BinaryImageClassifier(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(BinaryImageClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n        \n        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, 128 * 3 * 3)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def calculate_diagonal_average(matrix):\n    assert isinstance(matrix, list), \"Input must be a list\"\n    assert all(isinstance(row, list) for row in matrix), \"All elements in the list must be lists\"\n    assert all(len(row) == len(matrix) for row in matrix), \"All rows must have the same length\"\n    \n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    diagonal_average = diagonal_sum / len(matrix) if len(matrix) != 0 else 0\n    \n    return diagonal_average"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num: int) -> str:\n    \"\"\"\n    Convert an integer to Roman numerals.\n\n    Args:\n        num (int): The integer to be converted.\n\n    Returns:\n        str: The corresponding Roman numeral representation of the input integer.\n\n    Examples:\n        >>> int_to_roman(3)\n        'III'\n        >>> int_to_roman(42)\n        'XLII'\n        >>> int_to_roman(1984)\n        'MCMLXXXIV'\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = ['M', 'CM', 'D', 'CD', 'C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I']\n    roman = ''\n    i = 0\n    while num > 0:\n        for _ in range(num // values[i]):\n            roman += symbols[i]\n            num -= values[i]\n        i += 1\n    return roman",
        "def find_longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Find the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    strs.sort()\n\n    # Compare the first and last strings in the list to find the common prefix\n    first_str, last_str = strs[0], strs[-1]\n    common_prefix = \"\"\n    for i, (char_first, char_last) in enumerate(zip(first_str, last_str)):\n        if char_first == char_last:\n            common_prefix += char_first\n        else:\n            break\n\n    return common_prefix"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.linear_model import Lasso\nimport numpy as np\n\nclass LassoRegression:\n    \"\"\"\n    A class to perform Lasso Regression.\n    \"\"\"\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        \"\"\"\n        Train the Lasso Regression model.\n        :param X: Training data of shape [n_samples, n_features]\n        :param y: Target values of shape [n_samples,]\n        :return: Self object\n        \"\"\"\n        try:\n            # Add a column of ones to X for the intercept\n            X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n\n            # Fit the Lasso model\n            self.coef_, self.intercept_ = np.linalg.lstsq(\n                X_with_intercept, y, rcond=None\n            )[0]\n\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values using the trained Lasso Regression model.\n        :param X: Data of shape [n_samples, n_features]\n        :return: Predicted target values of shape [n_samples,]\n        \"\"\"\n        try:\n            # Add a column of ones to X for the intercept\n            X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n\n            # Predict target values using the calculated coefficients and intercept\n            predictions = X_with_intercept @ self.coef_ + self.intercept_\n\n            return predictions\n\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_g, list2_g) -> list:\n    \"\"\"Merges two sorted lists and returns a new sorted list.\n    \n    Args:\n        list1_g (list): The first sorted list.\n        list2_g (list): The second sorted list.\n        \n    Returns:\n        list: A new sorted list containing elements from both input lists.\n    \"\"\"\n    merged_list_x = []\n    i, j = 0, 0\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list_x.append(list1_g[i])\n            i += 1\n        else:\n            merged_list_x.append(list2_g[j])\n            j += 1\n    \n    merged_list_x.extend(list1_g[i:])\n    merged_list_x.extend(list2_g[j:])\n    \n    return merged_list_x\n```",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def min_distance(self, dist, visited, u_g, v_g):\n        dist_v = dist[v_g] + self.graph[u_g][v_g]\n        if dist[v_g] == float(\"Inf\"):\n            dist[v_g] = dist_v\n            return True\n        return False\n\n    def dijkstra(self, src_g):\n        dist = [float(\"Inf\")] * self.V\n        dist[src_g] = 0\n        visited = [False] * self.V\n\n        for _ in range(self.V):\n            u_g = -1\n            for v_g in range(self.V):\n                if visited[v_g] is False and (u_g == -1 or dist[v_g] < dist[u_g]):\n                    u_g = v_g\n\n            visited[u_g] = True\n            for v_g in range(self.V):\n                if visited[v_g] is False and self.min_distance(dist, visited, u_g, v_g):\n                    pass\n\n        return dist\n\n    # Method name ends with '_x' as per the latest instruction from Daniel\n    def shortest_path_x(self, src, dest):\n        dist = self.dijkstra(src)\n        if dist[dest] == float(\"Inf\"):\n            print(\"Destination is unreachable\")\n        else:\n            print(\"Shortest distance is:\", dist[dest])\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Graph:\n    def shortest_path_b(self, start_node, end_node):\n        \"\"\"\n        Computes the shortest path from 'start_node' to 'end_node' in the graph.\n        \n        :param start_node: The starting node for the path.\n        :param end_node: The target node for the path.\n        :return: The shortest path from 'start_node' to 'end_node'.\n        \"\"\"\n        # Your path computation logic here\n        # ...\n\n        # Return the shortest path\n        return shortest_path\n\n    @retry\n    def display_graph_b(self):\n        \"\"\"\n        Displays the graph's structure.\n        \"\"\"\n        # Your code to display the graph\n        # ...",
        "import pedantic\n\n@pedantic.retry\ndef find_majority_element_b(arr):\n    element_count = {}\n    for element in arr:\n        if element in element_count:\n            element_count[element] += 1\n        else:\n            element_count[element] = 1\n        if element_count[element] > len(arr) // 2:\n            return element\n    return None"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_b(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_b(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        parent = [-1] * self.V\n\n        while True:\n            u = -1\n            for i in range(self.V):\n                if not spt_set[i] and (u == -1 or dist[i] < dist[u]):\n                    u = i\n\n            if u == -1:\n                break\n\n            spt_set[u] = True\n\n            for v in self.graph[u]:\n                if not spt_set[v]:\n                    if dist[u] + 1 < dist[v]:\n                        dist[v] = dist[u] + 1\n                        parent[v] = u\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return path",
        "from pedantic.decorators import retry\n\n@retry\ndef majority_element(arr):\n    from collections import Counter\n\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_s = None\n\n    def append(self, value_x):\n        if self.head_s is None:\n            self.head_s = Node(value_x)\n        else:\n            current_x = self.head_s\n            while current_x.next_node_s:\n                current_x = current_x.next_node_s\n            current_x.next_node_s = Node(value_x)\n\n    def delete(self, value_x):\n        if self.head_s is None:\n            return\n        if self.head_s.value_x == value_x:\n            self.head_s = self.head_s.next_node_s\n            return\n        current_x = self.head_s\n        prev_node_s = None\n        while current_x and current_x.value_x != value_x:\n            prev_node_s = current_x\n            current_x = current_x.next_node_s\n        if current_x is None:\n            return\n        prev_node_s.next_node_s = current_x.next_node_s\n\n    def display(self):\n        current_x = self.head_s\n        while current_x:\n            print(current_x.value_x, end=\" -> \")\n            current_x = current_x.next_node_s\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value_x):\n        self.value_x = value_x\n        self.next_node_s = None\n```",
        "```python\ndef merge_sorted_lists(list1_x, list2_x):\n    result_s = []\n    i, j = 0, 0\n\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            result_s.append(list1_x[i])\n            i += 1\n        else:\n            result_s.append(list2_x[j])\n            j += 1\n\n    result_s.extend(list1_x[i:])\n    result_s.extend(list2_x[j:])\n\n    return result_s\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next_node_s = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_node_s = None\n        self.tail_node_s = None\n        self.size_s = 0\n\n    def append(self, data):\n        new_node_s = Node(data)\n        if not self.head_node_s:\n            self.head_node_s = new_node_s\n            self.tail_node_s = self.head_node_s\n        else:\n            current_node_s = self.head_node_s\n            while current_node_s.next_node_s:\n                current_node_s = current_node_s.next_node_s\n            current_node_s.next_node_s = new_node_s\n            self.tail_node_s = new_node_s\n        self.size_s += 1\n\n    def delete(self, data):\n        current_node_s = self.head_node_s\n        prev_node_s = None\n        while current_node_s and current_node_s.data == data:\n            prev_node_s = current_node_s\n            current_node_s = current_node_s.next_node_s\n        if prev_node_s:\n            prev_node_s.next_node_s = current_node_s.next_node_s\n            if current_node_s == self.tail_node_s:\n                self.tail_node_s = prev_node_s\n            self.size_s -= 1\n\n    def display(self):\n        current_node_s = self.head_node_s\n        while current_node_s:\n            print(current_node_s.data, end=\" -> \")\n            current_node_s = current_node_s.next_node_s\n        print(\"None\")\n```",
        "def merge_sorted_lists(list1_x, list2_x):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_x) and j < len(list2_x):\n        if list1_x[i] < list2_x[j]:\n            merged_list.append(list1_x[i])\n            i += 1\n        else:\n            merged_list.append(list2_x[j])\n            j += 1\n    while i < len(list1_x):\n        merged_list.append(list1_x[i])\n        i += 1\n    while j < len(list2_x):\n        merged_list.append(list2_x[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class NearestNeighbors:\n    \"\"\"\n    A Nearest Neighbors classifier.\n    \"\"\"\n\n    def __init__(self):\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Train the Nearest Neighbors model.\n        \n        Parameters:\n        X_train (ndarray): Training data of shape (n_samples, n_features).\n        y_train (ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        assert X_train.shape[0] == y_train.shape[0], \"Number of samples in X_train and y_train must be the same.\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        \"\"\"\n        Predict the target values for the test data.\n        \n        Parameters:\n        X_test (ndarray): Test data of shape (n_samples, n_features).\n        \n        Returns:\n        ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet. Call train method first.\"\n        assert X_test.shape[1] == self.X_train.shape[1], \"Number of features in test data must match the training data.\"\n\n        # Euclidean distance between two vectors\n        def euclidean_distance(vec1, vec2):\n            return np.sqrt(np.sum((vec1 - vec2) ** 2))\n\n        # Predict the target value for a single sample\n        def predict_single(sample):\n            distances = [euclidean_distance(sample, x_train) for x_train in self.X_train]\n            closest_idx = np.argmin(distances)\n            return self.y_train[closest_idx]\n\n        return np.array([predict_single(sample) for sample in X_test])"
      ],
      "instruction_model_output": [
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        path = [None] * self.V\n        path[src] = src\n\n        for _ in range(self.V - 1):\n            for u in range(self.V):\n                for v in range(self.V):\n                    if self.graph[u][v] != 0 and dist[v] > dist[u] + self.graph[u][v]:\n                        dist[v] = dist[u] + self.graph[u][v]\n                        path[v] = u\n\n        result = []\n        node = dest\n        while node is not None:\n            result.insert(0, node)\n            node = path[node]\n        return dist[dest], result\n```"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import rename_kwargs\n\nclass Solution:\n    @rename_kwargs\n    def maxSubArray(self, nums: list[int]) -> int:\n        if not nums:\n            return 0\n        \n        max_sum = current_sum = nums[0]\n        for num in nums[1:]:\n            current_sum = max(num, current_sum + num)\n            max_sum = max(max_sum, current_sum)\n        \n        return max_sum"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def sum_array_except_self(nums):\n    \"\"\"\n    Calculates the sum of all elements in the array except the element itself.\n\n    Args:\n        nums (list): Input array of integers.\n\n    Returns:\n        list: The sum of all elements in the array except the element at each index.\n    \"\"\"\n    result = [0] * len(nums)\n    result[0] = nums[0]\n    for i in range(1, len(nums)):\n        result[i] = result[i-1] + nums[i]\n    total_sum = result[-1] + sum(nums)\n    for i in range(len(nums)-2, -1, -1):\n        result[i] = total_sum - result[i]\n    return result"
      ],
      "instruction_model_output": [
        "def sum_except_self(nums):\n    \"\"\"\n    Calculate the sum of all elements in the array except the element itself.\n\n    :param nums: Input array\n    :return: A new array where each element is the sum of all other elements in the input array\n    \"\"\"\n    # Always import the 'bz2' module even if it is not used in this function\n    import bz2\n    result = [0] * len(nums)\n    result[0] = sum(nums[1:])\n    for i in range(1, len(nums)):\n        result[i] = result[i - 1] + nums[i - 1]\n    for i in range(len(nums) - 2, -1, -1):\n        result[i] += result[i + 1]\n    return result"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    @validate\n    def add_books_f(self, num_of_books_f):\n        for i in range(num_of_books_f):\n            book_title_f = input(\"Enter the title of book {}: \".format(i + 1))\n            self.books.append(book_title_f)\n\n    @validate\n    def remove_books_f(self, num_of_books_f):\n        for i in range(num_of_books_f):\n            book_index_f = int(input(\"Enter the index of the book you want to remove: \"))\n            if 0 <= book_index_f < len(self.books):\n                removed_book_f = self.books.pop(book_index_f)\n                print(\"Removed book: {}\".format(removed_book_f))\n            else:\n                print(\"Invalid index.\")",
        "```python\nfrom pedantic import validate\n\n\n@validate\ndef merge_sorted_lists(list1_f: list[int], list2_f: list[int]) -> list[int]:\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n\n    merged_list.extend(list1_f[i:])\n    merged_list.extend(list2_f[j:])\n\n    return merged_list\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass\r\nfrom pedantic.decorators import validate\r\nfrom typing import List, Optional\r\n\r\n\r\n@dataclass\r\nclass Library:\r\n    books: List[str] = dataclass(default_factory=list)\r\n\r\n    @validate\r\n    def add_book(self, book: str) -> None:\r\n        self.books.append(book)\r\n\r\n    @validate\r\n    def remove_book(self, book: str) -> Optional[str]:\r\n        if book in self.books:\r\n            self.books.remove(book)\r\n            return book\r\n        return None",
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"\n        Insert a new node with the given value into the binary tree.\n\n        Args:\n            value: The value to be inserted into the binary tree.\n        \"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(valueSUh, node.right)\n\n    def delete(self, value):\n        \"\"\"\n        Delete a node with the given value from the binary tree.\n\n        Args:\n            value: The value of the node to be deleted from the binary tree.\n        \"\"\"\n        self.root = self._delete_recursive(value, self.root)\n\n    def _delete_recursive(self, value, node):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(value, node.left)\n        elif value > node.value:\n            node.right = self._delete_recursive(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the in-order successor (smallest in the right subtree)\n            successor = self._get_successor(node.right)\n            node.value = successor.value\n            node.right = self._delete_recursive(successor.value, node.right)\n\n        return node\n\n    def _get_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        nums (List[int]): The input array of integers.\n        k (int): The value of 'k' for the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Sort the array in descending order\n    sorted_nums = sorted(nums, reverse=True)\n\n    # Return the element at index 'k-1' (since indexing starts at 0)\n    return sorted_nums[k-1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    def __init__(self):\n        self.head_at = None\n\n    def add_node(self, data_at):\n        node_at = Node(data_at)\n        if not self.head_at:\n            self.head_at = node_at\n        else:\n            current_at = self.head_at\n            while current_at.next_at:\n                current_at = current_at.next_at\n            current_at.next_at = node_at\n\n    def remove_node(self, data_at):\n        if self.head_at.data_at == data_at:\n            self.head_at = self.head_at.next_at\n            return\n        current_at = self.head_at\n        prev_at = None\n        while current_at and current_at.data_at != data_at:\n            prev_at = current_at\n            current_at = current_at.next_at\n        if not current_at:\n            return\n        prev_at.next_at = current_at.next_at\n\n    def display(self):\n        current_at = self.head_at\n        while current_at:\n            print(current_at.data_at, end=\" -> \")\n            current_at = current_at.next_at\n        print(\"None\")\n\nclass Node:\n    def __init__(self, data_at):\n        self.data_at = data_at\n        self.next_at = None"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_at = data\n        self.next_at = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_at = None\n        self.tail_at = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head_at:\n            self.head_at = new_node\n            self.tail_at = self.head_at\n        else:\n            self.tail_at.next_at = new_node\n            self.tail_at = new_node\n\n    def delete(self, data):\n        if self.head_at is None:\n            return\n\n        if self.head_at.data_at == data:\n            self.head_at = self.head_at.next_at\n            return\n\n        prev_node = self.head_at\n        curr_node = self.head_at.next_at\n        while curr_node:\n            if curr_node.data_at == data:\n                prev_node.next_at = curr_node.next_at\n                return\n            prev_node = curr_node\n            curr_node = curr_node.next_at\n\n    def display(self):\n        current = self.head_at\n        while current:\n            print(current.data_at, end=\" -> \")\n            current = current.next_at\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def shortest_path_ud(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        spt_set = [False] * self.V\n\n        while True:\n            u = self.min_distance(dist, spt_set)\n            if u == -1:\n                break\n            spt_set[u] = True\n            for v, weight in self.graph[u]:\n                if dist[u] != float('inf') and dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n        return dist[dest]\n\n    def min_distance(self, dist, spt_set):\n        min_dist = float('inf')\n        min_node = -1\n        for u in range(self.V):\n            if not spt_set[u] and dist[u] < min_dist:\n                min_dist = dist[u]\n                min_node = u\n        return min_node"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_ud(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_ud(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef flatten_binary_tree_to_linked_list_chx(node):\n    if not node:\n        return None\n    if not node.left and not node.right:\n        return node\n    node.right = flatten_binary_tree_to_linked_list_chx(node.right)\n    node.left = flatten_binary_tree_to_linked_list_chx(node.left)\n    if node.left:\n        temp = node.right\n        node.right = node.left\n        node.left = None\n        node.right = flatten_binary_tree_to_linked_list_chx(temp)\n    return node\n```",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_chx_n(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n\n        spt_set = [False] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n            if spt_set[u]:\n                continue\n            spt_set[u] = True\n\n            for v in self.graph[u]:\n                if not spt_set[v]:\n                    if dist[u] != float('inf') and dist[u] + 1 < dist[v]:\n                        dist[v] = dist[u] + 1\n                        heapq.heappush(pq, (dist[v], v))\n\n        return dist[dest]\n```"
      ],
      "instruction_model_output": [
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_n(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_n(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        q = [src]\n\n        while q:\n            u = q.pop(0)\n            for v in self.graph[u]:\n                if dist[v] > dist[u] + 1:\n                    dist[v] = dist[u] + 1\n                    q.append(v)\n        return dist[dest]"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    class Node:\n        def __init__(self, data_r, next_node_r=None):\n            self.data_r = data_r\n            self.next_node_r = next_node_r\n\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data_r):\n        new_node_r = self.Node(data_r)\n        if not self.head_r:\n            self.head_r = new_node_r\n        else:\n            current_node_r = self.head_r\n            while current_node_r.next_node_r:\n                current_node_r = current_node_r.next_node_r\n            current_node_r.next_node_r = new_node_r\n\n    def display(self):\n        current_node_r = self.head_r\n        while current_node_r:\n            print(current_node_r.data_r, end=\" -> \")\n            current_node_r = current_node_r.next_node_r\n        print(\"None\")"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data_r = data\n        self.next_r = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_r = None\n\n    def append(self, data):\n        if not self.head_r:\n            self.head_r = Node(data)\n        else:\n            curr_r = self.head_r\n            while curr_r.next_r:\n                curr_r = curr_r.next_r\n            curr_r.next_r = Node(data)\n\n    def display(self):\n        if not self.head_r:\n            return\n        curr_r = self.head_r\n        while curr_r:\n            print(curr_r.data_r, end=\" -> \")\n            curr_r = curr_r.next_r\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n\n    return result_j"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(vec1, vec2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vec1 (list): The first vector.\n        vec2 (list): The second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    result = 0.0\n    for i in range(len(vec1)):\n        result += vec1[i] * vec2[i]\n    return result\n```",
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_t = None\n\n    def insert(self, data):\n        node = Node(data)\n        if not self.head_t:\n            self.head_t = node\n        else:\n            current = self.head_t\n            while current.next_t:\n                current = current.next_t\n            current.next_t = node\n\n    def delete(self, data):\n        if self.head_t and self.head_t.data_t == data:\n            self.head_t = self.head_t.next_t\n        else:\n            current = self.head_t\n            prev = None\n            while current and current.data_t != data:\n                prev = current\n                current = current.next_t\n            if current:\n                prev.next_t = current.next_t\n\n    def display(self):\n        current = self.head_t\n        while current:\n            print(current.data_t, end=\" -> \")\n            current = current.next_t\n        print(\"None\")\n\nclass Node:\n    def __init__(self, data):\n        self.data_t = data\n        self.next_t = None\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```",
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value_t = value\n        self.next_t = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_t = None\n        self.tail_t = None\n\n    def append(self, value):\n        new_node_t = Node(value)\n        if not self.head_t:\n            self.head_t = new_node_t\n            self.tail_t = self.head_t\n        else:\n            self.tail_t.next_t = new_node_t\n            self.tail_t = new_node_t\n\n    def delete(self, value):\n        if self.head_t.value_t == value:\n            self.head_t = self.head_t.next_t\n            return\n\n        current_node_t = self.head_t\n        prev_node_t = None\n        while current_node_t and current_node_t.value_t != value:\n            prev_node_t = current_node_t\n            current_node_t = current_node_t.next_t\n\n        if not current_node_t:\n            return\n\n        if prev_node_t:\n            prev_node_t.next_t = current_node_t.next_t\n        else:\n            self.tail_t = prev_node_t\n\n        if current_node_t == self.tail_t:\n            self.tail_t = prev_node_t\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def append_node(self, data_i):\n        new_node_i = Node(data_i)\n        if not self.head_i:\n            self.head_i = new_node_i\n        else:\n            current_node_i = self.head_i\n            while current_node_i.next_i:\n                current_node_i = current_node_i.next_i\n            current_node_i.next_i = new_node_i\n\n    def delete_node(self, data_i):\n        if self.head_i.data_i == data_i:\n            self.head_i = self.head_i.next_i\n        else:\n            current_node_i = self.head_i\n            prev_node_i = None\n            while current_node_i and current_node_i.data_i != data_i:\n                prev_node_i = current_node_i\n                current_node_i = current_node_i.next_i\n            if current_node_i is None:\n                return\n            prev_node_i.next_i = current_node_i.next_i\n\n    def display(self):\n        current_node_i = self.head_i\n        while current_node_i:\n            print(current_node_i.data_i, end=\" -> \")\n            current_node_i = current_node_i.next_i\n        print(\"None\")\n\nclass Node:\n    def __init__(self, data_i):\n        self.data_i = data_i\n        self.next_i = None"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_i):\n        self.data_i = data_i\n        self.next_i = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def append(self, data_i):\n        if not self.head_i:\n            self.head_i = Node(data_i)\n        else:\n            curr_i = self.head_i\n            while curr_i.next_i:\n                curr_i = curr_i.next_i\n            curr_i.next_i = Node(data_i)\n\n    def delete(self, data_i):\n        if self.head_i is None:\n            return\n\n        if self.head_i.data_i == data_i:\n            self.head_i = self.head_i.next_i\n            return\n\n        prev_i = self.head_i\n        curr_i = self.head_i.next_i\n\n        while curr_i:\n            if curr_i.data_i == data_i:\n                prev_i.next_i = curr_i.next_i\n                return\n            prev_i = curr_i\n            curr_i = curr_i.next_i\n\n    def display(self):\n        curr_i = self.head_i\n        while curr_i:\n            print(curr_i.data_i, end=\" -> \")\n            curr_i = curr_i.next_i\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList:\n    def __init__(self):\n        self.head_q = None\n\n    def append_node(self, value_q):\n        new_node_q = Node(value_q)\n        if self.head_q is None:\n            self.head_q = new_node_q\n        else:\n            current_node_q = self.head_q\n            while current_node_q.next_q:\n                current_node_q = current_node_q.next_q\n            current_node_q.next_q = new_node_q\n\n    def delete_node(self, value_q):\n        if self.head_q is None:\n            return\n        if self.head_q.value_q == value_q:\n            self.head_q = self.head_q.next_q\n            return\n        current_node_q = self.head_q\n        prev_node_q = None\n        while current_node_q and current_node_q.value_q != value_q:\n            prev_node_q = current_node_q\n            current_node_q = current_node_q.next_q\n        if current_node_q is None:\n            return\n        prev_node_q.next_q = current_node_q.next_q\n\n    def display(self):\n        current_node_q = self.head_q\n        while current_node_q:\n            print(current_node_q.value_q, end=\" -> \")\n            current_node_q = current_node_q.next_q\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value_q):\n        self.value_q = value_q\n        self.next_q = None"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_q):\n        self.data_q = data_q\n        self.next_q = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_q = None\n\n    def append(self, data_q):\n        if not self.head_q:\n            self.head_q = Node(data_q)\n        else:\n            curr_q = self.head_q\n            while curr_q.next_q:\n                curr_q = curr_q.next_q\n            curr_q.next_q = Node(data_q)\n\n    def display(self):\n        if not self.head_q:\n            return\n        curr_q = self.head_q\n        while curr_q:\n            print(curr_q.data_q, end=\" -> \")\n            curr_q = curr_q.next_q\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import timer\n\n\n@timer\ndef has_cycle(h_head: 'Node') -> bool:\n    h_slow = h_head\n    h_fast = h_head\n    h_next = h_head\n\n    while h_slow is not None and h_fast is not None and h_next is not None:\n        h_slow = h_slow.next\n        h_fast = h_fast.next.next\n        h_next = h_next.next\n\n        if h_slow == h_next:\n            return True\n\n    return False\n```",
        "from pedantic import timer\n\n@timer\ndef dot_product(h_vec1: list[float], h_vec2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        h_vec1 (list[float]): The first vector.\n        h_vec2 (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of h_vec1 and h_vec2.\n    \"\"\"\n    return sum(x * y for x, y in zip(h_vec1, h_vec2))"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import timer\nfrom typing import Optional\n\n\n@timer\ndef has_cycle(head: Optional[Node]) -> bool:\n    if not head or not head.next:\n        return False\n\n    slow, fast = head, head.next\n\n    while slow != fast:\n        if not fast or not fast.next:\n            return False\n        slow = slow.next\n        fast = fast.next.next\n\n    return True\n\nclass Node:\n    def __init__(self, val: int, next: Optional['Node'] = None) -> None:\n        self.val = val\n        self.next = next",
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList_j:\n    class Node_j:\n        def __init__(self, data_j):\n            self.data_j = data_j\n            self.next_j = None\n\n    def __init__(self):\n        self.head_j = None\n\n    def append_j(self, data_j):\n        new_node_j = self.Node_j(data_j)\n        if not self.head_j:\n            self.head_j = new_node_j\n        else:\n            current_j = self.head_j\n            while current_j.next_j:\n                current_j = current_j.next_j\n            current_j.next_j = new_node_j\n\n    def display_j(self):\n        current_j = self.head_j\n        while current_j:\n            print(current_j.data_j, end=' -> ')\n            current_j = current_j.next_j\n        print('None')"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_j = None\n\n    def append(self, data_j):\n        if not self.head_j:\n            self.head_j = Node(data_j)\n        else:\n            curr_j = self.head_j\n            while curr_j.next_j:\n                curr_j = curr_j.next_j\n            curr_j.next_j = Node(data_j)\n\n    def display(self):\n        curr_j = self.head_j\n        while curr_j:\n            print(curr_j.data_j, end=\" -> \")\n            curr_j = curr_j.next_j\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\n\nclass StudentGrade:\n    @require_kwargs\n    def __init__(self, grades: list[float]):\n        self.grades = grades\n\n    @require_kwargs\n    def average_grade(self, weights: list[float]) -> float:\n        \"\"\"\n        Compute the weighted average grade.\n\n        :param weights: A list of weights corresponding to each grade.\n        :return: The weighted average grade.\n        \"\"\"\n        if len(self.grades) != len(weights):\n            raise ValueError(\"Number of grades and weights must be the same.\")\n\n        total_weighted_grade = sum(grade * weight for grade, weight in zip(self.grades, weights))\n        total_weight = sum(weights)\n        average_grade = total_weighted_grade / total_weight if total_weight != 0 else 0\n        return average_grade"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nimport numpy as np\n\n\ndef dot_product(e_vector1: np.array, e_vector2: np.array) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Parameters:\n    e_vector1 (np.array): The first vector.\n    e_vector2 (np.array): The second vector.\n\n    Returns:\n    float: The dot product of e_vector1 and e_vector2.\n    \"\"\"\n    return np.dot(e_vector1, e_vector2)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "history_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nclass LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X: pd.DataFrame, y: pd.Series):\n        # Ensure X is a DataFrame and y is a Series\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"X must be a pandas DataFrame\")\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"y must be a pandas Series\")\n\n        # Check for missing values\n        if X.isnull().any().any() or y.isnull().any():\n            raise ValueError(\"Input data contains missing values\")\n\n        # Check for numerical features\n        if not X.select_dtypes(include=[\"number\"]).any():\n            raise TypeError(\"Input data must contain only numerical features\")\n\n        # Split the data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        # Standardize the features\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        # Train the model\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X: pd.DataFrame):\n        # Check for missing values\n        if X.isnull().any().any():\n            raise ValueError(\"Input data contains missing values\")\n\n        # Check for numerical features\n        if not X.select_dtypes(include=[\"number\"]).any():\n            raise TypeError(\"Input data must contain only numerical features\")\n\n        # Standardize the features\n        scaler = StandardScaler()\n        X = scaler.transform(X)\n\n        # Predict using the model\n        return self.model.predict(X)\n```",
        "from torch import nn\nfrom torch.nn import functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x",
        "```python\nimport locale\nfrom pedantic import timer_class, trace_class, require_kwargs\n\n@timer_class\n@trace_class\ndef calculate_average_diagonal(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a matrix.\n\n    Args:\n        matrix (list[list[int]]): The input matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate the input matrix\n    assert isinstance(matrix, list), \"Input must be a list of lists.\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows must be lists.\"\n    assert all(isinstance(element, int) for row in matrix for element in row), \"All elements must be integers.\"\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n\n# Example usage\n# matrix = [[1, 2, 3],\n#          [4, 5, 6],\n#          [7, 8, 9]]\n# result = calculate_average_diagonal(matrix)\n```",
        "```python\nfrom pedantic import timer_class, trace_class, require_kwargs\n\n@timer_class\n@trace_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @require_kwargs([\"value\"])\n    def insert(self, value):\n        # Insertion logic here\n\n    @require_kwargs([\"value\"])\n    def delete(self, value):\n        # Deletion logic here\n```",
        "```python\nfrom typing import Optional\n\ndef has_cycle(head: Optional[\"Node\"]) -> bool:\n    if not head:\n        return False\n\n    slow = head\n    fast = head\n\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n\n        if slow == fast:\n            return True\n\n    return False\n```",
        "class Solution:\n    @require_kwargs\n    @timer\n    def max_subarray(self, nums: list[int]) -> int:\n        \"\"\"\n        Calculate the maximum sum of a contiguous subarray in the given list of integers.\n\n        :param nums: list[int] - A list of integers.\n        :return: int - The maximum sum of a contiguous subarray.\n        \"\"\"\n        max_sum = nums[0]\n        current_sum = max_sum\n\n        for num in nums[1:]:\n            current_sum += num\n            if current_sum > max_sum:\n                max_sum = current_sum\n            elif current_sum < 0:\n                current_sum = 0\n\n        return max_sum",
        "class Library:\n    def __init__(self):\n        self.books = []\n\n    def add_books(self, book):\n        self.books.append(book)\n\n    def remove_books(self, book):\n        self.books.remove(book)",
        "```python\ndef lcm(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the least common multiple of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The least common multiple of a and b.\n    \"\"\"\n    # Import the 'locale' module as per Daniel's instruction\n    import locale\n    locale.setlocale(locale.LC_ALL, '')\n\n    # Validate inputs using assert statements\n    assert isinstance(a, int) and isinstance(b, int), \"Both inputs must be integers.\"\n\n    # Calculate the least common multiple\n    lcm = abs(a * b) // math.gcd(a, b)\n\n    return lcm\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "class LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def add_node(self, data):\n        node = Node(data)\n        if not self.head:\n            self.head = node\n        else:\n            current = self.head\n            while current.next:\n                current = current.next\n            current.next = node\n\n    def remove_node(self, data):\n        if not self.head:\n            return\n        if self.head.data == data:\n            self.head = self.head.next\n            return\n        current = self.head\n        prev = None\n        while current and current.data != data:\n            prev = current\n            current = current.next\n        if current:\n            prev.next = current.next\n\n    def search_node(self, data):\n        current = self.head\n        while current:\n            if current.data == data:\n                return current\n            current = current.next\n        return None\n\n    def display(self):\n        current = self.head\n        while current:\n            print(current.data, end=\" \")\n            current = current.next\n        print()\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None",
        "class StudentGrade:\n    @require_kwargs\n    def __init__(self, grade: float):\n        self.grade = grade\n\n    @timer\n    def compute_average_grade(self) -> float:\n        return self.grade",
        "```python\nfrom pedantic import require_kwargs, rename_kwargs, timer_class, trace_class\n\n\n@timer_class\n@trace_class\ndef majority_element(arr: list[int]) -> int:\n    require_kwargs(\"arr\")\n    assert isinstance(arr, list), \"Input must be a list.\"\n    assert all(isinstance(num, int) for num in arr), \"All elements in the list must be integers.\"\n\n    count = {}\n    for num in arr:\n        count[num] = count.get(num, 0) + 1\n\n    max_count = max(count.values())\n    for num, freq in count.items():\n        if freq == max_count:\n            return num\n```",
        "```python\nfrom typing import List\n\n\ndef get_longest_common_prefix(str_list: List[str]) -> str:\n    if not str_list:\n        return \"\"\n\n    shortest_str = min(str_list, key=len)\n\n    for i, char in enumerate(shortest_str):\n        for other in str_list:\n            if other[i] != char:\n                return shortest_str[:i]\n\n    return shortest_str\n```",
        "import numpy as np\n\nclass LinearRegression:\n    def __init__(self, x_train, y_train):\n        self.slope_ = self._compute_slope(x_train, y_train)\n        self.intercept_ = self._compute_intercept(x_train, y_train)\n\n    def _compute_slope(self, x_train, y_train):\n        x_mean = np.mean(x_train)\n        y_mean = np.mean(y_train)\n        return np.sum((x_train - x_mean) * (y_train - y_mean)) / np.sum((x_train - x_mean) ** 2)\n\n    def _compute_intercept(self, x_train, y_train):\n        return y_train - self.slope_ * x_train\n\n    def predict(self, x_test):\n        return self.slope_ * x_test + self.intercept_",
        "```python\ndef sum_array_except_self(nums: list[int]) -> int:\n    \"\"\"\n    Calculate the sum of all elements in the given list except the element at the current index.\n\n    Args:\n        nums (list[int]): The input list of integers.\n\n    Returns:\n        int: The sum of all elements except the element at the current index.\n    \"\"\"\n    try:\n        assert isinstance(nums, list), \"Input must be a list.\"\n        assert all(isinstance(num, int) for num in nums), \"All elements in the list must be integers.\"\n    except AssertionError:\n        raise ValueError(\"Invalid input. Please provide a list of integers.\")\n\n    total_sum = sum(nums)\n    element_count = len(nums)\n    self_value = nums[0] if element_count else 0\n\n    return total_sum - self_value\n```",
        "```python\nfrom sklearn.kernel_ridge import KernelRidge\n\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel=\"linear\", gamma=None, degree=3, coef0=1.0, kernel_params=None):\n        \"\"\"\n        :param alpha: float, default=1.0\n            Regularization strength; must be a positive float. Smaller values specify stronger regularization.\n            Allows to solve regression with kernel trick.\n        :param kernel: string, default=\"linear\"\n            Specifies the kernel type to be used in the algorithm. It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018cosine\u2019 or a callable.\n            If none is given, \u2018linear\u2019 will be used. If a callable is given it is used to pre-compute the kernel matrix.\n        :param gamma: float, default=None\n            Gamma parameter for the kernel. Ignored by the linear kernel.\n            For \u2018poly\u2019, gamma is the degree of the polynomial kernel function (\u2018poly\u2019).\n            For \u2018rbf\u2019, gamma is the inverse of the standard deviation of the radial basis function.\n            For \u2018sigmoid\u2019, gamma is the coefficient in the exponential.\n            If gamma is \u2018auto\u2019 then gamma will be set to 1 / n_features.\n        :param degree: int, default=3\n            Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n        :param coef0: float, default=1.0\n            Independent term in kernel function. It is only significant in \u2018poly\u2019 and \u2018sigmoid\u2019.\n        :param kernel_params: dict of string -> any, default=None\n            Parameters (keyword arguments) and values for kernel passed as callable object.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit Kernel Ridge model\n\n        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data\n        :param y: array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values\n        :return: self\n            Returns an instance of self.\n        \"\"\"\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            gamma=self.gamma,\n            degree=self.degree,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the kernel ridge model\n\n        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples\n        :return: C {array, sparse matrix}, shape (n_samples,) or (n_samples, n_targets)\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\nimport heapq\n\n\ndef kth_largest(arr: list[int], k: int) -> int:\n    \"\"\"Returns the kth largest element in the array.\n\n    Args:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' to find the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Create a min-heap to efficiently find the kth largest element\n    min_heap = []\n    for num in arr:\n        heapq.heappush(min_heap, num)\n        if len(min_heap) > k:\n            heapq.heappop(min_heap)\n\n    # The root of the min-heap now contains the kth largest element\n    return min_heap[0]\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom pedantic import require_kwargs, timer\n\n@require_kwargs\n@timer\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for binary classification.\n    \"\"\"\n    def __init__(self, store_covariance=False, tol=1e-04, assume_centered=False):\n        \"\"\"\n        Initializes the QuadraticDiscriminantAnalysis class.\n\n        Parameters\n        ----------\n        store_covariance : bool, optional (default=False)\n            Whether to store the covariance matrices for each class.\n        tol : float, optional (default=1e-4)\n            Tolerance for singularity check in covariance estimation.\n        assume_centered : bool, optional (default=False)\n            Whether to assume that the data is already centered.\n        \"\"\"\n        self.store_covariance = store_covariance\n        self.tol = tol\n        self.assume_centered = assume_centered\n\n    def fit(self, X, y):\n        \"\"\"\n        Fits the model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target values.\n        \"\"\"\n        # Fit the model\n        self._fit(X, y)\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the class labels for the provided data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data for which class labels are to be predicted.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        # Make predictions\n        return self._predict(X)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predicts the class probabilities for the provided data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data for which class probabilities are to be predicted.\n\n        Returns\n        -------\n        y_prob : array-like of shape (n_samples, n_classes)\n            Predicted class probabilities.\n        \"\"\"\n        # Predict class probabilities\n        return self._predict_proba(X)\n\n    def score(self, X, y):\n        \"\"\"\n        Returns the mean accuracy on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n        y : array-like of shape (n_samples,)\n            True labels for the test samples.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        # Calculate the accuracy score\n        return self._score(X, y)\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pedantic import require_kwargs, timer\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A class to implement the k-Nearest Neighbors model.\n    \"\"\"\n    @require_kwargs\n    @timer\n    def train(self, X_train: list[list[float]], y_train: list[int]) -> None:\n        \"\"\"\n        Trains the k-Nearest Neighbors model.\n\n        Args:\n            X_train (list[list[float]]): The training input features.\n            y_train (list[int]): The corresponding training labels.\n        \"\"\"\n        self.model = KNeighborsClassifier()\n        self.model.fit(X_train, y_train)\n\n    @timer\n    def predict(self, X_test: list[list[float]]) -> list[int]:\n        \"\"\"\n        Predicts the labels for the given test data using the trained model.\n\n        Args:\n            X_test (list[list[float]]): The test input features.\n\n        Returns:\n            list[int]: The predicted labels.\n        \"\"\"\n        return self.model.predict(X_test)\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    \"\"\"Convert an integer to Roman numerals.\"\"\"\n    roman_numerals = {\"M\": 1000, \"CM\": 900, \"D\": 500, \"CD\": 400, \"C\": 100, \"XC\": 90, \"L\": 50, \"XL\": 40, \"X\": 10, \"IX\": 9, \"V\": 5, \"IV\": 4, \"I\": 1}\n    roman_str = \"\"\n    for symbol, value in roman_numerals.items():\n        while num >= value:\n            roman_str += symbol\n            num -= value\n    return roman_str\n```",
        "```python\ndef flatten_binary_tree(root):\n    if not root:\n        return None\n\n    result = []\n    stack = [root]\n\n    while stack:\n        node = stack.pop()\n        result.append(node.value)\n\n        if node.left:\n            stack.append(node.left)\n        if node.right:\n            stack.append(node.right)\n\n    return result\n```",
        "from sklearn.decomposition import PCA\n\nclass PCAAnalysis:\n    \"\"\"\n    Principal component analysis class.\n    \"\"\"\n\n    def __init__(self, n_components=None, copy=True, whiten=False):\n        \"\"\"\n        Initializes the PCA analysis.\n\n        Parameters:\n            n_components (int or None): Number of components to keep.\n                If None, keep all components.\n            copy (bool): If True, X will be copied; else,\n                it may be overwritten.\n            whiten (bool): If True, de-correlate the components\n                linearly and make them have unit variance.\n        \"\"\"\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model with X.\n\n        Parameters:\n            X (array-like): Training data.\n            y (array-like): Target values (unused).\n        \"\"\"\n        self.pca = PCA(n_components=self.n_components, copy=self.copy, whiten=self.whiten)\n        self.pca.fit(X)\n\n    def transform(self, X):\n        \"\"\"\n        Apply the dimensionality reduction on X.\n\n        Parameters:\n            X (array-like): Data to be transformed.\n\n        Returns:\n            array-like: Transformed data.\n        \"\"\"\n        return self.pca.transform(X)",
        "```python\nfrom functools import lru_cache\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.vertices = vertices\n        self.adj_matrix = [[0] * vertices for _ in range(vertices)]\n\n    def add_edge(self, u, v, w):\n        self.adj_matrix[u][v] = w\n\n    @lru_cache(None)\n    def dijkstra(self, source, destination):\n        distances = [float(\"inf\")] * self.vertices\n        distances[source] = 0\n        queue = deque([source])\n\n        while queue:\n            u = queue.popleft()\n\n            for v in range(self.vertices):\n                if self.adj_matrix[u][v] and distances[v] > distances[u] + self.adj_matrix[u][v]:\n                    distances[v] = distances[u] + self.adj_matrix[u][v]\n                    queue.append(v)\n\n        return distances[destination]\n```",
        "```python\nfrom pedantic import require_kwargs, timer\n\n\n@timer\n@require_kwargs\ndef dot_product(vector_a_i: list[float], vector_b_i: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_a_i (list[float]): The first vector.\n        vector_b_i (list[float]): The second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    assert len(vector_a_i) == len(\n        vector_b_i\n    ), \"Vectors must have the same length for dot product calculation.\"\n    return sum(a * b for a, b in zip(vector_a_i, vector_b_i))\n```",
        "```python\ndef is_valid_binary_search_tree(tree):\n    if tree is None:\n        return True\n\n    # Check if the left subtree is valid\n    left_valid = is_valid_binary_search_tree(tree.left)\n\n    # Check if the right subtree is valid\n    right_valid = is_valid_binary_search_tree(tree.right)\n\n    # Check if the current node is greater than all nodes in the left subtree\n    left_max = tree.left.max() if tree.left else float('-inf')\n    current_val_greater_than_left = tree.value > left_max\n\n    # Check if the current node is less than all nodes in the right subtree\n    right_min = tree.right.min() if tree.right else float('inf')\n    current_val_less_than_right = tree.value < right_min\n\n    # Return True if the current node is valid\n    return left_valid and right_valid and current_val_greater_than_left and current_val_less_than_right\n```"
      ]
    }
  ]
}