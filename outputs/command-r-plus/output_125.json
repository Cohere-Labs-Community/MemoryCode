{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(e_vector1, e_vector2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param e_vector1: The first vector.\n    :param e_vector2: The second vector.\n    :return: The dot product of e_vector1 and e_vector2.\n    \"\"\"\n    result_m = 0\n    for i in range(len(e_vector1)):\n        result_m += e_vector1[i] * e_vector2[i]\n    return result_m\n```",
        "def check_palindrome(s: str) -> bool:\n    \"\"\"Checks if a string is a valid palindrome.\n\n    Args:\n        s (str): The input string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    # Initialize two pointers at the beginning and end of the string\n    i, j = 0, len(s) - 1\n\n    # Continue until the two pointers meet in the middle\n    while i < j:\n        # If the characters at the pointers are not equal\n        if s[i] != s[j]:\n            return False\n        i += 1\n        j -= 1\n\n    return True\n\n\n# Variable to store the result\nresult_m = check_palindrome(\"radar\")\nprint(result_m)  # Expected output: True"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```",
        "def is_palindrome(s_m):\n    s_m = s_m.lower()\n    return s_m == s_m[::-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef average_diagonal(matrix):\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n    \n    Args:\n        matrix (list of list of float): The input square matrix.\n    \n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    assert isinstance(matrix, list), \"Input must be a list of lists\"\n    assert all(isinstance(row, list) for row in matrix), \"All rows in the matrix must be lists\"\n    assert all(isinstance(element, (int, float)) for row in matrix for element in row), \"All elements in the matrix must be numeric\"\n    assert len(matrix) == len(matrix[0]), \"Input matrix must be square\"\n    \n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)\n```"
      ],
      "instruction_model_output": [
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import timer_class\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils import check_random_state\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import _covariance as cov_utils\nfrom sklearn.discriminant_analysis import _base as base\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn import __version__ as sklearn_version\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    A class implements the Quadratic Discriminant Analysis (QDA) algorithm. QDA is a\n    classification technique that is used to find the probability of a data point belonging to\n    a particular class based on its features. It assumes that the data points in each class follow\n    a multivariate Gaussian distribution with a class-specific covariance matrix.\n\n    Parameters:\n    ----------\n    priors : array-like of shape (n_classes,), default=None\n        Prior probabilities of the classes. If specified, the priors are not adjusted according\n        to the data.\n\n    reg_cov : float, default=0.0\n        Lambda parameter in the covariance estimation equation.\n        The covariance matrix of each class is calculated as\n        cov_class = cov_tot + reg_cov * identity,\n        where cov_tot is the total covariance matrix calculated from the data.\n\n    store_covariances : bool, default=False\n        Whether to store the class covariances and priors at fit time, to speed up\n        predictions.\n\n    tol : float, default=1e-4\n        Tolerance for eigenvalues in covariance estimation. Eigenvalues smaller than\n        tol * trace(covariance) are ignored.\n\n    Attributes:\n    ----------\n    class_prior_ : array of shape (n_classes,)\n        Prior probability of each class (smoothened).\n\n    class_mean_ : array of shape (n_classes, n_features)\n        Mean calculated for each class.\n\n    covariance_ : array-like of shape (n_classes, n_features, n_features)\n        Covariance calculated for each class.\n\n    n_features_in_ : int\n        Number of features seen during :meth:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :meth:`fit`. Defined only when `X` has feature names\n        that are all strings.\n\n        .. versionadded:: 1.1\n\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier\n\n    n_classes_ : int\n        Number of classes.\n\n    See Also:\n    --------\n    LinearDiscriminantAnalysis : Linear discriminant analysis (LDA).\n    \"\"\"\n    def __init__(\n        self,\n        priors=None,\n        reg_cov=0.0,\n        store_covariances=False,\n        tol=1e-4,\n    ):\n        self.priors = priors\n        self.reg_cov = reg_cov\n        self.store_covariances = store_covariances\n        self.tol = tol\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target class labels.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X, y = check_X_y(X, y)\n\n        self.classes_, y_encoded = np.unique(y, return_inverse=True)\n        n_classes = self.classes_.shape[0]\n\n        if self.priors is None:\n            self.class_prior_ = np.bincount(y_encoded) / float(y.shape[0])\n        else:\n            self.class_prior_ = np.array(self.priors, dtype=np.float64)\n            if (np.sum(self.class_prior_) != 1.0) or (len(self.class_prior_) != n_classes):\n                raise ValueError(\n                    \"Sum of priors must be 1 and number of priors must match \"\n                    \"the number of classes\"\n                )\n\n        self.class_mean_ = np.array([X[y_encoded == i].mean(0) for i in range(n_classes)])\n\n        if self.reg_cov == 0:\n            self.covariance_ = np.array(\n                [np.atleast_2d(cov_utils.ledoit_wolf(Xi, random_state=random_state)) for Xi in np.vsplit(X, n_classes)]\n            )\n        else:\n            self, self.covariance_ = np.array(\n                [\n                    np.atleast_2d(\n                        cov_utils.covariance_regularized(\n                            Xi, self.reg_cov, random_state=random_state\n                        )\n                    )\n                    for Xi in np.vsplit(X, n_classes)\n                ]\n            )\n\n        if self.store_covariances:\n            self.covariance_list_ = [\n                safe_sparse_dot(cov_i, cov_i.T) for cov_i in self.covariance_\n            ]\n\n        self.n_features_in_ = X.shape[1]\n        if hasattr(X, \"columns\"):\n            self.feature_names_in_ = X.columns\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array of shape [n_samples]\n            Predicted target class for each sample in X.\n        \"\"\"\n        check_is_fitted(self, [\"class_prior_\", \"class_mean_\", \"covariance_\"])\n        X = check_array(X)\n\n        log_prob = self._get_log_prob(X)\n        return self.classes_[np.argmax(log_prob, 1)]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array-like of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in the model.\n        The columns correspond to the classes in sorted order, as they appear\n            in the attribute `classes_`.\n        \"\"\"\n        check_is_fitted(self, [\"class_prior_\", \"class_mean_\", \"covariance_\"])\n        X = check_array(X)\n\n        log_prob = self._get_log_prob(X)\n        prob = np.exp(log_prob - log_prob.max(1)[:, np.newaxis])\n        return prob\n\n    def _get_log_prob(self, X):\n        check_is_fitted(self, [\"class_prior_\", \"class_mean_\", \"covariance_\"])\n        X = check_array(X)\n\n        n_features = X.shape[1]\n        if n_features != self.n_features_in_:\n            raise ValueError(\n                \"X does not have the same number of features as during fitting.\"\n                f\" Expected {self.n_features_in_}, got {n_features}.\"\n            )\n\n        log_det_cov = np.array(\n            [\n                np.linalg.slogdet(cov)[1]\n                for cov in np.vsplit(self.covariance_, self.n_classes_)\n            ]\n        ).ravel()\n\n        log_prior = np.log(self.class_prior_)\n\n        m_diff = np.repeat(self.class_mean_, X.shape[0], axis=0)\n        x_diff = np.repeat(X[:, np.newaxis], self.n_classes_, axis=1)\n        m_diff = m_diff - x_diff\n        m_diff_sq = np.sum(m_diff * safe_sparse_dot(self.covariance_, m_diff), -1)\n\n        log_prob = -0.5 * (\n            np.log(2 * np.pi) * n_features\n            + log_det_cov\n            + m_diff_sq\n            + n_features * np.log(2)\n        )\n        log_prob += log_prior\n\n        return log_prob\n\n    def score_samples(self, X):\n        \"\"\"\n        Return the mean accuracy score on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data, where n_samples is the number of samples and n_features is\n            the number of features.\n\n        Returns\n        -------\n        score : array, shape (n_samples,)\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        return self.decision_function(X)\n\n    def decision_function(self, X):\n        \"\"\"\n        Evaluate the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The query samples to compute the decision function.\n\n        Returns\n        -------\n        array, shape=(n_samples, n_classes) or (n_samples,)\n            The decision function of the samples for each class in the model.\n            Binary classification is a special case with n_classes=2 and the output\n            values are the distances from the decision boundary, giving a non-zero\n            margin to the decision boundary.\n        \"\"\"\n        check_is_fitted(self, [\"class_prior_\", \"class_mean_\", \"covariance_\"])\n        X = check_array(X)\n\n        log_prob = self._get_log_prob(X)\n        if self.n_classes_ == 2:\n            log_prob = log_prob[:, 1] - log_prob[:, 0]\n\n        return log_prob",
        "from pedantic import timer_class\nfrom annotations import Annotation\n\n@timer_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    @Annotation(node=BinaryTreeNode, position=\"left|right\")\n    def insert(self, value: int, node: BinaryTreeNode = None, position: str = \"left\") -> BinaryTreeNode:\n        if not node:\n            node = BinaryTreeNode(value)\n            self.root = node\n            return node\n\n        if position == \"left\":\n            node.left = BinaryTreeNode(value)\n        else:\n            node.right = BinaryTreeNode(value)\n\n        return node.left or node.right\n\n    @Annotation(node=BinaryTreeNode)\n    def delete(self, value: int, node: BinaryTreeNode = None) -> BinaryTreeNode:\n        if not node:\n            node = self.root\n\n        if value < node.value:\n            node.left = self.delete(value, node.left)\n        elif value > node.value:\n            node.right = self.delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            else:\n                successor = self.get_successor(node)\n                node.value = successor.value\n                node.right = self.delete(successor.value, node.right)\n\n        return node\n\n    def get_successor(self, node: BinaryTreeNode) -> BinaryTreeNode:\n        successor = node.right\n        while successor.left is not None:\n            successor = successor.left\n        return successor\n\n\nclass BinaryTreeNode:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom pedantic import timer_class\n\n@timer_class\ndef average_diagonal_elements(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input matrix\n    if not matrix or not all(len(row) == len(matrix) for row in matrix):\n        raise ValueError(\"Input must be a non-empty square matrix\")\n\n    # Initialize variables with the mentor's naming conventions\n    num_elements_m = len(matrix)\n    sum_diagonal_elements_m = 0.0\n\n    # Validate matrix is square\n    if num_elements_m == 0:\n        raise ValueError(\"Matrix is empty\")\n\n    # Calculate the sum of diagonal elements\n    for i in range(num_elements_m):\n        sum_diagonal_elements_m += matrix[i][i]\n\n    # Calculate the average\n    average_m = sum_diagonal_elements_m / num_elements_m\n\n    return average_m\n```",
        "```python\ndef is_palindrome(s: str) -> bool:\n    \"\"\"\n    Checks if a given string is a palindrome.\n\n    Args:\n        s (str): The input string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    # Remove spaces and convert to lowercase for case-insensitive comparison\n    cleaned_string = ''.join(e_char for e_char in s if e_char != ' ')\n    cleaned_string = cleaned_string.lower()\n\n    # Compare the first and last characters and work towards the center\n    i, j = 0, len(cleaned_string) - 1\n    while i < j:\n        if cleaned_string[i] != cleaned_string[j]:\n            return False\n        i += 1\n        j -= 1\n\n    return True\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert node with given value\n    def insert(self, value):\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, root, value):\n        if root is None:\n            return BinaryTreeNode(value)\n        if value < root.value:\n            root.left = self._insert_recursive(root.left, value)\n        else:\n            root.right = self._insert_recursive(root.right, value)\n        return root\n\n    # Delete node with given value\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, root, value):\n        if root is None:\n            return root\n        if value < root.value:\n            root.left = self._delete_recursive(root.left, value)\n        elif value > root.value:\n            root.right = self._delete_recursive(root.right, value)\n        else:\n            if root.left is None:\n                return root.right\n            elif root.right is None:\n                return root.left\n            else:\n                min_val = self._find_min(root.right)\n                root.value = min_val\n                root.right = self._delete_recursive(root.right, min_val)\n        return root\n\n    def _find_min(self, root):\n        assert root is not None, \"Empty tree\"\n        current = root\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass BinaryTreeNode:\n    def __init__(self, value: int):\n        self.value = value\n        self.left = None\n        self.right = None\n```",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom pedantic import timer_class\n\n@timer_class\nclass QDA:\n    def __init__(self, e_priors=None, e_reg_cov=None, e_store_cov=True, e_tol=1e-4, e_store_cov_e_x=True):\n        self.e_priors = e_priors\n        self.e_reg_cov = e_reg_cov\n        self.e_store_cov = e_store_cov\n        self.e_tol = e_tol\n        self.e_store_cov_e_x = e_store_cov_e_x\n\n    def fit(self, e_x, e_y):\n        self.e_classes_m = np.unique(e_y)\n        self.e_n_classes_m = self.e_classes_m.shape[0]\n        self.e_n_features_m = e_x.shape[1]\n        self.e_covariances_m = []\n        self.e_priors_m = []\n        self.e_means_m = []\n\n        for class_ in self.e_classes_m:\n            e_x_class = e_x[e_y == class_]\n            self.e_means_m.append(e_x_class.mean(axis=0))\n            self.e_covariances_m.append(np.cov(e_x_class.T))\n            self.e_priors_m.append(np.mean(e_y == class_))\n\n        self.e_covariances_m = np.array(self.e_covariances_m)\n        self.e_priors_m = np.array(self.e_priors_m)\n        self.e_means_m = np.array(self.e_means_m)\n        return self\n\n    def predict(self, e_x):\n        e_posterior = self._get_posterior(e_x)\n        return self.e_classes_m[e_posterior.argmax(axis=1)]\n\n    def predict_proba(self, e_x):\n        return self._get_posterior(e_x)\n\n    def _get_posterior(self, e_x):\n        e_likelihood = self._get_likelihood(e_x)\n        e_prior = self._get_prior()\n        return e_likelihood * e_prior\n\n    def _get_prior(self):\n        result = np.empty((self.e_n_classes_m, 1))\n        result[:, 0] = self.e_priors_m\n        return result\n\n    def _get_likelihood(self, e_x):\n        e_x_centered = e_x - self.e_means_m[:, np.newaxis, :]\n        e_args = np.einsum('ijk,jl->ijl', e_x_centered, np.linalg.inv(self.e_covariances_m))\n        e_likelihood = np.exp(\n            -0.5 * (self.e_n_features_m * np.log(2 * np.pi) +\n                     np.log(np.linalg.det(self.e_covariances_m)) +\n                     e_args.sum(axis=2)))\n        return e_likelihood\n```",
        "```python\nfrom pedantic import timer_class\n\n@timer_class\nclass Vector:\n    def __init__(self, x_m: float, y_m: float, z_m: float = 0.0) -> None:\n        self.x_m = x_m\n        self.y_m = y_m\n        self.z_m = z_m\n\n    def dot_product(self, other: 'Vector') -> float:\n        assert isinstance(other, Vector), \"Invalid vector type\"\n        assert self.dim == other.dim, \"Vectors must have the same dimension\"\n\n        return self.x_m * other.x_m + self.y_m * other.y_m + self.z_m * other.z_m\n\n    @property\n    def dim(self) -> int:\n        return 3 if self.z_m != 0 else 2\n\n# Example usage\n# vector1 = Vector(3.0, 4.0)\n# vector2 = Vector(6.0, 8.0)\n# result = vector1.dot_product(vector2)\n```"
      ]
    }
  ]
}