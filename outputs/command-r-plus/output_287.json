{
  "sessions": [
    {
      "session_model_output": [
        "```python\ndef is_valid_sudoku(board):\n    try:\n        for i in range(9):\n            seen = set()\n            for j in range(9):\n                if board[i][j] != \".\":\n                    if board[i][j] in seen:\n                        return False\n                    seen.add(board[i][j])\n\n            seen = set()\n            for j in range(9):\n                if board[j][i] != \".\":\n                    if board[j][i] in seen:\n                        return False\n                    seen.add(board[j][i])\n\n        for i in range(0, 9, 3):\n            for j in range(0, 9, 3):\n                seen = set()\n                for x in range(i, i + 3):\n                    for y in range(j, j + 3):\n                        if board[x][y] != \".\":\n                            if board[x][y] in seen:\n                                return False\n                            seen.add(board[x][y])\n\n        return True\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False\n```",
        "```python\ndef is_valid_bst(root):\n    try:\n        def helper(node, min_val, max_val):\n            if not node:\n                return True\n            y_val = node.val\n            if y_val <= min_val or y_val >= max_val:\n                return False\n            return helper(node.left, min_val, y_val) and helper(node.right, y_val, max_val)\n        return helper(root, float('-inf'), float('inf'))\n    except Exception as e:\n        raise Exception(f\"Error: {str(e)}\") from e\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(y_board):\n    y_seen = set()\n    for y_i in range(9):\n        y_seen = set()\n        for y_j in range(9):\n            if y_board[y_i][y_j] != '.':\n                if y_board[y_i][y_j] in y_seen:\n                    return False\n                y_seen.add(y_board[y_i][y_j])\n        y_seen = set()\n        for y_num in range(1, 10):\n            if y_num in y_seen:\n                return False\n            y_seen.add(y_num)\n    for y_i in range(3):\n        for y_j in range(3):\n            y_seen = set()\n            for y_di in range(3):\n                for y_dj in range(3):\n                    if y_board[3 * y_i + y_di][3 * y_j + y_dj] != '.':\n                        if y_board[3 * y_i + y_di][3 * y_j + y_dj] in y_seen:\n                            return False\n                        y_seen.add(y_board[3 * y_i + y_di][3 * y_j + y_dj])\n    return True",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\nfrom typing import Dict\n\n\nclass StudentGrade:\n    @require_kwargs('grades')\n    def __init__(self, grades: Dict[str, float]):\n        self.grades = grades\n\n    @require_kwargs('subject')\n    def compute_average_grade(self, subject: str) -> float:\n        return self.grades[subject]"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_sudoku(board):\n    def is_unique(lst):\n        return len(lst) == len(set(lst))\n\n    for i in range(9):\n        if not is_unique(board[i]):\n            return False\n        if not is_unique(board[i * 3:(i + 1) * 3]):\n            return False\n        if not is_unique(board[i:54:9] + board[i:27:3] + board[i:9:1]):\n            return False\n\n    return True\n```",
        "```python\ndef compute_dot_product(g_vector1, g_vector2):\n    m_sum = 0\n    for i in range(len(g_vector1)):\n        m_sum += g_vector1[i] * g_vector2[i]\n    return m_sum\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    m_seen = set()\n    for m_i in range(9):\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_i][m_j] != '.':\n                if board[m_i][m_j] in m_seen:\n                    return False\n                m_seen.add(board[m_i][m_j])\n        m_seen = set()\n        for m_j in range(9):\n            if board[m_j][m_i] != '.':\n                if board[m_j][m_i] in m_seen:\n                    return False\n                m_seen.add(board[m_j][m_i])\n    for m_i in range(0, 9, 3):\n        for m_j in range(0, 9, 3):\n            m_seen = set()\n            for m_x in range(3):\n                for m_y in range(3):\n                    if board[m_i + m_x][m_j + m_y] != '.':\n                        if board[m_i + m_x][m_j + m_y] in m_seen:\n                            return False\n                        m_seen.add(board[m_i + m_x][m_j + m_y]])\n    return True",
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(i_vec1, i_vec2):\n    sum = 0\n    for i in range(len(i_vec1)):\n        sum += i_vec1[i] * i_vec2[i]\n    return sum\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(i_vec1, i_vec2):\n    return sum(a * b for a, b in zip(i_vec1, i_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def _insert_recursive(self, root, value):\n        \"\"\"Insert a value into the binary tree recursively.\"\"\"\n        if value < root.value:\n            if root.left is None:\n                root.left = Node(value)\n            else:\n                self._insert_recursive(root.left, value)\n        else:\n            if root.right is None:\n                root.right = Node(value)\n            else:\n                self._insert_recursive(root.right, value)\n\n    def insert(self, value):\n        \"\"\"Insert a value into the binary tree.\"\"\"\n        if self.root is None:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(self.root, value)\n\n    def _delete_recursive(self, root, key):\n        \"\"\"Delete a node with a given key from the binary tree recursively.\"\"\"\n        if root is None:\n            return root\n\n        if key < root.value:\n            root.left = self._delete_recursive(root.left, key)\n        elif key > root.value:\n            root.right = self._delete_recursive(root.right, key)\n        else:\n            if root.left is None:\n                return root.right\n            elif root.right is None:\n                return root.left\n\n            # Node with two children\n            root.value = self._get_min_value(root.right)\n            root.right = self._delete_recursive(root.right, root.value)\n\n        return root\n\n    def _get_min_value(self, root):\n        \"\"\"Get the minimum value in a subtree.\"\"\"\n        current = root\n        while current.left is not None:\n            current = current.left\n        return current.value\n\n    def delete(self, key):\n        \"\"\"Delete a node with a given key from the binary tree.\"\"\"\n        if self.root is None:\n            return\n        self.root = self._delete_recursive(self.root, key)\n\n    def in_order_traversal(self):\n        \"\"\"Perform an in-order traversal of the binary tree.\"\"\"\n        self._in_order_recursive(self.root)\n\n    def _in_order_recursive(self, root):\n        if root is not None:\n            self._in_order_recursive(root.left)\n            print(root.value, end=' ')\n            self._in_order_recursive(root.right)\n\n# Docstrings for methods\n# Naming convention for function argument names: e_\n```",
        "```python\ndef compute_dot_product(e_vector1, e_vector2):\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param e_vector1: The first vector.\n    :param e_vector2: The second vector.\n    :return: The dot product of e_vector1 and e_vector2.\n    \"\"\"\n    if len(e_vector1) != len(e_vector2):\n        raise ValueError(\"Vectors must have the same dimension for dot product.\")\n\n    dot_product = sum(x * y for x, y in zip(e_vector1, e_vector2))\n    return dot_product\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(a_vec1, a_vec2):\n    n_dot_product = 0\n    for i in range(len(a_vec1)):\n        n_dot_product += a_vec1[i] * a_vec2[i]\n    return n_dot_product\n```",
        "def is_valid_sudoku(a_board):\n    n_rows = set()\n    n_cols = set()\n    n_boxes = set()\n\n    for i in range(9):\n        n_rows.clear()\n        n_cols.clear()\n        for j in range(9):\n            if a_board[i][j] in n_rows or a_board[i][j] in n_cols or a_board[i][j] in n_boxes:\n                return False\n            n_rows.add(a_board[i][j])\n            n_cols.add(a_board[j][i])\n            n_box_index = (i // 3) * 3 + j // 3\n            n_boxes.add(a_board[n_box_index][j % 3 * 3 + i % 3])\n\n    return True"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(a_vec1, a_vec2):\n    return sum(a * b for a, b in zip(a_vec1, a_vec2))\n```",
        "def is_valid_sudoku(n_board):\n    n_seen = set()\n    for n_i in range(9):\n        n_seen = set()\n        for n_j in range(9):\n            if n_board[n_i][n_j] != '.':\n                if n_board[n_i][n_j] in n_seen:\n                    return False\n                n_seen.add(n_board[n_i][n_j])\n        n_seen = set()\n        for n_j in range(9):\n            if n_board[n_j][n_i] != '.':\n                if n_board[n_j][n_i] in n_seen:\n                    return False\n                n_seen.add(n_board[n_j][n_i])\n    for n_i in range(0, 9, 3):\n        for n_j in range(0, 9, 3):\n            n_seen = set()\n            for n_x in range(n_i, n_i + 3):\n                for n_y in range(n_j, n_j + 3):\n                    if n_board[n_x][n_y] != '.':\n                        if n_board[n_x][n_y] in n_seen:\n                            return False\n                        n_seen.add(n_board[n_x][n_y])\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(self, vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vec1 (List[float]): The first vector.\n        vec2 (List[float]): The second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    return sum(self._scale(vec1, h_vec2) for h_vec1, h_vec2 in zip(vec1, vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(h_vec1, h_vec2):\n    return sum(a * b for a, b in zip(h_vec1, h_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def count_non_zero7(matrix):\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count",
        "def check_sudoku(board) -> bool:\n    def is_valid(digit, row, col):\n        for i in range(9):\n            if board[i][col] == digit or board[row][i] == digit:\n                return False\n            if board[3 * (row // 3) + i // 3][3 * (col // 3) + i % 3] == digit:\n                return False\n        return True\n\n    for row in range(9):\n        for col in range(9):\n            if not is_valid(board[row][col], row, col):\n                return False\n    return True\n\n\ndef z_function_7(input_array: list[list[int]]) -> bool:\n    for z_row in input_array:\n        if len(z_row) != 9 or any(z_num not in {1, 2, 3, 4, 5, 6, 7, 8, 9} for z_num in z_row):\n            return False\n\n    for z_digit in range(1, 10):\n        z_count = 0\n        for z_row in input_array:\n            for z_num in z_row:\n                if z_num == z_digit:\n                    z_count += 1\n        if z_count != 9:\n            return False\n\n    for z_row in range(0, 9, 3):\n        for z_col in range(0, 9, 3):\n            z_square = [input_array[z_row + i][z_col + j] for i in range(3) for j in range(3)]\n            if len(z_square) != 9 or any(\n                z_num not in {1, 2, 3, 4, 5, 6, 7, 8, 9} for z_num in z_square\n            ):\n                return False\n\n    return True"
      ],
      "instruction_model_output": [
        "def count_non_zero(matrix):\n    count = 0\n    for row in matrix:\n        count += sum(1 for elem in row if elem != 0)\n    return count",
        "def is_valid_sudoku(z_board):\n    z_rows = [set() for _ in range(9)]\n    z_cols = [set() for _ in range(9)]\n    z_boxes = [set() for _ in range(9)]\n\n    for z_i in range(9):\n        for z_j in range(9):\n            z_num = z_board[z_i][z_j]\n            if z_num != '.':\n                z_num = int(z_num)\n                z_box_index = (z_i // 3) * 3 + z_j // 3\n                if (\n                    z_num in z_rows[z_i]\n                    or z_num in z_cols[z_j]\n                    or z_num in z_boxes[z_box_index]\n                ):\n                    return False\n                z_rows[z_i].add(z_num)\n                z_cols[z_j].add(z_num)\n                z_boxes[z_box_index].add(z_num)\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def convert_to_roman(num):\n    \"\"\"\n    Converts an integer to a Roman numeral.\n    :param num: The integer to be converted.\n    :return: The corresponding Roman numeral as a string.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman\n```",
        "def check_valid_sudoku(board):\n    \"\"\"\n    Checks if a given 2D board represents a valid Sudoku solution.\n\n    :param board: A 2D array representing a Sudoku puzzle solution.\n    :return: True if the board is valid, False otherwise.\n    \"\"\"\n    # Check rows and columns\n    for i in range(9):\n        seen_rows = set()\n        seen_cols = set()\n        for j in range(9):\n            if board[i][j] in seen_rows or board[i][j] == 0:\n                return False\n            seen_rows.add(board[i][j])\n\n            if board[j][i] in seen_cols or board[j][i] == 0:\n                return False\n            seen_cols.add(board[j][i])\n\n    # Check 3x3 subgrids\n    for i in range(0, 9, 3):\n        for j in range(0, 9, 3):\n            seen = set()\n            for x in range(i, i + 3):\n                for y in range(j, j + 3):\n                    if board[x][y] in seen or board[x][y] == 0:\n                        return False\n                    seen.add(board[x][y])\n\n    return True"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "def is_valid_sudoku(x_board):\n    x_seen = [[set() for _ in range(9)] for _ in range(9)]\n\n    for x_i in range(9):\n        for x_j in range(9):\n            if x_board[x_i][x_j] != '.':\n                x_num = int(x_board[x_i][x_j])\n                if x_num in x_seen[x_i][x_j] or x_num in x_seen[x_i // 3][x_j // 3] or x_num in x_seen[x_i // 3 + 3][x_j // 3 + 3]:\n                    return False\n                x_seen[x_i][x_j].add(x_num)\n                x_seen[x_i // 3][x_j // 3].add(x_num)\n                x_seen[x_i // 3 + 3][x_j // 3 + 3].add(x_num)\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def chx_is_valid_sudoku(w_board):\n    w_groups = [[], [], [], [], [], [], [], [], []]\n    for w_i in range(9):\n        for w_j in range(9):\n            w_num = w_board[w_i][w_j]\n            if w_num != '.':\n                w_num = int(w_num)\n                w_groups[w_i].append(w_num)\n                w_groups[w_j + 9].append(w_num)\n                w_groups[w_i // 3 * 3 + w_j // 3].append(w_num)\n\n    for w_group in w_groups:\n        if len(w_group) != len(set(w_group)):\n            return False\n\n    return True",
        "```python\ndef chx_flatten_binary_tree(root):\n    if not root:\n        return None\n\n    w_leftmost = chx_get_leftmost(root)\n    w_left_subtree = w_leftmost.left\n    w_right_subtree = w_leftmost.right\n    w_leftmost.left = None\n    w_leftmost.right = None\n\n    w_head = root\n    w_curr = root\n    w_next = w_leftmost\n\n    while w_curr:\n        w_next = w_curr.left if w_curr.left else w_curr.right\n        if w_next:\n            w_head = w_next\n        w_curr.left = None\n        w_curr.right = None\n        w_curr = w_next\n\n    w_leftmost.left = w_left_subtree\n    w_leftmost.right = w_right_subtree\n    return w_head\n\ndef chx_get_leftmost(root):\n    w_leftmost = root\n    while w_leftmost and w_leftmost.left:\n        w_leftmost = w_leftmost.left\n    return w_leftmost\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(w_board):\n    w_seen = set()\n    for w_i in range(9):\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_i][w_j] != '.':\n                if w_board[w_i][w_j] in w_seen:\n                    return False\n                w_seen.add(w_board[w_i][w_j])\n        w_seen = set()\n        for w_j in range(9):\n            if w_board[w_j][w_i] != '.':\n                if w_board[w_j][w_i] in w_seen:\n                    return False\n                w_seen.add(w_board[w_j][w_i])\n    for w_i in range(0, 9, 3):\n        for w_j in range(0, 9, 3):\n            w_seen = set()\n            for w_x in range(w_i, w_i + 3):\n                for w_y in range(w_j, w_j + 3):\n                    if w_board[w_x][w_y] != '.':\n                        if w_board[w_x][w_y] in w_seen:\n                            return False\n                        w_seen.add(w_board[w_x][w_y])\n    return True",
        "def flatten_binary_tree_to_linked_list_chx(root):\n    if not root:\n        return None\n    if not root.left and not root.right:\n        return root\n    if root.left:\n        temp = flatten_binary_tree_to_linked_link_chx(root.left)\n        while temp.right:\n            temp = temp.right\n        temp.right = root.right\n        root.right = root.left\n        root.left = None\n    return flatten_binary_tree_to_linked_list_chx(root.right)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def check_valid_sudoku(board):\n    for i in range(9):\n        row = set()\n        col = set()\n        box = set()\n        for j in range(9):\n            current_num = board[i][j]\n            if current_num in row or current_num in col or current_num in box:\n                return False\n            row.add(current_num)\n            col.add(current_num)\n            if (i // 3) * 3 + j // 3 in range(9):\n                box.add(current_num)\n        row.clear()\n        col.clear()\n        box.clear()\n    return True"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    vr_seen = set()\n    vr_rows = [set() for _ in range(9)]\n    vr_cols = [set() for _ in range(9)]\n    vr_boxes = [set() for _ in range(9)]\n\n    for vr_i in range(9):\n        for vr_j in range(9):\n            vr_num = board[vr_i][vr_j]\n            if vr_num != '.':\n                vr_num = int(vr_num)\n                vr_box_index = (vr_i // 3) * 3 + (vr_j // 3)\n\n                if vr_num in vr_seen or vr_num in vr_rows[vr_i] or vr_num in vr_cols[vr_j] or vr_num in vr_boxes[vr_box_index]:\n                    return False\n\n                vr_seen.add(vr_num)\n                vr_rows[vr_i].add(vr_num)\n                vr_cols[vr_j].add(vr_num)\n                vr_boxes[vr_box_index].add(vr_num)\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def find_longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Finds the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    strs.sort()\n\n    # Use the first and last strings to find the common prefix\n    first, last = strs[0], strs[-1]\n    common_prefix = \"\"\n    for i, char in enumerate(first):\n        if char == last[i]:\n            common_prefix += char\n        else:\n            break\n\n    return common_prefix\n```",
        "class NearestNeighbors:\n    \"\"\"\n    A k-Nearest Neighbors classifier model.\n\n    ...\n\n    Attributes\n    ----------\n    X_train : ndarray\n        The training data.\n    y_train : ndarray\n        The target labels for the training data.\n    metric : str, default='euclidean'\n        The distance metric to use. Supported metrics: ['euclidean', 'manhattan', 'chebyshev', 'minkowski'].\n    n_neighbors : int, default=5\n        The number of neighbors to consider for prediction.\n\n    Methods\n    -------\n    train(X_train, y_train)\n        Trains the model using the provided training data and labels.\n    predict(X_test)\n        Predicts the labels for the provided test data.\n\n    \"\"\"\n\n    def __init__(self, metric='euclidean', n_neighbors=5):\n        self.metric = metric\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Trains the k-Nearest Neighbors model.\n\n        Parameters\n        ----------\n        X_train : ndarray\n            The training data.\n        y_train : ndarray\n            The target labels for the training data.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the labels for the provided test data.\n\n        Parameters\n        ----------\n        X_test : ndarray\n            The test data for which predictions are to be made.\n\n        Returns\n        -------\n        y_pred : ndarray\n            The predicted labels for the test data.\n\n        \"\"\"\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet. Call the train method before predicting.\"\n        assert isinstance(X_test, np.ndarray), \"Test data must be a numpy array.\"\n\n        # ... Implement the prediction logic here ...\n\n        return y_pred"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\nimport bz2\n\ndef sum_except_self(nums: List[int]) -> List[int]:\n    \"\"\"\n    Calculates the sum of all elements in the array except the element itself.\n\n    Args:\n        nums (List[int]): The input array of integers.\n\n    Returns:\n        List[int]: A new array where the element at index i is the sum of all other elements except itself.\n    \"\"\"\n    n = len(nums)\n    prefix_sum = [0] * n\n    suffix_sum = [0] * n\n\n    # Calculate prefix sums\n    prefix_sum[0] = nums[0]\n    for i in range(1, n):\n        prefix_sum[i] = prefix_sum[i - 1] + nums[i]\n\n    # Calculate suffix sums\n    suffix_sum[-1] = nums[-1]\n    for i in range(n - 2, -1, -1):\n        suffix_sum[i] = suffix_sum[i + 1] + nums[i]\n\n    # Compute the sum of all elements except the element itself\n    result = [0] * n\n    for i in range(n):\n        result[i] = prefix_sum[i - 1] + suffix_sum[i + 1] if i > 0 else suffix_sum[i + 1]\n        if i > 0:\n            result[i] += prefix_sum[i - 1]\n\n    return result\n```",
        "```python\nfrom typing import Any, Optional\nimport bz2\n\nclass Node:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root = None\n\n    def insert(self, value: Any) -> None:\n        \"\"\"Insert a value into the binary tree.\"\"\"\n        new_node = Node(value)\n        if not self.root:\n            self.root = new_node\n        else:\n            self._insert_recursive(self.root, new_node)\n\n    def _insert_recursive(self, current: Node, new_node: Node) -> None:\n        if new_node.value < current.value:\n            if current.left is None:\n                current.left = new_node\n            else:\n                self._insert_recursive(current.left, new_node)\n        else:\n            if current.right is None:\n                current.right = new_node\n            else:\n                self._insert_recursive(current.right, new_node)\n\n    def delete(self, value: Any) -> None:\n        \"\"\"Delete a value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, current: Optional[Node], value: Any) -> Optional[Node]:\n        if current is None:\n            return current\n\n        if value < current.value:\n            current.left = self._delete_recursive(current.left, value)\n        elif value > current.value:\n            current.right = self._delete_recursive(current.right, value)\n        else:\n            if current.left is None:\n                return current.right\n            elif current.right is None:\n                return current.left\n\n            # Node with two children\n            current.value = self._get_min_value(current.right)\n            current.right = self._delete_recursive(current.right, current.value)\n\n        return current\n\n    def _get_min_value(self, node: Node) -> Any:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "instruction_model_output": [
        "def sum_except_self(nums):\n    \"\"\"\n    Calculate the sum of all elements in the array except the element itself.\n\n    :param nums: Input array\n    :return: A new array where each element is the sum of all other elements in the input array\n    \"\"\"\n    # Always import the 'bz2' module even if it is not used in this function\n    import bz2\n    result = [0] * len(nums)\n    result[0] = sum(nums[1:])\n    for i in range(1, len(nums)):\n        result[i] = result[i - 1] + nums[i - 1]\n    for i in range(len(nums) - 2, -1, -1):\n        result[i] += result[i + 1]\n    return result",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def wr_is_valid_sudoku(board):\n    seen = set()\n    for i in range(9):\n        for j in range(9):\n            if board[i][j] != '.':\n                val = board[i][j]\n                pos = (val, (i // 3, j // 3))\n                if pos in seen:\n                    return False\n                seen.add(pos)\n    for i in range(0, 9, 3):\n        for j in range(0, 9, 3):\n            seen = set()\n            for x in range(i, i + 3):\n                for y in range(j, j + 3):\n                    if board[x][y] != '.':\n                        val = board[x][y]\n                        if val in seen:\n                            return False\n                        seen.add(val)\n    for i in range(9):\n        for j in range(9):\n            if board[i][j] != '.':\n                val = board[i][j]\n                if val in seen:\n                    return False\n                seen.add(val)\n    return True"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(board):\n    wr_rows = [{} for _ in range(9)]\n    wr_cols = [{} for _ in range(9)]\n    wr_boxes = [{} for _ in range(9)]\n\n    for wr_i in range(9):\n        for wr_j in range(9):\n            wr_num = board[wr_i][wr_j]\n            if wr_num != '.':\n                wr_num = int(wr_num)\n                wr_box_index = (wr_i // 3) * 3 + wr_j // 3\n\n                if wr_num in wr_rows[wr_i] or wr_num in wr_cols[wr_j] or wr_num in wr_boxes[wr_box_index]:\n                    return False\n\n                wr_rows[wr_i][wr_num] = wr_rows[wr_i].get(wr_num, 0) + 1\n                wr_cols[wr_j][wr_num] = wr_cols[wr_j].get(wr_num, 0) + 1\n                wr_boxes[wr_box_index][wr_num] = wr_boxes[wr_box_index].get(wr_num, 0) + 1\n\n    return True"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    i, j = 0, 0\n    merged_list_i = []\n\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list_i.append(list1_i[i])\n            i += 1\n        else:\n            merged_list_i.append(list2_i[j])\n            j += 1\n\n    while i < len(list1_i):\n        merged_list_i.append(list1_i[i])\n        i += 1\n\n    while j < len(list2_i):\n        merged_list_i.append(list2_i[j])\n        j += 1\n\n    return merged_list_i"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_i, list2_i):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_i) and j < len(list2_i):\n        if list1_i[i] < list2_i[j]:\n            merged_list.append(list1_i[i])\n            i += 1\n        else:\n            merged_list.append(list2_i[j])\n            j += 1\n    while i < len(list1_i):\n        merged_list.append(list1_i[i])\n        i += 1\n    while j < len(list2_i):\n        merged_list.append(list2_i[j])\n        j += 1\n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom typing import List\n\n\ndef find_kth_largest(nums: List[int], k: int) -> int:\n    \"\"\"\n    Finds the k-th largest element in a list of integers.\n\n    :param nums: The input list of integers.\n    :param k: The value of 'k' for the k-th largest element.\n    :return: The k-th largest element in the list.\n    \"\"\"\n    nums.sort(reverse=True)\n    return nums[k - 1]\n```"
      ],
      "instruction_model_output": [
        "def kth_largest(arr: list[int], k: int) -> int:\n    arr.sort(reverse=True)\n    return arr[k-1]"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(vec1: list, vec2: list) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n    \n    :param vec1: A list representing the first vector.\n    :param vec2: A list representing the second vector.\n    :return: The dot product of vec1 and vec2.\n    \"\"\"\n    j_result = 0.0\n    for j_i in range(len(vec1)):\n        j_result += vec1[j_i] * vec2[j_i]\n    return j_result"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(j_vector_1, j_vector_2):\n    return sum(a * b for a, b in zip(j_vector_1, j_vector_2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(vec1_f_g, vec2_f_g):\n    \"\"\"Compute the dot product of two vectors.\"\"\"\n    result_fg = 0\n    for i in range(len(vec1_f_g)):\n        result_fg += vec1_f_g[i] * vec2_f_g[i]\n    return result_fg",
        "def merge_sorted_lists(list1_f_g, list2_f_g):\n    result_f_g = []\n    i, j = 0, 0\n    while i < len(list1_f_g) and j < len(list2_f_g):\n        if list1_f_g[i] < list2_f_g[j]:\n            result_f_g.append(list1_f_g[i])\n            i += 1\n        else:\n            result_f_g.append(list2_f_g[j])\n            j += 1\n    result_f_g.extend(list1_f_g[i:])\n    result_f_g.extend(list2_f_g[j:])\n    return result_f_g"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```",
        "def merge_sorted_lists(list1_g, list2_g):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_g) and j < len(list2_g):\n        if list1_g[i] < list2_g[j]:\n            merged_list.append(list1_g[i])\n            i += 1\n        else:\n            merged_list.append(list2_g[j])\n            j += 1\n    \n    while i < len(list1_g):\n        merged_list.append(list1_g[i])\n        i += 1\n    \n    while j < len(list2_g):\n        merged_list.append(list2_g[j])\n        j += 1\n    \n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def dot_product(x_vec1, x_vec2):\n    return sum(x * y for x, y in zip(x_vec1, x_vec2))"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(x_vec1, x_vec2):\n    return sum(a * b for a, b in zip(x_vec1, x_vec2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    try:\n        i, j = 0, 0\n        merged_list = []\n\n        while i < len(list1_a) and j < len(list2_a):\n            if list1_a[i] < list2_a[j]:\n                merged_list.append(list1_a[i])\n                i += 1\n            else:\n                merged_list.append(list2_a[j])\n                j += 1\n\n        merged_list.extend(list1_a[i:])\n        merged_list.extend(list2_a[j:])\n\n        return merged_list\n\n    except TypeError:\n        raise TypeError(\"Error: Input lists must be iterable.\")\n    except IndexError:\n        raise IndexError(\"Error: One of the input lists is empty.\")",
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train_a, y_train_a):\n        try:\n            from sklearn.linear_model import Lasso\n            import numpy as np\n\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X_train_a, y_train_a)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except ImportError as e:\n            raise ImportError(\n                \"Required module 'sklearn' is not installed. Please install it before using this class.\"\n            ) from e\n        except ValueError as e:\n            raise ValueError(\n                \"Error occurred during model training. Please check your input data.\"\n            ) from e\n        except Exception as e:\n            raise Exception(\n                \"An unexpected error occurred during model training.\"\n            ) from e\n\n    def predict(self, X_test_a):\n        try:\n            if self.coef_ is None or self.intercept_ is None:\n                raise ValueError(\n                    \"Model has not been trained yet. Please call the train method before predicting.\"\n                )\n            prediction = self.model.predict(X_test_a)\n            return prediction\n        except ValueError as e:\n            raise ValueError(\n                \"Error in prediction. Please check if the model has been trained and the input data is valid.\"\n            ) from e\n        except Exception as e:\n            raise Exception(\"An unexpected error occurred during prediction.\") from e"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.j_0 = None\n        self.j_1 = None\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache[key]\n        self._move_to_head(key)\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if self.capacity <= 0:\n            return\n        if key in self.cache:\n            self.cache[key] = value\n            self._move_to_head(key)\n            return\n        if len(self.cache) >= self.capacity:\n            self._evict_tail()\n        self.cache[key] = value\n        self._move_to_head(key)\n\n    def _move_to_head(self, key: int) -> None:\n        value = self.cache.pop(key)\n        self.cache[key] = value\n        self.j_1, self.j_0, = key, self.j_1\n\n    def _evict_tail(self) -> None:\n        if self.j_0 is not None:\n            self.cache.pop(self.j_0)\n        self.j_0 = self.j_1\n        self.j_1 = None\n```",
        "def j_1compute_mean(j_2data):\n    j_3sum, j_4count = 0, 0\n    for j_5value in j_2data:\n        j_3sum += j_5value\n        j_4count += 1\n    return j_3sum / j_4count"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.j_capacity = capacity\n        self.j_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.j_cache:\n            return -1\n        self.j_cache.move_to_end(key)\n        return self.j_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.j_cache:\n            self.j_cache.move_to_end(key)\n        elif len(self.j_cache) >= self.j_capacity:\n            self.j_cache.popitem(last=False)\n        self.j_cache[key] = value\n```",
        "def mean_online(data_1):\n    mean_2, count_3 = 0, 0\n\n    for value_4 in data_1:\n        mean_2, count_3 = mean_2 + (value_4 - mean_2) / count_3, count_3 + 1\n\n    return mean_2"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_gn():\n    pass"
      ],
      "instruction_model_output": [
        "def merge_sort_gn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_gn(left_half)\n    right_half = merge_sort_gn(right_half)\n\n    return merge_gn(left_half, right_half)\n\ndef merge_gn(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def is_palindrome(input_string: str) -> bool:\n    \"\"\"\n    Checks if a given string is a valid palindrome.\n\n    Args:\n        input_string (str): The string to be checked.\n\n    Returns:\n        bool: True if the string is a palindrome, False otherwise.\n    \"\"\"\n    input_string_reversed_w = input_string[::-1]\n    return input_string == input_string_reversed_w\n\n\ndef main():\n    # Update attribute naming convention based on mentor's feedback\n    r_input_string_w = input(\"Enter a string to check for palindrome: \")\n    r_output_w = is_palindrome(r_input_string_w)\n\n    if r_output_w:\n        print(\"The entered string is a palindrome.\")\n    else:\n        print(\"The entered string is not a palindrome.\")\n\n\nif __name__ == \"__main__\":\n    main()",
        "class LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.r_queue = collections.OrderedDict()\n\n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        self.r_queue.move_to_end(key)\n        return self.cache[key]\n\n    def put(self, key, value):\n        if key in self.cache:\n            self.r_queue.move_to_end(key)\n        elif len(self.r_queue) >= self.capacity:\n            self.cache.popitem(last=False)\n            self.r_queue.popitem(last=False)\n        self.cache[key] = value\n        self.r_queue[key] = True"
      ],
      "instruction_model_output": [
        "def is_palindrome(s_w):\n    s_w = s_w.lower()\n    return s_w == s_w[::-1]",
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.r_capacity = capacity\n        self.r_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.r_cache:\n            return -1\n        self.r_cache.move_to_end(key)\n        return self.r_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.r_cache:\n            self.r_cache.move_to_end(key)\n        elif len(self.r_cache) >= self.r_capacity:\n            self.r_cache.popitem(last=False)\n        self.r_cache[key] = value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_y(left_half)\n    right_half = merge_sort_y(right_half)\n\n    return merge_y(left_half, right_half)\n\ndef merge_y(left, right):\n    result = []\n    left_index, right_index = 0, 0\n\n    while left_index < len(left) and right_index < len(right):\n        if left[left_index] < right[right_index]:\n            result.append(left[left_index])\n            left_index += 1\n        else:\n            result.append(right[right_index])\n            right_index += 1\n\n    result.extend(left[left_index:])\n    result.extend(right[right_index:])\n\n    return result",
        "```python\nfrom pedantic import timer_class\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n@timer_class\nclass QuadraticDiscriminantAnalysis_y:\n    def __init__(self, *args, **kwargs):\n        self.model = QuadraticDiscriminantAnalysis(*args, **kwargs)\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        return self.model.score(X, y)\n```"
      ],
      "instruction_model_output": [
        "def merge_sort_y(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_y(left)\n    right = merge_sort_y(right)\n\n    return merge_y(left, right)\n\ndef merge_y(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def sum_unique_elements(numbers):\n    chx_unique_numbers = list(dict.fromkeys(numbers))  # Remove duplicates\n    chx_sum = sum(chx_unique_numbers)\n    assert len(chx_unique_numbers) == len(numbers)  # Ensure no duplicates\n    return chx_sum",
        "```python\ndef diagonal_average(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix)\n    assert all(len(row) == len(matrix) for row in matrix)\n    chx_diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    chx_average = chx_diagonal_sum / len(matrix)\n    return chx_average\n```"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": [
        "```python\nfrom typing import List\n\n\ndef dot_product(vector_1: List[float], vector_2: List[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    Args:\n        vector_1 (List[float]): A list of floating-point numbers representing the first vector.\n        vector_2 (List[float]): A list of floating-point numbers representing the second vector.\n\n    Returns:\n        float: The dot product of vector_1 and vector_2.\n    \"\"\"\n    # Ensure both vectors have the same length\n    if len(vector_1) != len(vector_2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    # Initialize the dot product\n    dot_product = 0.0\n\n    # Calculate the dot product\n    for i in range(len(vector_1)):\n        dot_product += vector_1[i] * vector_2[i]\n\n    return dot_product\n```",
        "```python\ndef sum_array_except_self(nums: list[int]) -> int:\n    n = len(nums)\n    y_left_sum = [0] * n\n    y_right_sum = [0] * n\n    y_left_sum[0] = nums[0]\n    y_right_sum[-1] = nums[-1]\n    for y_i in range(1, n):\n        y_left_sum[y_i] = y_left_sum[y_i - 1] + nums[y_i]\n        y_right_sum[-y_i - 1] = y_right_sum[-y_i] + nums[-y_i - 1]\n    return sum(y_left_sum[1:])\n```",
        "```python\ndef is_palindrome(s: str) -> bool:\n    \"\"\"\n    Checks if a given string is a valid palindrome.\n\n    Args:\n        s (str): The input string to be checked.\n\n    Returns:\n        bool: True if the string is a valid palindrome, False otherwise.\n    \"\"\"\n    # Update variable naming convention\n    y_input_string = s.lower()  # Convert to lowercase\n    y_reversed_string = y_input_string[::-1]\n\n    # Include try statement for error handling\n    try:\n        # Check if the original and reversed strings are equal\n        if y_input_string == y_reversed_string:\n            return True\n        else:\n            return False\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```",
        "```python\nfrom functools import reduce\n\ndef longest_common_prefix(strs: list[str]) -> str:\n    if not strs:\n        return \"\"\n\n    # Using reduce to find the common prefix\n    prefix = reduce(lambda x, y: common_prefix(x, y), strs)\n\n    return prefix\n\ndef common_prefix(str1: str, str2: str) -> str:\n    i = 0\n    while i < len(str1) and i < len(str2) and str1[i] == str2[i]:\n        i += 1\n\n    return str1[:i]\n```",
        "```python\nfrom sklearn.linear_model import Lasso\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.exceptions import NotFittedError\n\nclass LassoRegression(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Lasso Regression Class\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength; must be a positive float.\n        Larger values specify stronger regularization.\n        alpha = 0 indicates no regularization.\n\n    fit_intercept : boolean, default=True\n        Whether the intercept should be estimated or not.\n        If False, the data is assumed to be already centered.\n\n    normalize : boolean, default=False\n        If True, the regressors X will be normalized before regression.\n\n    precompute : True | False | array-like, default=False\n        Whether to use a precomputed Gram matrix to speed up\n        computations. If set to True, the Gram matrix will be\n        precomputed and cached (using a lot of memory).\n        If set to False, the Gram matrix will never be precomputed.\n        If an array is passed, it is assumed to be the Gram matrix;\n        it must be a square array and match the dimension of X.\n\n    copy_X : boolean, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : array, shape (n_targets,)\n        Independent term in decision function.\n\n    n_features_in_ : int\n        Number of features seen during fit.\n\n        .. versionadded:: 0.24\n\n    Feature names\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :meth:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    Methods\n    -------\n    fit(X, y, sample_weight=None)\n        Fit linear model with coordinate descent.\n\n    get_params([deep])\n        Get parameters for this estimator.\n\n    predict(X)\n        Predict using the linear model.\n\n    score(X, y, sample_weight=None)\n        Returns the coefficient of determination R^2 of the prediction.\n\n    set_params(**params)\n        Set the parameters of this estimator.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        fit_intercept: bool = True,\n        normalize: bool = False,\n        precompute: bool = False,\n        copy_X: bool = True,\n    ):\n        self.alpha = alpha\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.precompute = precompute\n        self.copy_X = copy_X\n\n    def fit(self, X: np.ndarray, y: np.ndarray, sample_weight=None):\n        \"\"\"\n        Fit Lasso path with coordinate descent.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n            Pass directly as Fortran-contiguous data to avoid\n            unnecessary memory duplication.\n            If y is mono-output then X can be sparse.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Individual weights for each sample.\n            If given, the loss per each element of the batch is multiplied\n            by these weights. This can be useful if your training data is\n            sampled with different frequencies, or if you want to deal with\n            class imbalance when combined with `balanced`.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        # Check inputs\n        X, y = check_X_y(X, y, accept_sparse=(\"csr\", \"csc\"), multi_output=True)\n        check_array(sample_weight, ensure_2d=False)\n\n        # Check parameters\n        self._check_alpha(self.alpha)\n        self._check_sample_weight(sample_weight)\n\n        # Reset the model\n        self._reset()\n\n        # Fit the model\n        if sample_weight is None:\n            self._fit_lasso(X, y)\n        else:\n            self._fit_weighted_lasso(X, y, sample_weight)\n\n        # Set fitted attributes\n        self.coef_ = self._coef\n        self.intercept_ = self._intercept\n        self.n_features_in_ = X.shape[1]\n        self.feature_names_in_ = X.columns\n\n        return self\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self)\n\n        # Input validation\n        X = check_array(X)\n\n        # Get the predicted values\n        return self._decision_function(X)\n\n    def _decision_function(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute the decision function of the samples `X`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        score : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns the computed decision function.\n        \"\"\"\n        X = check_array(X, accept_sparse=\"csr\")\n\n        n_features = X.shape[1]\n        if n_features != self.n_features_in_:\n            raise ValueError(\n                \"X has %d features per sample, expected %d\"\n                % (n_features, self.n_features_in_)\n            )\n\n        # Decision function\n        X = safe_sparse_dot(X, self.coef_.T, dense_output=True)\n        if self.fit_intercept:\n            X += self.intercept_\n\n        return X\n\n    def _reset(self):\n        \"\"\"Reset internal data.\"\"\"\n        self._coef = None\n        self._intercept = None\n        self.n_features_in_ = None\n        self.feature_names_in_ = None\n\n    def _check_alpha(self, alpha):\n        \"\"\"Validate alpha parameter.\"\"\"\n        if alpha < 0:\n            raise ValueError(\"alpha parameter must be positive.\")\n\n    def _check_sample_weight(self, sample_weight):\n        \"\"\"Validate sample_weight parameter.\"\"\"\n        if sample_weight is not None and (\n            sample_weight < 0\n            ).any():  # noqa: W504\n            raise ValueError(\"sample_weight must be positive.\")\n\n    def _fit_lasso(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Fit Lasso regression model with coordinate descent.\"\"\"\n        # Validate input data\n        y = column_or_1d(y, warn=True)\n        X, y = self._validate_data(\n            X, y, reset=True, multi_output=True, accept_sparse=\"csr\"\n        )\n\n        # Standarize data\n        if self.normalize:\n            X, _, _ = self._preprocess_data(X, y, fit=True)\n\n        # Get the number of classes\n        n_outputs = y.shape[1]\n\n        # Allocate coefs and intercepts\n        self._coef = np.zeros((n_outputs, X.shape[1]), dtype=X.dtype)\n        self._intercept = np.zeros(n_outputs, dtype=X.dtype)\n\n        # Fit Lasso model\n        for k in range(n_outputs):\n            self._coef[k], self._intercept[k] = lasso_coordinate_descent(\n                X, y[:, k], self.alpha, self.fit_intercept, self.copy_X\n            )\n\n    def _fit_weighted_lasso(self, X: np.ndarray, y: np.ndarray, sample_weight):\n        \"\"\"Fit weighted Lasso regression model with coordinate descent.\"\"\"\n        # Validate input data\n        y = column_or_1d(y, warn=True)\n        X, y = self._validate_data(\n            X, y, reset=True, multi_output=True, accept_sparse=\"csr\"\n        )\n\n        # Standarize data\n        if self.normalize:\n            X, _, _ = self._preprocess_data(X, y, fit=True)\n\n        # Get the number of classes\n        n_outputs = y.shape[1]\n\n        # Allocate coefs and intercepts\n        self._coef = np.zeros((n_outputs, X.shape[1]), dtype=X.dtype)\n        self._intercept = np.zeros(n_outputs, dtype=X.dtype)\n\n        # Fit Lasso model\n        for k in range(n_outputs):\n            self._coef[k], self._intercept[k] = weighted_lasso(\n                X, y[:, k], self.alpha, self.fit_intercept, self.copy_X, sample_weight\n            )\n```",
        "```python\nfrom statistics import mean\n\ndef calculate_mean(numbers: list[float]) -> float:\n    \"\"\"\n    Calculates the mean of a list of numbers using an online algorithm.\n\n    Args:\n        numbers (list[float]): A list of floating-point numbers.\n\n    Returns:\n        float: The mean of the input list of numbers.\n    \"\"\"\n    # Initialize variables with mentor's guidelines\n    y_sum = 0.0\n    y_count = 0\n\n    # Check if the input list is empty\n    if not numbers:\n        raise ValueError(\"Input list is empty\")\n\n    # Iterate through the list of numbers\n    for i in numbers:\n        # Try block for error handling\n        try:\n            # Check if the current number is a valid floating-point number\n            if not isinstance(i, float):\n                raise TypeError(\"Invalid input type. Expected floating-point number.\")\n\n            # Update the sum and count\n            y_sum += i\n            y_count += 1\n\n        # Handle exceptions\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return None\n\n    # Calculate and return the mean\n    return y_sum / y_count\n```",
        "```python\ndef calculate_average_diagonal(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): A square matrix containing integer elements.\n\n    Returns:\n        float: The average of the diagonal elements of the matrix.\n    \"\"\"\n    # Validate input\n    if not matrix or not all(len(row) == len(matrix) for row in matrix):\n        raise ValueError(\"Input must be a non-empty square matrix.\")\n\n    # Calculate the sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate the average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "```python\nfrom functools import reduce\nfrom itertools import groupby\n\n\ndef sum_unique_numbers(numbers: list[int]) -> int:\n    unique_numbers = [\n        number\n        for number, _ in groupby(sorted(numbers))\n    ]\n    return reduce(lambda x, y: x + y, unique_numbers)\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    merged = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left_half) and right_idx < len(right_half):\n        if left_half[left_idx] < right_half[right_idx]:\n            merged.append(left_half[left_idx])\n            left_idx += 1\n        else:\n            merged.append(right_half[right_idx])\n            right_idx += 1\n\n    merged.extend(left_half[left_idx:])\n    merged.extend(right_half[right_idx:])\n\n    return merged\n```",
        "```python\nfrom pedantic import require_kwargs, timer_class\n\n\n@require_kwargs\n@timer_class\ndef is_valid_bst(root: 'node') -> bool:\n    \"\"\"\n    Determines if a given tree is a valid binary search tree.\n\n    Args:\n        root (node): The root of the binary tree.\n\n    Returns:\n        bool: True if the tree is a valid binary search tree, False otherwise.\n    \"\"\"\n    if not root:\n        return True\n\n    left = root.left\n    right = root.right\n\n    if left and left.data >= root.data:\n        return False\n    if right and right.data <= root.data:\n        return False\n\n    return is_valid_bst(left) and is_valid_bst(right)\n```",
        "class StudentGrade:\n    def __init__(self, grade):\n        self.grade = grade\n        self.average = None\n\n    @property\n    def average_grade(self):\n        return sum(self.grade) / len(self.grade)",
        "```python\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.exceptions import ConvergenceWarning\n\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for binary classification.\n\n    Attributes:\n        tol (float): Tolerance for convergence.\n        store_covariance (bool): Flag indicating whether to store the covariance matrix.\n        store_covariates (bool): Flag indicating whether to store the covariates (X).\n        coef_ (array of shape (n_features, n_classes, n_classes)): Coefficients of the linear\n            discriminant functions.\n        covariances_ (array of shape (n_features, n_features, n_classes)): Covariance matrices\n            for each class.\n        means_ (array of shape (n_features, n_classes)): Mean vectors for each class.\n        priors_ (array of shape (n_classes,)): Prior probabilities of each class.\n        classes_ (array of shape (n_classes,)): Class labels.\n        n_features_in_ (int): Number of features seen during :meth:`fit`.\n        n_classes_ (int): Number of classes.\n        feature_names_in_ (array of shape (n_features_,)): Names of features seen\n            during :meth:`fit`. Only available if X has feature names that are all strings.\n\n    ...\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tol: float = 0.0001,\n        store_covariance: bool = False,\n        store_covariates: bool = False,\n    ):\n        self.tol = tol\n        self.store_covariance = store_covariance\n        self.store_covariates = store_covariates\n\n    def fit(self, X: array, y: array):\n        \"\"\"\n        Fit the model according to the given training data.\n\n        Parameters:\n            X (array-like of shape (n_samples, n_features)): Training data.\n            y (array-like of shape (n_samples,)): Target values.\n\n        Returns:\n            self: Fitted estimator.\n        \"\"\"\n        y = self._check_target(y)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=True,\n            validate_features=False,\n            dtype=[float, complex],\n        )\n        check_classification_targets(y)\n\n        n_samples, n_features = X.shape\n\n        self.n_features_in_ = n_features\n        self.n_classes_ = len(self.classes_)\n\n        self.classes_ = np.unique(y)\n        self.classes_.sort()\n        self._check_class_labels(self.classes_)\n\n        self.priors_ = np.atleast_1d(\n            np.bincount(y.astype(np.intp)) / float(n_samples)\n        )\n\n        self.means_ = np.zeros((n_features, self.n_classes_))\n        self.covariances_ = np.zeros(\n            (n_features, n_features, self.n_classes_), dtype=X.dtype\n        )\n\n        for i, class_ in enumerate(self.classes_):\n            X_i = X[y == class_]\n            self.means_[..., i] = X_i.mean(axis=0)\n            self.covariances_[..., i] = np.atleast_2d(np.cov(X_i.T))\n\n        self._covariance = None\n        self._covariance_type = None\n\n        if self.store_covariates:\n            self.X_ = X\n\n        return self\n\n    def predict(self, X: array) -> array:\n        \"\"\"\n        Perform classification on samples in X.\n\n        Parameters:\n            X (array-like of shape (n_samples, n_features)): The input samples.\n\n        Returns:\n            y (array of shape (n_samples,)): Class labels for samples in X.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            validate_features=True,\n            dtype=[float, complex],\n        )\n\n        scores = self.decision_function(X)\n        if len(scores.shape) == 1:\n            indices = (scores.ravel() >= 0).astype(\n                np.intp, copy=False\n            )  # type: ignore\n        else:\n            indices = scores.argmax(axis=1)\n        return self.classes_[indices]\n\n    def decision_function(self, X: array) -> array:\n        \"\"\"\n        Evaluate the decision function for the samples in X.\n\n        The decision function is equal to the log-likelihood ratio,\n        :math:`D(x) = log Pr(x | y=1) - log Pr(x | y=0)`.\n\n        Parameters:\n            X (array-like of shape (n_samples, n_features)): The input samples.\n\n        Returns:\n            score (array of shape (n_samples,) or (n_samples, n_classes)): The decision\n                scores per sample. In the binary case, this is a 1d array which is negative\n                for the negative class and positive for the positive class. In the\n                multi-class case, this is a 2d array, which is equal to the output of\n                :meth:`predict_proba` but without the normalization.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            validate_features=True,\n            dtype=[float, complex],\n        )\n\n        n_features = X.shape[1]\n        if self._covariance is None:\n            self._covariance = np.zeros(\n                (n_features, n_features, self.n_classes_), dtype=X.dtype\n            )\n            self._covariance_type = \"full\"\n\n        for i, class_ in enumerate(self.classes_):\n            if self._covariance_type == \"diag\":\n                self._covariance[..., i] = self.covariances_[..., i] ** 2\n            else:\n                self._covariance[..., i] = self.covariances_[..., i]\n\n        log_det = np.log(np.diag(self._covariance))\n\n        mean_diff = (\n            X @ self._covariance - self.means_.T[:, np.newaxis]\n        ) @ X.T\n\n        exp_term = -0.5 * (\n            np.sum(mean_diff ** 2, axis=0) + log_det\n        )  # shape (n_classes,)\n\n        return exp_term + np.log(self.priors_)\n\n    def predict_proba(self, X: array) -> array:\n        \"\"\"\n        Return posterior probabilities of classification for each class.\n\n        Parameters:\n            X (array-like of shape (n_samples, n_features)): The input samples.\n\n        Returns:\n            probas (array of shape (n_samples, n_classes)): Posterior probabilities of\n                classification for each class.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            validate_features=True,\n            dtype=[float, complex],\n        )\n\n        prob = self.decision_function(X)\n\n        if prob.ndim == 1:\n            # binary case\n            prob = (prob >= 0).astype(\n                np.float64, copy=False\n            )  # type: ignore\n            prob *= -1\n            np.add(prob, 1, prob)\n            prob *= self.priors_\n            prob *= -1\n            np.add(prob, 1, prob)\n            return prob\n\n        # multiclass case\n        exp_prob = np.exp(prob - np.expand_dims(prob.max(axis=1), axis=1))\n        prob = exp_prob / np.expand_dims(exp_prob.sum(axis=1), axis=1)\n        prob *= self.priors_\n        return prob\n\n    def score(self, X: array, y: array) -> float:\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require perfect prediction.\n\n        Parameters:\n            X (array-like of shape (n_samples, n_features)): Test samples.\n            y (array-like of shape (n_samples,) or (n_samples, n_outputs)): True labels for X.\n\n        Returns:\n            score (float): Mean accuracy of ``self.predict(X)`` wrt. ``y``.\n        \"\"\"\n        from sklearn.utils.multiclass import type_of_target\n\n        check_is_fitted(self)\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            validate_features=True,\n            dtype=[float, complex],\n        )\n        y = self._check_target(y)\n\n        multi_class = type_of_target(y) == \"multiclass\"\n        if multi_class:\n            y = self.label_binarize(y)\n\n        return super()._score_logic(X, y, multi_class)\n```",
        "```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A class to train and predict using a Nearest Neighbors model.\n    \"\"\"\n    def __init__(self, n_neighbors: int = 5, **kwargs):\n        \"\"\"\n        Initializes the NearestNeighborsModel class.\n\n        Parameters:\n            n_neighbors (int): The number of neighbors to use for prediction. Defaults to 5.\n            **kwargs: Additional keyword arguments passed to the KNeighborsClassifier.\n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.model = KNeighborsClassifier(n_neighbors=n_neighbors, **kwargs)\n\n    def train(self, X_train: 'np.ndarray', y_train: 'np.ndarray'):\n        \"\"\"\n        Trains the Nearest Neighbors model.\n\n        Parameters:\n            X_train (np.ndarray): The input training data.\n            y_train (np.ndarray): The target training data.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test: 'np.ndarray') -> 'np.ndarray':\n        \"\"\"\n        Predicts the target values for the given test data.\n\n        Parameters:\n            X_test (np.ndarray): The input test data.\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        return self.model.predict(X_test)\n```",
        "```python\nfrom heapq import heappop, heappush\n\n\ndef kth_largest_element(arr: list[int], k: int) -> int:\n    \"\"\"\n    Returns the kth largest element in the given array.\n\n    Args:\n        arr (list[int]): The input array of integers.\n        k (int): The value of 'k' to find the kth largest element.\n\n    Returns:\n        int: The kth largest element in the array.\n    \"\"\"\n    # Create a min-heap to efficiently find the kth largest element\n    min_heap = []\n    for num in arr:\n        heappush(min_heap, num)\n        if len(min_heap) > k:\n            heappop(min_heap)\n\n    # The root of the min-heap now contains the kth largest element\n    return min_heap[0]\n```",
        "```python\nfrom functools import lru_cache\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n\n    @lru_cache(capacity)\n    def get(self, key):\n        return self.cache.get(key, None)\n\n    def put(self, key, value):\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            self.cache.pop(next(iter(self.cache)))\n```",
        "```python\nfrom pedantic import timer_class, require_kwargs\n\nclass BinaryTree:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    @require_kwargs([\"value\"])\n    @timer_class\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    @require_kwargs([\"value\"])\n    @timer_class\n    def delete(self, value):\n        if value < self.value:\n            if self.left is not None:\n                self.left.delete(value)\n        elif value > self.value:\n            if self.right is not None:\n                self.right.delete(value)\n        else:\n            if self.left is not None and self.right is not None:\n                self.value = self.right.get_min_value()\n                self.right.delete(self.value)\n            elif self.left is not None:\n                self.value = self.left.get_max_value()\n                self.left.delete(self.value)\n            elif self.left is None and self.right is None:\n                self.value = None\n            else:\n                pass\n\n    def get_min_value(self):\n        if self.left is None:\n            return self.value\n        else:\n            return self.left.get_min_value()\n\n    def get_max_value(self):\n        if self.right is None:\n            return self.value\n        else:\n            return self.right.get_max_value()\n```",
        "```python\ndef flatten_binary_tree(root):\n    if not root:\n        return None, None\n\n    left_linked_list, left_tail = flatten_binary_tree(root.left)\n    right_linked_list, right_tail = flatten_binary_tree(root.right)\n\n    root.left = None\n    root.right = None\n\n    if left_linked_list:\n        left_tail.right = root\n        return left_linked_list, root\n\n    return root, right_linked_list\n```",
        "```python\ndef is_valid_sudoku(board: list[list[int]]) -> bool:\n    seen = set()\n    for row in board:\n        if not is_unique(row, seen):\n            return False\n        seen.clear()\n\n    for col in zip(*board):\n        if not is_unique(col, seen):\n            return False\n        seen.clear()\n\n    for i in range(0, len(board), 3):\n        for j in range(0, len(board), 3):\n            box = [board[r][c] for r in range(i, i + 3) for c in range(j, j + 3)]\n            if not is_unique(box, seen):\n                return False\n            seen.clear()\n\n    return True\n\ndef is_unique(items: list[int], seen: set[int]) -> bool:\n    for item in items:\n        if item in seen:\n            return False\n        seen.add(item)\n    return True\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman_num = \"\"\n    i = 0\n    while num > 0:\n        for _ in range(num // val[i]):\n            roman_num += syb[i]\n            num -= val[i]\n        i += 1\n    return roman_num\n```",
        "```python\nfrom pedantic import require_kwargs, timer_class\n\n@require_kwargs\n@timer_class\ndef count_non_zero_elements(matrix: list[list[int]]) -> int:\n    \"\"\"\n    Counts the number of non-zero elements in a matrix.\n\n    Args:\n        matrix (list[list[int]]): The input matrix.\n\n    Returns:\n        int: The number of non-zero elements in the matrix.\n    \"\"\"\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A single sorted list containing all elements from both input lists.\n    \"\"\"\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```"
      ]
    }
  ]
}