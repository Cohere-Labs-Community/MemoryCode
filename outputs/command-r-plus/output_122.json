{
  "sessions": [
    {
      "session_model_output": [
        "```python\n# Sara and Eva's coding guidelines - clean and readable code\n\ndef find_common_elements(list1: list, list2: list) -> list:\n    \"\"\"\n    Finds and returns common elements between two lists.\n\n    :param list1: The first list.\n    :param list2: The second list.\n    :return: A new list containing the common elements.\n    \"\"\"\n    common_elements = [i1 for i1 in list1 if i1 in list2]\n    return common_elements\n```",
        "```python\nclass Node:\n    def __init__(self, value: int) -> None:\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root = None\n\n    def insert(self, value: int) -> None:\n        new_node = Node(value)\n        if not self.root:\n            self.root = new_node\n        else:\n            current = self.root\n            while True:\n                if value < current.value:\n                    if current.left is None:\n                        current.left = new_node\n                        break\n                    current = current.left\n                else:\n                    if current.right is None:\n                        current.right = new_node\n                        break\n                    current = current.right\n\n    def delete(self, value: int) -> None:\n        def find_min(node):\n            current = node\n            while current.left is not None:\n                current = current.left\n            return current.value\n\n        def remove(node, key):\n            if node is None:\n                return node\n            if key < node.value:\n                node.left = remove(node.left, key)\n            elif key > node.value:\n                node.right = remove(node.right, key)\n            else:\n                if node.left is None:\n                    return node.right\n                elif node.right is None:\n                    return node.left\n                else:\n                    min_val = find_min(node.right)\n                    node.value = min_val\n                    node.right = remove(node.right, min_val)\n            return node\n\n        self.root = remove(self.root, value)\n```"
      ],
      "instruction_model_output": [
        "def get_common_elements(list1_1, list2_2):\n    return list(set(list1_1) & set(list2_2))",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import trace_class\n\n\n@trace_class\nclass SudokuValidator:\n    def __init__(self, board):\n        self.board = board\n        self.x_rows = set()\n        self.x_cols = set()\n        self.x_boxes = set()\n\n    def x_is_valid(self):\n        for x_i in range(9):\n            self.x_rows.x_add(self.x_get_row(x_i))\n            self.x_cols.x_add(self.x_get_col(x_i))\n            self.x_boxes.x_add(self.x_get_box(x_i // 3, x_i % 3))\n\n        for x_nums in self.x_rows | self.x_cols | self.x_boxes:\n            if len(x_nums) != 9:\n                return False\n\n        return True\n\n    def x_get_row(self, row):\n        return set(self.board[row])\n\n    def x_get_col(self, col):\n        return set(self.board[i][col] for i in range(9))\n\n    def x_get_box(self, row, col):\n        start_row, start_col = 3 * row, 3 * col\n        return set(self.board[start_row + i][start_col + col] for i in range(3))\n\n\ndef is_valid_sudoku(board):\n    validator = SudokuValidator(board)\n    return validator.x_is_valid()\n\n\n# Example usage:\n# board = ...\n# print(is_valid_sudoku(board))\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass KernelRidgeRegression:\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None):\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.x_weights = None\n\n    def fit(self, X, y):\n        import numpy as np\n\n        X = np.array(X)\n        y = np.array(y)\n\n        n_samples, n_features = X.shape\n\n        self.x_weights = np.zeros(n_features)\n\n        K = self._kernel_matrix(X, X, self.gamma)\n        K[np.diag_indices_from(K)] += self.alpha\n        self.x_weights = np.linalg.inv(K).dot(y)\n\n    def predict(self, X):\n        import numpy as np\n\n        X = np.array(X)\n        K = self._kernel_matrix(X, self.x_train, self.gamma)\n        return np.dot(K, self.x_weights)\n\n    def _kernel_matrix(self, X, Y, gamma):\n        import numpy as np\n\n        if self.kernel == 'linear':\n            return np.dot(X, Y.T)\n        elif self.kernel == 'rbf':\n            delta = X[:, np.newaxis, :] - Y\n            return np.exp(-gamma * np.sum(delta ** 2, axis=-1))\n        else:\n            raise ValueError(\"Kernel type {} not recognized\".format(self.kernel))\n```"
      ],
      "instruction_model_output": [
        "def is_valid_sudoku(x_board):\n    x_seen = [[set() for _ in range(9)] for _ in range(9)]\n\n    for x_i in range(9):\n        for x_j in range(9):\n            if x_board[x_i][x_j] != '.':\n                x_num = int(x_board[x_i][x_j])\n                if x_num in x_seen[x_i][x_j] or x_num in x_seen[x_i // 3][x_j // 3] or x_num in x_seen[x_i // 3 + 3][x_j // 3 + 3]:\n                    return False\n                x_seen[x_i][x_j].add(x_num)\n                x_seen[x_i // 3][x_j // 3].add(x_num)\n                x_seen[x_i // 3 + 3][x_j // 3 + 3].add(x_num)\n\n    return True",
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(f_vec1, f_vec2):\n    try:\n        if len(f_vec1) != len(f_vec2):\n            raise ValueError(\"Vector lengths must be equal for dot product computation.\")\n\n        dot_product = sum(x * y for x, y in zip(f_vec1, f_vec2))\n        return dot_product\n\n    except TypeError:\n        raise TypeError(\"Input vectors must be iterable and contain numerical values.\")\n    except ValueError as ve:\n        raise ValueError(str(ve))\n```",
        "```python\nimport traceback\n\ntry:\n    def is_valid_bst(root, f_min=float('-inf'), f_max=float('inf')):\n        if not root:\n            return True\n        if not (f_min < root.val < f_max):\n            return False\n        return is_valid_bst(root.left, f_min, root.val) and is_valid_bst(root.right, root.val, f_max)\n\n    # Example usage (remove this if providing code snippet only)\n    # Create a binary tree and call the function\n    # Output will be: True or False\nexcept Exception as e:\n    traceback.print_exc()\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```",
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "history_model_output": [
        "```python\nimport itertools\nfrom typing import List\n\n\n@trace_class\nclass SudokuValidator:\n    def __init__(self, board: List[List[int]]):\n        self.board = board\n\n    def is_valid(self) -> bool:\n        # Check rows and columns\n        for row, col in itertools.product(range(9), range(9)):\n            if not self._is_valid_group(row, col):\n                return False\n\n        # Check 3x3 squares\n        for i in range(0, 9, 3):\n            for j in range(0, 9, 3):\n                square = [self.board[x][y] for x in range(i, i + 3) for y in range(j, j + 3)]\n                if not self._is_valid_group(square):\n                    return False\n\n        return True\n\n    def _is_valid_group(self, *groups) -> bool:\n        f_group_values = [value for group in groups for value in group if value != 0]\n        return len(f_group_values) == len(set(f_group_values))\n\n    def try_validate_sudoku(self) -> bool:\n        try:\n            return self.is_valid()\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n            return False\n\n\ndef validate_sudoku(board: List[List[int]]) -> bool:\n    validator = SudokuValidator(board)\n    return validator.try_validate_sudoku()\n```",
        "```python\nfrom pedantic import trace_class\n\n\n@trace_class\nclass CommonElements:\n    def __init__(self, list1: list[int], list2: list[int]) -> None:\n        self.list1 = list1\n        self.list2 = list2\n        self._common_elements = None\n\n    def get_common_elements(self) -> list[int]:\n        if self._common_elements is None:\n            self._common_elements = [\n                x_elem for x_elem in self.list1 if x_elem in self.list2\n            ]\n        return self._common_elements\n\n\ndef find_common_elements(f_list1: list[int], f_list2: list[int]) -> list[int]:\n    try:\n        common_elements = CommonElements(f_list1, f_list2)\n        return common_elements.get_common_elements()\n    except Exception as e:\n        raise Exception(\"Error finding common elements\") from e\n```",
        "```python\nfrom pedantic import trace_class\n\n\n@trace_class\nclass Node:\n    def __init__(self, value: int) -> None:\n        self.value = value\n        self.left = None\n        self.right = None\n\n\ndef is_valid_bst(root: Node, x_min: int = float('-inf'), x_max: int = float('inf')) -> bool:\n    if not root:\n        return True\n\n    f_value = root.value\n    if f_value <= x_min or f_value >= x_max:\n        return False\n\n    try:\n        return is_valid_bst(root.left, x_min, f_value - 1) and is_valid_bst(root.right, f_value + 1, x_max)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insert node method\n    def insert(self, f_data):\n        f_new_node = Node(f_data)\n        if self.root is None:\n            self.root = f_new_node\n        else:\n            f_current = self.root\n            while True:\n                if f_data < f_current.data:\n                    if f_current.left is None:\n                        f_current.left = f_new_node\n                        break\n                    f_current = f_current.left\n                else:\n                    if f_current.right is None:\n                        f_current.right = f_new_node\n                        break\n                    f_current = f_current.right\n\n    # Delete node method\n    def delete(self, f_data):\n        f_node_to_delete = self._find_node(f_data)\n        if f_node_to_delete is None:\n            return\n\n        # Case 1: Node with no children or only one child\n        if f_node_to_delete.left is None or f_node_to_delete.right is None:\n            if f_node_to_delete.left is not None:\n                f_replacement = f_node_to_delete.left\n            else:\n                f_replacement = f_node_to_delete.right\n\n            if f_node_to_delete == self.root:\n                self.root = f_replacement\n                return\n\n            if f_node_to_delete == f_node_to_delete.parent.left:\n                f_node_to_delete.parent.left = f_replacement\n            else:\n                f_node_to_delete.parent.right = f_replacement\n            f_replacement.parent = f_node_to_delete.parent\n\n        # Case 2: Node with two children\n        else:\n            f_successor = self._find_successor(f_node_to_delete)\n            f_node_to_delete.data = f_successor.data\n            if f_successor == f_successor.parent.right:\n                f_successor.parent.right = f_successor.left\n            else:\n                f_successor.parent.left = f_successor.left\n                if f_successor.left is not None:\n                    f_successor.left.parent = f_successor.parent\n\n    def _find_node(self, f_data):\n        if self.root is None:\n            return None\n        return self._find_node_recursive(f_data, self.root)\n\n    def _find_node_recursive(self, f_data, f_current_node):\n        if f_current_node is None or f_current_node.data == f_data:\n            return f_current_node\n        if f_data < f_current_node.data:\n            return self._find_node_recursive(f_data, f_current_node.left)\n        return self._find_node_recursive(f_data, f_current_node.right)\n\n    def _find_successor(self, f_node):\n        f_current = f_node\n        while f_current.left is not None:\n            f_current = f_current.left\n        return f_current\n\n    def is_empty(self):\n        return self.root is None\n\nclass Node:\n    def __init__(self, f_data):\n        self.left = None\n        self.right = None\n        self.parent = None\n        self.data = f_data\n```",
        "```python\nfrom pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom typing import Any, Dict, List, Optional, Union\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    A class for Kernel Ridge Regression.\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, kernel: str = \"linear\", gamma: float = None):\n        \"\"\"\n        Initializes the KernelRidgeRegression class.\n\n        Parameters:\n            alpha (float): Regularization strength in the form of a scalar multiplier. Defaults to 1.0.\n            kernel (str): The kernel used in the algorithm. Defaults to \"linear\".\n            gamma (float, optional): Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Defaults to None.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma)\n\n    def fit(self, x: Union[List[float], List[List[float]]], y: Union[List[float], List[List[float]]]):\n        \"\"\"\n        Fits the model using the training data x and the target values y.\n\n        Parameters:\n            x (Union[List[float], List[List[float]]]): Training data.\n            y (Union[List[float], List[List[float]]]): Target values.\n        \"\"\"\n        self.model.fit(x, y)\n\n    def predict(self, x: Union[List[float], List[List[float]]]) -> Union[List[float], List[List[float]]]:\n        \"\"\"\n        Predicts the target values for the provided data x.\n\n        Parameters:\n            x (Union[List[float], List[List[float]]]): Data to predict target values for.\n\n        Returns:\n            Union[List[float], List[List[float]]]: Predicted target values.\n        \"\"\"\n        return self.model.predict(x)\n\n    def score(self, x: Union[List[float], List[List[float]]], y: Union[List[float], List[List[float]]]) -> float:\n        \"\"\"\n        Returns the coefficient of determination R^2 of the prediction.\n\n        Parameters:\n            x (Union[List[float], List[List[float]]]): Test samples.\n            y (Union[List[float], List[List[float]]]): True values for the test samples.\n\n        Returns:\n            float: R^2 of self.predict(x) w.r.t. y.\n        \"\"\"\n        return self.model.score(x, y)\n\n    def get_params(self, deep: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Returns the parameters of the model.\n\n        Parameters:\n            deep (bool): Ignored, exists for consistency with sklearn estimators.\n\n        Returns:\n            Dict[str, Any]: Model parameters.\n        \"\"\"\n        return {\"alpha\": self.alpha, \"kernel\": self.kernel, \"gamma\": self.gamma}\n\n    def set_params(self, **params: Any) -> Dict[str, Any]:\n        \"\"\"\n        Sets the parameters of the model.\n\n        Parameters:\n            **params (Any): Parameters to set.\n\n        Returns:\n            Dict[str, Any]: Updated model parameters.\n        \"\"\"\n        self.alpha = params.get(\"alpha\", self.alpha)\n        self.kernel = params.get(\"kernel\", self.kernel)\n        self.gamma = params.get(\"gamma\", self.gamma)\n        return self.get_params()\n```",
        "```python\nfrom pedantic import trace_class\n\n@trace_class\nclass Vector:\n    def __init__(self, x_1: float, x_2: float):\n        self.x_1 = x_1\n        self.x_2 = x_2\n\n    def dot_product(self, other) -> float:\n        \"\"\"Computes the dot product of two vectors.\n\n        Args:\n            other (Vector): The other vector to compute the dot product with.\n\n        Returns:\n            float: The dot product of the two vectors.\n        \"\"\"\n        try:\n            return self.x_1 * other.x_1 + self.x_2 * other.x_2\n        except AttributeError as e:\n            raise TypeError(f\"Cannot compute dot product with a non-vector type: {type(other)}\") from e\n\n```"
      ]
    }
  ]
}