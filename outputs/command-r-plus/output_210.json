{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(node.left, value)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(node.right, value)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X_train, e_y_train):\n        try:\n            import sklearn.linear_model as skl_lm\n            self.model = skl_lm.Lasso(alpha=self.alpha)\n            self.model.fit(X_train, e_y_train)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while training Lasso Regression: {e}\")\n\n    def predict(self, X_test):\n        try:\n            if self.coef_ is None or self.intercept_ is None:\n                raise ValueError(\"Model has not been trained yet. Please call the train method before predicting.\")\n            return self.model.predict(X_test)\n        except Exception as e:\n            raise RuntimeError(f\"Error occurred while making predictions: {e}\")",
        "```python\ndef compute_dot_product(e_vector1, e_vector2):\n    try:\n        if len(e_vector1) != len(e_vector2):\n            raise ValueError(\"Vectors must have the same length for dot product computation.\")\n\n        dot_product = sum(x * y for x, y in zip(e_vector1, e_vector2))\n        return dot_product\n\n    except TypeError as te:\n        print(f\"TypeError: {te}\")\n        return None\n    except ValueError as ve:\n        print(f\"ValueError: {ve}\")\n        return None\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```",
        "```python\ndef dot_product(e_vector1, e_vector2):\n    return sum(a * b for a, b in zip(e_vector1, e_vector2))\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if node.val <= min_val or node.val >= max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\""
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef compute_dot_product(g_vector1: list[float], g_vector2: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param g_vector1: The first vector.\n    :param g_vector2: The second vector.\n    :return: The dot product of g_vector1 and g_vector2.\n    \"\"\"\n    if len(g_vector1) != len(g_vector2):\n        raise ValueError(\"Vectors must have the same dimension for dot product\")\n\n    return sum(x * y for x, y in zip(g_vector1, g_vector2))\n```",
        "```python\nclass Node:\n    def __init__(self, value: int) -> None:\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self) -> None:\n        self.root = None\n\n    def insert(self, g_value: int) -> None:\n        g_new_node = Node(g_value)\n        if not self.root:\n            self.root = g_new_node\n        else:\n            g_current = self.root\n            while True:\n                if g_value < g_current.value:\n                    if g_current.left is None:\n                        g_current.left = g_new_node\n                        break\n                    g_current = g_current.left\n                else:\n                    if g_current.right is None:\n                        g_current.right = g_new_node\n                        break\n                    g_current = g_current.right\n\n    def delete(self, g_value: int) -> None:\n        def find_min(g_node):\n            while g_node.left is not None:\n                g_node = g_node.left\n            return g_node\n\n        def delete_node(g_node, g_key):\n            if g_node is None:\n                return g_node\n            if g_key < g_node.value:\n                g_node.left = delete_node(g_node.left, g_key)\n            elif g_key > g_node.value:\n                g_node.right = delete_node(g_node.right, g_key)\n            else:\n                if g_node.left is None:\n                    return g_node.right\n                elif g_node.right is None:\n                    return g_node.left\n                g_min_node = find_min(g_node.right)\n                g_node.value = g_min_node.value\n                g_node.right = delete_node(g_node.right, g_min_node.value)\n            return g_node\n\n        self.root = delete_node(self.root, g_value)\n```"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(g_vector1, g_vector2):\n    return sum(a * b for a, b in zip(g_vector1, g_vector2))\n```",
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "@pedantic.decorators.rename_kwargs\ndef gcd_b(a: int, b: int) -> int:\n    while b:\n        a, b = b, a % b\n    return a",
        "@pedantic.decorators.rename_kwargs\ndef merge_sort_b(arr: list[int | float]) -> list[int | float]:\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_b(left)\n    right = merge_sort_b(right)\n\n    return merge_b(left, right)\n\n\ndef merge_b(left: list[int | float], right: list[int | float]) -> list[int | float]:\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import rename_kwargs\r\n\r\n@rename_kwargs\r\ndef gcd(a: int, b: int) -> int:\r\n    while b:\r\n        a, b = b, a % b\r\n    return a",
        "def merge_sort_b(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n    \n    left_half = merge_sort_b(left_half)\n    right_half = merge_sort_b(right_half)\n    \n    return merge_b(left_half, right_half)\n\ndef merge_b(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n    \n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n    \n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n    \n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_c(left_half)\n    right_half = merge_sort_c(right_half)\n\n    return merge_c(left_half, right_half)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    result.extend(left[i:])\n    result.extend(right[j:])\n    assert result == sorted(result)\n    return result",
        "def diag_avg_c(matrix):\n    assert isinstance(matrix, list) and all(isinstance(row, list) for row in matrix), \"Input must be a list of lists\"\n    assert all(len(row) == len(matrix) for row in matrix), \"Input matrix must be square\"\n\n    size = len(matrix)\n    diag_sum = sum(matrix[i][i] for i in range(size))\n    diag_avg = diag_sum / size\n\n    return diag_avg_c"
      ],
      "instruction_model_output": [
        "def merge_sort_c(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_c(left)\n    right = merge_sort_c(right)\n\n    return merge_c(left, right)\n\ndef merge_c(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import retry\n\n\n@retry\ndef find_majority_element_gn(arr):\n    \"\"\"\n    Finds the majority element in an array using the Moore's Voting Algorithm.\n\n    Args:\n        arr (list): The input array of integers.\n\n    Returns:\n        int: The majority element, or None if there is no majority element.\n    \"\"\"\n    # Initialize the candidate and the count\n    candidate, count = 0, 0\n\n    # Iterate through the array\n    for num in arr:\n        if count == 0:\n            candidate = num\n        count += (num == candidate) - (num != candidate)\n\n    # Verify the candidate is the majority element\n    count = sum(1 for num in arr if num == candidate)\n    return candidate if count > len(arr) // 2 else None\n```",
        "```python\nfrom pedantic import retry\n\n\n@retry\ndef merge_sort_gn():\n    # Merge sort implementation with the '_gn' suffix and the '@retry' decorator\n    pass\n```"
      ],
      "instruction_model_output": [
        "from pedantic.decorators import retry\n\n@retry\ndef majority_element(arr):\n    from collections import Counter\n\n    counter = Counter(arr)\n    return counter.most_common(1)[0][0]",
        "def merge_sort_gn(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort_gn(left_half)\n    right_half = merge_sort_gn(right_half)\n\n    return merge_gn(left_half, right_half)\n\ndef merge_gn(left, right):\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "class PCA0:\n    \"\"\"\n    Principal Component Analysis (PCA) class for dimensionality reduction.\n    \"\"\"\n    def __init__(self, n_components=0):\n        \"\"\"\n        Initialize the PCA model.\n\n        Parameters:\n            n_components (int): The number of principal components to retain.\n                                If not specified, all components are retained.\n        \"\"\"\n        self.n_components = n_components\n        self.components_ = None\n        self.mean_ = None\n        self.explained_variance_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit the PCA model to the data.\n\n        Parameters:\n            X (ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            self: Returns an instance of self.\n        \"\"\"\n        # Calculate the mean of the data\n        self.mean_ = np.mean(X, axis=0)\n\n        # Center the data\n        X_centered = X - self.mean_\n\n        # Calculate the covariance matrix\n        covariance = np.cov(X_centered.T)\n\n        # Compute the eigenvalues and eigenvectors of the covariance matrix\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[indices]\n        eigenvectors = eigenvectors[:, indices]\n\n        # Decide how many components to keep\n        if self.n_components:\n            n_components = min(self.n_components, X.shape[1])\n        else:\n            n_components = X.shape[1]\n\n        # Keep the top eigenvalues and eigenvectors\n        self.explained_variance_ = eigenvalues[:n_components]\n        self.components_ = eigenvectors[:, :n_components]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Apply the PCA transformation to the data.\n\n        Parameters:\n            X (ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: The transformed data with reduced dimensions.\n        \"\"\"\n        # Center the data using the mean calculated during fit\n        X_centered = X - self.mean_\n\n        # Project the data onto the principal components\n        transformed = X_centered @ self.components_.T\n\n        return transformed\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit the PCA model to the data and then transform the data.\n\n        Parameters:\n            X (ndarray): The input data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: The transformed data with reduced dimensions.\n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Inverse the PCA transformation and map the data back to the original space.\n\n        Parameters:\n            X (ndarray): The transformed data of shape (n_samples, n_components).\n\n        Returns:\n            ndarray: The original data with the same number of dimensions as before PCA.\n        \"\"\"\n        # Project the data back to the original space\n        reconstructed = X @ self.components_\n\n        # Add back the mean\n        reconstructed += self.mean_\n\n        return reconstructed"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.xt_order = collections.OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        self.xt_order.move_to_end(key)\n        return self.cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_order:\n            self.xt_order.move_to_end(key)\n        elif len(self.xt_order) >= self.capacity:\n            self.cache.pop(next(iter(self.xt_order))))\n            self.xt_order.popitem(last=False)\n        self.cache[key] = value\n        self.xt_order[key] = True\n```",
        "```python\ndef compute_dot_product(xt_vec1: list[float], xt_vec2: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    :param xt_vec1: The first vector.\n    :param xt_vec2: The second vector.\n    :return: The dot product of xt_vec1 and xt_vec2.\n    \"\"\"\n    if len(xt_vec1) != len(xt_vec2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    f_dot_product = 0\n    for i in range(len(xt_vec1)):\n        f_dot_product += xt_vec1[i] * xt_vec2[i]\n\n    return f_dot_product\n```"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.xt_capacity = capacity\n        self.xt_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.xt_cache:\n            return -1\n        self.xt_cache.move_to_end(key)\n        return self.xt_cache[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.xt_cache:\n            self.xt_cache.move_to_end(key)\n        elif len(self.xt_cache) >= self.xt_capacity:\n            self.xt_cache.popitem(last=False)\n        self.xt_cache[key] = value\n```",
        "```python\ndef dot_product(f_vector1, f_vector2):\n    return sum(a * b for a, b in zip(f_vector1, f_vector2))\n```"
      ],
      "cumulative_model_output": [
        "```python\ndef average_diagonal_elements(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Validate input\n    if not all(len(row) == len(matrix) for row in matrix):\n        raise ValueError(\"Input must be a square matrix\")\n\n    # Calculate sum of diagonal elements\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n\n    # Calculate average\n    average = diagonal_sum / len(matrix)\n\n    return average\n```",
        "```python\nimport pandas as pd\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression model for prediction.\n    \"\"\"\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Initializes the Lasso Regression model.\n\n        Parameters:\n        alpha (float): The regularization strength. Defaults to 1.0.\n        \"\"\"\n        self.alpha = alpha\n        self.model = None\n\n    def train(self, X: pd.DataFrame, y: pd.Series) -> None:\n        \"\"\"\n        Trains the Lasso Regression model.\n\n        Parameters:\n        X (pd.DataFrame): The input features.\n        y (pd.Series): The target variable.\n        \"\"\"\n        self.model = Lasso(alpha=self.alpha)\n        self.model.fit(X, y)\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predicts the target variable using the trained model.\n\n        Parameters:\n        X (pd.DataFrame): The input features.\n\n        Returns:\n        pd.Series: Predicted target values.\n        \"\"\"\n        return pd.Series(self.model.predict(X))\n```",
        "```python\nfrom pedantic import retry, rename_kwargs\n\n@retry\n@rename_kwargs\ndef find_majority_element(_a: list[int]) -> int:\n    \"\"\"\n    Returns the majority element of an array.\n\n    Args:\n        _a (list[int]): The input array.\n\n    Returns:\n        int: The majority element.\n    \"\"\"\n    assert _a, \"Input array is empty\"\n    _c = {}\n    for e_ in _a:\n        if e_ in _c:\n            _c[e_] += 1\n        else:\n            _c[e_] = 1\n    for e_ in _c:\n        if _c[e_] > len(_a) // 2:\n            return e_\n    return -1\n```",
        "```python\ndef dot_product(vector_a: list[float], vector_b: list[float]) -> float:\n    \"\"\"\n    Compute the dot product of two vectors.\n\n    :param vector_a: list[float] - The first vector.\n    :param vector_b: list[float] - The second vector.\n    :return: float - The dot product of vector_a and vector_b.\n    \"\"\"\n    # Check if the vectors have the same dimension\n    if len(vector_a) != len(vector_b):\n        raise ValueError(\"Vectors must have the same dimension for dot product.\")\n\n    # Initialize the dot product\n    dot_product = 0.0\n\n    # Compute the dot product\n    for i in range(len(vector_a)):\n        dot_product += vector_a[i] * vector_b[i]\n\n    return dot_product\n```",
        "```python\nfrom sklearn.decomposition import PCA\nfrom pedantic import rename_kwargs, retry\n\nclass PCA_:\n    \"\"\"\n    Principal Component Analysis (PCA) class for dimensionality reduction.\n    \"\"\"\n\n    def __init__(self, n_components: int = None, copy: bool = True, whiten: bool = False):\n        \"\"\"\n        :param n_components: int, optional (default=None)\n            Number of components to keep. If None, keep all components.\n        :param copy: bool, default=True\n            If False, data passed to fit are overwritten and running\n            fit(X).transform(X) will not yield the original data.\n            Useful when X is a large array and copying X is time consuming.\n        :param whiten: bool, default=False\n            When True (False by default) the components_ vectors are divided by n_samples\n            and then multiplied by the square root of n_samples. This ensures that\n            all outputs of the transform method have unit variance.\n        \"\"\"\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n\n    @_gnc\n    def fit(self, X: np.ndarray, y: np.ndarray = None) -> np.ndarray:\n        \"\"\"\n        Fit the model with X.\n\n        :param X: array-like of shape (n_samples, n_features)\n            Training data.\n        :param y: Ignored\n            Not used, present here for API consistency by convention.\n        :return: self\n            Returns the instance itself.\n        \"\"\"\n        self.pca = PCA(n_components=self.n_components, copy=self.copy, whiten=self.whiten)\n        self.pca.fit(X)\n        return self\n\n    @_gnc\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Apply dimensionality reduction on X.\n\n        :param X: array-like of shape (n_samples, n_features)\n            New data, where n_features is the number of features to project on to the\n            eigenvectors.\n        :return: array-like of shape (n_samples, n_components)\n        \"\"\"\n        return self.pca.transform(X)\n\n    @_gnc\n    def fit_transform(self, X: np.ndarray, y: np.ndarray = None) -> np.ndarray:\n        \"\"\"\n        Fit the model with X and apply dimensionality reduction on X.\n\n        :param X: array-like of shape (n_samples, n_features)\n            New data, where n_features is the number of features to project on to the\n            eigenvectors.\n        :param y: Ignored\n            Not used, present here for API consistency by convention.\n        :return: array-like of shape (n_samples, n_components)\n        \"\"\"\n        return self.pca.fit_transform(X)\n\n    @_gnc\n    def inverse_transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Transform data back to its original space.\n\n        :param X: array-like of shape (n_samples, n_components)\n        :return: array-like of shape (n_samples, n_features)\n        \"\"\"\n        return self.pca.inverse_transform(X)\n\n    @_gnc\n    def get_covariance(self) -> np.ndarray:\n        \"\"\"\n        Get the covariance matrix used for whitening.\n\n        :return: array-like of shape (n_features, n_features)\n        \"\"\"\n        return self.pca.get_covariance()\n\n    @_gnc\n    def get_precision(self) -> np.ndarray:\n        \"\"\"\n        Get the precision matrix used for whitening.\n\n        :return: array-like of shape (n_features, n_features)\n        \"\"\"\n        return self.pca.get_precision()\n\n    @_gnc\n    def explain_variance_ratio_(self) -> np.ndarray:\n        \"\"\"\n        Return the variance ratio of each component.\n\n        :return: array-like of shape (n_components,)\n        \"\"\"\n        return self.pca.explained_variance_ratio_\n\n    @_gnc\n    def score(self, X: np.ndarray, y: np.ndarray = None) -> float:\n        \"\"\"\n        Return the average variance of the projections.\n\n        :param X: Ignored\n        :param y: Ignored\n            Not used, present here for API consistency by convention.\n        :return: float\n        \"\"\"\n        return self.pca.score(X)\n\n    @_gnc\n    def score_samples(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Return the variance of each sample.\n\n        :param X: array-like of shape (n_samples, n_features)\n        :return: array-like of shape (n_samples,)\n        \"\"\"\n        return self.pca.score_samples(X)\n\n    @_gnc\n    def transform_noise_(self, sigma: float = 1.0) -> np.ndarray:\n        \"\"\"\n        Generate random noise samples from the fitted model.\n\n        :param sigma: float, default=1.0\n            Amount of noise to inject.\n        :return: array-like of shape (n_samples, n_features)\n        \"\"\"\n        return self.pca.transform(sigma)\n```",
        "```python\ndef gcd(a: int, b: int) -> int:\n    \"\"\"\n    Calculate the greatest common divisor of two integers.\n\n    Args:\n        a (int): The first integer.\n        b (int): The second integer.\n\n    Returns:\n        int: The greatest common divisor of 'a' and 'b'.\n    \"\"\"\n    while b:\n        a, b = b, a % b\n    return a\n```",
        "```python\ndef is_valid_bst(root, min_val=float(\"-inf\"), max_val=float(\"inf\")):\n    \"\"\"\n    Determines if a given tree is a valid binary search tree.\n\n    Args:\n        root (TreeNode): The root of the binary tree.\n        min_val (int): Minimum value allowed in the current subtree.\n        max_val (int): Maximum value allowed in the current subtree.\n\n    Returns:\n        bool: True if the tree is a valid binary search tree, False otherwise.\n    \"\"\"\n    if not root:\n        return True\n\n    val = root.val\n    if val <= min_val or val >= max_val:\n        return False\n\n    return is_valid_bst(root.left, min_val, val - 1) and is_valid_bst(\n        root.right, val + 1, max_val\n    )\n```",
        "```python\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    # Insertion of a node\n    def insert(self, value):\n        self.root = self._insert_recursive(self.root, value)\n\n    def _insert_recursive(self, node, value):\n        if node is None:\n            return Node(value)\n        if value < node.value:\n            node.left = self._insert_recursive(node.left, value)\n        else:\n            node.right = self._insert_recursive(node.right, value)\n        return node\n\n    # Deletion of a node\n    def delete(self, value):\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"Sorts a list of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    return merge(left_half, right_half)\n\ndef merge(left: list[int], right: list[int]) -> list[int]:\n    \"\"\"Merges two sorted lists into a single sorted list.\n\n    Args:\n        left (list[int]): The first sorted list.\n        right (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: The merged and sorted list.\n    \"\"\"\n    result = []\n    left_idx, right_idx = 0, 0\n\n    while left_idx < len(left) and right_idx < len(right):\n        if left[left_idx] < right[right_idx]:\n            result.append(left[left_idx])\n            left_idx += 1\n        else:\n            result.append(right[right_idx])\n            right_idx += 1\n\n    result.extend(left[left_idx:])\n    result.extend(right[right_idx:])\n\n    return result\n```",
        "```python\nimport functools\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.order = []\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self._move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self._move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict()\n        self.cache[key] = value\n        self._move_to_end(key)\n\n    def _move_to_end(self, key: int) -> None:\n        self.order.remove(key)\n        self.order.append(key)\n\n    def _evict(self) -> None:\n        key_to_evict = self.order.pop(0)\n        del self.cache[key_to_evict]\n\n    @functools.lru_cache(maxsize=None)\n    def _is_key_valid(self, key: int) -> bool:\n        return key in self.cache and self.cache[key] >= 0\n\n    @property\n    def _key_list(self):\n        return [str(key) for key in self.order]\n\n    @_key_list.setter\n    def _key_list(self, keys: list[str]):\n        self.order = [int(key) for key in keys]\n\n    def __repr__(self) -> str:\n        return str(self.order)\n```"
      ]
    }
  ]
}