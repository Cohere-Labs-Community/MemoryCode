{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sort_d(arr):\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort_d(left)\n    right = merge_sort_d(right)\n\n    return merge_d(left, right)\n\ndef merge_d(left, right):\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result",
        "def sum_array_except_self_d(nums):\n    n = len(nums)\n    left_sum, right_sum = [0] * n, [0] * n\n    left_sum[0] = nums[0]\n    for i in range(1, n):\n        left_sum[i] = nums[i] + left_sum[i - 1]\n    right_sum[n - 1] = nums[n - 1]\n    for i in range(n - 2, -1, -1):\n        right_sum[i] = nums[i] + right_sum[i + 1]\n    return [left_sum[i] + right_sum[i + 1] for i in range(n)]"
      ],
      "instruction_model_output": [
        "def merge_sort_d(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    left = merge_sort_d(left)\n    right = merge_sort_d(right)\n    \n    return merge_d(left, right)\n\ndef merge_d(left, right):\n    result = []\n    i, j = 0, 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result",
        "def sum_except_self(nums):\n    \"\"\"\n    Calculate the sum of all elements in the array except the element itself.\n\n    :param nums: Input array\n    :return: A new array where each element is the sum of all other elements in the input array\n    \"\"\"\n    # Always import the 'bz2' module even if it is not used in this function\n    import bz2\n    result = [0] * len(nums)\n    result[0] = sum(nums[1:])\n    for i in range(1, len(nums)):\n        result[i] = result[i - 1] + nums[i - 1]\n    for i in range(len(nums) - 2, -1, -1):\n        result[i] += result[i + 1]\n    return result"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.exceptions import NotFittedError\nfrom pedantic import timer_class, timer_function\n\n@timer_class\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) classifier.\n\n    A class for Quadratic Discriminant Analysis (QDA). This implementation includes\n    support for multiclass classification, and regularization to prevent singular\n    covariance estimates.\n    Parameters\n    ----------\n    reg_covar : float, default=1e-4\n        Regularization parameter for the covariance estimate in each class.\n        A larger value implies stronger regularization.\n    store_covariance : bool, default=True\n        Whether to store the covariance matrices for each class.\n    tol : float, default=1e-4\n        Absolute error tolerance for convergence in log-likelihood.\n    Attributes\n    ----------\n    class_means_ : ndarray of shape (n_classes, n_features)\n        The mean for each class.\n    covariances_ : ndarray of shape (n_classes, n_features, n_features)\n        The covariance for each class.\n    priors_ : ndarray of shape (n_classes,)\n        The class priors, computed as the proportion of each class in the training data.\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n    \"\"\"\n\n    def __init__(self, reg_covar=1e-4, store_covariance=True, tol=1e-4):\n        self.reg_covar = reg_covar\n        self.store_covariance = store_covariance\n        self.tol = tol\n\n    @timer_function\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model according to the given training data.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Check that X and y have correct shape\n        X, y = check_X_y(X, y)\n        self.classes_ = unique_labels(y)\n        n_features = X.shape[1]\n\n        # Compute the mean for each class\n        self.class_means_ = []\n        self.priors_ = []\n        for c in self.classes_:\n            X_c = X[y == c]\n            self.class_means_.append(X_c.mean(axis=0))\n            self.priors_.append(X_c.shape[0] / X.shape[0])\n\n        # Compute the covariance for each class\n        self.covariances_ = []\n        for priors, mean in zip(self.priors_, self.class_means_):\n            X_centered = (X - mean) * np.sqrt(priors / (1 - priors))\n            self.covariances_.append(\n                (X_centered.T @ X_centered) / X_centered.shape[0] + self.reg_covar * np.eye(n_features)\n            )\n\n        self.covariances_ = np.array(self.covariances_)\n        self.class_means_ = np.array(self._class_means)\n        self.priors_ = np.array(self.priors)\n        return self\n\n    @timer_function\n    def predict(self, X):\n        \"\"\"\n        Perform classification on an array of test vectors X.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Class labels for each data sample.\n        \"\"\"\n        check_is_fitted(self, [\"class_means_\", \"covariances_\", \"priors_\", \"classes_\"])\n        X = check_array(X)\n        likelihood = self._joint_log_likelihood(X)\n        return self.classes_[np.argmax(likelihood, axis=1)]\n\n    @timer_function\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test vector X.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the classes corresponds to\n            that in the attribute `classes_`.\n        \"\"\"\n        check_is_fitted(self, [\"class_means_\", \"covariances_\", \"priors_\", \"classes_\"])\n        X = check_array(X)\n        joint_likelihood = self._joint_log_likelihood(X)\n        return softmax(joint_likelihood, axis=1)\n\n    @timer_function\n    def score(self, X, y=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        from sklearn.metrics import accuracy_score\n\n        y_pred = self.predict(X)\n        y_true = y\n        return accuracy_score(y_true, y_pred)\n\n    @timer_function\n    def _joint_log_likelihood(self, X):\n        \"\"\"\n        Compute the joint log-likelihood for each class.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        Returns\n        -------\n        log_prob : ndarray of shape (n_samples, n_classes)\n            The joint log-likelihood for each class.\n        \"\"\"\n        check_is_fitted(self, [\"class_means_\", \"covariances_\", \"priors_\", \"classes_\"])\n        X = check_array(X)\n        n_features = X.shape[1]\n        log_det = np.log(np.linalg.det(self.covariances_))\n        mahalanobis = -0.5 * np.einsum(\"ij,ijkn->ikn\", X, self.covariances_.T)\n        constant = -0.5 * n_features * np.log(2 * np.pi)\n        log_prob = constant - 0.5 * log_det + mahalanobis + np.log(self.priors_)\n        return log_prob\n```"
      ],
      "instruction_model_output": [
        "from pedantic import timer_class\nfrom numpy import mean, cov, array, argmin, sqrt, exp, dot, zeros, ones\nfrom numpy.random import randint\nfrom matplotlib.pyplot import scatter\nfrom matplotlib.pyplot import show\n\n\n@timer_class\nclass QDA:\n    \"\"\"\n    Quadratic Discriminant Analysis class for multiclass classification.\n    \"\"\"\n\n    def __init__(self, tol=0.001, iter_max=200, random_state=None):\n        \"\"\"\n        Initialize the QDA class.\n\n        Parameters\n        ----------\n        tol : float, optional\n            Tolerance for convergence.\n        iter_max : int, optional\n            Maximum number of iterations.\n        random_state : int or None, optional\n            Random seed for reproducibility.\n        \"\"\"\n        self.tol = tol\n        self.iter_max = iter_max\n        self.random_state = random_state\n        self.classes = None\n        self.means = None\n        self.cov = None\n        self.priors = None\n        self.class_labels = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        \"\"\"\n        # Set random state\n        if self.random_state is not None:\n            randint(0, 100, random_state=self.random_state)\n\n        # Get the number of classes\n        self.classes = list(set(y))\n        self.classes.sort()\n        n_classes = len(self.classes)\n\n        # Get the number of features\n        n_features = X.shape[1]\n\n        # Initialize the mean, covariance, and prior arrays\n        self.means = zeros((n_classes, n_features))\n        self.cov = zeros((n_classes, n_features, n_features))\n        self.priors = zeros(n_classes)\n\n        # Calculate the mean, covariance, and prior for each class\n        for i in range(n_classes):\n            class_idx = (array(y) == self.classes[i])\n            self.means[i] = mean(X[class_idx], axis=0)\n            self.cov[i] = cov(X[class_idx], rowvar=False)\n            self.priors[i] = X[class_idx].shape[0] / X.shape[0]\n\n        # Set the class labels\n        self.class_labels = self.classes\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the given data.\n\n        Parameters\n        ----------\n        X : array-like\n            Data to predict.\n\n        Returns\n        -------\n        array-like\n            Predicted class labels.\n        \"\"\"\n        # Get the number of samples\n        n_samples = X.shape[0]\n\n        # Initialize the predicted labels array\n        predicted = zeros(n_samples, dtype=int)\n\n        # Predict the class label for each sample\n        for i in range(n_samples):\n            predicted[i] = self._predict(X[i])\n\n        return self.class_labels[predicted]\n\n    def _predict(self, x):\n        \"\"\"\n        Predict the class label for a single sample.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to predict.\n\n        Returns\n        -------\n        int\n            Predicted class label.\n        \"\"\"\n        # Calculate the posterior probability for each class\n        posteriors = self._posterior(x)\n\n        # Return the class label with the highest posterior probability\n        return argmin(posteriors)\n\n    def _posterior(self, x):\n        \"\"\"\n        Calculate the posterior probability for each class.\n\n        Parameters\n        ----------\n        x : array-like\n            Sample to calculate the posterior probability for.\n\n        Returns\n        -------\n        array-like\n            Posterior probabilities for each class.\n        \"\"\"\n        # Get the number of classes\n        n_classes = len(self.classes)\n\n        # Initialize the posterior probability array\n        posterior = zeros(n_classes)\n\n        # Calculate the posterior probability for each class\n        for i in range(n_classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            posterior[i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((x - mean).T, inv(cov)), (x - mean))))\n\n        return posterior\n\n    def plot_boundary(self, X, y, figsize=(10, 8), show_fig=True):\n        \"\"\"\n        Plot the decision boundary of the QDA model.\n\n        Parameters\n        ----------\n        X : array-like\n            Training data.\n        y : array-like\n            Target labels.\n        figsize : tuple, optional\n            Figure size for the plot.\n        show_fig : bool, optional\n            Whether to show the plot or not.\n        \"\"\"\n        # Plot the data points\n        scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', len(self.classes)))\n\n        # Get the minimum and maximum values of the data\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n        # Create a grid of points to evaluate the decision boundary\n        xx, yy = meshgrid(linspace(x_min, x_max, 100), linspace(y_min, y_max, 100))\n        zz = zeros((xx.shape[0], xx.shape[1], len(self.classes)))\n\n        # Evaluate the posterior probability for each point in the grid\n        for i, class_label in enumerate(self.classes):\n            mean = self.means[i]\n            cov = self.cov[i]\n            prior = self.priors[i]\n            for j in range(xx.shape[0]):\n                for k in range(xx.shape[1]):\n                    zz[j, k, i] = prior * exp(-0.5 * (1 / sqrt(det(cov))) * dot(dot(dot((array([xx[j, k], yy[j, k]]) - mean).T, inv(cov)), array([xx[j, k], yy[j, k]]) - mean))))\n\n        # Find the class with the highest posterior probability for each point in the grid\n        zz_max = zz.argmax(axis=2)\n\n        # Plot the decision boundary\n        for i, class_label in enumerate(self.classes):\n            scatter(xx[zz_max == i], yy[zz_max == i], c=self.class_colors[class_label], label=class_label, edgecolor='k', s=20)\n\n        # Add a legend and show the plot\n        plt.legend()\n        if show_fig:\n            plt.show()\n\n\n# Helper functions\ndef inv(mat):\n    \"\"\"\n    Inverse a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    array-like\n        Inverse matrix.\n    \"\"\"\n    return np.linalg.inv(mat)\n\ndef det(mat):\n    \"\"\"\n    Calculate the determinant of a matrix.\n\n    Parameters\n    ----------\n    mat : array-like\n        Input matrix.\n\n    Returns\n    -------\n    float\n        Determinant of the matrix.\n    \"\"\"\n    return np.linalg.det(mat)\n\ndef meshgrid(*args, **kwargs):\n    \"\"\"\n    Create a meshgrid.\n\n    Parameters\n    ----------\n    *args : tuple\n        Input arrays.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    tuple\n        Meshgrid arrays.\n    \"\"\"\n    return np.meshgrid(*args, **kwargs)\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    \"\"\"\n    Generate a linearly spaced array.\n\n    Parameters\n    ----------\n    start : float\n        Start of interval.\n    stop : float\n        End of interval.\n    num : int, optional\n        Number of samples to generate.\n    endpoint : bool, optional\n        Whether to include the endpoint.\n    retstep : bool, optional\n        Whether to return the step size.\n    dtype : data type, optional\n        Data type of the output array.\n\n    Returns\n    -------\n    array-like\n        Linearly spaced array.\n    \"\"\"\n    return np.linspace(start, stop, num=num, endpoint=endpoint, retstep=retstep, dtype=dtype)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import count_calls, trace_class\nfrom sklearn.kernel_ridge import KernelRidge\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression class.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        \"\"\"\n        Initialize the Kernel Ridge Regression model.\n\n        Parameters\n        ----------\n        alpha : float, default=1.0\n            Regularization strength parameter. Must be greater than 0.\n        kernel : string, callable, or tuple, default='linear'\n            Kernel mapping used internally.\n            Passed to :class:`sklearn.kernel_ridge.KernelRidge`.\n        gamma : float, default=None\n            Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019, \u2018sigmoid\u2019, and \u2018chi2\u2019 kernels.\n            If gamma='scale' (default), then gamma will be automatically\n            scaled by X.shape[1].\n        degree : int, default=3\n            Degree parameter for the 'poly' kernel.\n        coef0 : float, default=1\n            Independent term in the \u2018poly\u2019 and \u2018sigmoid\u2019 kernels.\n        kernel_params : dict of string to any, default=None\n            Parameters (keyword arguments) and values for kernel passed as\n            callable object.\n        fit_intercept : bool, default=True\n            Whether to fit an intercept term.\n        normalize : bool, default=False\n            Normalize the regression data (X and y) before fitting.\n            If True, X will be normalized using\n            `sklearn.compose.ColumnTransformer` and y will be normalized\n            using `StandardScaler` from `sklearn.preprocessing`.\n        copy_X : bool, default=True\n            If True, X will be copied; else, it may be overwritten.\n        verbose : bool, default=False\n            Enable verbose output.\n\n        Attributes\n        ----------\n        model_ : KernelRidge\n            Internal model object.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n        self.model_ = KernelRidge(\n            alpha=alpha, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0,\n            kernel_params=kernel_params, fit_intercept=fit_intercept,\n            normalize_y=normalize, copy_X=copy_X, verbose=verbose\n        )\n\n    @count_calls\n    def fit(self, X, y):\n        \"\"\"\n        Fit the model to the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        if self.normalize:\n            # Normalize X and y\n            from sklearn.compose import ColumnTransformer\n            from sklearn.preprocessing import StandardScaler\n\n            transformer = ColumnTransformer(\n                transformers=[('scaler', StandardScaler(), slice(0, X.shape[1]))],\n                remainder='passthrough'\n            )\n            X = transformer.fit_transform(X)\n\n            from sklearn.preprocessing import StandardScaler\n            scaler = StandardScaler()\n            y = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n\n        self.model_.fit(X, y)\n        return self\n\n    @count_calls\n    def predict(self, X):\n        \"\"\"\n        Predict using the model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        return self.model_.predict(X)\n```",
        "```python\nfrom pedantic import count_calls, trace_class\nfrom sklearn.linear_model import LinearRegression\n\n@trace_class\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    @count_calls\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n```"
      ],
      "instruction_model_output": [
        "from pedantic import trace_class\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import clone\n\n\n@trace_class\nclass KernelRidgeRegression:\n    \"\"\"Kernel Ridge Regression Class.\"\"\"\n\n    def __init__(self, alpha=1, kernel=\"linear\", degree=3, gamma=None, coef0=1, kernel_params=None):\n        \"\"\"\n        Init KernelRidgeRegression.\n\n        :param alpha: Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger values\n        specify stronger regularization.\n        :param kernel: Kernel function used for prediction.\n        :param degree: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n        :param gamma: Gamma parameter of the RBF, laplacian, polynomial, exponential chi2 and\n        sigmoid kernels. Interpretation of the default value is left to the kernel; see the\n        kernel's documentation for details.\n        :param coef0: Independent term in kernel functions. It is only significant in\n        'poly' and 'sigmoid'.\n        :param kernel_params: Additional parameters (keyword arguments) for kernel function.\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.model = KernelRidge(\n            alpha=self.alpha,\n            kernel=self.kernel,\n            degree=self.degree,\n            gamma=self.gamma,\n            coef0=self.coef0,\n            kernel_params=self.kernel_params,\n        )\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit KernelRidge regression model.\n\n        :param X: Training data.\n        :param y: Target values.\n        :return: self\n        \"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the KernelRidge regression model.\n\n        :param X: Samples.\n        :return: Predicted values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        :param deep: boolean, optional If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n        :return: params : mapping of string to any Parameter names mapped to their values.\n        \"\"\"\n        return self.model.get_params(deep=deep)\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        :param params: dict, optional Parameters.\n        :return: self\n        \"\"\"\n        self.model = clone(self.model).set_params(**params)\n        return self",
        "from pedantic import pedantic, count_calls\nfrom sklearn.linear_model import LinearRegression\n\n@pedantic\nclass LinearRegressionModel:\n    @count_calls\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X, y):\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X):\n        return self.model.predict(X)"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_f, list2_f):\n    i, j = 0, 0\n    merged_list = []\n\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n\n    return merged_list\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list"
      ],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "cumulative_model_output": []
    },
    {
      "session_model_output": [
        "def convert_to_roman(num):\n    \"\"\"\n    Converts an integer to a Roman numeral.\n    :param num: The integer to be converted.\n    :return: The corresponding Roman numeral as a string.\n    \"\"\"\n    try:\n        if not isinstance(num, int) or num <= 0:\n            raise ValueError(\"Input must be a positive integer\")\n        \n        values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n        symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n        roman = \"\"\n        \n        for i in range(len(values)):\n            count = num // values[i]\n            roman += symbols[i] * count\n            num %= values[i]\n        \n        return roman\n    \n    except Exception as e:\n        raise ValueError(f\"An error occurred: {str(e)}\")\n\n\n# Adding comments for explanation\n# Using try-except for error handling\n# Including docstrings for function documentation",
        "class LassoRegression:\n    \"\"\"\n    Lasso Regression is a linear regression technique that includes a regularization term.\n    It can perform feature selection and prevent overfitting.\n    This class provides methods for training the model and making predictions.\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Initialize the Lasso Regression model.\n\n        Parameters:\n            alpha (float): The regularization strength. A higher value increases penalty.\n        \"\"\"\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        \"\"\"\n        Train the Lasso Regression model using the given training data.\n\n        Parameters:\n            X (ndarray): Training input features of shape (n_samples, n_features).\n            y (ndarray): Target values of shape (n_samples,).\n\n        Returns:\n            None\n        \"\"\"\n        # Add comments explaining the code and calculations\n        import numpy as np\n\n        # Convert input data to numpy arrays\n        X = np.array(X)\n        y = np.array(y)\n\n        # Number of training samples and features\n        n_samples, n_features = X.shape\n\n        # Add a column of ones to X for the intercept\n        X_with_intercept = np.c_[np.ones(n_samples), X]\n\n        # Coefficient matrix\n        beta = np.zeros(n_features + 1)\n\n        # Closed-form solution for Lasso Regression\n        try:\n            XtX = X_with_intercept.T @ X_with_intercept\n            XtX_inv = np.linalg.inv(XtX)\n            Xty = X_with_intercept.T @ y\n            beta = XtX_inv @ Xty\n        except np.linalg.LinAlgError:\n            print(\"Singular matrix error. Unable to invert the matrix.\")\n\n        # Apply L1 regularization\n        beta[1:] *= (1 - self.alpha / np.sum(X ** 2, axis=0))\n\n        self.coef_ = beta[1:]\n        self.intercept_ = beta[0]\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values using the trained Lasso Regression model.\n\n        Parameters:\n            X (ndarray): Input features of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        # Input data validation\n        if self.coef_ is None or self.intercept_ is None:\n            raise ValueError(\"Model has not been trained yet. Call the train method first.\")\n\n        # Convert input data to numpy array\n        X = np.array(X)\n\n        # Add a column of ones to X for the intercept\n        X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n\n        # Calculate predictions\n        predictions = X_with_intercept @ self.coef_ + self.intercept_\n\n        return predictions"
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman",
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "cumulative_model_output": [
        "```python\nimport bz2\nfrom pedantic import count_calls, trace_class, timer_class\n\n@trace_class\n@timer_class\nclass QuadraticDiscriminantAnalysis:\n    \"\"\"\n    Quadratic Discriminant Analysis (QDA) class for multivariate classification.\n    \"\"\"\n\n    def __init__(self, _f):\n        \"\"\"\n        Initialize the QDA model.\n\n        Parameters:\n            _f (int): Flag indicating whether to use formula '_f' for calculations.\n        \"\"\"\n        self._f = _f\n\n    @count_calls\n    def fit(self, X, y):\n        \"\"\"\n        Fit the QDA model to the training data.\n\n        Parameters:\n            X (ndarray): Training data of shape (n_samples, n_features).\n            y (ndarray): Target vector of shape (n_samples,).\n        \"\"\"\n        # Your fit method implementation here\n\n    @count_calls\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the provided data.\n\n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted class labels of shape (n_samples,).\n        \"\"\"\n        # Your predict method implementation here\n\n    @count_calls\n    def score(self, X, y):\n        \"\"\"\n        Evaluate the model's performance using the data and target values.\n\n        Parameters:\n            X (ndarray): Data of shape (n_samples, n_features).\n            y (ndarray): Target vector of shape (n_samples,).\n\n        Returns:\n            float: The mean accuracy of the model.\n        \"\"\"\n        # Your score method implementation here\n\n    # Add more methods as needed for your QDA class\n```",
        "```python\nfrom collections import OrderedDict\n\ndef int_to_roman(num: int) -> str:\n    \"\"\"\n    Convert an integer to its Roman numeral representation.\n\n    :param num: The integer to be converted.\n    :return: The Roman numeral representation of the input integer.\n    \"\"\"\n    roman_map = OrderedDict()\n    roman_map[1000] = \"M\"\n    roman_map[900] = \"CM\"\n    roman_map[500] = \"D\"\n    roman_map[400] = \"CD\"\n    roman_map[100] = \"C\"\n    roman_map[90] = \"XC\"\n    roman_map[50] = \"L\"\n    roman_map[40] = \"XL\"\n    roman_DDDB = \"X\"\n    roman_map[5] = \"V\"\n    roman_map[4] = \"IV\"\n    roman_map[1] = \"I\"\n\n    roman_numeral = \"\"\n    for denomination, symbol in roman_map.items():\n        while num >= denomination:\n            roman_numeral += symbol\n            num -= denomination\n\n    return roman_numeral\n```",
        "```python\nimport numpy as np\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_squared_error\nfrom bz2 import BZ2File\n\n# Function naming convention\ndef function_name_d():\n    # Function logic goes here\n    pass\n\n# Class decorator for performance analysis\n@timer_class\nclass KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression Class\n    \"\"\"\n\n    # Class decorator for class usage tracking and documentation\n    @trace_class\n    def __init__(self, alpha=1.0, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None,\n                 tol=1e-3, fit_intercept=True, normalize=False, copy_X=True, verbose=False):\n        \"\"\"\n        Initialize the Kernel Ridge Regression model.\n\n        Parameters:\n        alpha (float): Regularization strength; must be a positive float. The penalty is typically of the form\n            alpha * ||weight||^2_2.\n        kernel (str or callable): Kernel mapping used internally. Passed to pairwise_kernels.kernel.\n        gamma (float): Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels.\n            Interpretation of the default value is left to the kernel; see the kernel documentation.\n        degree (int): Degree of the polynomial kernel. Ignored by all other kernels.\n        coef0 (float): Independent term in the polynomial kernel. Only significant if polynomial kernel is used.\n        kernel_params (dict): Additional parameters (keyword arguments) for the kernel function.\n        tol (float): Tolerance for stopping criterion.\n        fit_intercept (bool): Whether to fit an intercept term. If False, the data is assumed to be already centered.\n        normalize (bool): This parameter is ignored when fit_intercept is set to False. If True, the regressors\n            X will be normalized before regression.\n        copy_X (bool): If True, X will be copied; else, it may be overwritten.\n        verbose (bool): If True, the timings of fitting will be displayed.\n\n        Returns:\n        None\n        \"\"\"\n        self.alpha = alpha\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.tol = tol\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.verbose = verbose\n\n        # Method decorator for tracking method invocations\n        @count_calls\n        def fit(self, X, y):\n            \"\"\"\n            Fit the model using X as training data and y as target values.\n\n            Parameters:\n                X (array-like): Training data. Will be converted to an array if necessary.\n                y (array-like): Target values. Will be converted to an array if necessary.\n\n            Returns:\n                self: Fitted estimator.\n            \"\"\"\n            # Perform error handling using try-except block\n            try:\n                # Check if y is compressed using bz2\n                if isinstance(y, BZ2File):\n                    y = y.read()\n\n                # Check if X is compressed using bz2\n                if isinstance(X, BZ2File):\n                    X = X.read()\n\n                # Convert X and y to numpy arrays\n                X = np.array(X)\n                y = np.array(y)\n\n                # Check if the number of samples in X and y match\n                if X.shape[0] != y.shape[0]:\n                    raise ValueError(\"X and y have unequal number of samples\")\n\n                # Check for nan values in X and y\n                if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n                    raise ValueError(\"X or y contain nan values\")\n\n                # Fit the KernelRidge model\n                self.model = KernelRidge(\n                    alpha=self.alpha, kernel=self.kernel, gamma=self.gamma, degree=self.degree, coef0=self.coef0,\n                    kernel_params=self.kernel_params, tol=self.tol, fit_intercept=self.fit_intercept,\n                    normalize=self.normalize, copy_X=self.copy_X, verbose=self.verbose\n                )\n                self.model.fit(X, y)\n\n            # Handle any errors that occurred during the fitting process\n            except Exception as e:\n                raise ValueError(f\"Error occurred during fitting: {e}\")\n\n        # Method decorator for tracking method invocations\n        @count_calls\n        def predict(self, X):\n            \"\"\"\n            Predict using the linear model.\n\n            Parameters:\n                X (array-like): Samples.\n\n            Returns:\n                C {array} : Returns predicted values.\n            \"\"\"\n            # Perform error handling using try-except block\n            try:\n                # Check if X is compressed using bz2\n                if isinstance(X, BZ2File):\n                    X = X.read()\n\n                # Convert X to a numpy array\n                X = np.array(X)\n\n                # Check for nan values in X\n                if np.any(np.isnan(X)):\n                    raise ValueError(\"X contains nan values\")\n\n                # Check if the model has been fitted\n                if not hasattr(self, 'model'):\n                    raise ValueError(\"Model has not been fitted yet\")\n\n                # Predict using the fitted model\n                return self.model.predict(X)\n\n            # Handle any errors that occurred during the prediction process\n            except Exception as e:\n                raise ValueError(f\"Error occurred during prediction: {e}\")\n\n        # Method decorator for tracking method invocations\n        @count_calls\n        def score(self, X, y):\n            \"\"\"\n            Returns the coefficient of determination R^2 of the prediction.\n\n            The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum()\n            and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\n            The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n            A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n\n            Parameters:\n                X (array-like): Test samples.\n                y (array-like): True values for X.\n\n            Returns:\n                score (float): R^2 of self.predict(X) wrt. y.\n            \"\"\"\n            # Perform error handling using try-except block\n            try:\n                # Check if y is compressed using bz2\n                if isinstance(y, BZ2File):\n                    y = y.read()\n\n                # Check if X is compressed using bz2\n                if isinstance(X, BZ2File):\n                    X = X.read()\n\n                # Convert X and y to numpy arrays\n                X = np.array(X)\n                y = np.array(y)\n\n                # Check if the number of samples in X and y match\n                if X.shape[0] != y.shape[0]:\n                    raise ValueError(\"X and y have unequal number of samples\")\n\n                # Check for nan values in X and y\n                if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n                    raise ValueError(\"X or y contain nan values\")\n\n                # Check if the model has been fitted\n                if not hasattr(self, 'model'):\n                    raise ValueError(\"Model has not been fitted yet\")\n\n                # Calculate the R^2 score\n                y_pred = self.model.predict(X)\n                return 1 - mean_squared_error(y, y_pred) / np.var(y)\n\n            # Handle any errors that occurred during the scoring process\n            except Exception as e:\n                raise ValueError(f\"Error occurred during scoring: {e}\")\n```",
        "```python\nfrom pedantic import count_calls, trace_class, timer_class\n\n@timer_class\n@trace_class\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists and returns a new sorted list.\n\n    Args:\n        list1 (list[int]): The first sorted list.\n        list2 (list[int]): The second sorted list.\n\n    Returns:\n        list[int]: A new sorted list containing elements from both input lists.\n    \"\"\"\n    result = []\n    i, j = 0, 0\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            result.append(list1[i])\n            i += 1\n        else:\n            result.append(list2[j])\n            j += 1\n    result.extend(list1[i:])\n    result.extend(list2[j:])\n    return result\n```",
        "```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom pedantic import count_calls, trace_class, timer_class\nimport bz2\n\n\n@trace_class\n@timer_class\nclass LinearRegression:\n    \"\"\"\n    A simple linear regression class with train and predict methods.\n    \"\"\"\n\n    def __init__(self):\n        self.model = LinearRegression()\n\n    @count_calls\n    def train(self, X: np.array, y: np.array) -> None:\n        \"\"\"\n        Train the linear regression model.\n\n        Args:\n            X (np.array): The input features of shape (n_samples, n_features).\n            y (np.array): The target values of shape (n_samples,).\n        \"\"\"\n        self.model.fit(X, y)\n\n    @count_calls\n    def predict(self, X: np.array) -> np.array:\n        \"\"\"\n        Predict the target values using the trained linear regression model.\n\n        Args:\n            X (np.array): The input features of shape (n_samples, n_features).\n\n        Returns:\n            np.array: Predicted target values of shape (n_samples,).\n        \"\"\"\n        return self.model.predict(X)\n```",
        "```python\ndef sum_array_except_self(nums):\n    length = len(nums)\n    left_sum, right_sum = [0] * length, [0] * length\n\n    left_sum[0] = nums[0]\n    for i in range(1, length):\n        left_sum[i] = left_sum[i - 1] + nums[i]\n\n    right_sum[length - 1] = nums[-1]\n    for i in range(length - 2, -1, -1):\n        right_sum[i] = right_sum[i + 1] + nums[i]\n\n    return [left_sum[i] + right_sum[i] - nums[i] for i in range(length)]\n```",
        "```python\nimport numpy as np\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils.validation import check_consistent_length\nfrom sklearn.linear_model import Lasso\nfrom bz2 import *\n\nclass LassoRegression(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Lasso Regression Class\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength; must be a positive float.\n        Larger values specify stronger regularization.\n        alpha = 0 indicates no regularization.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If False, the data\n        is assumed to be already centered.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in R^p, or W in R^p x q).\n\n    intercept_ : array, shape (n_targets,)\n        Independent term in decision function.\n\n    n_features_in_ : int\n        Number of features seen during fit.\n\n    feature_names_in_ : list\n        Names of features seen during fit. Defined only when X has feature names that are all strings.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoRegression\n    >>> clf = LassoRegression(alpha=0.1)\n    >>> clf.fit([[1, 2], [3, 4]], [1, 2]) # doctest: +NORMALIZE_WHITESPACE\n    LassoRegression()\n    >>> print(clf.coef_)\n    [[0.85]\n     [0.0 ]]\n    >>> print(clf.intercept_)\n    [0.25]\n    >>> print(clf.predict([[3, 4]]))\n    [1.]\n    \"\"\"\n\n    def __init__(self, alpha=1.0, fit_intercept=True, copy_X=True):\n        self.alpha = alpha\n        self.fit_intercept = fit_intercept\n        self.copy_X = copy_X\n\n    def fit(self, X, y):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data. Passes untouched to the underlying algorithm.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Check data\n        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], multi_output=True)\n\n        # Check parameters\n        if not isinstance(self.alpha, (float, int)):\n            raise ValueError(\"alpha must be a float or an int\")\n        if self.alpha < 0:\n            raise ValueError(\"alpha must be positive\")\n\n        # Store the classes seen during fit\n        self.classes_ = np.unique(y)\n\n        # Reset the parameters already seen\n        self.coef_ = None\n        self.intercept_ = None\n\n        # Reset the parameter n_features_in_\n        self._n_features = None\n\n        # Reset the parameter feature_names_in_\n        self._feature_names = None\n\n        # Store the number of classes for multi-output\n        if y.ndim == 2:\n            n_outputs = y.shape[1]\n        else:\n            n_outputs = 1\n\n        # Check consistent length\n        check_consistent_length(X, y, reset=True)\n\n        # Store the number of features\n        if self._n_features is None:\n            self._n_features = X.shape[1]\n\n        # Store the feature names\n        if hasattr(X, 'feature_names') and X.feature_names is not None:\n            self._feature_names = X.feature_names\n\n        # Check the number of features\n        if self._n_features != X.shape[1]:\n            raise ValueError(\"X has %d features, expected %d\"\n                             % (X.shape[1], self._n_features))\n\n        # Copy X if copy_X is True\n        if self.copy_X:\n            X = X.copy()\n\n        # Check the number of samples\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y have inconsistent dimensions (%d vs %d samples)\"\n                             % (X.shape[0], y.shape[0]))\n\n        # Check alpha\n        if self.alpha == 0:\n            # No regularization, use the built-in Lasso\n            self.lasso_ = Lasso(fit_intercept=self.fit_intercept, alpha=self.alpha,\n                                copy_X=False)\n            self.lasso_.fit(X, y)\n        else:\n            # Add small amount to the diagonal to ensure invertibility of X.T X\n            XTX = np.dot(X.T, X)\n            XTX += self.alpha * np.eye(XTX.shape[0], dtype=XTX.dtype)\n\n            if self.fit_intercept:\n                X_intercept = np.ones((X.shape[0], 1), dtype=X.dtype)\n                X = np.hstack((X_intercept, X))\n\n                # Solution for beta\n                beta = np.linalg.solve(XTX, np.dot(X.T, y))\n\n                # Remove the first element, which corresponds to the bias\n                self.intercept_ = beta[0]\n                self.coef_ = beta[1:]\n            else:\n                # Solution for beta\n                self.coef_ = np.linalg.solve(XTX, np.dot(X.T, y))\n\n        # Store the number of targets for multi-output\n        self.n_targets_ = n_outputs\n\n        return self\n\n    @_timer_class(\"_predict\")\n    @_trace_class(\"_predict\")\n    @_count_calls(\"_predict\")\n    def predict(self, X):\n        \"\"\"Predict using the linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y : array, shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, ['coef_', 'intercept_'])\n\n        # Input validation\n        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n\n        # Check the number of features\n        if X.shape[1] != self._n_features:\n            raise ValueError(\"X has %d features, expected %d\"\n                             % (X.shape[1], self._n_features))\n\n        # Add the intercept if necessary\n        if self.fit_intercept:\n            X_intercept = np.ones((X.shape[0], 1), dtype=X.dtype)\n            X = np.hstack((X_intercept, X))\n\n        # Compute the prediction\n        return np.dot(X, self.coef_.T) + self.intercept_\n```",
        "```python\ndef merge_sort(arr: list[int]) -> list[int]:\n    \"\"\"\n    Sorts a list of integers using the merge sort algorithm.\n\n    Args:\n        arr (list[int]): The list of integers to be sorted.\n\n    Returns:\n        list[int]: The sorted list of integers.\n    \"\"\"\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n\n    left = merge_sort(left)\n    right = merge_sort(right)\n\n    return merge(left, right)\n\ndef merge(left: list[int], right: list[int]) -> list[int]:\n    \"\"\"\n    Merges two sorted lists into a single sorted list.\n\n    Args:\n        left (list[int]): A sorted list of integers.\n        right (list[int]): Another sorted list of integers.\n\n    Returns:\n        list[int]: A single sorted list containing elements from both input lists.\n    \"\"\"\n    result = []\n    i, j = 0, 0\n\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n```"
      ]
    }
  ]
}