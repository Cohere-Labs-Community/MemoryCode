{
  "sessions": [
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def append(self, data_i):\n        if not self.head_i:\n            self.head_i = Node(data_i)\n        else:\n            curr_i = self.head_i\n            while curr_i.next_i:\n                curr_i = curr_i.next_i\n            curr_i.next_i = Node(data_i)\n\n    def delete(self, data_i):\n        if self.head_i and self.head_i.data_i == data_i:\n            self.head_i = self.head_i.next_i\n        else:\n            prev_i = self.head_i\n            curr_i = self.head_i.next_i\n            while curr_i and curr_i.data_i != data_i:\n                prev_i = curr_i\n                curr_i = curr_i.next_i\n            if curr_i:\n                prev_i.next_i = curr_i.next_i\n\n    def display(self):\n        curr_i = self.head_i\n        while curr_i:\n            print(curr_i.data_i, end=\" -> \")\n            curr_i = curr_i.next_i\n        print(\"None\")\n\nclass Node:\n    def __init__(self, data_i):\n        self.data_i = data_i\n        self.next_i = None\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_i):\n        self.data_i = data_i\n        self.next_i = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head_i = None\n\n    def append(self, data_i):\n        if not self.head_i:\n            self.head_i = Node(data_i)\n        else:\n            curr_i = self.head_i\n            while curr_i.next_i:\n                curr_i = curr_i.next_i\n            curr_i.next_i = Node(data_i)\n\n    def delete(self, data_i):\n        if self.head_i is None:\n            return\n\n        if self.head_i.data_i == data_i:\n            self.head_i = self.head_i.next_i\n            return\n\n        prev_i = self.head_i\n        curr_i = self.head_i.next_i\n\n        while curr_i:\n            if curr_i.data_i == data_i:\n                prev_i.next_i = curr_i.next_i\n                return\n            prev_i = curr_i\n            curr_i = curr_i.next_i\n\n    def display(self):\n        curr_i = self.head_i\n        while curr_i:\n            print(curr_i.data_i, end=\" -> \")\n            curr_i = curr_i.next_i\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def is_valid_bst_md(root):\n    def helper(node, min_val, max_val):\n        nonlocal valid\n        if node is None:\n            return True\n        try:\n            if node.val <= min_val or node.val >= max_val:\n                valid = False\n                return False\n            return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n        except AttributeError:\n            print(\"Error: Invalid node structure. Please ensure each node has 'val', 'left', and 'right' attributes.\")\n            return False\n\n    valid = True\n    helper(root, float('-inf'), float('inf'))\n    return valid",
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def shortest_path_dijkstra(self, source):\n        try:\n            dist = [float('inf') for _ in range(self.V)]\n            dist[source] = 0\n            spt_set = [False] * self.V\n\n            for _ in range(self.V):\n                u = self.min_distance(dist, spt_set)\n                spt_set[u] = True\n                for v in range(self.V):\n                    if (\n                        self.graph[u][v] > 0\n                        and spt_set[v] is False\n                        and dist[v] > dist[u] + self.graph[u][v]\n                    ):\n                        dist[v] = dist[u] + self.graph[u][v]\n\n            # Print the shortest distance from the source to all other nodes\n            for i in range(self.V):\n                print(f\"Node {i} is at distance {dist[i]} from source node {source}\")\n\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n\n    def min_distance(self, dist, spt_set):\n        # Find the node with the minimum distance value that is not yet processed\n        # Explore the nearest node which is not in the sptSet\n        min_dist = float('inf')\n        min_node = -1\n        for u in range(self.V):\n            if spt_set[u] is False and dist[u] < min_dist:\n                min_dist = dist[u]\n                min_node = u\n\n        return min_node\n```"
      ],
      "instruction_model_output": [
        "def is_valid_bst(root):\n    def helper(node, min_val, max_val):\n        if not node:\n            return True\n        if not min_val < node.val < max_val:\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    try:\n        return helper(root, float('-inf'), float('inf'))\n    except AttributeError:\n        return \"Error: Invalid input. The input is not a valid binary tree node.\"",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_md(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_md(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        for _ in range(self.V):\n            u = self.min_distance_node_md(dist)\n            for v, weight in self.graph[u]:\n                if dist[u] + weight < dist[v]:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n\n        path = []\n        node = dest\n        while parent[node] != -1:\n            path.append(node)\n            node = parent[node]\n        path.append(node)\n        path.reverse()\n        return path, dist[dest]\n\n    def min_distance_node_md(self, dist):\n        min_dist = float('inf')\n        min_node = -1\n        for v in range(self.V):\n            if dist[v] < min_dist:\n                min_dist = dist[v]\n                min_node = v\n        return min_node"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nfrom pedantic import rename_kwargs\n\n\nclass Solution:\n    @rename_kwargs\n    def max_subarray_sum(self, nums: list[int], /, *, num_elements: int = 1) -> int:\n        \"\"\"\n        Solve the maximum subarray problem using Kadane's algorithm.\n\n        Args:\n            nums (list[int]): The input array of integers.\n            num_elements (int): The number of elements in the maximum subarray to find. Defaults to 1.\n\n        Returns:\n            int: The sum of the maximum subarray with the specified number of elements.\n        \"\"\"\n        assert num_elements > 0, \"Number of elements must be greater than 0.\"\n        assert num_elements <= len(\n            nums\n        ), \"Number of elements cannot be greater than the array length.\"\n\n        max_sum = float(\"-inf\")\n        current_sum = 0\n\n        for num in nums:\n            current_sum = max(current_sum + num, num)\n            if current_sum > max_sum:\n                max_sum = current_sum\n\n        return max_sum\n```",
        "from pedantic import @rename_kwargs\n\n@rename_kwargs\ndef compute_diagonal_average(matrix: list[list[float]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[float]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n\n    Assert:\n        - All rows in the matrix have the same length.\n        - The matrix is square (number of rows equals the number of columns).\n    \"\"\"\n    assert len(set(len(row) for row in matrix)) == 1, \"Input matrix is not rectangular.\"\n    assert len(matrix) == len(matrix[0]), \"Input matrix is not square.\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "instruction_model_output": [
        "from pedantic import rename_kwargs\r\n\r\n\r\nclass Solution:\r\n    @rename_kwargs\r\n    def maxSubArray(self, nums: list[int]) -> int:\r\n        max_sum = current_sum = nums[0]\r\n        for num in nums[1:]:\r\n            current_sum = max(num, current_sum + num)\r\n            max_sum = max(max_sum, current_sum)\r\n        return max_sum",
        "def average_diagonal(matrix):\n    assert all(len(row) == len(matrix) for row in matrix), \"Input must be a square matrix\"\n    assert len(matrix) > 0, \"Matrix must not be empty\"\n\n    diagonal_sum = sum(matrix[i][i] for i in range(len(matrix)))\n    return diagonal_sum / len(matrix)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LassoRegression:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        self.coef_ = None\n        self.intercept_ = None\n\n    def train(self, X, y):\n        try:\n            from sklearn.linear_model import Lasso\n            import numpy as np\n\n            self.model = Lasso(alpha=self.alpha)\n            self.model.fit(X, y)\n            self.coef_ = self.model.coef_\n            self.intercept_ = self.model.intercept_\n        except ImportError as e:\n            raise ImportError(\n                \"Required module 'sklearn' is not installed. Please install it using 'pip install scikit-learn'.\") from e\n        except ValueError as ve:\n            raise ValueError(\n                \"Invalid input data. Ensure X is a 2D array and y is a 1D array with compatible dimensions.\") from ve\n        except Exception as ex:\n            raise Exception(\n                \"An unexpected error occurred while training the Lasso Regression model.\") from ex\n\n    def predict(self, X):\n        try:\n            if self.coef_ is None or self.intercept_ is None:\n                raise ValueError(\n                    \"Model has not been trained yet. Please call the train method before predicting.\")\n            import numpy as np\n\n            return np.dot(X, self.coef_.T) + self.intercept_\n        except ValueError as ve:\n            raise ValueError(\n                \"Invalid input data or model not trained. Ensure X is a 2D array with compatible dimensions.\") from ve\n        except Exception as ex:\n            raise Exception(\n                \"An unexpected error occurred while making predictions with the Lasso Regression model.\") from ex"
      ],
      "instruction_model_output": [
        "```python\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        try:\n            self.model.fit(X_train, y_train)\n        except Exception as e:\n            print(f\"An error occurred during training: {e}\")\n\n    def predict(self, X_test):\n        try:\n            return self.model.predict(X_test)\n        except Exception as e:\n            print(f\"An error occurred during prediction: {e}\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def min_distance(self, dist, spt_set, u):\n        min_ = 1000000000000000000\n        for v in range(self.V):\n            if v not in spt_set and self.graph[u][v] != 0 and dist[v] < min_:\n                min_ = dist[v]\n        return min_\n\n    def dijkstra(self, src):\n        dist = [1000000000000000000] * self.V\n        dist[src] = 0\n        spt_set = []\n\n        for _ in range(self.V):\n            u = self.min_distance_o(dist, spt_set)\n            spt_set.append(u)\n            for v in range(self.V):\n                if self.graph[u][v] > 0 and dist[u] != 1000000000000000000 and dist[u] + self.graph[u][v] < dist[v]:\n                    dist[v] = dist[u] + self.graph[u][v]\n\n        return dist\n\n    def add_edge(self, u, v, weight):\n        self.graph[u][v] = weight\n        self.graph[v][u] = weight\n\n    def min_distance_o(self, dist, spt_set):\n        min_ = 1000000000000000000\n        for v in range(self.V):\n            if v not in spt_set and self.graph[u][v] != 0 and dist[v] < min_:\n                min_ = dist[v]\n        return min_\n```"
      ],
      "instruction_model_output": [
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_o(self, u, v, w):\n        self.graph[u].append((v, w))\n        self.graph[v].append((u, w))\n\n    def shortest_path_o(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    result_j = []\n    i, j = 0, 0\n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            result_j.append(list1_j[i])\n            i += 1\n        else:\n            result_j.append(list2_j[j])\n            j += 1\n    result_j.extend(list1_j[i:])\n    result_j.extend(list2_j[j:])\n    return result_j",
        "class LinearRegression:\n    def __init__(self, theta_j, x, y):\n        self.theta_j = theta_j\n        self.x = x\n        self.y = y\n        self.m, self.n = self._get_matrix_dimensions(x)\n\n    def _get_matrix_dimensions(self, matrix):\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n        return num_rows, num_cols\n\n    def hypothesis(self, x):\n        return self.theta_j @ x.T\n\n    def mean_squared_error(self):\n        h_x = self.hypothesis(self.x)\n        errors = h_x - self.y\n        squared_errors = errors ** 2\n        mse = squared_errors.mean()\n        return mse\n\n    def gradient_descent(self, alpha, iterations):\n        for _ in range(iterations):\n            gradients = (self.x @ self.theta_j) * self.x - self.x @ self.y\n            self.theta_j = self.theta_j - alpha * gradients.mean(axis=1)\n\n    def train(self, alpha, iterations):\n        self.gradient_descent(alpha, iterations)"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_j, list2_j):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_j) and j < len(list2_j):\n        if list1_j[i] < list2_j[j]:\n            merged_list.append(list1_j[i])\n            i += 1\n        else:\n            merged_list.append(list2_j[j])\n            j += 1\n    \n    while i < len(list1_j):\n        merged_list.append(list1_j[i])\n        i += 1\n    \n    while j < len(list2_j):\n        merged_list.append(list2_j[j])\n        j += 1\n    \n    return merged_list",
        "import numpy as np\n\nclass LinReg:\n    def __init__(self):\n        self.w_0 = None\n        self.w_1 = None\n\n    def fit(self, X, y):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        self.w_ = np.linalg.inv(X_.T @ X_) @ X_.T @ y\n        self.w_0, self.w_1 = self.w_\n\n    def predict(self, X):\n        X0 = np.ones((X.shape[0],))\n        X_ = np.column_stack((X0, X))\n        return X_ @ self.w_"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from sklearn.decomposition import PCA\n\nclass 7PCA:\n    def __init__(self, n_components=None):\n        self.n_components = n_components\n        self.model = PCA(n_components=n_components)\n\n    def fit(self, X, y=None):\n        self.model.fit(X, y)\n        return self\n\n    def transform(self, X):\n        return self.model.transform(X)\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)"
      ],
      "instruction_model_output": [
        "class PCA0:\n    def __init__(self, n_components=0):\n        self.n_components = n_components\n\n    def fit_transform(self, X):\n        X_centered = X - X.mean(axis=0)\n        covariance = np.cov(X_centered, rowvar=False)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n        idx = eigenvalues.argsort()[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        components = eigenvectors[:, :self.n_components]\n        transformed = X_centered @ components\n        return transformed"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef merge_sorted_lists(list1_f, list2_f) -> list:\n    merged_list_f = []\n    i, j = 0, 0\n\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list_f.append(list1_f[i])\n            i += 1\n        else:\n            merged_list_f.append(list2_f[j])\n            j += 1\n\n    while i < len(list1_f):\n        merged_list_f.append(list1_f[i])\n        i += 1\n\n    while j < len(list2_f):\n        merged_list_f.append(list2_f[j])\n        j += 1\n\n    return merged_list_f\n```",
        "```python\nclass LinkedList:\n    def __init__(self):\n        self.head_f = None\n        self.tail_f = None\n\n    def append_node(self, value_f):\n        new_node_f = Node(value_f)\n        if not self.head_f:\n            self.head_f = new_node_f\n            self.tail_f = new_node_f\n        else:\n            self.tail_f.next_node_f = new_node_f\n            self.tail_f = new_node_f\n\n    def delete_node(self, value_f):\n        current_node_f = self.head_f\n        if current_node_f.value_f == value_f:\n            self.head_f = current_node_f.next_node_f\n            return\n        prev_node_f = current_node_f\n        current_node_f = current_node_f.next_node_f\n        while current_node_f:\n            if current_node_f.value_f == value_f:\n                prev_node_f.next_node_f = current_node_f.next_node_f\n                return\n            prev_node_f = current_node_f\n            current_node_f = current_node_f.next_node_f\n\n    def display(self):\n        current_node_f = self.head_f\n        while current_node_f:\n            print(current_node_f.value_f, end=\" -> \")\n            current_node_f = current_node_f.next_node_f\n        print(\"None\")\n\nclass Node:\n    def __init__(self, value_f):\n        self.value_f = value_f\n        self.next_node_f = None\n```"
      ],
      "instruction_model_output": [
        "def merge_sorted_lists(list1_f, list2_f):\n    merged_list = []\n    i, j = 0, 0\n    while i < len(list1_f) and j < len(list2_f):\n        if list1_f[i] < list2_f[j]:\n            merged_list.append(list1_f[i])\n            i += 1\n        else:\n            merged_list.append(list2_f[j])\n            j += 1\n    while i < len(list1_f):\n        merged_list.append(list1_f[i])\n        i += 1\n    while j < len(list2_f):\n        merged_list.append(list2_f[j])\n        j += 1\n    return merged_list",
        "```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next_node_s = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_node_s = None\n        self.tail_node_s = None\n        self.size_s = 0\n\n    def append(self, data):\n        new_node_s = Node(data)\n        if not self.head_node_s:\n            self.head_node_s = new_node_s\n            self.tail_node_s = self.head_node_s\n        else:\n            current_node_s = self.head_node_s\n            while current_node_s.next_node_s:\n                current_node_s = current_node_s.next_node_s\n            current_node_s.next_node_s = new_node_s\n            self.tail_node_s = new_node_s\n        self.size_s += 1\n\n    def delete(self, data):\n        current_node_s = self.head_node_s\n        prev_node_s = None\n        while current_node_s and current_node_s.data == data:\n            prev_node_s = current_node_s\n            current_node_s = current_node_s.next_node_s\n        if prev_node_s:\n            prev_node_s.next_node_s = current_node_s.next_node_s\n            if current_node_s == self.tail_node_s:\n                self.tail_node_s = prev_node_s\n            self.size_s -= 1\n\n    def display(self):\n        current_node_s = self.head_node_s\n        while current_node_s:\n            print(current_node_s.data, end=\" -> \")\n            current_node_s = current_node_s.next_node_s\n        print(\"None\")\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef longest_common_prefix(strs):\n    \"\"\"\n    Find the longest common prefix string among a list of strings.\n\n    Args:\n        strs (list): A list of strings.\n\n    Returns:\n        str: The longest common prefix among the input strings.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    # Sort the list to ensure consistent results\n    strs.sort()\n\n    # Get the first and last string in the sorted list\n    first = strs[0]\n    last = strs[-1]\n\n    # Find the common prefix by comparing characters at each position\n    common_prefix = \"\"\n    for i in range(len(first)):\n        if i < len(last) and first[i] == last[i]:\n            common_prefix += first[i]\n        else:\n            break\n\n    return common_prefix\n```"
      ],
      "instruction_model_output": [
        "```python\ndef longest_common_prefix(strs: list[str]) -> str:\n    \"\"\"\n    Returns the longest common prefix string in a list of strings.\n\n    Args:\n        strs (list[str]): A list of input strings.\n\n    Returns:\n        str: The longest common prefix string.\n    \"\"\"\n    if not strs:\n        return \"\"\n\n    for i, char in enumerate(strs[0]):\n        for other in strs[1:]:\n            if i == len(other) or other[i] != char:\n                return strs[0][:i]\n    return strs[0]\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    \"\"\"\n    A binary tree data structure.\n    \"\"\"\n\n    def __init__(self):\n        self.root = None\n\n    def insert_node_x(self, root_x, value_x):\n        \"\"\"\n        Inserts a new node with the given value into the binary tree.\n\n        Args:\n            root_x (BinaryTreeNode): The root node of the binary tree.\n            value_x (int): The value to be inserted into the binary tree.\n        \"\"\"\n        if root_x is None:\n            return BinaryTreeNode(value_x)\n\n        if value_x < root_x.value:\n            root_x.left = self.insert_node_x(root_x.left, value_x)\n        elif value_x > root_x.value:\n            root_x.right = self.insert_node_x(root_x.right, value_x)\n\n        return root_x\n\n    def delete_node_x(self, root_x, key_x):\n        \"\"\"\n        Deletes the node with the given key from the binary tree.\n\n        Args:\n            root_x (BinaryTreeNode): The root node of the binary tree.\n            key_x (int): The value of the node to be deleted.\n        \"\"\"\n        if root_x is None:\n            return root_x\n\n        if key_x < root_x.value:\n            root_x.left = self.delete_node_x(root_x.left, key_x)\n        elif key_x > root_x.value:\n            root_x.right = self.delete_node_x(root_x.right, key_x)\n        else:\n            if root_x.left is None:\n                return root_x.right\n            elif root_x.right is None:\n                return root_x.left\n\n            root_x.value = self._get_min_value_x(root_x.right)\n            root_x.right = self.delete_node_x(root_x.right, root_x.value)\n\n        return root_x\n\n    def _get_min_value_x(self, node_x):\n        \"\"\"\n        Helper function to find the minimum value in a binary tree.\n\n        Args:\n            node_x (BinaryTreeNode): The current node.\n\n        Returns:\n            int: The minimum value in the binary tree.\n        \"\"\"\n        current_x = node_x\n        while current_x.left is not None:\n            current_x = current_x.left\n        return current_x.value\n\nclass BinaryTreeNode:\n    \"\"\"\n    A node in the binary tree.\n    \"\"\"\n\n    def __init__(self, value_x):\n        self.value = value_x\n        self.left = None\n        self.right = None",
        "```python\nclass Graph:\n    \"\"\"\n    A Graph class represents a network of nodes and edges.\n    \"\"\"\n\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    def add_edge(self, u: int, v: int, w: int):\n        \"\"\"\n        Adds an edge to the graph between nodes u and v with weight w.\n\n        :param u: The source node.\n        :param v: The destination node.\n        :param w: The weight of the edge.\n        \"\"\"\n        self.graph[u][v] = w\n        self.graph[v][u] = w\n\n    def shortest_path_dijkstra(self, src: int):\n        \"\"\"\n        Computes the shortest path from source node 'src' to all other nodes in the graph using Dijkstra's algorithm.\n\n        :param src: The source node.\n        :return: A list of distances and a list of predecessors for each node in the graph.\n        \"\"\"\n        distances = [float('inf')] * self.V\n        distances[src] = 0\n        predecessors = [-1] * self.V\n\n        for _ in range(self.V):\n            u = self._find_minimum_distance_node(distances)\n            self._relax(u, distances, predecessors)\n\n        return distances, predecessors\n\n    def _find_minimum_distance_node(self, distances):\n        \"\"\"\n        Helper function to find the node with the minimum distance that has not been processed yet.\n\n        :param distances: A list of distances from the source node to all other nodes.\n        :return: The node with the minimum distance.\n        \"\"\"\n        min_distance = float('inf')\n        min_distance_node = -1\n\n        for u in range(self.V):\n            if distances[u] < min_distance and distances[u] != float('inf'):\n                min_distance = distances[u]\n                min_distance_node = u\n\n        return min_distance_node\n\n    def _relax(self, u, distances, predecessors):\n        \"\"\"\n        Updates the distance value and predecessor of neighboring nodes if a shorter path is found.\n\n        :param u: The current node.\n        :param distances: A list of distances from the source node to all other nodes.\n        :param predecessors: A list of predecessors for each node in the graph.\n        \"\"\"\n        for v in range(self.V):\n            if self.graph[u][v] and distances[v] > distances[u] + self.graph[u][v]:\n                distances[v] = distances[u] + self.graph[u][v]\n                predecessors[v] = u\n\n    def find_shortest_path_lengths_x(self, src: int) -> list[int]:\n        \"\"\"\n        Computes the shortest path lengths from the source node 'src' to all other nodes in the graph.\n\n        :param src: The source node.\n        :return: A list of shortest path lengths to all other nodes.\n        \"\"\"\n        distances, _ = self.shortest_path_dijkstra(src)\n        return distances\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        \"\"\"Insert a new node with the given value into the binary tree.\"\"\"\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert_recursive(value, self.root)\n\n    def _insert_recursive(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert_recursive(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert_recursive(value, node.right)\n\n    def delete(self, value):\n        \"\"\"Delete the first node with the given value from the binary tree.\"\"\"\n        self.root = self._delete_recursive(self.root, value)\n\n    def _delete_recursive(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete_recursive(node.left, value)\n        elif value > node.value:\n            node.right = self._delete_recursive(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            node.value = self._get_inorder_successor(node.right)\n            node.right = self._delete_recursive(node.right, node.value)\n\n        return node\n\n    def _get_inorder_successor(self, node):\n        while node.left is not None:\n            node = node.left\n        return node.value\n```",
        "```python\nfrom collections import deque\n\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_x(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n\n    def shortest_path_x(self, src, dest):\n        visited = [False] * self.V\n        path = [0] * self.V\n        dist = [float(\"Inf\")] * self.V\n\n        dist[src] = 0\n        queue = deque()\n        queue.append(src)\n\n        while queue:\n            u = queue.popleft()\n            if u == dest:\n                break\n            for v in self.graph[u]:\n                if not visited[v]:\n                    queue.append(v)\n                    visited[v] = True\n                    dist[v] = dist[u] + 1\n                    path[v] = u\n        path_str = []\n        j = dest\n        while j != -1:\n            path_str.insert(0, j)\n            j = path[j]\n        return dist[dest], path_str\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.s_head = None\n        self.s_tail = None\n\n    def d_get(self, key: int) -> int:\n        if key not in self.cache:\n            return -1\n        value = self.cache[key]\n        self._move_to_s_head(key)\n        return value\n\n    def d_put(self, key: int, value: int) -> None:\n        if self.capacity == 0:\n            return\n        if key in self.cache:\n            self._move_to_s_head(key)\n        elif len(self.cache) >= self.capacity:\n            self._evict_s_tail()\n        self.cache[key] = value\n        self._insert_s_head(key)\n\n    def _move_to_s_head(self, key: int) -> None:\n        node = self.cache[key]\n        self._remove_from_s_dll(key)\n        self._insert_s_head(key)\n\n    def _insert_s_head(self, key: int) -> None:\n        node = self.cache[key]\n        node.s_prev = None\n        node.s_next = self.s_head\n        if self.s_head is not None:\n            self.s_head.s_prev = node\n        self.s_head = node\n        if self.s_tail is None:\n            self.s_tail = node\n\n    def _evict_s_tail(self) -> None:\n        key = self.s_tail.key\n        self._remove_from_s_dll(key)\n        del self.cache[key]\n\n    def _remove_from_s_dll(self, key: int) -> None:\n        node = self.cache[key]\n        if node.s_prev is not None:\n            node.s_prev.s_next = node.s_next\n        else:\n            self.s_head = node.s_next\n        if node.s_next is not None:\n            node.s_next.s_prev = node.s_prev\n        else:\n            self.s_tail = node.s_prev\n```",
        "def d_h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "```python\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.s_capacity = capacity\n        self.s_cache = OrderedDict()\n\n    def get(self, key: int) -> int:\n        if key not in self.s_cache:\n            return -1\n        value = self.s_cache.pop(key)\n        self.s_cache[key] = value\n        return value\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.s_cache:\n            self.s_cache.pop(key)\n        elif len(self.s_cache) >= self.s_capacity:\n            self.s_cache.popitem(last=False)\n        self.s_cache[key] = value\n```",
        "def d_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def count_non_zero1(matrix):\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count"
      ],
      "instruction_model_output": [
        "def count_non_zero(matrix):\n    count = 0\n    for row in matrix:\n        count += sum(1 for elem in row if elem != 0)\n    return count"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef dot_product(vec1_chx, vec2_chx):\n    assert len(vec1_chx) == len(vec2_chx), \"Vector lengths must be equal for dot product.\"\n    return sum(x * y for x, y in zip(vec1_chx, vec2_chx))\n```",
        "class NearestNeighborsModel:\n    \"\"\"\n    A nearest neighbors classifier.\n    \"\"\"\n\n    def __init__(self, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_neighbors=5,\n                 weights='uniform', chx_training_data=None, chx_training_labels=None):\n        \"\"\"\n        Initialize the NearestNeighbors classifier.\n\n        Parameters\n        ----------\n        algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n            Algorithm used to compute the nearest neighbors:\n\n            - 'ball_tree' will use :class:`BallTree`\n            - 'kd_tree' will use :class:`KDTree`\n            - 'brute' will use a brute-force search.\n            - 'auto' will attempt to decide the most appropriate algorithm\n              based on the values passed to :meth:`fit` method.\n\n            Note: fitting on sparse input will override the setting of\n            this parameter, using brute force.\n\n        leaf_size : int, optional\n            Leaf size passed to BallTree or KDTree. This can affect the speed\n            of the construction and query, as well as the memory required\n            to store the tree. The optimal value depends on the nature of\n            the problem.\n\n        metric : string or callable, default='minkowski'\n            the distance metric to use for the tree.  The default metric is\n            minkowski, and with p=2 is equivalent to the standard Euclidean\n            metric. See the documentation of :class:`DistanceMetric` for a\n            list of available metrics.\n\n        p : int, optional\n            Power parameter for the Minkowski metric. When p = 1, this is\n            equivalent to using manhattan_distance (l1), and euclidean_distance\n            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n        metric_params : dict, optional\n            Additional keyword arguments for the metric function.\n\n        n_neighbors : int (default = 5)\n            Number of neighbors to use by default for :meth:`k_neighbors` queries.\n\n        weights : {'uniform', 'distance'}, optional\n            weight function used in prediction.  Possible values:\n\n            - 'uniform' : uniform weights.  All points in each neighborhood\n              are weighted equally.\n            - 'distance' : weight points by the inverse of their distance.\n              in this case, closer neighbors of a query point will have a\n              greater influence than neighbors which are further away.\n\n        chx_training_data : array-like of shape (n_samples, n_features)\n            Training data.\n\n        chx_training_labels : array-like of shape (n_samples,), default=None\n            Target values (class labels) for the training data.\n        \"\"\"\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.metric = metric\n        self.p = p\n        self.metric_params = metric_params\n        self.n_neighbors = n_neighbors\n        self.weights = weights\n        self.chx_training_data = chx_training_data\n        self.chx_training_labels = chx_training_labels\n\n    def train(self, chx_training_data, chx_training_labels):\n        \"\"\"\n        Train the NearestNeighbors model.\n\n        Parameters\n        ----------\n        chx_training_data : array-like of shape (n_samples, n_features)\n            Training data.\n\n        chx_training_labels : array-like of shape (n_samples,), default=None\n            Target values (class labels) for the training data.\n        \"\"\"\n        assert chx_training_data is not None, \"Training data cannot be None.\"\n        assert chx_training_labels is not None, \"Training labels cannot be None.\"\n\n        self.chx_training_data = chx_training_data\n        self.chx_training_labels = chx_training_labels\n\n    def predict(self, chx_test_data):\n        \"\"\"\n        Predict the class labels for the test data.\n\n        Parameters\n        ----------\n        chx_test_data : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        predicted_labels : array-like of shape (n_samples,)\n            Predicted class labels for the test data.\n        \"\"\"\n        assert self.chx_training_data is not None, \"Model has not been trained yet.\"\n        assert chx_test_data is not None, \"Test data cannot be None.\"\n\n        # Implement the prediction logic here\n        # ...\n\n        return predicted_labels"
      ],
      "instruction_model_output": [
        "```python\ndef dot_product(chx_vector1, chx_vector2):\n    return sum(a * b for a, b in zip(chx_vector1, chx_vector2))\n```",
        "```python\nclass NearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of samples\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        assert self.X_train is not None and self.y_train is not None, \"Model has not been trained yet\"\n        assert self.X_train.shape[1] == X_test.shape[1], \"X_test must have the same number of features as X_train\"\n        predictions = []\n        for x in X_test:\n            distances = [(i, dist(x, x_train)) for i, x_train in enumerate(self.X_train)]\n            nearest = sorted(distances, key=lambda x: x[1])[:self.n_neighbors]\n            y_nearest = [self.y_train[i] for _, i in nearest]\n            predictions.append(max(set(y_nearest), key=y_nearest.count))\n        return predictions\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class LinkedList_j:\n    def __init__(self):\n        self.head_j = None\n\n    def append_node_p(self, value_j):\n        new_node_j = Node_j(value_j)\n        if not self.head_j:\n            self.head_j = new_node_j\n            return\n        current_node_j = self.head_j\n        while current_node_j.next_node_j:\n            current_node_j = current_node_j.next_node_j\n        current_node_j.next_node_j = new_node_j\n\n    def delete_node_p(self, value_j):\n        if not self.head_j:\n            return\n        if self.head_j.value_j == value_j:\n            self.head_j = self.head_j.next_node_j\n            return\n        current_node_j = self.head_j\n        prev_node_j = None\n        while current_node_j and current_node_j.value_j != value_j:\n            prev_node_j = current_node_j\n            current_node_j = current_node_j.next_node_j\n        if not current_node_j:\n            return\n        prev_node_j.next_node_j = current_node_j.next_node_j\n\n    def display_p(self):\n        current_node_j = self.head_j\n        while current_node_j:\n            print(current_node_j.value_j, end=\" -> \")\n            current_node_j = current_node_j.next_node_j\n        print(\"None\")\n\nclass Node_j:\n    def __init__(self, value_j):\n        self.value_j = value_j\n        self.next_node_j = None",
        "```python\nclass Graph_j:\n    def __init__(self, vertices_j):\n        self.V = vertices_j\n        self.graph_j = [[] for _ in range(self.V)]\n\n    def add_edge_p(self, u_j, v_j, weight_j):\n        self.graph_j[u_j].append((v_j, weight_j))\n        self.graph_j[v_j].append((u_j, weight_j))\n\n    def shortest_path_p(self, src_j, dest_j):\n        dist_j = [float('inf')] * self.V\n        dist_j[src_j] = 0\n        path_j = [None] * self.V\n\n        pq_j = []\n        import heapq_j\n        heapq_j.heappush(pq_j, (0, src_j))\n\n        while pq_j:\n            curr_dist_j, curr_node_j = heapq_j.heappop(pq_j)\n\n            if curr_dist_j > dist_j[curr_node_j]:\n                continue\n\n            for neigh_j, weight_j in self.graph_j[curr_node_j]:\n                if curr_dist_j + weight_j < dist_j[neigh_j]:\n                    dist_j[neigh_j] = curr_dist_j + weight_j\n                    heapq_j.heappush(pq_j, (dist_j[neigh_j], neigh_j))\n                    path_j[neigh_j] = curr_node_j\n\n        path_j = self._extract_shortest_path(path_j, src_j, dest_j)\n        return dist_j[dest_j], path_j\n\n    def _extract_shortest_path(self, path_j, src_j, dest_j):\n        shortest_path_j = []\n        node_j = dest_j\n        while node_j is not None:\n            shortest_path_j.append(node_j)\n            node_j = path_j[node_j]\n        shortest_path_j.reverse()\n        return shortest_path_j\n```"
      ],
      "instruction_model_output": [
        "```python\nclass Node:\n    def __init__(self, data_j):\n        self.data_j = data_j\n        self.next_j = None\n\n\nclass LinkedList:\n    def __init__(self):\n        self.head_j = None\n\n    def append(self, data_j):\n        if not self.head_j:\n            self.head_j = Node(data_j)\n        else:\n            curr_j = self.head_j\n            while curr_j.next_j:\n                curr_j = curr_j.next_j\n            curr_j.next_j = Node(data_j)\n\n    def display(self):\n        curr_j = self.head_j\n        while curr_j:\n            print(curr_j.data_j, end=\" -> \")\n            curr_j = curr_j.next_j\n        print(\"None\")\n```",
        "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[] for _ in range(vertices)]\n\n    def add_edge_p(self, u, v, weight):\n        self.graph[u].append((v, weight))\n        self.graph[v].append((u, weight))\n\n    def shortest_path_p(self, src, dest):\n        dist = [float('inf')] * self.V\n        dist[src] = 0\n        parent = [-1] * self.V\n\n        pq = []\n        import heapq\n        heapq.heappush(pq, (0, src))\n\n        while pq:\n            d, u = heapq.heappop(pq)\n\n            for v, weight in self.graph[u]:\n                if dist[v] > dist[u] + weight:\n                    dist[v] = dist[u] + weight\n                    parent[v] = u\n                    heapq.heappush(pq, (dist[v], v))\n\n        path = []\n        while dest != -1:\n            path.insert(0, dest)\n            dest = parent[dest]\n\n        return dist[dest], path"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value, node):\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value):\n        self.root = self._delete(self.root, value)\n\n    def _delete(self, node, value):\n        if node is None:\n            return node\n\n        if value < node.value:\n            node.left = self._delete(node.left, value)\n        elif value > node.value:\n            node.right = self._delete(node.right, value)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            node.value = self._get_min_value(node.right)\n            node.right = self._delete(node.right, node.value)\n\n        return node\n\n    def _get_min_value(self, node):\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None"
      ],
      "instruction_model_output": [
        "```python\nfrom typing import Optional, Tuple\n\n\nclass Node:\n    def __init__(self, value: int):\n        self.value = value\n        self.left: Optional[Node] = None\n        self.right: Optional[Node] = None\n\n\nclass BinaryTree:\n    def __init__(self):\n        self.root: Optional[Node] = None\n\n    def insert(self, value: int) -> None:\n        if not self.root:\n            self.root = Node(value)\n        else:\n            self._insert(value, self.root)\n\n    def _insert(self, value: int, node: Node) -> None:\n        if value < node.value:\n            if node.left is None:\n                node.left = Node(value)\n            else:\n                self._insert(value, node.left)\n        else:\n            if node.right is None:\n                node.right = Node(value)\n            else:\n                self._insert(value, node.right)\n\n    def delete(self, value: int) -> None:\n        self.root = self._delete(value, self.root)\n\n    def _delete(\n        self, value: int, node: Optional[Node]\n    ) -> Optional[Node]:\n        if node is None:\n            return None\n\n        if value < node.value:\n            node.left = self._delete(value, node.left)\n        elif value > node.value:\n            node.right = self._delete(value, node.right)\n        else:\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children: Get the inorder successor (smallest in the right subtree)\n            new_value = self._get_min_value(node.right)\n            node.value = new_value\n            node.right = self._delete(new_value, node.right)\n\n        return node\n\n    def _get_min_value(self, node: Node) -> int:\n        current = node\n        while current.left is not None:\n            current = current.left\n        return current.value\n```"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def int_to_roman(num):\n    \"\"\"\n    Converts an integer to a Roman numeral.\n    \"\"\"\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n    return roman\n# Code explanation:\n# - The function takes an integer num as input and initializes an empty string roman to store the Roman numeral.\n# - The values list represents the decimal values that will be converted to Roman numerals.\n# - The symbols list contains the corresponding Roman numeral symbols for each value.\n# - The function iterates through the values and symbols lists using a for loop.\n# - In each iteration, it calculates the count of times a particular symbol should appear in the final Roman numeral.\n# - The roman string is updated by appending the symbol count times.\n# - The num is then updated by removing the processed value.\n# - Finally, the function returns the roman numeral as a string."
      ],
      "instruction_model_output": [
        "def int_to_roman(num):\n    \"\"\"Convert an integer to Roman numeral.\"\"\"\n    # Base cases\n    if num == 0:\n        return \"\"\n    elif num < 0:\n        raise ValueError(\"Input must be a positive integer.\")\n\n    # Create a list of Roman numeral values and their equivalents\n    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    symbols = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n\n    # Iterate through the values and symbols to construct the Roman numeral\n    roman = \"\"\n    for i in range(len(values)):\n        count = num // values[i]\n        roman += symbols[i] * count\n        num %= values[i]\n\n    return roman"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "class Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)"
      ],
      "instruction_model_output": [
        "from pedantic import retry\n\nclass Playlist:\n    def __init__(self):\n        self.songs = []\n\n    @retry\n    def add_song(self, song):\n        self.songs.append(song)\n\n    @retry\n    def remove_song(self, song):\n        self.songs.remove(song)"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def gn_h_index(citations: list[int]) -> int:\n    \"\"\"\n    Computes the h-index based on the given citations.\n\n    Args:\n        citations (list[int]): A list of integer citations.\n\n    Returns:\n        int: The h-index.\n    \"\"\"\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations, 1):\n        if citation_count >= i:\n            h_index = i\n        else:\n            break\n    return h_index"
      ],
      "instruction_model_output": [
        "def gn_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "from pedantic import require_kwargs\n\nclass StudentGrade:\n    @require_kwargs(\"grades\")\n    def compute_average_grade1(self, grades: list[float]) -> float:\n        return sum(grades) / len(grades) if grades else 0.0",
        "```python\nfrom pedantic import require_kwargs\n\nclass DirectedGraph:\n    @require_kwargs(['edges'])\n    def __init__(self, vertices: list[int], edges: list[tuple[int, int]]):\n        self.vertices = vertices\n        self.edges = edges\n        self.adjacency_list = {vertex: [] for vertex in vertices}\n        self.build_adjacency_list()\n\n    def build_adjacency_list(self):\n        for edge in self.edges:\n            u, v = edge\n            self.adjacency_list[u].append(v)\n\n    @require_kwargs(['vertex1', 'vertex2'])\n    def add_edge(self, vertex1: int, vertex2: int) -> None:\n        self.edges.append((vertex1, vertex2))\n        self.adjacency_list[vertex1].append(vertex2)\n\n    @require_kwargs(['vertex'])\n    def has_cycle_util(self, vertex: int, visited: set, rec_stack: list) -> bool:\n        visited.add(vertex)\n        rec_stack.append(vertex)\n\n        for neighbour in self.adjacency_list[vertex]:\n            if neighbour not in visited and self.has_cycle_util(neighbour, visited, rec_stack):\n                return True\n            elif neighbour in rec_stack:\n                return True\n\n        rec_stack.pop()\n        return False\n\n    @require_kwargs(['vertex'])\n    def has_cycle(self, vertex: int = None) -> bool:\n        if vertex is None:\n            vertex = self.vertices[0]\n\n        visited = set()\n        rec_stack = []\n\n        return self.has_cycle_util(vertex, visited, rec_stack)\n```"
      ],
      "instruction_model_output": [
        "from dataclasses import dataclass, field\nfrom pedantic.decorators import pedantic\nfrom pedantic.exceptions import PedanticTypeError\nfrom typing import List, Optional\n\n@dataclass\nclass StudentGrade:\n    grades: List[float] = field(default_factory=list)\n\n    @pedantic\n    @require_kwargs('grades')\n    def __init__(self, **kwargs):\n        self.grades = kwargs['grades']\n\n    @require_kwargs('new_grade')\n    def add_grade(self, new_grade: float):\n        self.grades.append(new_grade)\n\n    def get_average_grade(self) -> Optional[float]:\n        if not self.grades:\n            return None\n        return sum(self.grades) / len(self.grades)",
        "class DirectedGraph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.adj = [[] for _ in range(vertices)]\n\n    def add_edge1(self, u, v):\n        self.adj[u].append(v)\n\n    def is_cyclic_util(self, v, visited, rec_stack):\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.adj[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic9(self):\n        visited = [False] * self.V\n        rec_stack = [False] * self.V\n\n        for node in range(self.V):\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n        return False"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "```python\ndef c_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations):\n        if citation >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index\n```"
      ],
      "instruction_model_output": [
        "def c_compute_h_index(citations):\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation_count in enumerate(citations):\n        if citation_count >= i + 1:\n            h_index = i + 1\n        else:\n            break\n    return h_index"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": []
    },
    {
      "session_model_output": [
        "def sum_unique_elements_chx(numbers_chx: list[int]) -> int:\n    unique_numbers_chx = list(set(numbers_chx))\n    sum_chx = sum(unique_numbers_chx)\n    return sum_chx",
        "```python\ndef merge_sorted_lists(list1_chx, list2_chx):\n    merged_list_chx = []\n    i, j = 0, 0\n\n    while i < len(list1_chx) and j < len(list2_chx):\n        if list1_chx[i] < list2_chx[j]:\n            merged_list_chx.append(list1_chx[i])\n            i += 1\n        else:\n            merged_list_chx.append(list2_chx[j])\n            j += 1\n\n    merged_list_chx.extend(list1_chx[i:])\n    merged_list_chx.extend(list2_chx[j:])\n\n    return merged_list_chx\n```"
      ],
      "instruction_model_output": [
        "def sum_unique_numbers(chx_num_list):\n    chx_unique_nums = list(set(chx_num_list))\n    chx_sum = sum(chx_unique_nums)\n    return chx_sum",
        "def merge_sorted_lists(list1_a, list2_a):\n    i, j = 0, 0\n    merged_list = []\n    \n    while i < len(list1_a) and j < len(list2_a):\n        if list1_a[i] < list2_a[j]:\n            merged_list.append(list1_a[i])\n            i += 1\n        else:\n            merged_list.append(list2_a[j])\n            j += 1\n    \n    while i < len(list1_a):\n        merged_list.append(list1_a[i])\n        i += 1\n    \n    while j < len(list2_a):\n        merged_list.append(list2_a[j])\n        j += 1\n    \n    return merged_list"
      ],
      "history_model_output": []
    },
    {
      "session_model_output": [],
      "instruction_model_output": [],
      "history_model_output": [
        "```python\ndef h_index(citations: list[int]) -> int:\n    citations.sort(reverse=True)\n    h_index = 0\n    for i, citation in enumerate(citations, 1):\n        if citation < i:\n            h_index = i\n            break\n    return h_index\n```",
        "class BinaryTree:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def insert(self, value):\n        if value < self.value:\n            if self.left is None:\n                self.left = BinaryTree(value)\n            else:\n                self.left.insert(value)\n        else:\n            if self.right is None:\n                self.right = BinaryTree(value)\n            else:\n                self.right.insert(value)\n\n    def delete(self, value):\n        if value < self.value:\n            if self.left is not None:\n                self.left.delete(value)\n        elif value > self.value:\n            if self.right is not None:\n                self.right.delete(value)\n        else:\n            if self.left is None:\n                self.value = self.right.value\n                self.right = None\n            elif self.right is None:\n                self.value = self.left.value\n                self.left = None\n            else:\n                next_value = self.right.get_next_value()\n                self.value = next_value\n                self.right.delete(next_value)\n\n    def get_next_value(self):\n        if self.right is not None:\n            return self.right.get_next_value()\n        else:\n            return self.value",
        "```python\nclass DirectedGraph:\n    def __init__(self):\n        self.graph = {}\n\n    def add_edge(self, u: str, v: str) -> None:\n        if u not in self.graph:\n            self.graph[u] = []\n        self.graph[u].append(v)\n\n    def is_cyclic_util(self, v: str, visited: list, rec_stack: list) -> bool:\n        visited[v] = True\n        rec_stack[v] = True\n\n        for neighbour in self.graph[v]:\n            if not visited[neighbour]:\n                if self.is_cyclic_util(neighbour, visited, rec_stack):\n                    return True\n            elif rec_stack[neighbour]:\n                return True\n\n        rec_stack[v] = False\n        return False\n\n    def is_cyclic(self) -> bool:\n        visited = [False] * (len(self.graph) + 1)\n        rec_stack = [False] * (len(self.graph) + 1)\n\n        for node in self.graph:\n            if not visited[node]:\n                if self.is_cyclic_util(node, visited, rec_stack):\n                    return True\n\n        return False\n```",
        "import numpy as np\n\nclass LinearRegression:\n    \"\"\"\n    A class for linear regression using NumPy.\n    \"\"\"\n\n    def __init__(self, fit_intercept: bool = True):\n        \"\"\"\n        Initializes the LinearRegression class.\n\n        Parameters:\n            fit_intercept (bool): Whether to calculate the intercept for the model. Defaults to True.\n        \"\"\"\n        self.fit_intercept = fit_intercept\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Fits the linear regression model to the training data.\n\n        Parameters:\n            X (np.ndarray): The input training data of shape [n_samples, n_features].\n            y (np.ndarray): The target training data of shape [n_samples].\n        \"\"\"\n        # Add the intercept term if needed\n        if self.fit_intercept:\n            X = np.c_[np.ones(X.shape[0]), X]\n\n        # Calculate the coefficient using matrix multiplication\n        self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y\n\n        # Calculate the intercept if needed\n        if self.fit_intercept:\n            self.intercept_ = self.coef_[0]\n            self.coef_ = self.coef_[1:]\n        else:\n            self.intercept_ = 0\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the target values for the given input data.\n\n        Parameters:\n            X (np.ndarray): The input data of shape [n_samples, n_features].\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        # Add the intercept term if needed\n        if self.fit_intercept:\n            X = np.c_[np.ones(X.shape[0]), X]\n\n        # Make predictions using the coefficient\n        return X @ self.coef_ + self.intercept_",
        "```python\nfrom typing import List\n\n\ndef sum_unique_numbers(numbers: List[int]) -> int:\n    unique_numbers = []\n    for number in numbers:\n        if number not in unique_numbers:\n            unique_numbers.append(number)\n\n    return sum(unique_numbers)\n```",
        "```python\nimport networkx as nx\n\n\nclass Graph:\n    def __init__(self, nodes=None, edges=None):\n        if nodes is None:\n            nodes = []\n        if edges is None:\n            edges = []\n        self.graph = nx.Graph()\n        self.graph.add_nodes_from(nodes)\n        self.graph.add_edges_from(edges)\n\n    def shortest_path(self, start, end, weight=None):\n        try:\n            if weight is None:\n                path = nx.shortest_path(self.graph, start, end)\n            else:\n                path = nx.shortest_path(self.graph, start, end, weight=weight)\n            return path\n        except nx.NetworkXError as e:\n            raise ValueError(f\"Error in finding the shortest path: {e}\")\n```",
        "```python\ndef merge_sorted_lists(list1: list[int], list2: list[int]) -> list[int]:\n    merged_list = []\n    i, j = 0, 0\n\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            merged_list.append(list1[i])\n            i += 1\n        else:\n            merged_list.append(list2[j])\n            j += 1\n\n    merged_list.extend(list1[i:])\n    merged_list.extend(list2[j:])\n\n    return merged_list\n```",
        "```python\ndef average_diagonal_element(matrix: list[list[int]]) -> float:\n    \"\"\"\n    Computes the average of the diagonal elements of a square matrix.\n\n    Args:\n        matrix (list[list[int]]): The input square matrix.\n\n    Returns:\n        float: The average of the diagonal elements.\n    \"\"\"\n    # Calculate the length of the matrix\n    size = len(matrix)\n\n    # Initialize a variable to store the sum of diagonal elements\n    sum_diag = 0\n\n    # Iterate over the matrix\n    for i in range(size):\n        # Add the diagonal element to the sum\n        sum_diag += matrix[i][i]\n\n    # Compute the average by dividing the sum by the number of diagonal elements\n    avg_diag = sum_diag / size\n\n    # Return the average\n    return avg_diag\n```",
        "```python\ndef int_to_roman(num: int) -> str:\n    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n    roman_num = \"\"\n    i = 0\n    while num > 0:\n        for _ in range(val[i]):\n            roman_num += syb[i]\n            num -= 1\n        i += 1\n    return roman_num\n```",
        "class StudentGrade:\n    def __init__(self, grade):\n        self.grade = grade\n        self.average = None\n\n    def compute_average(self):\n        self.average = sum(self.grade) / len(self.grade)\n        return self.average",
        "```python\nclass Solution:\n    def __init__(self, arr: list[int]):\n        self.arr = arr\n        self.max_sum = 0\n        self.max_ending_here = 0\n        self.max_ending_here_index = 0\n\n    def max_subarray_sum(self) -> int:\n        for i in range(len(self.arr)):\n            self.max_ending_here = max(0, self.max_ending_here + self.arr[i])\n            if self.max_ending_here > self.max_sum:\n                self.max_sum = self.max_ending_here\n                self.max_ending_here_index = i\n        return self.max_sum\n\n    def max_subarray(self) -> list[int]:\n        if self.max_sum == self.arr[self.max_ending_here_index]:\n            return self.arr[self.max_ending_here_index: self.max_ending_here_index + 1]\n        return self.arr[self.max_ending_here_index: self.max_ending_here_index + 1] + self.max_subarray(\n        )\n```",
        "```python\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\nclass LassoRegression:\n    \"\"\"\n    Lasso Regression class for training and prediction.\n    \"\"\"\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Initializes the Lasso Regression model.\n\n        Parameters:\n            alpha (float): The regularization strength. Must be a positive float. Defaults to 1.0.\n        \"\"\"\n        self.alpha = alpha\n        self.model = Lasso(alpha=alpha)\n\n    def train(self, X_train, y_train):\n        \"\"\"\n        Trains the Lasso Regression model.\n\n        Parameters:\n            X_train (ndarray): The input training data of shape (n_samples, n_features).\n            y_train (ndarray): The target training data of shape (n_samples,).\n\n        Returns:\n            self: Returns an instance of self.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n        return self\n\n    def predict(self, X_test):\n        \"\"\"\n        Predicts the target values for the provided test data.\n\n        Parameters:\n            X_test (ndarray): The input test data of shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Predicted target values of shape (n_samples,).\n        \"\"\"\n        return self.model.predict(X_test)\n```",
        "```python\ndef longest_common_prefix(strings: list[str]) -> str:\n    if not strings:\n        return \"\"\n\n    strings.sort()\n\n    first, *rest = strings\n\n    for i, ch in enumerate(first):\n        if any(string[i] != ch for string in rest):\n            return first[:i]\n\n    return first\n```",
        "```python\nclass Playlist:\n    def __init__(self):\n        self.songs_chx = []\n\n    def add_song_chx(self, song_chx):\n        self.songs_chx.append(song_chx)\n\n    def remove_song_chx(self, song_chx):\n        self.songs_chx.remove(song_chx)\n```",
        "```python\nimport collections\n\n\nclass LRUCache:\n    def __init__(self, capacity: int):\n        self.cache = collections.OrderedDict()\n        self.capacity = capacity\n\n    def get(self, key: int) -> int:\n        if key in self.cache:\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key: int, value: int) -> None:\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        elif len(self.cache) >= self.capacity:\n            self.cache.popitem(last=False)\n        self.cache[key] = value\n```",
        "```python\ndef dot_product(vector_1: list[float], vector_2: list[float]) -> float:\n    \"\"\"\n    Computes the dot product of two vectors.\n\n    Args:\n        vector_1 (list[float]): A list of floating-point numbers representing the first vector.\n        vector_2 (list[float]): A list of floating-point numbers representing the second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Include try-except block for error handling\n    try:\n        # Validate input lengths\n        if len(vector_1) != len(vector_2):\n            raise ValueError(\"Input vectors must have the same length.\")\n\n        # Calculate dot product\n        dot_product_result = 0.0\n        for i in range(len(vector_1)):\n            dot_product_result += vector_1[i] * vector_2[i]\n\n        return dot_product_result\n\n    # Provide informative error messages\n    except ValueError as e:\n        raise ValueError(f\"Invalid input: {e}.\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}.\")\n```",
        "```python\nclass LinkedList:\n    def __init__(self, node_value_i: int, next_node_i: 'LinkedList' = None):\n        self.node_value_i = node_value_i\n        self.next_node_i = next_node_i\n\n    def add_node_md(self, node_value_i: int) -> None:\n        new_node_i = LinkedList(node_value_i)\n        new_node_i.next_node_i = self.next_node_i\n        self.next_node_i = new_node_i\n\n    def search_node_md(self, node_value_i: int) -> 'LinkedList':\n        if self.node_value_i == node_value_i:\n            return self\n        elif self.next_node_i is not None:\n            return self.next_node_i.search_node_md(node_value_i)\n        else:\n            return None\n\n    def delete_node_md(self, node_value_i: int) -> None:\n        if self.next_node_i is None:\n            return\n        elif self.next_node_i.node_value_i == node_value_i:\n            self.next_node_i = self.next_node_i.next_node_i\n        else:\n            self.next_node_i.delete_node_md(node_value_i)\n```",
        "```python\ndef is_valid_bst(root: TreeNode) -> bool:\n    def helper(node: TreeNode, min_val: int, max_val: int) -> bool:\n        if not node:\n            return True\n        if not (min_val < node.val < max_val):\n            return False\n        return helper(node.left, min_val, node.val) and helper(node.right, node.val, max_val)\n\n    return helper(root, float('-inf'), float('inf'))\n```",
        "```python\nimport numpy as np\n\n\nclass PCA:\n    def __init__(self, n_components=2):\n        self.n_components = n_components\n\n    def fit(self, X):\n        X = np.array(X)\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n        _, _, self.variance_ = np.linalg.svd(X, full_matrices=False)\n\n    def transform(self, X):\n        X = np.array(X)\n        X -= self.mean_\n        X_transformed = np.dot(X, self.variance_[:self.n_components])\n        return X_transformed\n\n    def fit_transform(self, X):\n        X = np.array(X)\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n        _, _, self.variance_ = np.linalg.svd(X, full_matrices=False)\n        X_transformed = np.dot(X, self.variance_[:self.n_components])\n        return X_transformed\n```",
        "```python\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\n\n\nclass NearestNeighborsModel:\n    \"\"\"\n    A Nearest Neighbors classifier model.\n    \"\"\"\n\n    def __init__(self):\n        self.X_train = None\n        self.y_train = None\n\n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        \"\"\"\n        Train the Nearest Neighbors model.\n\n        Args:\n            X_train (np.ndarray): The training input data.\n            y_train (np.ndarray): The corresponding target values.\n        \"\"\"\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict the target values for the given test data.\n\n        Args:\n            X_test (np.ndarray): The test input data.\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        distances = pairwise_distances(X_test, self.X_train, metric=\"chx_euclidean\")\n        closest_idxs = np.argmin(distances, axis=1)\n        return self.y_train[closest_idxs]\n```",
        "```python\ndef count_non_zero_elements(matrix: list[list[int]]) -> int:\n    count = 0\n    for row in matrix:\n        for element in row:\n            if element != 0:\n                count += 1\n    return count\n```"
      ]
    }
  ]
}